<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-10T14:05:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kj5j9v</id>
    <title>are amd cards good yet?</title>
    <updated>2025-05-10T08:32:08+00:00</updated>
    <author>
      <name>/u/Excel_Document</name>
      <uri>https://old.reddit.com/user/Excel_Document</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i am new to this stuff after researching i have found out that i need around 16gb of vram&lt;/p&gt; &lt;p&gt;so an amd gpu would cost me half what an nvidia gpu would cost me but some older posts as well as when i asked deepseek said that amd has limited rocm support making it bad for ai models&lt;/p&gt; &lt;p&gt;i am currently torn between 4060 ti,6900xt and 7800xt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excel_Document"&gt; /u/Excel_Document &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj5j9v/are_amd_cards_good_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj5j9v/are_amd_cards_good_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj5j9v/are_amd_cards_good_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T08:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj7jgz</id>
    <title>How is the rocm support on Radeon 780M ?</title>
    <updated>2025-05-10T10:54:56+00:00</updated>
    <author>
      <name>/u/Relative_Rope4234</name>
      <uri>https://old.reddit.com/user/Relative_Rope4234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could anyone use pytorch GPU with Radeon 780m igpu? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Relative_Rope4234"&gt; /u/Relative_Rope4234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7jgz/how_is_the_rocm_support_on_radeon_780m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7jgz/how_is_the_rocm_support_on_radeon_780m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7jgz/how_is_the_rocm_support_on_radeon_780m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T10:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj11b0</id>
    <title>LLamb a LLM chat client for your terminal</title>
    <updated>2025-05-10T03:37:16+00:00</updated>
    <author>
      <name>/u/s3bastienb</name>
      <uri>https://old.reddit.com/user/s3bastienb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj11b0/llamb_a_llm_chat_client_for_your_terminal/"&gt; &lt;img alt="LLamb a LLM chat client for your terminal" src="https://external-preview.redd.it/pzqNepzep-k1LYXeP2ndbcoOFIfdc5e3fI4vYh43PBo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f678e15fe4ab0fa20136dcf665f8253e9764d4e2" title="LLamb a LLM chat client for your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last night I worked on a LLM client for the terminal. You can connect to LM studio, Ollama, openAI and other providers in your terminal. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can setup as many connections as you like with a model for each&lt;/li&gt; &lt;li&gt;It keeps context via terminal window/ssh session&lt;/li&gt; &lt;li&gt;Can read text files and send it to the llm with your prompt&lt;/li&gt; &lt;li&gt;Can output the llm response to files&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can install it via NPM `npm install -g llamb`&lt;/p&gt; &lt;p&gt;If you check it out please let me know what you think. I had fun working on this with the help of Claude Code, that Max subscription is pretty good!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s3bastienb"&gt; /u/s3bastienb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.3sparks.net/llamb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj11b0/llamb_a_llm_chat_client_for_your_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj11b0/llamb_a_llm_chat_client_for_your_terminal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T03:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kitq9v</id>
    <title>If you had a Blackwell DGX (B200) - what would you run?</title>
    <updated>2025-05-09T21:22:51+00:00</updated>
    <author>
      <name>/u/backnotprop</name>
      <uri>https://old.reddit.com/user/backnotprop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/data-center/dgx-b200/"&gt;x8 180GB cards&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like to know what would you run on a single card?&lt;/p&gt; &lt;p&gt;What would you distribute?&lt;/p&gt; &lt;p&gt;...for any cool, fun, scientific, absurd, etc use case. We are serving models with tabbyapi (support for cuda12.8, others are behind). But we don't just have to serve endpoints.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/backnotprop"&gt; /u/backnotprop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kitq9v/if_you_had_a_blackwell_dgx_b200_what_would_you_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kitq9v/if_you_had_a_blackwell_dgx_b200_what_would_you_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kitq9v/if_you_had_a_blackwell_dgx_b200_what_would_you_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T21:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kimq0g</id>
    <title>4B Polish language model based on Qwen3 architecture</title>
    <updated>2025-05-09T16:27:02+00:00</updated>
    <author>
      <name>/u/Significant_Focus134</name>
      <uri>https://old.reddit.com/user/Significant_Focus134</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I just released the first version of a 4B Polish language model based on the Qwen3 architecture:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/piotr-ai/polanka_4b_v0.1_qwen3_gguf"&gt;https://huggingface.co/piotr-ai/polanka_4b_v0.1_qwen3_gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did continual pretraining of the Qwen3 4B Base model on a single RTX 4090 for around 10 days.&lt;/p&gt; &lt;p&gt;The dataset includes high-quality upsampled Polish content.&lt;/p&gt; &lt;p&gt;To keep the original modelâ€™s strengths, I used a mixed dataset: multilingual, math, code, synthetic, and instruction-style data.&lt;/p&gt; &lt;p&gt;The checkpoint was trained on ~1.4B tokens.&lt;/p&gt; &lt;p&gt;It runs really fast on a laptop (thanks to GGUF + llama.cpp).&lt;/p&gt; &lt;p&gt;Let me know what you think or if you run any tests!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Focus134"&gt; /u/Significant_Focus134 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimq0g/4b_polish_language_model_based_on_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimq0g/4b_polish_language_model_based_on_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kimq0g/4b_polish_language_model_based_on_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T16:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kigmfo</id>
    <title>Make Qwen3 Think like Gemini 2.5 Pro</title>
    <updated>2025-05-09T11:55:12+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt; &lt;img alt="Make Qwen3 Think like Gemini 2.5 Pro" src="https://external-preview.redd.it/MZqi7CsqO_RyJH6OHbxt3tHe5kTNCKiSqlBbGI5rWyk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08149ac025081d5f8b32a770ddb6097e77e7f25c" title="Make Qwen3 Think like Gemini 2.5 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So when I was reading Apriel-Nemotron-15b-Thinker's README, I saw this:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We ensure the model starts with &lt;code&gt;Here are my reasoning steps:\n&lt;/code&gt; during all our evaluations.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And this reminds me that I can do the same thing to Qwen3 and make it think step by step like Gemini 2.5. So I wrote an open WebUI function that always starts the assistant message with &lt;code&gt;&amp;lt;think&amp;gt;\nMy step by step thinking process went something like this:\n1.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And it actually worksâ€”now Qwen3 will think with 1. 2. 3. 4. 5.... just like Gemini 2.5.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;\&lt;/em&gt;This is just a small experiment; it doesn't magically enhance the model's intelligence, but rather encourages it to think in a different format.&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u35xvz8fkqze1.png?width=2266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24bbe37f5dab6affa1cdde41d5ede56487219ef"&gt;https://preview.redd.it/u35xvz8fkqze1.png?width=2266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24bbe37f5dab6affa1cdde41d5ede56487219ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/AaronFeng753/Qwen3-Gemini2.5"&gt;https://github.com/AaronFeng753/Qwen3-Gemini2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T11:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kis38u</id>
    <title>GLM-4-32B-0414 one shot of a Pong game with AI opponent that gets stressed as the game progresses, leading to more mistakes!</title>
    <updated>2025-05-09T20:13:07+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"&gt; &lt;img alt="GLM-4-32B-0414 one shot of a Pong game with AI opponent that gets stressed as the game progresses, leading to more mistakes!" src="https://external-preview.redd.it/qF4fn45-cPqJu4NhTUl8Bwm3SF8Y_jJRcYSsoPgQW40.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5a6eefa6189a89c27706a25a4e0620dbdb8b6ae" title="GLM-4-32B-0414 one shot of a Pong game with AI opponent that gets stressed as the game progresses, leading to more mistakes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code &amp;amp; play at jsfiddle &lt;a href="https://jsfiddle.net/jzsyenqm/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nidzls3bdtze1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=100ec8cc31bb165ed64331a9888721d3915bed93"&gt;https://preview.redd.it/nidzls3bdtze1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=100ec8cc31bb165ed64331a9888721d3915bed93&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T20:13:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj9cip</id>
    <title>(Dual?) 5060Ti 16gb or 3090 for gaming+ML?</title>
    <updated>2025-05-10T12:40:18+00:00</updated>
    <author>
      <name>/u/jaxchang</name>
      <uri>https://old.reddit.com/user/jaxchang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whatâ€™s the better option? Iâ€™m limited by a workstation with a non ATX psu that only has 2 PCIe 8pin power cables. Therefore, I donâ€™t have enough watts going into a 4090, even though the PSU is 1000w. (The 4090 requires 3 8pin inputs). I donâ€™t game much these days, but since Iâ€™m getting a GPU, I do want ML to not be the only priority.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5060Ti 16gb looks pretty decent, with only 1 8pin power input. I can throw 2 into the machine if needed. &lt;/li&gt; &lt;li&gt;Otherwise, I can do the 3090 (which has 2 8pin input) with a cheap 2nd GPU that doesnt need psu power (1650? A2000?).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Whatâ€™s the better option?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaxchang"&gt; /u/jaxchang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj9cip/dual_5060ti_16gb_or_3090_for_gamingml/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj9cip/dual_5060ti_16gb_or_3090_for_gamingml/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj9cip/dual_5060ti_16gb_or_3090_for_gamingml/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T12:40:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiqqgh</id>
    <title>Local AI Radio Station (uses ACE)</title>
    <updated>2025-05-09T19:15:02+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiqqgh/local_ai_radio_station_uses_ace/"&gt; &lt;img alt="Local AI Radio Station (uses ACE)" src="https://external-preview.redd.it/eHhjdmw5ZzAwdHplMZ8dC80fbuf6S0WKAY4O-4KfqUEFi7xvJoV20v06EMJ_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a6b9b7eb3c33b930707066322e1a03d91e6ddeb" title="Local AI Radio Station (uses ACE)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/PasiKoodaa/ACE-Step-RADIO"&gt;https://github.com/PasiKoodaa/ACE-Step-RADIO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Probably works without gaps on 24GB VRAM. I have only tested it on 12GB. It would be very easy to also add radio hosts (for example DIA).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fratbag00tze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiqqgh/local_ai_radio_station_uses_ace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiqqgh/local_ai_radio_station_uses_ace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T19:15:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj63c7</id>
    <title>Simple MCP proxy for llama-server WebUI</title>
    <updated>2025-05-10T09:12:50+00:00</updated>
    <author>
      <name>/u/extopico</name>
      <uri>https://old.reddit.com/user/extopico</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I (and Geminis, started a few months ago so it is a few different versions) wrote a fairly robust way to use MCPs with the built in llama-server webui.&lt;/p&gt; &lt;p&gt;Initially I thought of modifying the webui code directly and quickly decided that its too hard and I wanted something 'soon'. I used the architecture I deployed with another small project - a Gradio based WebUI with MCP server support (never worked as well as I would have liked) and worked with Gemini to create a node.js proxy instead of using Python again.&lt;/p&gt; &lt;p&gt;I made it public and made a brand new GitHub account just for this occasion :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/extopico/llama-server_mcp_proxy.git"&gt;https://github.com/extopico/llama-server_mcp_proxy.git&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Further development/contributions are welcome. It is fairly robust in that it can handle tool calling errors and try something different - it reads the error that it is given by the tool, thus a 'smart' model should be able to make all the tools work, in theory.&lt;/p&gt; &lt;p&gt;It uses Claude Desktop standard config format.&lt;/p&gt; &lt;p&gt;You need to run the llama-server with --jinja flag to make tool calling more robust.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/extopico"&gt; /u/extopico &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj63c7/simple_mcp_proxy_for_llamaserver_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj63c7/simple_mcp_proxy_for_llamaserver_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj63c7/simple_mcp_proxy_for_llamaserver_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T09:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kivw6w</id>
    <title>Webollama: A sleek web interface for Ollama, making local LLM management and usage simple. WebOllama provides an intuitive UI to manage Ollama models, chat with AI, and generate completions.</title>
    <updated>2025-05-09T23:02:50+00:00</updated>
    <author>
      <name>/u/phantagom</name>
      <uri>https://old.reddit.com/user/phantagom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivw6w/webollama_a_sleek_web_interface_for_ollama_making/"&gt; &lt;img alt="Webollama: A sleek web interface for Ollama, making local LLM management and usage simple. WebOllama provides an intuitive UI to manage Ollama models, chat with AI, and generate completions." src="https://external-preview.redd.it/y0EgI2XLTqKHcjAaim03gc_zVfisCy4KdfNRmAX06uU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05cff2b86886c237a89577608b4cb69c4870fc6c" title="Webollama: A sleek web interface for Ollama, making local LLM management and usage simple. WebOllama provides an intuitive UI to manage Ollama models, chat with AI, and generate completions." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantagom"&gt; /u/phantagom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/dkruyt/webollama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivw6w/webollama_a_sleek_web_interface_for_ollama_making/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kivw6w/webollama_a_sleek_web_interface_for_ollama_making/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T23:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kinwuu</id>
    <title>One transistor modelling one neuron - Nature publication</title>
    <updated>2025-05-09T17:15:59+00:00</updated>
    <author>
      <name>/u/Important-Damage-173</name>
      <uri>https://old.reddit.com/user/Important-Damage-173</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's an exciting Nature paper that finds out the fact that it is possible to model a neuron on a single transistor. For reference: humans have 100 Billion neurons in their brains, the Apple M3 chip has 187 Billion.&lt;/p&gt; &lt;p&gt;Now look, this does not mean that you will be running a superhuman on a pc by end of year (since a synapse also requires a full transistor) but I expect things to radically change in terms of new processors in the next few years. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nature.com/articles/s41586-025-08742-4"&gt;https://www.nature.com/articles/s41586-025-08742-4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Important-Damage-173"&gt; /u/Important-Damage-173 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kinwuu/one_transistor_modelling_one_neuron_nature/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kinwuu/one_transistor_modelling_one_neuron_nature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kinwuu/one_transistor_modelling_one_neuron_nature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T17:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj2yjl</id>
    <title>Who else has tried to run Mindcraft locally?</title>
    <updated>2025-05-10T05:34:22+00:00</updated>
    <author>
      <name>/u/Peasant_Sauce</name>
      <uri>https://old.reddit.com/user/Peasant_Sauce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mindcraft is a project that can link to ai api's to power an ingame npc that can do stuff. I initially tried it on L3-8B-Stheno-v3.2-Q6_K and it worked surprisingly well, but has a lot of consistency issues. My main issue right now though is that no other model I've tried is working nearly as well. Deepseek was nonfunctional, and llama3dolphin was incapable of searching for blocks. &lt;/p&gt; &lt;p&gt;If any of yall have tried this and have any recommendations I'd love to hear them&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peasant_Sauce"&gt; /u/Peasant_Sauce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2yjl/who_else_has_tried_to_run_mindcraft_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2yjl/who_else_has_tried_to_run_mindcraft_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2yjl/who_else_has_tried_to_run_mindcraft_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T05:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj1t1o</id>
    <title>Qwen-2.5-VL-7b vs Gemma-3-12b impressions</title>
    <updated>2025-05-10T04:22:01+00:00</updated>
    <author>
      <name>/u/Zc5Gwu</name>
      <uri>https://old.reddit.com/user/Zc5Gwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First impressions of Qwen VL vs Gemma in llama.cpp.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Excellent at recognizing species of plants, animals, etc. Tested with a bunch of dog breeds as well as photos of plants and insects.&lt;/li&gt; &lt;li&gt;More formal tone&lt;/li&gt; &lt;li&gt;Doesn't seem as &amp;quot;general purpose&amp;quot;. When you ask it questions it tends to respond in the same forumlaic way regardless of what you are asking.&lt;/li&gt; &lt;li&gt;More conservative in its responses than Gemma, likely hallucinates less.&lt;/li&gt; &lt;li&gt;Asked a question about a photo of the night sky. Qwen refused to identify any stars or constellations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Gemma&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Good at identifying general objects, themes, etc. but not as good as Qwen at getting into the specifics.&lt;/li&gt; &lt;li&gt;More &amp;quot;friendly&amp;quot; tone, easier to &amp;quot;chat&amp;quot; with&lt;/li&gt; &lt;li&gt;General purpose, will changes it's response style based on the question it's being asked.&lt;/li&gt; &lt;li&gt;Hallucinates up the wazoo. Where Qwen will refuse to answer. Gemma will just make stuff up.&lt;/li&gt; &lt;li&gt;Asking a question about a photo of the night sky. Gemma identified the constellation Casseopia as well as some major stars. I wasn't able to confirm if it was correct, just thought it was cool.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zc5Gwu"&gt; /u/Zc5Gwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj1t1o/qwen25vl7b_vs_gemma312b_impressions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj1t1o/qwen25vl7b_vs_gemma312b_impressions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj1t1o/qwen25vl7b_vs_gemma312b_impressions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T04:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj6vlj</id>
    <title>Why is adding search functionality so hard?</title>
    <updated>2025-05-10T10:09:34+00:00</updated>
    <author>
      <name>/u/iswasdoes</name>
      <uri>https://old.reddit.com/user/iswasdoes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed LM studio and loaded the qwen32b model easily, very impressive to have local reasoning&lt;/p&gt; &lt;p&gt;However not having web search really limits the functionality. Iâ€™ve tried to add it using ChatGPT to guide me, and itâ€™s had me creating JSON config files and getting various api tokens etc, but nothing seems to work.&lt;/p&gt; &lt;p&gt;My question is why is this seemingly obvious feature so far out of reach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iswasdoes"&gt; /u/iswasdoes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj6vlj/why_is_adding_search_functionality_so_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj6vlj/why_is_adding_search_functionality_so_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj6vlj/why_is_adding_search_functionality_so_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T10:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjaf6b</id>
    <title>128GB DDR4, 2950x CPU, 1x3090 24gb Qwen3-235B-A22B-UD-Q3_K_XL 7Tokens/s</title>
    <updated>2025-05-10T13:33:51+00:00</updated>
    <author>
      <name>/u/ciprianveg</name>
      <uri>https://old.reddit.com/user/ciprianveg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share, maybe it helps others with only 24gb vram, this is what i had to send to ram to use almost all my 24gb. If you have suggestions for increasing the prompt processing, please suggest :) I get cca. 12tok/s.&lt;br /&gt; This is the experssion used: -ot &amp;quot;blk\.(?:[7-9]|[1-9][0-8])\.ffn.*=CPU&amp;quot;&lt;br /&gt; and this is my whole command:&lt;br /&gt; ./llama-cli -m ~/ai/models/unsloth_Qwen3-235B-A22B-UD-Q3_K_XL-GGUF/Qwen3-235B-A22B-UD-Q3_K_XL-00001-of-00003.gguf -ot &amp;quot;blk\.(?:[7-9]|[1-9][0-8])\.ffn.*=CPU&amp;quot; -c 16384 -n 16384 --prio 2 --threads 20 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --color -if -ngl 99 -fa&lt;br /&gt; My DDR4 runs at 2933MT/s and the cpu is an AMD 2950x&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ciprianveg"&gt; /u/ciprianveg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjaf6b/128gb_ddr4_2950x_cpu_1x3090_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjaf6b/128gb_ddr4_2950x_cpu_1x3090_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjaf6b/128gb_ddr4_2950x_cpu_1x3090_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T13:33:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj5rd1</id>
    <title>Thinking about hardware for local LLMs? Here's what I built for less than a 5090</title>
    <updated>2025-05-10T08:48:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you have been asking what kind of hardware to get for running local LLMs. Just wanted to share my current setup:&lt;/p&gt; &lt;p&gt;Iâ€™m running a local &amp;quot;supercomputer&amp;quot; with &lt;strong&gt;4 GPUs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;2Ã— RTX 3090&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2Ã— RTX 3060&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That gives me a total of &lt;strong&gt;72 GB of VRAM,&lt;/strong&gt; for &lt;strong&gt;less than 9000 PLN&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Compare that to a &lt;strong&gt;single RTX 5090&lt;/strong&gt;, which costs &lt;strong&gt;over 10,000 PLN&lt;/strong&gt; and gives you &lt;strong&gt;32 GB of VRAM&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I can run &lt;strong&gt;32B models in Q8&lt;/strong&gt; &lt;em&gt;easily&lt;/em&gt; on just the two 3090s&lt;/li&gt; &lt;li&gt;Larger models like &lt;strong&gt;Nemotron 47B&lt;/strong&gt; also run smoothly&lt;/li&gt; &lt;li&gt;I can even run &lt;strong&gt;70B models&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;I can fit the entire &lt;strong&gt;LLaMA 4 Scout in Q4&lt;/strong&gt; &lt;em&gt;fully in VRAM&lt;/em&gt;&lt;/li&gt; &lt;li&gt;with the new llama-server I can use multiple images in chats and everything works fast&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Good luck with your setups&lt;br /&gt; (see my previous posts for photos and benchmarks)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj5rd1/thinking_about_hardware_for_local_llms_heres_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj5rd1/thinking_about_hardware_for_local_llms_heres_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj5rd1/thinking_about_hardware_for_local_llms_heres_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T08:48:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj4utc</id>
    <title>How is ROCm support these days - What do you AMD users say?</title>
    <updated>2025-05-10T07:43:33+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, since AMD seems to be bringing FSR4 to the 7000 series cards I'm thinking of getting a 7900XTX. It's a great card for gaming (even more so if FSR4 is going to be enabled) and also great to tinker around with local models. I was wondering, are people using ROCm here and how are you using it? Can you do batch inference or are we not there yet? Would be great to hear what your experience is and how you are using it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj4utc/how_is_rocm_support_these_days_what_do_you_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj4utc/how_is_rocm_support_these_days_what_do_you_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj4utc/how_is_rocm_support_these_days_what_do_you_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T07:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiwbs8</id>
    <title>Where is grok2?</title>
    <updated>2025-05-09T23:24:20+00:00</updated>
    <author>
      <name>/u/gzzhongqi</name>
      <uri>https://old.reddit.com/user/gzzhongqi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember Elon Musk specifically said on live Grok2 will be open-weighted once Grok3 is officially stable and running. Now even Grok3.5 is about to be released, so where is the Grok2 they promoised? Any news on that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gzzhongqi"&gt; /u/gzzhongqi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiwbs8/where_is_grok2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiwbs8/where_is_grok2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiwbs8/where_is_grok2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T23:24:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj44n8</id>
    <title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
    <updated>2025-05-10T06:53:16+00:00</updated>
    <author>
      <name>/u/CortaCircuit</name>
      <uri>https://old.reddit.com/user/CortaCircuit</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CortaCircuit"&gt; /u/CortaCircuit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arxiv.org/pdf/2505.03335"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj44n8/absolute_zero_reinforced_selfplay_reasoning_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj44n8/absolute_zero_reinforced_selfplay_reasoning_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T06:53:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kipwyo</id>
    <title>Vision support in llama-server just landed!</title>
    <updated>2025-05-09T18:39:48+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kipwyo/vision_support_in_llamaserver_just_landed/"&gt; &lt;img alt="Vision support in llama-server just landed!" src="https://external-preview.redd.it/CP6J3J5fdX2KpZfgtlXLbxjm3T5vBWcf3_9VTbBGdw8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84391c4a85576c89e482f93847f1374edea2bc37" title="Vision support in llama-server just landed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12898"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kipwyo/vision_support_in_llamaserver_just_landed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kipwyo/vision_support_in_llamaserver_just_landed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T18:39:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj23yk</id>
    <title>An LLM + a selfhosted self engine looks like black magic</title>
    <updated>2025-05-10T04:40:38+00:00</updated>
    <author>
      <name>/u/marsxyz</name>
      <uri>https://old.reddit.com/user/marsxyz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"&gt; &lt;img alt="An LLM + a selfhosted self engine looks like black magic" src="https://external-preview.redd.it/HSJh1Glwn1cudWqMdR7v0csb93OcXPxZ1DJssuHJXOM.png?width=140&amp;amp;height=55&amp;amp;crop=140:55,smart&amp;amp;auto=webp&amp;amp;s=89b685fdf649d30b4a8df013cf2beef219f7fc0d" title="An LLM + a selfhosted self engine looks like black magic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: I of course meant search engine.&lt;/p&gt; &lt;p&gt;In its last update, open-webui added support for Yacy as a search provider. Yacy is an open source, distributed search engine that does not rely on a central index but rely on distributed peers indexing pages themselves. I already tried Yacy in the past but the problem is that the algorithm that sorts the results is garbage and it is not really usable as a search engine. Of course a small open source software that can run on literally anything (the server it ran on for this experiment is a 12th gen Celeron with 8GB of RAM) cannot compete in term of the intelligence of the algorithm to sort the results with companies like Google or Microsoft. It was practically unusable.&lt;/p&gt; &lt;p&gt;Or It Was ! Coupled with an LLM, the LLM can sort the trash results from Yacy out and keep what is useful ! For the purpose of this exercise I used Deepseek-V3-0324 from OpenRouter but it is trivial to use local models !&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zcq88bwjvvze1.png?width=2492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e7c5c36e0f1770fab88f7baed53cd25e1014d07"&gt;https://preview.redd.it/zcq88bwjvvze1.png?width=2492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e7c5c36e0f1770fab88f7baed53cd25e1014d07&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That means that we can now have selfhosted AI models that learn from the Web ... without relying on Google or any central entity at all !&lt;/p&gt; &lt;p&gt;Some caveats; 1. Of course this is inferior to using google or even duckduckgo, I just wanted to share that here because I think you'll find it cool. 2. You need a solid CPU to have a lot of concurrent research, my Celeron gets hammered to 100% usage at each query. (open-webui and a bunch of other services are running on this server, that must not help). That's not your average LocalLLama rig costing my yearly salary ahah.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7q2mkkshvvze1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56b75972d9a1e4e98c7cdfe111dad47b7f87cbeb"&gt;https://preview.redd.it/7q2mkkshvvze1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56b75972d9a1e4e98c7cdfe111dad47b7f87cbeb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marsxyz"&gt; /u/marsxyz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T04:40:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj89gq</id>
    <title>ManaBench: A Novel Reasoning Benchmark Based on MTG Deck Building</title>
    <updated>2025-05-10T11:40:13+00:00</updated>
    <author>
      <name>/u/Jake-Boggs</name>
      <uri>https://old.reddit.com/user/Jake-Boggs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj89gq/manabench_a_novel_reasoning_benchmark_based_on/"&gt; &lt;img alt="ManaBench: A Novel Reasoning Benchmark Based on MTG Deck Building" src="https://external-preview.redd.it/z_Ta6BgN-0E4xjWqloxN8S0IMfl-GG_lbgHPaHjOU5s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9fd95a896e5e0a2b1e6245ca34120093288ab9d" title="ManaBench: A Novel Reasoning Benchmark Based on MTG Deck Building" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to share a new benchmark I've developed called &lt;strong&gt;ManaBench&lt;/strong&gt;, which tests LLM reasoning abilities using Magic: The Gathering deck building as a proxy.&lt;/p&gt; &lt;h1&gt;What is ManaBench?&lt;/h1&gt; &lt;p&gt;ManaBench evaluates an LLM's ability to reason about complex systems by presenting a simple but challenging task: given a 59-card MTG deck, select the most suitable 60th card from six options.&lt;/p&gt; &lt;p&gt;This isn't about memorizing card knowledge - all the necessary information (full card text and rules) is provided in the prompt. It's about reasoning through complex interactions, understanding strategic coherence, and making optimal choices within constraints.&lt;/p&gt; &lt;h1&gt;Why it's a good benchmark:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strategic reasoning&lt;/strong&gt;: Requires understanding deck synergies, mana curves, and card interactions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System optimization&lt;/strong&gt;: Tests ability to optimize within resource constraints&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expert-aligned&lt;/strong&gt;: The &amp;quot;correct&amp;quot; answer is the card that was actually in the human-designed tournament deck&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hard to game&lt;/strong&gt;: Large labs are unlikely to optimize for this task and the questions are private&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Results for Local Models vs Cloud Models&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/adlxg53bxxze1.png?width=1065&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39c1fe2aff1b4a5906b11bbd112d1bc53706b544"&gt;ManaBench Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Looking at these results, several interesting patterns emerge:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama models underperform expectations&lt;/strong&gt;: Despite their strong showing on many standard benchmarks, Llama 3.3 70B scored only 19.5% (just above random guessing at 16.67%), and Llama 4 Maverick hit only 26.5%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Closed models dominate&lt;/strong&gt;: o3 leads the pack at 63%, followed by Claude 3.7 Sonnet at 49.5%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance correlates with but differentiates better than LMArena scores&lt;/strong&gt;: Notice how the spread between models is much wider on ManaBench&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b3zyiwuoxxze1.png?width=814&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21d07b7fdad90b4fe3eb16b860f14617b3872fa0"&gt;ManaBench vs LMArena&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What This Means for Local Model Users&lt;/h1&gt; &lt;p&gt;If you're running models locally and working on tasks that require complex reasoning (like game strategy, system design, or multi-step planning), these results suggest that current open models may struggle more than benchmarks like MATH or LMArena would indicate.&lt;/p&gt; &lt;p&gt;This isn't to say local models aren't valuable - they absolutely are! But it's useful to understand their relative strengths and limitations compared to cloud alternatives.&lt;/p&gt; &lt;h1&gt;Looking Forward&lt;/h1&gt; &lt;p&gt;I'm curious if these findings match your experiences. The current leaderboard aligns very well with my results using many of these models personally.&lt;/p&gt; &lt;p&gt;For those interested in the technical details, my &lt;a href="https://boggs.tech/posts/evaluating-llm-reasoning-with-mtg-deck-building/"&gt;full writeup&lt;/a&gt; goes deeper into the methodology and analysis.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: The specific benchmark questions are not being publicly released to prevent contamination of future training data. If you are a researcher and would like access, please reach out.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jake-Boggs"&gt; /u/Jake-Boggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj89gq/manabench_a_novel_reasoning_benchmark_based_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj89gq/manabench_a_novel_reasoning_benchmark_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj89gq/manabench_a_novel_reasoning_benchmark_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T11:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj2j6q</id>
    <title>Seed-Coder 8B</title>
    <updated>2025-05-10T05:07:09+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"&gt; &lt;img alt="Seed-Coder 8B" src="https://external-preview.redd.it/qN4W2OErTr-fXyFZh4FVGoCZMT9K6nHi3_DvqJJHr5c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bcd7116e2911f655490d68be32d15c7b0a893b6" title="Seed-Coder 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bytedance has released a new 8B code-specific model that outperforms both Qwen3-8B and Qwen2.5-Coder-7B-Inst. I am curious about the performance of its base model in code FIM tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wbtmpay50wze1.jpg?width=8348&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b7e6bb5d9a152ed6594e5683f582f9d5f9fb81d9"&gt;https://preview.redd.it/wbtmpay50wze1.jpg?width=8348&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b7e6bb5d9a152ed6594e5683f582f9d5f9fb81d9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ByteDance-Seed/Seed-Coder"&gt;github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Instruct"&gt;HF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Base"&gt;Base Model HF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T05:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj7l8p</id>
    <title>AMD eGPU over USB3 for Apple Silicon by Tiny Corp</title>
    <updated>2025-05-10T10:58:23+00:00</updated>
    <author>
      <name>/u/zdy132</name>
      <uri>https://old.reddit.com/user/zdy132</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7l8p/amd_egpu_over_usb3_for_apple_silicon_by_tiny_corp/"&gt; &lt;img alt="AMD eGPU over USB3 for Apple Silicon by Tiny Corp" src="https://external-preview.redd.it/2BON-N6TCd_ctm0tqr4moZr228fviTa5r-AUavBUN3Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de7825d238122dad4a6420788d2290a151b8da31" title="AMD eGPU over USB3 for Apple Silicon by Tiny Corp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zdy132"&gt; /u/zdy132 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/__tinygrad__/status/1920960070055080107"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7l8p/amd_egpu_over_usb3_for_apple_silicon_by_tiny_corp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj7l8p/amd_egpu_over_usb3_for_apple_silicon_by_tiny_corp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T10:58:23+00:00</published>
  </entry>
</feed>
