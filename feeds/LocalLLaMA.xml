<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-03T11:36:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mg1e80</id>
    <title>Any news about the open source models that OpenAI promised to release ?</title>
    <updated>2025-08-02T21:10:32+00:00</updated>
    <author>
      <name>/u/NeedleworkerDull7886</name>
      <uri>https://old.reddit.com/user/NeedleworkerDull7886</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sam Altman promised imminent release of open source/weight models . It seems we haven‚Äôt heard anything new in the past few weeks, have we?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeedleworkerDull7886"&gt; /u/NeedleworkerDull7886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg1e80/any_news_about_the_open_source_models_that_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T21:10:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgd3lh</id>
    <title>HRM model was trained on test?</title>
    <updated>2025-08-03T07:21:00+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/"&gt; &lt;img alt="HRM model was trained on test?" src="https://preview.redd.it/e4op2j02argf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8eece1e33073e0e4fe5801c6917464a55d051355" title="HRM model was trained on test?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e4op2j02argf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgd3lh/hrm_model_was_trained_on_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T07:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfxas1</id>
    <title>Qwen moe in C</title>
    <updated>2025-08-02T18:14:18+00:00</updated>
    <author>
      <name>/u/1Hesham</name>
      <uri>https://old.reddit.com/user/1Hesham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped something I'm really excited about! üöÄ I was scrolling through my feed and saw Sebastian Raschka, PhD 's incredible Qwen3 MoE implementation in PyTorch. The educational clarity of his code just blew me away - especially how he broke down the Mixture of Experts architecture in his LLMs-from-scratch repo. That got me thinking... what if I could bring this to pure C? ü§î Inspired by Andrej Karpathy's legendary llama2.c approach (seriously, if you haven't seen it, check it out), I decided to take on the challenge of implementing Qwen3's 30B parameter model with 128 experts in a single C file. The result? Qwen_MOE_C - a complete inference engine that: ‚úÖ Handles sparse MoE computation (only 8 out of 128 experts active) ‚úÖ Supports Grouped Query Attention with proper head ratios ‚úÖ Uses memory mapping for efficiency (~30GB models) ‚úÖ Zero external dependencies (just libc + libm) The beauty of this approach is the same as llama2.c - you can understand every line, it's hackable, and it runs anywhere C runs. No frameworks, no dependencies, just pure computational transparency. Huge thanks to Sebastian Raschka for the reference implementation and educational materials, and to Andrej Karpathy for showing us that simplicity is the ultimate sophistication in ML systems. Sometimes the best way to truly understand something is to build it from scratch. üõ†Ô∏è Link to the project: &lt;a href="https://github.com/h9-tec/Qwen_MOE_C"&gt;https://github.com/h9-tec/Qwen_MOE_C&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1Hesham"&gt; /u/1Hesham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfxas1/qwen_moe_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T18:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgfs7l</id>
    <title>Do you also get weird behavior from Qwen3-Coder-30B-A3B?</title>
    <updated>2025-08-03T10:18:02+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was using this model as an assistant to modify code in a C++ file with ~roughly 800 lines of code. However, the model did a lot of mistakes, and it constantly corrected itself (in the same reply) in a way like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Here is the modification of the code:&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt;code**&lt;/p&gt; &lt;p&gt;But on a second thought, that was not a good implementation, here is a better method to do it:&lt;/p&gt; &lt;p&gt;&lt;em&gt;\&lt;/em&gt;code**&lt;/p&gt; &lt;p&gt;But on a third thought, here is a better way to do it...&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And each method it provided had mistakes, such as missing lines of code. I then tried something very simple with HTML and prompted:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;How do I set a left, right and bottom border on a div in CSS?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It then provided me with 4 methods. Notice how &lt;strong&gt;method 1&lt;/strong&gt; and &lt;strong&gt;method 4&lt;/strong&gt; is the &lt;em&gt;exact same&lt;/em&gt; code repeated. &lt;strong&gt;Method 2&lt;/strong&gt; is also almost the exact same code but just with an added &lt;code&gt;border: none;&lt;/code&gt; line of code. &lt;/p&gt; &lt;p&gt;Also, &lt;strong&gt;method 3&lt;/strong&gt; has a mistake where it do not to set a border on the right side of the div, it wrote &lt;code&gt;border-width: 0 0 2px 2px;&lt;/code&gt; but the correct code would be &lt;code&gt;border-width: 0 2px 2px 2px;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The output:&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Method 1: Using Individual Border Properties&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;div { border-left: 2px solid black; border-right: 2px solid black; border-bottom: 2px solid black; } &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Method 2: Using the Shorthand border Property&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;div { border: none; /* Reset all borders first */ border-left: 2px solid black; border-right: 2px solid black; border-bottom: 2px solid black; } &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Method 3: Using border-width, border-style, and border-color&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;div { border-width: 0 0 2px 2px; /* top right bottom left */ border-style: solid; border-color: black; } &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Method 4: More Concise Shorthand&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;div { border-left: 2px solid black; border-right: 2px solid black; border-bottom: 2px solid black; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm using Unsloth's UD-Q5_K_XL quant with the recommended settings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Temperature: &lt;strong&gt;0,7&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Top K: &lt;strong&gt;20&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Repeat Penalty: &lt;strong&gt;1,05&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Top P: &lt;strong&gt;0,8&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyone else having similar odd behavior with this model? Might the quant/jinja be broken currently?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgfs7l/do_you_also_get_weird_behavior_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T10:18:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfs9qn</id>
    <title>[GUIDE] Running Qwen-30B (Coder/Instruct/Thinking) with CPU-GPU Partial Offloading - Tips, Tricks, and Optimizations</title>
    <updated>2025-08-02T14:44:41+00:00</updated>
    <author>
      <name>/u/AliNT77</name>
      <uri>https://old.reddit.com/user/AliNT77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using &lt;code&gt;llama.cpp&lt;/code&gt; with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here‚Äôs what actually works.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;KV Quantization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;KV cache quantization matters a lot&lt;/strong&gt;. If you're offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. Use &lt;code&gt;q5_1&lt;/code&gt; for a good balance of memory usage and performance. It works well in PPL tests and in practice.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Offloading Strategy&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You're bottlenecked by your &lt;strong&gt;system RAM bandwidth&lt;/strong&gt; when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.&lt;/li&gt; &lt;li&gt;Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU‚Äôs VRAM limit. More offloading = slower inference.blk\.(1[6-9]|[2-4][0-9])\.ffn_.*._=CPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Memory Tuning for CPU Offloading&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;System memory speed has a major impact on throughput when using partial offloading.&lt;/li&gt; &lt;li&gt;Run your RAM at the highest stable speed. Overclock and tighten timings if you're comfortable doing so.&lt;/li&gt; &lt;li&gt;On &lt;strong&gt;AM4&lt;/strong&gt; platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.&lt;/li&gt; &lt;li&gt;On &lt;strong&gt;AM5&lt;/strong&gt;, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.&lt;/li&gt; &lt;li&gt;Poor memory tuning will bottleneck your CPU offloading even with a fast processor.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ubatch (Prompt Batch Size)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Higher &lt;code&gt;ubatch&lt;/code&gt; values significantly improve prompt processing (PP) performance.&lt;/li&gt; &lt;li&gt;Try values like &lt;code&gt;768&lt;/code&gt; or &lt;code&gt;1024&lt;/code&gt;. You‚Äôll use more VRAM, but it‚Äôs often worth it for the speedup.&lt;/li&gt; &lt;li&gt;If you‚Äôre VRAM-limited, lower this until it fits.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Extra Performance Boost&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Set this environment variable for a 5‚Äì10% performance gain:Launch like this: LLAMA_SET_ROWS=1 ./llama-server -md /path/to/model etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Speculative Decoding Tips (SD)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Speculative decoding is supported in &lt;code&gt;llama.cpp&lt;/code&gt;, but there are a couple important caveats:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;KV cache quant affects acceptance rate heavily.&lt;/strong&gt; Using &lt;code&gt;q4_0&lt;/code&gt; for the draft model‚Äôs KV cache &lt;em&gt;halves&lt;/em&gt; the acceptance rate in my testing. Use &lt;code&gt;q5_1&lt;/code&gt; or even &lt;code&gt;q8_0&lt;/code&gt; for the draft model KV cache for much better performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Draft model context handling is broken after filling the draft KV cache.&lt;/strong&gt; Once the draft model‚Äôs context fills up, performance tanks. Right now it‚Äôs better to run the draft with full context size. Reducing it actually hurts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Draft parameters matter a lot&lt;/strong&gt;. In my testing, using &lt;code&gt;--draft-p-min 0.85 --draft-min 2 --draft-max 12&lt;/code&gt; gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For SD, try using &lt;strong&gt;Qwen 3 0.6B&lt;/strong&gt; as the draft model. It‚Äôs fast and works well, as long as you avoid the issues above.&lt;/p&gt; &lt;p&gt;If you‚Äôve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AliNT77"&gt; /u/AliNT77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T14:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfvxdo</id>
    <title>What would it take to support Multi-Token-Prediction (MTP) in llama.cpp? feat. GLM 4.5</title>
    <updated>2025-08-02T17:17:04+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15026"&gt;A new PR&lt;/a&gt; was created to support GLM 4.5's models in llama.cpp, as the original, highly anticipated &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939"&gt;#14939&lt;/a&gt; seemed to get stuck. The new PR description reads: &amp;quot;&lt;strong&gt;this PR will NOT attempt to implement MTP&lt;/strong&gt;&amp;quot;, with great progress being made in short time. (Amazing!!!)&lt;/p&gt; &lt;p&gt;Given that MTP is supposed to achieve a 5x (or equally significant) inference speedup (correct me if I am wrong), why do we not increase community efforts in trying to enable MTP for these and all models going forward? We heard before that it's not optimisations that will advance Local LLMs, but architecture shifts, and this could be in the same level als MoEs in terms of efficacy.&lt;/p&gt; &lt;p&gt;Disclaimer: I am eternally grateful for everybody's contribution to the field, as LLMs allow me to code what I couldn't code before. But I have in no way the foundational understanding, knowledge or experience to contribute, so I am really thankful for all efforts from the involved people on github!&lt;/p&gt; &lt;p&gt;PS: does MTP already work on/with MLX?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfvxdo/what_would_it_take_to_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T17:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgg3mh</id>
    <title>Successfully running INSTINCT MI50 on Win11</title>
    <updated>2025-08-03T10:38:30+00:00</updated>
    <author>
      <name>/u/Desperate-Sir-5088</name>
      <uri>https://old.reddit.com/user/Desperate-Sir-5088</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/"&gt; &lt;img alt="Successfully running INSTINCT MI50 on Win11" src="https://b.thumbs.redditmedia.com/ac8XNi9a_cAzytIhKD5Gg-Gwa0GLMmvvfiuUz9ia33E.jpg" title="Successfully running INSTINCT MI50 on Win11" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, poor GPU guys&lt;/p&gt; &lt;p&gt;A few days ago, I purchased the 32GB version of MI50 from Alibaba, and it arrived at my doorstep via UPS in just a few days, accompanied by a rather loud blower.&lt;/p&gt; &lt;p&gt;Some married guys might understand, but I‚Äôve been using an m-ATX case I bought about 15 years ago, and there‚Äôs no room for the MI50 since the 4070ti is already in there. I went ahead and used a PCIe riser cable to mount it on the side of my desk, and then I finally got down to ‚Äúreal‚Äù work.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80"&gt;https://preview.redd.it/oe4uyadb4sgf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e8c9de636b034bd89f57dddca7291dcce101a80&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the reasons the MI50 was rejected is that AMD only developed drivers for Linux and has since discontinued support, as most people are aware. That's why the ‚Äú32GB‚Äù model ended up in my hands.&lt;/p&gt; &lt;p&gt;Of course, some experts claim they can force-install the Radeon Pro VII BIOS, but that seemed too challenging for me, and after reading many posts stating that the ‚ÄúOriginal MI50‚Äù cannot be BIOS-re-flashed, I had given up.&lt;/p&gt; &lt;p&gt;First, take a look at the results: the MI50 is running with GTX 4070ti or alone on Windows.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9"&gt;https://preview.redd.it/pvictbhu4sgf1.png?width=1707&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=49102c07c665ed9635a4c99e9a3aed46da15c6a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3cabctch4sgf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ed2201e24962d94990ea9bc149d8ec389982b96"&gt;4070+MI50 (22GB only)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lrtre5656sgf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f14e51095d28e301c62418f12f52586a62861ee2"&gt;MI50 works alone upto 30GB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guys, hold your horses. I'm aware there are a few issues here.&lt;/p&gt; &lt;p&gt;1) It's recognized as a Radeon Pro VII&lt;/p&gt; &lt;p&gt;2) It runs on LM STUDIO, which some people really dislike&lt;/p&gt; &lt;p&gt;3) Even if it's recognized as Vulkan, you can't use the combined VRAM of both cards‚Äîonly twice the VRAM of the first graphics card&lt;/p&gt; &lt;p&gt;(On my PC, it's 12+12GB instead of 12+32GB)&lt;/p&gt; &lt;p&gt;-&amp;gt; However, I haven't tested it yet, but if you get a 32GB 5090 or V100, it might work with 32+32, and being able to steal GTX's prompt processing ability is an extra bonus.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Anyway, there are only three things you need to do.&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Disable Secure Boot in the CMOS BIOS.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run PowerShell in administrator mode and enter the following command:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;bcdedit.exe -set TESTSIGNING on&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download and install the Polaris-Vega-Navi driver created by the real pros.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://rdn-id.com/"&gt;R.ID - AMD 3rd Party Drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All risks are on you, but I think it's better than getting divorced by your wife over buying an RTX 6000,&lt;/p&gt; &lt;p&gt;The blower fan sent by the Ali seller is very effective, but it's incredibly loud. The GPU also gets quite hot, so you might want to find a way to adjust the fan speed.&lt;/p&gt; &lt;p&gt;P.S. Could you please share a link to a guide on how to install ROCM to support MI50 on Ubuntu 24.04 LTS? I tried version 6.3.3, but it doesn't recognize it at all. Do I really have to rebuild PyTorch from scratch?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate-Sir-5088"&gt; /u/Desperate-Sir-5088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgg3mh/successfully_running_instinct_mi50_on_win11/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T10:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgdh6r</id>
    <title>is the P102-100 still a viable option for LLM?</title>
    <updated>2025-08-03T07:45:44+00:00</updated>
    <author>
      <name>/u/Boricua-vet</name>
      <uri>https://old.reddit.com/user/Boricua-vet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdh6r/is_the_p102100_still_a_viable_option_for_llm/"&gt; &lt;img alt="is the P102-100 still a viable option for LLM?" src="https://preview.redd.it/oy25ru8gergf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd72000cba8efed634dc539ff393fe099624df46" title="is the P102-100 still a viable option for LLM?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen thousands of posts of people asking what card to buy and there is two points of view. One is buy expensive 3090, or even more expensive 5000 series or, buy cheap and try it. This post will cover why the P102-100 is still relevant and why it is simply the best budget card to get at 60 dollars.&lt;/p&gt; &lt;p&gt;If you are just doing LLM, Vision and no image or video generation. This is hands down the best budget card to get all because of its memory bandwidth. This list covers entry level cards form all series. Yes I know there are better cards but I am comparing the P102-100 with all entry level cards only and those better cards are 10x more.This is for the budget build people.&lt;/p&gt; &lt;p&gt;2060 - 336.0 GB/s - $150 8GB&lt;br /&gt; 3060 - 360.0 GB/s - $200+ 8GB&lt;/p&gt; &lt;p&gt;4060 - 272.0 GB/s - $260+ 8GB&lt;/p&gt; &lt;p&gt;5060 - 448.0 GB/s - $350+ 8GB&lt;/p&gt; &lt;p&gt;P102-100 - 440.3 GB/s - $60 10GB.&lt;/p&gt; &lt;p&gt;Is the P102-100 faster than an&lt;/p&gt; &lt;p&gt;entry 2060 = yes&lt;/p&gt; &lt;p&gt;entry 3060 = yes&lt;/p&gt; &lt;p&gt;entry 4060 = yes.&lt;/p&gt; &lt;p&gt;only a 5060 would be faster and not by much.&lt;/p&gt; &lt;p&gt;Does the P102-100 load slower, yes it takes about 1 second per GB on the model. PCie 1x4 =1GB/s but once the model is leaded it will be normal with no delays on all your queries.&lt;/p&gt; &lt;p&gt;I have attached screenshots of a bunch of models, all with 32K context so you can see what to expect. Compare those results with other entry cards using the same 32K context and you will for yourself. Make sure they are using 32K context as the P102-100 would also be faster with lower context.&lt;/p&gt; &lt;p&gt;so if you want to try LLM's and not go broke, the P102-100 is a solid card to try for 60 bucks. I have 2 of them and those results are using 2 cards so I have 20GB VRAM for 70 bucks at 35 each when I bought them. Now they would be 120 bucks. I am not sure if you can get 20GB VRAM for less than is as fast as this.&lt;/p&gt; &lt;p&gt;I hope this helps other people that have been afraid to try local private ai because of the costs. I hope this motivates you to at least try. It is just 60 bucks.&lt;/p&gt; &lt;p&gt;I will probably be updating this next week as I have a third card and I am moving up to 30GB. I should be able to run these models with higher context, 128k, 256k and even bigger models. I will post some updates for anyone interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boricua-vet"&gt; /u/Boricua-vet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oy25ru8gergf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdh6r/is_the_p102100_still_a_viable_option_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdh6r/is_the_p102100_still_a_viable_option_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T07:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfgj0g</id>
    <title>all I need....</title>
    <updated>2025-08-02T03:34:51+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"&gt; &lt;img alt="all I need...." src="https://preview.redd.it/ggc3dzhr0jgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=939ec0dfb5d8aad25a06b51c38644b3ee7d0d9cd" title="all I need...." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ggc3dzhr0jgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfgj0g/all_i_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T03:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg3d62</id>
    <title>Note to the Qwen team re. the new 30B A3B Coder and Instruct versions: Coder is lobotomized when compared to Instruct</title>
    <updated>2025-08-02T22:38:22+00:00</updated>
    <author>
      <name>/u/jackdareel</name>
      <uri>https://old.reddit.com/user/jackdareel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My own testing results are backed up by the private tests run on dubesor.de. Coder is significantly worse in coding related knowledge than Instruct. If Coder is fine tuned from Instruct, I can only surmise that the additional training on a plethora of programming languages and agentic abilities has resulted in a good dose of catastrophic forgetting.&lt;/p&gt; &lt;p&gt;The take away is that training data is king at these small model sizes, and that we need coders that are not overwhelmed in the attempt of making a generic Swiss Army knife for all programming use cases.&lt;/p&gt; &lt;p&gt;We need specialists for individual languages (or perhaps domains, such as web development). These should be at the Instruct level of general ability, with the added speciality of no negative consequence to the model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jackdareel"&gt; /u/jackdareel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3d62/note_to_the_qwen_team_re_the_new_30b_a3b_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T22:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgdur5</id>
    <title>üß† ICM+DPO: Used Qwen3's coherent understanding to improve Gemma3 at math - cross-model capability transfer with zero supervision</title>
    <updated>2025-08-03T08:10:25+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;Just released something that extends the recent &lt;a href="https://arxiv.org/abs/2506.10139"&gt;ICM paper&lt;/a&gt; in a big way - using one model's coherent understanding to improve a completely different model.&lt;/p&gt; &lt;h1&gt;Background: What is ICM?&lt;/h1&gt; &lt;p&gt;The original &lt;a href="https://arxiv.org/abs/2506.10139"&gt;&amp;quot;Unsupervised Elicitation of Language Models&amp;quot;&lt;/a&gt; paper showed something remarkable: &lt;strong&gt;models can generate their own training labels by finding internally coherent patterns&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Their key insight: pretrained models already understand concepts like mathematical correctness, but struggle to express this knowledge consistently. ICM finds label assignments that are &amp;quot;mutually predictable&amp;quot; - where each label can be predicted from all the others.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original ICM results&lt;/strong&gt;: Matched performance of golden supervision without any external labels. Pretty amazing, but only improved the same model using its own labels.&lt;/p&gt; &lt;h1&gt;Our extension: Cross-model capability transfer&lt;/h1&gt; &lt;p&gt;We took ICM further - &lt;strong&gt;what if we use one model's coherent understanding to improve a completely different model?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our process:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Used ICM on Qwen3 to extract its coherent math reasoning patterns&lt;/li&gt; &lt;li&gt;Generated DPO training data from Qwen3's coherent vs incoherent solutions&lt;/li&gt; &lt;li&gt;Trained Gemma3 on this data - &lt;strong&gt;Gemma3 learned from Qwen3's understanding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Zero external supervision, pure model-to-model knowledge transfer&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Results on local models&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3-0.6B&lt;/strong&gt;: 63.2 ‚Üí 66.0 MATH-500 (+4%) [original ICM self-improvement]&lt;br /&gt; &lt;strong&gt;Gemma3-1B&lt;/strong&gt;: 41.0 ‚Üí 45.6 MATH-500 (+11%) [&lt;strong&gt;novel: learned from Qwen3!&lt;/strong&gt;]&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The breakthrough&lt;/strong&gt;: Successfully transferred mathematical reasoning coherence from Qwen3 to improve Gemma3's abilities across different architectures.&lt;/p&gt; &lt;h1&gt;Why this matters beyond the original paper&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cross-model knowledge transfer&lt;/strong&gt; - use any strong model to improve your local models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Democratizes capabilities&lt;/strong&gt; - extract from closed/expensive models to improve open ones&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No training data needed&lt;/strong&gt; - pure capability extraction and transfer&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scales the ICM concept&lt;/strong&gt; - from self-improvement to ecosystem-wide improvement&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's available&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/codelion/icm"&gt;https://github.com/codelion/icm&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Both models&lt;/strong&gt;: Self-improved Qwen3 + Gemma3 (learned from Qwen3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transfer pipeline&lt;/strong&gt;: Extract from any model to improve another&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full writeup&lt;/strong&gt;: &lt;a href="https://huggingface.co/blog/codelion/internal-coherence-maximization"&gt;https://huggingface.co/blog/codelion/internal-coherence-maximization&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick start&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/codelion/icm.git &amp;amp;&amp;amp; cd icm &amp;amp;&amp;amp; pip install -e . # Extract coherent patterns from a strong model (teacher) icm run --model Qwen/Qwen2.5-Math-7B-Instruct --dataset gsm8k --max-examples 500 # Use those patterns to improve your local model (student) icm export --format dpo --output-path teacher_knowledge.jsonl # Train your model on teacher_knowledge.jsonl &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Anyone interested in trying capability transfer with their local models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdur5/icmdpo_used_qwen3s_coherent_understanding_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T08:10:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg6xia</id>
    <title>Any news on updated Qwen3-8B/14B versions?</title>
    <updated>2025-08-03T01:32:51+00:00</updated>
    <author>
      <name>/u/zyxwvu54321</name>
      <uri>https://old.reddit.com/user/zyxwvu54321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since Qwen3-235B-A22B and Qwen3-30B-A3B have been updated, is there any word on similar updates for Qwen3-8B or Qwen3-14B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zyxwvu54321"&gt; /u/zyxwvu54321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg6xia/any_news_on_updated_qwen38b14b_versions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T01:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfqejn</id>
    <title>Open-source model that is as intelligent as Claude Sonnet 4</title>
    <updated>2025-08-02T13:21:11+00:00</updated>
    <author>
      <name>/u/vishwa1238</name>
      <uri>https://old.reddit.com/user/vishwa1238</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spend about 300-400 USD per month on Claude Code with the max 5x tier. I‚Äôm unsure when they‚Äôll increase pricing, limit usage, or make models less intelligent. I‚Äôm looking for a cheaper or open-source alternative that‚Äôs just as good for programming as Claude Sonnet 4. Any suggestions are appreciated. &lt;/p&gt; &lt;p&gt;Edit: I don‚Äôt pay $300-400 per month. I have Claude Max subscription (100$) that comes with a Claude code. I used a tool called ccusage to check my usage, and it showed that I use approximately $400 worth of API every month on my Claude Max subscription. It works fine now, but I‚Äôm quite certain that, just like what happened with cursor, there will likely be a price increase or a higher rate limiting soon. &lt;/p&gt; &lt;p&gt;Thanks for all the suggestions. I‚Äôll try out Kimi2, R1, qwen 3, glm4.5 and Gemini 2.5 Pro and update how it goes in another post. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vishwa1238"&gt; /u/vishwa1238 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfqejn/opensource_model_that_is_as_intelligent_as_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T13:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mfuiri</id>
    <title>Qwen Code + Qwen Coder 30b 3A is insane</title>
    <updated>2025-08-02T16:17:55+00:00</updated>
    <author>
      <name>/u/Flashy_Management962</name>
      <uri>https://old.reddit.com/user/Flashy_Management962</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is just a little remark that if you haven't you definitely should try qwen code &lt;a href="https://github.com/QwenLM/qwen-code"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;br /&gt; I use qwen coder and qwen 3 30b thinking while the latter still needs some copy and pasting. I'm working on and refining a script for syncing my koreader metadata with obsidian for the plugin lineage (every highlight in own section). The last time I tried to edit it, I used Grok 4 and Claude Sonnet Thinking on Perplexity (its the only subscription I had until know) even with those models it was tedious and not really working. But with Qwen Code it looks very different to be honest. &lt;/p&gt; &lt;p&gt;The metadata is in written in lua which at first was a pain to parse right (remember, I actually cannot code by myself, I understand the logic and I can tell in natural language what is wrong, but nothing more) and I got qwen code running today with llama cpp and it almost integrated everything on the first try and I'm very sure that nothing of that was in the models trainingdata. We reached a point where - if we know a little bit - can let code be written for us almost without us needing to know what is happening at all, running on a local machine. Of course it is very advantageous to know what you are looking for.&lt;/p&gt; &lt;p&gt;So this is just a little recommendation, if you have not tried qwen code, do it. I guess its almost only really useful for people like me, who don't know jack shit about coding. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy_Management962"&gt; /u/Flashy_Management962 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mfuiri/qwen_code_qwen_coder_30b_3a_is_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T16:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg7qpa</id>
    <title>Announcing Olla - LLM Load Balancer, Proxy &amp; Model Unifier for Ollama / LM Studio &amp; OpenAI Compatible backends</title>
    <updated>2025-08-03T02:14:39+00:00</updated>
    <author>
      <name>/u/2shanigans</name>
      <uri>https://old.reddit.com/user/2shanigans</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/"&gt; &lt;img alt="Announcing Olla - LLM Load Balancer, Proxy &amp;amp; Model Unifier for Ollama / LM Studio &amp;amp; OpenAI Compatible backends" src="https://b.thumbs.redditmedia.com/aNDVZRvyIDhy6vwT1mm7Ch4ypRPRTf4JSNM_Np7grwg.jpg" title="Announcing Olla - LLM Load Balancer, Proxy &amp;amp; Model Unifier for Ollama / LM Studio &amp;amp; OpenAI Compatible backends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been working on an LLM proxy, balancer &amp;amp; model unifier based on a few other projects we've created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. &lt;/p&gt; &lt;p&gt;This was primarily after running into the same issues across several organisations - managing multiple LLM backend instances &amp;amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),&lt;/p&gt; &lt;p&gt;So some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.&lt;/p&gt; &lt;p&gt;Olla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.&lt;/p&gt; &lt;p&gt;Add your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.&lt;/p&gt; &lt;p&gt;The attempt to unify across providers wasn't as successful - as in, both LMStudio &amp;amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/thushan/olla"&gt;https://github.com/thushan/olla&lt;/a&gt; (golang)&lt;/p&gt; &lt;p&gt;Would love to know your thoughts. &lt;/p&gt; &lt;p&gt;Olla is still in its infancy, so we don't have auth implemented etc but there are plans in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2shanigans"&gt; /u/2shanigans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mg7qpa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T02:14:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mga3ox</id>
    <title>I made a prebuilt windows binary for ik_llama.cpp</title>
    <updated>2025-08-03T04:19:52+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/X5R/ik_llama.cpp"&gt;https://huggingface.co/X5R/ik_llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mga3ox/i_made_a_prebuilt_windows_binary_for_ik_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T04:19:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mggku0</id>
    <title>XBai-04 Is It Real?</title>
    <updated>2025-08-03T11:07:25+00:00</updated>
    <author>
      <name>/u/Ordinary_Mud7430</name>
      <uri>https://old.reddit.com/user/Ordinary_Mud7430</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/"&gt; &lt;img alt="XBai-04 Is It Real?" src="https://b.thumbs.redditmedia.com/ybTHvU3e25DWWBXsMwOWfbuGc2dzpO5QuzWGZyUC65s.jpg" title="XBai-04 Is It Real?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;WHAT THE DEVIL?&lt;/p&gt; &lt;p&gt;Another open model outperforms closed ones!&lt;br /&gt; XBai o4 beats OpenAI o3-mini and &lt;em&gt;confidently&lt;/em&gt; beats Anthropic's Claude Opus.&lt;/p&gt; &lt;p&gt;‚Ä¢Parameters: 32.8 B ‚Ä¢Training: Long-CoT RL + Process Reward Learning (SPRM) ‚Ä¢Benchmarks (High-Modus): ‚Ä¢AIME24: 86.5 ‚Ä¢AIME25: 77.9 ‚Ä¢LiveCodeBench v5: 67.2 ‚Ä¢C-EVAL: 89.7&lt;/p&gt; &lt;p&gt;üîóOpen source weights: &lt;a href="https://huggingface.co/MetaStoneTec/XBai-o4"&gt;https://huggingface.co/MetaStoneTec/XBai-o4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ordinary_Mud7430"&gt; /u/Ordinary_Mud7430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mggku0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mggku0/xbai04_is_it_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T11:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgdypr</id>
    <title>Best Medical Embedding Model Released</title>
    <updated>2025-08-03T08:17:31+00:00</updated>
    <author>
      <name>/u/DataNebula</name>
      <uri>https://old.reddit.com/user/DataNebula</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just dropped a new medical embedding model that's crushing the competition: &lt;a href="https://huggingface.co/lokeshch19/ModernPubMedBERT"&gt;https://huggingface.co/lokeshch19/ModernPubMedBERT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TL;DR: This model understands medical concepts better than existing solutions and has much fewer false positives.&lt;/p&gt; &lt;p&gt;The model is based on bioclinical modernbert, fine-tuned on PubMed title-abstract pairs using InfoNCE loss with 2048 token context.&lt;/p&gt; &lt;p&gt;The model demonstrates deeper comprehension of medical terminology, disease relationships, and clinical pathways through specialized training on PubMed literature. Advanced fine-tuning enabled nuanced understanding of complex medical semantics, symptom correlations, and treatment associations.&lt;/p&gt; &lt;p&gt;The model also exhibits deeper understanding to distinguish medical from non-medical content, significantly reducing false positive matches in cross-domain scenarios. Sophisticated discrimination capabilities ensure clear separation between medical terminology and unrelated domains like programming, general language, or other technical fields.&lt;/p&gt; &lt;p&gt;Download the model, test it on your medical datasets, and give it a ‚≠ê on the &lt;a href="https://huggingface.co/lokeshch19/ModernPubMedBERT"&gt;Hugging Face&lt;/a&gt; if it enhances your workflow!&lt;/p&gt; &lt;p&gt;Edit: Added evals to HF model card&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataNebula"&gt; /u/DataNebula &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgdypr/best_medical_embedding_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T08:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgc0v0</id>
    <title>I created an app to run local AI as if it were the App Store</title>
    <updated>2025-08-03T06:12:39+00:00</updated>
    <author>
      <name>/u/Deivih-4774</name>
      <uri>https://old.reddit.com/user/Deivih-4774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/"&gt; &lt;img alt="I created an app to run local AI as if it were the App Store" src="https://a.thumbs.redditmedia.com/DMpoJ6kbQBgxqPJp_64cY0H-qxr9jJD3Tr3RuFU7894.jpg" title="I created an app to run local AI as if it were the App Store" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I got tired of installing AI tools the hard way.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every time I wanted to try something like Stable Diffusion, RVC or a local LLM, it was the same nightmare:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;terminal commands, missing dependencies, broken CUDA, slow setup, frustration.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Dione&lt;/strong&gt; ‚Äî a desktop app that makes running local AI feel like using an App Store.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Browse and install AI tools with one click (like apps)&lt;/li&gt; &lt;li&gt;No terminal, no Python setup, no configs&lt;/li&gt; &lt;li&gt;Open-source, designed with UX in mind&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;You can try it&lt;/strong&gt; &lt;a href="https://getdione.app"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tools like Pinokio or open-source repos are powerful, but honestly‚Ä¶ &lt;strong&gt;most look like they were made by devs, for devs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I wanted something simple&lt;/strong&gt;. Something visual. Something you can give to your non-tech friend and it still works.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dione is my attempt to make local AI accessible without losing control or power.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would you use something like this? Anything confusing / missing?&lt;/p&gt; &lt;p&gt;The project is still evolving, and I‚Äôm fully open to ideas and contributions. Also, if you‚Äôre into self-hosted AI or building tools around it ‚Äî let‚Äôs talk!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://getdione.app/github"&gt;https://getdione.app/github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading &amp;lt;3!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deivih-4774"&gt; /u/Deivih-4774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mgc0v0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgc0v0/i_created_an_app_to_run_local_ai_as_if_it_were/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T06:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgbprh</id>
    <title>SmallThinker-21B-A3B-Instruct-QAT version</title>
    <updated>2025-08-03T05:53:55+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/"&gt; &lt;img alt="SmallThinker-21B-A3B-Instruct-QAT version" src="https://external-preview.redd.it/45xh02HDqqpsqpvhy9Hshpnf7PYGTT5vaQkpbv1dIDU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d016e4dfb71f7ddb2afc8c6c06ee782ba15871a" title="SmallThinker-21B-A3B-Instruct-QAT version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The larger SmallThinker MoE has been through a quantization aware training process. it's uploaded to the same gguf repo a bit later. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf"&gt;https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In llama.cpp m2 air 16gb, with the &lt;code&gt;sudo sysctl iogpu.wired_limit_mb=13000&lt;/code&gt; command, it's 30 t/s. &lt;/p&gt; &lt;p&gt;The model is CPU inference optimised for very low RAM provisions + fast disc, alongside sparsity optimizations, in their llama.cpp fork. The models are pre-trained from scratch. This group always had a good eye for inference optimizations, Always happy to see their works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF/blob/main/SmallThinker-21B-A3B-Instruct-QAT.Q4_0.gguf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbprh/smallthinker21ba3binstructqat_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T05:53:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg7abc</id>
    <title>Mac + Blackwell üëÄ</title>
    <updated>2025-08-03T01:51:15+00:00</updated>
    <author>
      <name>/u/Accomplished_Ad9530</name>
      <uri>https://old.reddit.com/user/Accomplished_Ad9530</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/"&gt; &lt;img alt="Mac + Blackwell üëÄ" src="https://preview.redd.it/u2mr83o6npgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=161c4ac9304218f08618c511e09178e7a7c08931" title="Mac + Blackwell üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a WIP, but it's looking like may be possible to pair Macs with NVIDIA soon!&lt;/p&gt; &lt;p&gt;Tweet: &lt;a href="https://x.com/anemll/status/1951307167417639101"&gt;https://x.com/anemll/status/1951307167417639101&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/anemll/anemll"&gt;https://github.com/anemll/anemll&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Ad9530"&gt; /u/Accomplished_Ad9530 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u2mr83o6npgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg7abc/mac_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T01:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg5xlb</id>
    <title>I created a persistent memory for an AI assistant I'm developing, and am releasing the memory system</title>
    <updated>2025-08-03T00:41:41+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ I just open-sourced a fully working persistent memory system for AI assistants!&lt;/p&gt; &lt;p&gt;üß† Features:&lt;/p&gt; &lt;p&gt;- Real-time memory capture across apps (LM Studio, VS Code, etc.)&lt;/p&gt; &lt;p&gt;- Semantic search via vector embeddings&lt;/p&gt; &lt;p&gt;- Tool call logging for AI self-reflection&lt;/p&gt; &lt;p&gt;- Cross-platform and fully tested&lt;/p&gt; &lt;p&gt;- Open source and modular&lt;/p&gt; &lt;p&gt;Built with: Python, SQLite, watchdog, and AI copilots like ChatGPT and GitHub Copilot ü§ù&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/savantskie/persistent-ai-memory"&gt;https://github.com/savantskie/persistent-ai-memory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg5xlb/i_created_a_persistent_memory_for_an_ai_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T00:41:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgbs6r</id>
    <title>We enabled Multi-GPU training in Unsloth AI ‚Äî a feature that‚Äôs usually paid ‚Äî using just 2 Copilot prompts!</title>
    <updated>2025-08-03T05:58:01+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/oevortex/unsloth"&gt;https://github.com/oevortex/unsloth&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgbs6r/we_enabled_multigpu_training_in_unsloth_ai_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T05:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mg3i48</id>
    <title>HRM solved thinking more than current "thinking" models (this needs more hype)</title>
    <updated>2025-08-02T22:44:39+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Article: &lt;a href="https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e"&gt;https://medium.com/@causalwizard/why-im-excited-about-the-hierarchical-reasoning-model-8fc04851ea7e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Context:&lt;/p&gt; &lt;p&gt;This insane new paper got 40% on ARC-AGI with an absolutely tiny model (27M params). It's seriously a revolutionary new paper that got way less attention than it deserved.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2506.21734"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A number of people have reproduced it if anyone is worried about that: &lt;a href="https://x.com/VictorTaelin/status/1950512015899840768"&gt;https://x.com/VictorTaelin/status/1950512015899840768&lt;/a&gt; &lt;a href="https://github.com/sapientinc/HRM/issues/12"&gt;https://github.com/sapientinc/HRM/issues/12&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mg3i48/hrm_solved_thinking_more_than_current_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-02T22:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgccyc</id>
    <title>ByteDance drops Seed-Prover</title>
    <updated>2025-08-03T06:34:03+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;ByteDance Seed-Prover proves math the way mathematicians do, not just explanations, but full formal proofs that a computer can verify using Lean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It writes Lean 4 code (a formal proof language), solves problems from competitions like IMO and Putnam, and gets the proof &lt;em&gt;checked&lt;/em&gt; by a compiler. &lt;/p&gt; &lt;p&gt;The key innovations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Lemma-first reasoning&lt;/strong&gt;: breaks problems into small reusable steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iterative refinement&lt;/strong&gt;: re-tries and improves failed proofs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Formal geometry engine&lt;/strong&gt;: solves insane geometry problems using a custom language and a C++ backend.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Performance? It formally solved &lt;strong&gt;5/6 IMO 2025 problems&lt;/strong&gt;, something no model has done before.&lt;/p&gt; &lt;p&gt;Check simple explanantion here : &lt;a href="https://www.youtube.com/watch?v=os1QcHEpgZQ"&gt;https://www.youtube.com/watch?v=os1QcHEpgZQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/abs/2507.23726"&gt;https://arxiv.org/abs/2507.23726&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgccyc/bytedance_drops_seedprover/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T06:34:03+00:00</published>
  </entry>
</feed>
