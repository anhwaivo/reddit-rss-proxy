<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-09T06:34:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j6sgez</id>
    <title>Introducing anemll-server: serve Anemll models locally with OpenAI API compatible endpoints</title>
    <updated>2025-03-08T22:13:46+00:00</updated>
    <author>
      <name>/u/BaysQuorv</name>
      <uri>https://old.reddit.com/user/BaysQuorv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Always wanted to connect a frontend to my Anemll models, and now you can too with hopefully any frontend! I have only tested with Open WebUI so far, but it shouldn't be an issue with other frontends unless they need meta endpoints other than /models which this server has.&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;OpenAI-compatible API&lt;/li&gt; &lt;li&gt;Streaming responses&lt;/li&gt; &lt;li&gt;System prompt, conversation history supported&lt;/li&gt; &lt;li&gt;Working with Open WebUI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you don't know what Anemll is, its a library that lets you run your llms on the ANE instead of the gpu. This draws like 1/5 the wattage on my base m4. But its still very early so there are limitations to it of course.&lt;/p&gt; &lt;p&gt;Thanks for the inspiration to &lt;a href="https://www.reddit.com/user/Anastasiosy/"&gt;https://www.reddit.com/user/Anastasiosy/&lt;/a&gt; with your Phi4 multimodal OpenAI api compatible server :)&lt;/p&gt; &lt;p&gt;And of course big thanks to the Anemll creators, I think this technology is truly amazing. Theres nothing more beautiful than checking macmon and seeing only 1.7W power draw for LLM inference&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/alexgusevski/anemll-server"&gt;https://github.com/alexgusevski/anemll-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Anemll/Anemll"&gt;https://github.com/Anemll/Anemll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaysQuorv"&gt; /u/BaysQuorv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6sgez/introducing_anemllserver_serve_anemll_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6sgez/introducing_anemllserver_serve_anemll_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6sgez/introducing_anemllserver_serve_anemll_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T22:13:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6lrcg</id>
    <title>FULL Cursor AI Agent System Prompt</title>
    <updated>2025-03-08T17:11:57+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cursor AI (Agent, Sonnet 3.7 based) full System Prompt now published! &lt;/p&gt; &lt;p&gt;You can check it out here: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6lrcg/full_cursor_ai_agent_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6lrcg/full_cursor_ai_agent_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6lrcg/full_cursor_ai_agent_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T17:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j706an</id>
    <title>Best LLMs for specific activities or tasks??</title>
    <updated>2025-03-09T05:00:13+00:00</updated>
    <author>
      <name>/u/Glittering-Cancel-25</name>
      <uri>https://old.reddit.com/user/Glittering-Cancel-25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With all of the LLMs out right now, it isn't easy to figure out which one to use for what activity/task. From your experience, which are the best for the following:&lt;/p&gt; &lt;p&gt;Writing (both basic and creative)&lt;br /&gt; Project work/management&lt;br /&gt; Reasoning/thinking&lt;br /&gt; Deep research&lt;br /&gt; Math (basic and complex)&lt;br /&gt; Web access to the most recent information &lt;/p&gt; &lt;p&gt;Feel free to add any other categories, but those are the ones that I mainly use LLMs for and that are most relevant to me. &lt;/p&gt; &lt;p&gt;Thanks in advance for everyone's input. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glittering-Cancel-25"&gt; /u/Glittering-Cancel-25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j706an/best_llms_for_specific_activities_or_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j706an/best_llms_for_specific_activities_or_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j706an/best_llms_for_specific_activities_or_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T05:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6y0jp</id>
    <title>OpenRouter-SDK</title>
    <updated>2025-03-09T02:55:02+00:00</updated>
    <author>
      <name>/u/Vzwjustin</name>
      <uri>https://old.reddit.com/user/Vzwjustin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well, I got bored today and I've been building this all day long.&lt;/p&gt; &lt;h1&gt;OpenRouter SDK with Integrated AI Orchestration&lt;/h1&gt; &lt;p&gt;This SDK provides a unified interface for working with AI models through OpenRouter, with integrated support for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Function/Tool Calling&lt;/strong&gt;: Register and execute functions that AI models can call&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CrewAI Agent Orchestration&lt;/strong&gt;: Create and manage multi-agent systems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector Databases&lt;/strong&gt;: Store and retrieve knowledge with semantic search&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflows&lt;/strong&gt;: Define and execute complex AI workflows with dependencies&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RESTful API Server&lt;/strong&gt;: Expose all SDK functionality through HTTP endpoints&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unified API&lt;/strong&gt;: Access all OpenRouter models through a single, consistent interface&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Function Registry&lt;/strong&gt;: Register functions that AI models can call and execute&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Management&lt;/strong&gt;: Create, configure, and orchestrate AI agents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Knowledge Management&lt;/strong&gt;: Store and retrieve knowledge with vector databases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow Orchestration&lt;/strong&gt;: Define complex workflows with multiple agents and tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API Server&lt;/strong&gt;: Full-featured REST API for all SDK functionality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/spotty118/OpenRouter-sdk"&gt;https://github.com/spotty118/OpenRouter-sdk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vzwjustin"&gt; /u/Vzwjustin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6y0jp/openroutersdk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6y0jp/openroutersdk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6y0jp/openroutersdk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T02:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1j68wr1</id>
    <title>Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great</title>
    <updated>2025-03-08T04:11:36+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt; &lt;img alt="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" src="https://a.thumbs.redditmedia.com/H0M55_ytNjyQLjluGxIhsq01_P1u9IVqCdRWbacevz8.jpg" title="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690"&gt;https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In case you're wondering right now it scores about a 66 global average but Qwen advertised it scores around 73 so maybe with more optimal settings it will get closer to that range&lt;/p&gt; &lt;p&gt;This rerun with be posted on Monday&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T04:11:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6a0s2</id>
    <title>Pov: when you overthink too much</title>
    <updated>2025-03-08T05:17:47+00:00</updated>
    <author>
      <name>/u/kernel348</name>
      <uri>https://old.reddit.com/user/kernel348</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt; &lt;img alt="Pov: when you overthink too much" src="https://preview.redd.it/m9paekz5hene1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b886e3b4eb343a109cd3fef74702179d30c3c20d" title="Pov: when you overthink too much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kernel348"&gt; /u/kernel348 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9paekz5hene1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6toz5</id>
    <title>Livrbench - Tomorrow qwq32b will be updated to score?</title>
    <updated>2025-03-08T23:11:53+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6toz5/livrbench_tomorrow_qwq32b_will_be_updated_to_score/"&gt; &lt;img alt="Livrbench - Tomorrow qwq32b will be updated to score?" src="https://preview.redd.it/o87gf0yssjne1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1519b5c3aee1feb44ba4d538614f2b37db684610" title="Livrbench - Tomorrow qwq32b will be updated to score?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o87gf0yssjne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6toz5/livrbench_tomorrow_qwq32b_will_be_updated_to_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6toz5/livrbench_tomorrow_qwq32b_will_be_updated_to_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T23:11:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6ma8i</id>
    <title>Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!</title>
    <updated>2025-03-08T17:35:45+00:00</updated>
    <author>
      <name>/u/Competitive-Bake4602</name>
      <uri>https://old.reddit.com/user/Competitive-Bake4602</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt; &lt;img alt="Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!" src="https://external-preview.redd.it/OERDGiS518l9lA6nng9dhSyETZuedB7NMNyJJW94EgQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e69639f639d057ebece140432310ca7d2b192b1" title="Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oadtfpm06ine1.png?width=1874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cae2c489fa65f3fa2770991a81a54562560bd2ab"&gt;https://preview.redd.it/oadtfpm06ine1.png?width=1874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cae2c489fa65f3fa2770991a81a54562560bd2ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re part of the open-source project &lt;a href="https://github.com/anemll/anemll"&gt;&lt;strong&gt;ANEMLL&lt;/strong&gt;&lt;/a&gt;, which is working to bring large language models (LLMs) to the Apple Neural Engine. This hardware has incredible potential, but there’s a catch—Apple hasn’t shared much about its inner workings, like memory speeds or detailed performance specs. That’s where you come in!&lt;/p&gt; &lt;p&gt;To help us understand the Neural Engine better, we’ve launched a new benchmark tool: &lt;a href="https://github.com/Anemll/anemll-bench"&gt;&lt;strong&gt;anemll-bench&lt;/strong&gt;&lt;/a&gt;. It measures the Neural Engine’s bandwidth, which is key for optimizing LLMs on Apple’s chips.&lt;/p&gt; &lt;p&gt;We’re especially eager to see results from &lt;strong&gt;Ultra&lt;/strong&gt; models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;M1 Ultra&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;M2 Ultra&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;And, if you’re one of the lucky few, &lt;strong&gt;M3 Ultra&lt;/strong&gt;!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(Max models like M2 Max, M3 Max, and M4 Max are also super helpful!)&lt;/p&gt; &lt;p&gt;If you’ve got one of these Macs, here’s how you can contribute:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Clone the repo&lt;/strong&gt;: &lt;a href="https://github.com/Anemll/anemll-bench"&gt;https://github.com/Anemll/anemll-bench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Run the benchmark&lt;/strong&gt;: Just follow the README—it’s straightforward!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Share your results&lt;/strong&gt;: Submit your JSON result via a &amp;quot;issues&amp;quot; or email&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why contribute?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You’ll help an open-source project make real progress.&lt;/li&gt; &lt;li&gt;You’ll get to see how your device stacks up.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious about the bigger picture? Check out the main ANEMLL project: &lt;a href="https://github.com/anemll/anemll"&gt;https://github.com/anemll/anemll&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thanks for considering this—every contribution helps us unlock the Neural Engine’s potential&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive-Bake4602"&gt; /u/Competitive-Bake4602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T17:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6noh8</id>
    <title>Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1...</title>
    <updated>2025-03-08T18:36:50+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"&gt; &lt;img alt="Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1..." src="https://external-preview.redd.it/7RbGl7PqZ_sGLzEC-up6MH5b7zJrrofblGxxk0WxBC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=132e3f4ed63e0faee796888e40ee542ef9d2a07c" title="Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Deveraux-Parker/FlappyAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:36:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6nct7</id>
    <title>Estimating how much the new NVIDIA RTX PRO 6000 Blackwell GPU should cost</title>
    <updated>2025-03-08T18:22:47+00:00</updated>
    <author>
      <name>/u/asssuber</name>
      <uri>https://old.reddit.com/user/asssuber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No price released yet, so let's figure out how much that card should cost:&lt;/p&gt; &lt;p&gt;Extra GDDR6 costs less than $8 per GB for the end consumer &lt;a href="https://arstechnica.com/gadgets/2024/01/review-radeon-7600-xt-offers-peace-of-mind-via-lots-of-ram-remains-a-midrange-gpu/"&gt;when installed in a GPU clamshell style&lt;/a&gt; like Nvidia is using here. GDDR7 chips seems to carry a &lt;a href="https://www.trendforce.com/presscenter/news/20240627-12207.html"&gt;20-30% premium&lt;/a&gt; over GDDR6 which I'm going to generalize to all other costs and margins related to putting it in a card, so we get less than $10 per GB.&lt;/p&gt; &lt;p&gt;Using the $2000 MSRP of the 32GB RTX 5090 as basis, the NVIDIA RTX PRO 6000 Blackwell with 96GB &lt;strong&gt;should cost less than $2700&lt;/strong&gt; *(see EDIT2) to the end consumer. Oh, the wonders of a competitive capitalistic market, free of monopolistic practices!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; It seems my sarcasm above, the &amp;quot;Funny&amp;quot; flair and my comment bellow weren't sufficient, so I will try to repeat here:&lt;/p&gt; &lt;p&gt;I'm estimating how much it SHOULD cost, because everyone over here seems to be keen on normalizing the exorbitant prices for extra VRAM at the top end cards, and this is wrong. I know nvidia will price it much higher, but that was not the point of my post.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT2:&lt;/strong&gt; The RTX PRO 6000 Blackwell will reportedly feature an almost fully enabled GB202 chip, with a bit more than 10% more CUDA cores than the RTX 5090, so using it's MSRP as base isn't sufficient. Think of the price as the fair price for an hypothetical RTX 5090 96GB instead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asssuber"&gt; /u/asssuber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j707rk</id>
    <title>I made MCP (Model Context Protocol) alternative solution, for OpenAI and all other LLMs, that is cheaper than Anthropic Claude</title>
    <updated>2025-03-09T05:02:27+00:00</updated>
    <author>
      <name>/u/SamchonFramework</name>
      <uri>https://old.reddit.com/user/SamchonFramework</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j707rk/i_made_mcp_model_context_protocol_alternative/"&gt; &lt;img alt="I made MCP (Model Context Protocol) alternative solution, for OpenAI and all other LLMs, that is cheaper than Anthropic Claude" src="https://external-preview.redd.it/r6RwKjQxXMDR-e5d7t4YxF5tZC1G1HG5fUFlHIeaPcI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57049fdbdeab552ae7b0d02c18658d895181b100" title="I made MCP (Model Context Protocol) alternative solution, for OpenAI and all other LLMs, that is cheaper than Anthropic Claude" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SamchonFramework"&gt; /u/SamchonFramework &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nestia.io/articles/llm-function-calling/i-made-mcp-alternative-solution.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j707rk/i_made_mcp_model_context_protocol_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j707rk/i_made_mcp_model_context_protocol_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T05:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dryj</id>
    <title>Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes</title>
    <updated>2025-03-08T09:41:19+00:00</updated>
    <author>
      <name>/u/2TierKeir</name>
      <uri>https://old.reddit.com/user/2TierKeir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt; &lt;img alt="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" src="https://external-preview.redd.it/aGlvZGVrdDZzZm5lMc_Az5p3qLdEN__5qSL7XTQoE-2LI7eWZo3yGOsqXnkB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7745bb1240348e2c2b8426f85b17a2fe6e2edeed" title="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2TierKeir"&gt; /u/2TierKeir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9xkdwav2sfne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6vmke</id>
    <title>Is RTX 3090 still the only king of price/performance for running local LLMs and diffusion models? (plus some rant)</title>
    <updated>2025-03-09T00:47:11+00:00</updated>
    <author>
      <name>/u/martinerous</name>
      <uri>https://old.reddit.com/user/martinerous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found a used MSI SUPRIM X RTX 3090 for 820EUR in a local store. I am so tempted to buy it. And also doubtful. Essentially, looking for an excuse to buy it.&lt;/p&gt; &lt;p&gt;Do I understand correctly that there seems to be no chance of having better (and not more expensive) alternatives with at least 24 GB RAM during the next months? Intel's rumored 24GB GPU might not even come out this year or ever.&lt;/p&gt; &lt;p&gt;Does MSI SUPRIM X RTX 3090 have good quality or are there any caveats?&lt;/p&gt; &lt;p&gt;I will power-limit it for sure. I have a mATX case that might not have that good airflow because of where it's located, and also I want the GPU to last as long as possible, being such an anxious person who upgrades rarely. Not yet sure what would be the right approach to limiting it for LLM use - powerlimit, undervolting, something else?&lt;/p&gt; &lt;p&gt;The specs of my other components:&lt;/p&gt; &lt;p&gt;Mobo: ASUS TUF Gaming B760M-Plus D4&lt;/p&gt; &lt;p&gt;RAM: 64 GB DDR4&lt;/p&gt; &lt;p&gt;CPU: i7 14700 (please don't degrade, knocking on wood, updated BIOS)&lt;/p&gt; &lt;p&gt;PSU: Seasonic Focus GX-850&lt;/p&gt; &lt;p&gt;Current GPU: 4060 Ti 16 GB&lt;/p&gt; &lt;p&gt;Case: Fractal Design Define Mini (should fit the 33cm SUPRIM, if I rearrange my hard drives).&lt;/p&gt; &lt;p&gt;Using Windows 11.&lt;/p&gt; &lt;p&gt;I know there are Macs with even more unified memory and the new AMD AI CPU with their &amp;quot;coming soon&amp;quot; devices, but the performance seems to be worse than 3090 and the price is so much higher (add 21% VAT in Europe).&lt;/p&gt; &lt;p&gt;Some personal rant follows, feel free to ignore it.&lt;/p&gt; &lt;p&gt;It's not a financial issue. I could afford even a Mac. I just cannot justify it psychologically. That's the consequence of growing up in a poor family where I could not afford even a cassette player and had to build one myself from parts that people threw out. Now I can afford everything I want but I need really good justification, otherwise, I always feel guilty for months because I spent so much.&lt;/p&gt; &lt;p&gt;I already went through similar anxious doubts when I bought a 4060 Ti 16GB some time ago naively thinking that &amp;quot;16GB is good enough&amp;quot;. Then 32B LLMs came, and then Flux, and now Wan video, and I want to &amp;quot;try it all&amp;quot; and have fun generating some content for my friends and relatives. I can run it on 4060 but I spend too much time tweaking settings and choosing the right quants to avoid outofmemory errors, and waiting too long for video generation to complete, just to find that it did not follow the prompt well enough and I need to regenerate.&lt;/p&gt; &lt;p&gt;Now about excuses. I can lie to myself that it is an investment in my work education. I'm a software developer (visually impaired since birth, BTW), but I'm working on boring ERP system integrations and not on AI. Still, I have already built my own LLM frontend for KoboldCpp/OpenRouter/Gemini. That was a development experience that might be useful in work someday... or most likely not. Also, I have experimented a bit in UnrealEngine and had an idea to create a 3D assistant avatar for LLM, but let's be real - I don't have enough time for everything. So, to be totally honest with myself, it is just a hobby.&lt;/p&gt; &lt;p&gt;How do you guys justify spending that much on GPUs? :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinerous"&gt; /u/martinerous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6vmke/is_rtx_3090_still_the_only_king_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6vmke/is_rtx_3090_still_the_only_king_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6vmke/is_rtx_3090_still_the_only_king_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T00:47:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6rngt</id>
    <title>Simple inference speed comparison of Deepseek-R1 between llama.cpp and ik_llama.cpp for CPU-only inference.</title>
    <updated>2025-03-08T21:36:58+00:00</updated>
    <author>
      <name>/u/U_A_beringianus</name>
      <uri>https://old.reddit.com/user/U_A_beringianus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a simple inference speed comparison of DeepSeek-R1 between &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;ik_llama.cpp&lt;/a&gt; for CPU-only inference. The latter is a fork of an old version of llama.cpp, but includes various recent optimizations and options that the original does not (yet?).&lt;br /&gt; Comparison is on linux, with a 16 core Ryzen 7 with 96GB RAM, using Q3 quants that are mem-mapped from nvme (~319GB). Initial context consists of merely one one-liner prompt.&lt;br /&gt; Options in &lt;strong&gt;bold&lt;/strong&gt; are exclusive to ik_llama.cpp, as of today.&lt;br /&gt; The quants in the mla/ directory are made with the fork, to support its use of the &amp;quot;-mla 1&amp;quot; command line option, which yields a significantly smaller requirement for KV-Cache space. &lt;/p&gt; &lt;p&gt;llama.cpp:&lt;br /&gt; llama-server -m DeepSeek-R1-Q3_K_M-00001-of-00007.gguf --host :: -fa -c 16384 -t 15 -ctk q8_0&lt;br /&gt; KV-Cache: 56120.00 MiB&lt;br /&gt; Token rate: 0.8 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m DeepSeek-R1-Q3_K_M-00001-of-00007.gguf --host :: -fa -c 16384 -t 15 -ctk q8_0&lt;br /&gt; KV-Cache: 56120.00 MiB&lt;br /&gt; Token rate: 1.1 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m DeepSeek-R1-Q3_K_M-00001-of-00007.gguf --host :: -fa -c 16384 -t 15 &lt;strong&gt;-fmoe&lt;/strong&gt; -ctk &lt;strong&gt;q8_KV&lt;/strong&gt;&lt;br /&gt; KV-Cache: 55632.00 MiB&lt;br /&gt; Token rate: 1.2 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m mla/DeepSeek-R1-Q3_K_M-00001-of-00030.gguf --host :: -fa -c 16384 -t 15 &lt;strong&gt;-mla 1&lt;/strong&gt; &lt;strong&gt;-fmoe&lt;/strong&gt; -ctk &lt;strong&gt;q8_KV&lt;/strong&gt;&lt;br /&gt; KV-Cache: 556.63 MiB (Yes, really, no typo. This would allow the use of much larger context.)&lt;br /&gt; Token rate: 1.6 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m mla/DeepSeek-R1-Q3_K_M-00001-of-00030.gguf --host :: -fa -c 16384 -t 15 &lt;strong&gt;-mla 1&lt;/strong&gt; &lt;strong&gt;-fmoe&lt;/strong&gt; (no KV cache quantization)&lt;br /&gt; KV-Cache: 1098.00 MiB&lt;br /&gt; Token rate: 1.6 t/s &lt;/p&gt; &lt;p&gt;Quants that work with MLA can be found there: &lt;a href="https://huggingface.co/daydream-org/DeepSeek-R1-GGUF-11446/tree/main/DeepSeek-R1-Q3_K_M"&gt;Q3&lt;/a&gt; &lt;a href="https://huggingface.co/gghfez/DeepSeek-R1-11446-Q2_K/tree/main"&gt;Q2&lt;/a&gt; &lt;a href="https://huggingface.co/gghfez/DeepSeek-R1-11446-Q4_K/tree/main"&gt;Q4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/U_A_beringianus"&gt; /u/U_A_beringianus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rngt/simple_inference_speed_comparison_of_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rngt/simple_inference_speed_comparison_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rngt/simple_inference_speed_comparison_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T21:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j67bxt</id>
    <title>16x 3090s - It's alive!</title>
    <updated>2025-03-08T02:43:38+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt; &lt;img alt="16x 3090s - It's alive!" src="https://b.thumbs.redditmedia.com/VvyYO_xrL0vczMCglIvOXlchOAjzJG3mEsXsV_k93PQ.jpg" title="16x 3090s - It's alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j67bxt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6iuyf</id>
    <title>NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp; 600W TBP</title>
    <updated>2025-03-08T14:56:53+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp;amp; 600W TBP" src="https://external-preview.redd.it/ipqoihUxtH0AdjsoCf5u0QWlmwf7QkIL9jnTAAb3HTw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6333201abf11bb51d15493e0484c12b4cafa2d16" title="NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp;amp; 600W TBP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-blackwell-gpu-more-cores-than-rtx-5090-24064-96-gb-gddr7-600w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T14:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6f61q</id>
    <title>QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7</title>
    <updated>2025-03-08T11:24:32+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt; &lt;img alt="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" src="https://preview.redd.it/opow8do3agne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00cbc87c29904f9341ccf656e804f32edd07064a" title="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/opow8do3agne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T11:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6z9aq</id>
    <title>Manifold now implements Model Context Protocol and indefinite TTS generation via WebGPU. Here is a weather forecast for Boston, MA.</title>
    <updated>2025-03-09T04:05:30+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6z9aq/manifold_now_implements_model_context_protocol/"&gt; &lt;img alt="Manifold now implements Model Context Protocol and indefinite TTS generation via WebGPU. Here is a weather forecast for Boston, MA." src="https://external-preview.redd.it/anhtenllMHA4bG5lMV78OOspL0cK_E8n1WAZwKPHs9OMmQEsahJ1oWnR6qQ0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f289895f4efbc2ce043570e85108d77fa3ef3b60" title="Manifold now implements Model Context Protocol and indefinite TTS generation via WebGPU. Here is a weather forecast for Boston, MA." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a20gd80p8lne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6z9aq/manifold_now_implements_model_context_protocol/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6z9aq/manifold_now_implements_model_context_protocol/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T04:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6xpvt</id>
    <title>How large is your local LLM context?</title>
    <updated>2025-03-09T02:38:39+00:00</updated>
    <author>
      <name>/u/iwinux</name>
      <uri>https://old.reddit.com/user/iwinux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm new to this rabbit hole. Never realized context is such a VRAM hog until I loaded my first model (Qwen2.5 Coder 14B Instruct &lt;code&gt;Q4_K_M&lt;/code&gt; GGUF) with LM Studio. On my Mac mini M2 Pro (32GB RAM), increasing context size from 32K to 64K almost eats up all RAM.&lt;/p&gt; &lt;p&gt;So I wonder, do you run LLMs with max context size by default? Or keep it as low as possible?&lt;/p&gt; &lt;p&gt;For my use case (coding, as suggested by the model), I'm already spoiled by Claude / Gemini's huge context size :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iwinux"&gt; /u/iwinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T02:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6u731</id>
    <title>NVIDIA RTX PRO 6000 Blackwell leaked: 24064 cores, 96GB G7 memory and 600W Double Flow Through cooler</title>
    <updated>2025-03-08T23:35:33+00:00</updated>
    <author>
      <name>/u/mbolaris</name>
      <uri>https://old.reddit.com/user/mbolaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-pro-6000-blackwell-leaked-24064-cores-96gb-g7-memory-and-600w-double-flow-through-cooler"&gt;https://videocardz.com/newz/nvidia-rtx-pro-6000-blackwell-leaked-24064-cores-96gb-g7-memory-and-600w-double-flow-through-cooler&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mbolaris"&gt; /u/mbolaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6u731/nvidia_rtx_pro_6000_blackwell_leaked_24064_cores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6u731/nvidia_rtx_pro_6000_blackwell_leaked_24064_cores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6u731/nvidia_rtx_pro_6000_blackwell_leaked_24064_cores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T23:35:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6i1ma</id>
    <title>Can't believe it, but the RTX 4090 actually exists and it runs!!!</title>
    <updated>2025-03-08T14:15:34+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt; &lt;img alt="Can't believe it, but the RTX 4090 actually exists and it runs!!!" src="https://b.thumbs.redditmedia.com/Jyu8XHnVjN10hhVg2LdYg8XX4xiDcuuwK_tm4Uimz_g.jpg" title="Can't believe it, but the RTX 4090 actually exists and it runs!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX 4090 96G version&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lqj0v5su4hne1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=595bfa005189d96dd1e6b8940b29dad9ae87cfc2"&gt;https://preview.redd.it/lqj0v5su4hne1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=595bfa005189d96dd1e6b8940b29dad9ae87cfc2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h10g0x915hne1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=aac7b20a79b477c60cac8c306a37e17b5034d4d1"&gt;https://preview.redd.it/h10g0x915hne1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=aac7b20a79b477c60cac8c306a37e17b5034d4d1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T14:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dzai</id>
    <title>Real-time token graph in Open WebUI</title>
    <updated>2025-03-08T09:56:58+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt; &lt;img alt="Real-time token graph in Open WebUI" src="https://external-preview.redd.it/dm1rY2E3dWl1Zm5lMeNo1g2VbIy6NNGx_1T_ctYYVLkaFt3bwpFyaChfDLc3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c66a72211f8ad427c81d09e427101d7638dfd38" title="Real-time token graph in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zscr76uiufne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6yxdr</id>
    <title>Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf (and Thinking/Reasoning MOES...) ... 34+ new models (Lllamas, Qwen - MOES and not Moes..)</title>
    <updated>2025-03-09T03:46:25+00:00</updated>
    <author>
      <name>/u/Dangerous_Fix_5526</name>
      <uri>https://old.reddit.com/user/Dangerous_Fix_5526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From David_AU ;&lt;/p&gt; &lt;p&gt;First two models based on Qwen's off the charts &amp;quot;QwQ 32B&amp;quot; model just released, with some extra power. Detailed instructions, and examples at each repo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;New Model, Free thinker, Extra Spicy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-abliterated-uncensored-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Regular, Not so Spicy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-gguf"&gt;https://huggingface.co/DavidAU/Qwen2.5-QwQ-35B-Eureka-Cubed-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AND Qwen/Llama Thinking/Reasoning MOES - all sizes, shapes ...&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;34 reasoning/thinking models (example generations, notes, instructions etc):&lt;/p&gt; &lt;p&gt;Includes Llama 3,3.1,3.2 and Qwens, DeepSeek/QwQ/DeepHermes in MOE and NON MOE config plus others:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-reasoning-deepseek-models-with-thinking-reasoning-67a41ec81d9df996fd1cdd60"&gt;https://huggingface.co/collections/DavidAU/d-au-reasoning-deepseek-models-with-thinking-reasoning-67a41ec81d9df996fd1cdd60&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For Qwens (12) only (Moes and/or Enhanced):&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-qwen-25-reasoning-thinking-reg-moes-67cbef9e401488e599d9ebde"&gt;https://huggingface.co/collections/DavidAU/d-au-qwen-25-reasoning-thinking-reg-moes-67cbef9e401488e599d9ebde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Separate source / full precision sections/collections at main repo here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU"&gt;https://huggingface.co/DavidAU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Special service note for Lmstudio users: &lt;/p&gt; &lt;p&gt;The issue with QwQs (32B from Qwen and mine 35B) re: Templates/Jinja templates has been fixed. Make sure you update to build 0.3.12 ; otherwise manually select CHATML template to work with the new QwQ models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Fix_5526"&gt; /u/Dangerous_Fix_5526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6yxdr/qwen25qwq35beurekacubedabliterateduncensoredgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T03:46:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6nzrk</id>
    <title>New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s</title>
    <updated>2025-03-08T18:51:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt; &lt;img alt="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" src="https://preview.redd.it/wfkxh0q5iine1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f63ab35ce1aa6589c56196d048a5e4231e07749f" title="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfkxh0q5iine1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6w3qq</id>
    <title>PSA: Deepseek special tokens don't use underscore/low line ( _ ) and pipe ( | ) characters.</title>
    <updated>2025-03-09T01:11:29+00:00</updated>
    <author>
      <name>/u/computemachines</name>
      <uri>https://old.reddit.com/user/computemachines</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6w3qq/psa_deepseek_special_tokens_dont_use/"&gt; &lt;img alt="PSA: Deepseek special tokens don't use underscore/low line ( _ ) and pipe ( | ) characters." src="https://preview.redd.it/4k4rbdxjdkne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1088de160301ef1df0ee665bb6dbee41c324644" title="PSA: Deepseek special tokens don't use underscore/low line ( _ ) and pipe ( | ) characters." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/computemachines"&gt; /u/computemachines &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4k4rbdxjdkne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6w3qq/psa_deepseek_special_tokens_dont_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6w3qq/psa_deepseek_special_tokens_dont_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-09T01:11:29+00:00</published>
  </entry>
</feed>
