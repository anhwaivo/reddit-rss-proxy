<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-02T12:11:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l1dujm</id>
    <title>GPT4All, AnythingLLM, Open WebUI, or other?</title>
    <updated>2025-06-02T09:45:23+00:00</updated>
    <author>
      <name>/u/BobbyNGa</name>
      <uri>https://old.reddit.com/user/BobbyNGa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't have the time I'd like to work on running LLMs locally, So far I have played with various models on GPT4All and a bit on AnythingLLM. In the interest of saving time, I am seeking opinions on which &amp;quot;front end&amp;quot; interface I should use with these various popular LLMs. I should note that I am most interested currently in developing a system for RAG or CAG. Most important to me right now is &amp;quot;chatting with my various documents.&amp;quot; Any thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BobbyNGa"&gt; /u/BobbyNGa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1dujm/gpt4all_anythingllm_open_webui_or_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1dujm/gpt4all_anythingllm_open_webui_or_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1dujm/gpt4all_anythingllm_open_webui_or_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T09:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1e4uz</id>
    <title>Any node based tools for general AI workflows?</title>
    <updated>2025-06-02T10:03:33+00:00</updated>
    <author>
      <name>/u/GamerWael</name>
      <uri>https://old.reddit.com/user/GamerWael</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking if anyone built any Comfy UI style tools for all sorts of general AI workflows like LLMs, STT, TTS, basic stuff like HTTP requests, custom functions, etc. Something like a mix of Comfy UI and n8n. The closest thing I found is a closed source tool florafauna.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GamerWael"&gt; /u/GamerWael &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e4uz/any_node_based_tools_for_general_ai_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e4uz/any_node_based_tools_for_general_ai_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e4uz/any_node_based_tools_for_general_ai_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T10:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0xubg</id>
    <title>Toolcalling in the reasoning trace as an alternative to agentic frameworks</title>
    <updated>2025-06-01T19:40:13+00:00</updated>
    <author>
      <name>/u/ExaminationNo8522</name>
      <uri>https://old.reddit.com/user/ExaminationNo8522</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://2084.substack.com/p/deep-reasoning-with-tools-toolcalling"&gt;Deep Reasoning With Tools: Toolcalling in the reasoning trace&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey, so I was working on training reasoning models to do interesting things, when I started wanting them to be more dynamic: not just predict based on static information but actively search the data space to get information. Thus I built this toolset to integrate toolcalling into the reasoning trace of the AI models, since then I could do wayyy more complex RL training to allow it to do stuff like reconciliation of accounts, or more complex trading. However, as I built it, I realized that its actually a nice alternative to traditional agentic frameworks - you don't have discrete steps so it can run as long or as short as you want, and it can be invoked with a single command versus having to handle multiple steps. Thoughts? What other weirder agentic frameworks have y'all seen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExaminationNo8522"&gt; /u/ExaminationNo8522 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0xubg/toolcalling_in_the_reasoning_trace_as_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0xubg/toolcalling_in_the_reasoning_trace_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0xubg/toolcalling_in_the_reasoning_trace_as_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:40:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1el53</id>
    <title>Any fast and multilingual TTS model trained with a lightweighted LLM?</title>
    <updated>2025-06-02T10:31:07+00:00</updated>
    <author>
      <name>/u/LewisJin</name>
      <uri>https://old.reddit.com/user/LewisJin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There were some work such as Orptheus, Octus, Zonos etc, however, they seems both only for English.&lt;/p&gt; &lt;p&gt;Am seeking for a model trained with multilingual and with emotion promptable.&lt;/p&gt; &lt;p&gt;Anyone are planing to train a one?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LewisJin"&gt; /u/LewisJin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1el53/any_fast_and_multilingual_tts_model_trained_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1el53/any_fast_and_multilingual_tts_model_trained_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1el53/any_fast_and_multilingual_tts_model_trained_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T10:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0qp75</id>
    <title>App-Use : Create virtual desktops for AI agents to focus on specific apps.</title>
    <updated>2025-06-01T14:46:57+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qp75/appuse_create_virtual_desktops_for_ai_agents_to/"&gt; &lt;img alt="App-Use : Create virtual desktops for AI agents to focus on specific apps." src="https://external-preview.redd.it/ejV6cmV3ODZ3YjRmMYsTHh_R0WswrUJBBa-0t3y7YsS9UlwJcbvZWkm9vo2Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e17e2eb7db050248c2423a2e18a718dcda868c6" title="App-Use : Create virtual desktops for AI agents to focus on specific apps." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;App-Use lets you scope agents to just the apps they need. Instead of full desktop access, say &amp;quot;only work with Safari and Notes&amp;quot; or &amp;quot;just control iPhone Mirroring&amp;quot; - visual isolation without new processes for perfectly focused automation.&lt;/p&gt; &lt;p&gt;Running computer-use on the entire desktop often causes agent hallucinations and loss of focus when they see irrelevant windows and UI elements. App-Use solves this by creating composited views where agents only see what matters, dramatically improving task completion accuracy&lt;/p&gt; &lt;p&gt;Currently macOS-only (Quartz compositing engine). &lt;/p&gt; &lt;p&gt;Read the full guide: &lt;a href="https://trycua.com/blog/app-use"&gt;https://trycua.com/blog/app-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v0fcznj6wb4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qp75/appuse_create_virtual_desktops_for_ai_agents_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0qp75/appuse_create_virtual_desktops_for_ai_agents_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T14:46:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l12cmi</id>
    <title>Playing generated games of Atari Style PingPong and Space Invaders, thanks to Qwen 3 8b! (Original non Deepseek version) This small model continues to amaze.</title>
    <updated>2025-06-01T22:51:37+00:00</updated>
    <author>
      <name>/u/c64z86</name>
      <uri>https://old.reddit.com/user/c64z86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l12cmi/playing_generated_games_of_atari_style_pingpong/"&gt; &lt;img alt="Playing generated games of Atari Style PingPong and Space Invaders, thanks to Qwen 3 8b! (Original non Deepseek version) This small model continues to amaze." src="https://external-preview.redd.it/fORXcgKVkaTCLfSuQUzrXrubR0RAsGHr5swRFkIXzZY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9570e8b102def9be8ddff84ea19c251af5011698" title="Playing generated games of Atari Style PingPong and Space Invaders, thanks to Qwen 3 8b! (Original non Deepseek version) This small model continues to amaze." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c64z86"&gt; /u/c64z86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ar_kFDHGbhQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l12cmi/playing_generated_games_of_atari_style_pingpong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l12cmi/playing_generated_games_of_atari_style_pingpong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T22:51:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0m8r0</id>
    <title>104k-Token Prompt in a 110k-Token Context with DeepSeek-R1-0528-UD-IQ1_S – Benchmark &amp; Impressive Results</title>
    <updated>2025-06-01T11:00:46+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The Prompts:&lt;/strong&gt; 1. &lt;a href="https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt.txt&lt;/a&gt; (Firefox: View -&amp;gt; Repair Text Encoding) 2. &lt;a href="https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt.txt&lt;/a&gt; (Firefox: View -&amp;gt; Repair Text Encoding)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Commands (on Windows):&lt;/strong&gt; &lt;code&gt; perl -pe 's/\n/\\n/' DeepSeek_Runescape_Massive_Prompt.txt | CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,2,1 ~/llama-b5355-bin-win-cuda12.4-x64/llama-cli -m DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf -t 36 --ctx-size 110000 -ngl 62 --flash-attn --main-gpu 0 --no-mmap --mlock -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; --simple-io &lt;/code&gt; &lt;code&gt; perl -pe 's/\n/\\n/' DeepSeek_Dipiloblop_Massive_Prompt.txt | CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,2,1 ~/llama-b5355-bin-win-cuda12.4-x64/llama-cli -m DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf -t 36 --ctx-size 110000 -ngl 62 --flash-attn --main-gpu 0 --no-mmap --mlock -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; --simple-io &lt;/code&gt; - Tips: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kysms8"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kysms8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Answers (first time I see a model provide such a good answer):&lt;/strong&gt; - &lt;a href="https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt_Answer.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Runescape_Massive_Prompt_Answer.txt&lt;/a&gt; - &lt;a href="https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt_Answer.txt"&gt;https://thireus.com/REDDIT/DeepSeek_Dipiloblop_Massive_Prompt_Answer.txt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Hardware:&lt;/strong&gt; &lt;code&gt; i9-7980XE - 4.2Ghz on all cores 256GB DDR4 F4-3200C14Q2-256GTRS - XMP enabled 1x 5090 (x16) 1x 3090 (x16) 1x 3090 (x8) Prime-X299-A-II &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The benchmark results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Runescape: ``` llama_perf_sampler_print: sampling time = 608.32 ms / 106524 runs ( 0.01 ms per token, 175112.36 tokens per second) llama_perf_context_print: load time = 190451.73 ms llama_perf_context_print: prompt eval time = 5188938.33 ms / 104276 tokens ( 49.76 ms per token, 20.10 tokens per second) llama_perf_context_print: eval time = 577349.77 ms / 2248 runs ( 256.83 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5768493.07 ms / 106524 tokens&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 608.32 ms / 106524 runs ( 0.01 ms per token, 175112.36 tokens per second) llama_perf_context_print: load time = 190451.73 ms llama_perf_context_print: prompt eval time = 5188938.33 ms / 104276 tokens ( 49.76 ms per token, 20.10 tokens per second) llama_perf_context_print: eval time = 577349.77 ms / 2248 runs ( 256.83 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5768493.22 ms / 106524 tokens &lt;code&gt; Dipiloblop: &lt;/code&gt; llama_perf_sampler_print: sampling time = 534.36 ms / 106532 runs ( 0.01 ms per token, 199364.47 tokens per second) llama_perf_context_print: load time = 177215.16 ms llama_perf_context_print: prompt eval time = 5101404.01 ms / 104586 tokens ( 48.78 ms per token, 20.50 tokens per second) llama_perf_context_print: eval time = 500475.72 ms / 1946 runs ( 257.18 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5603899.16 ms / 106532 tokens&lt;/p&gt; &lt;p&gt;llama_perf_sampler_print: sampling time = 534.36 ms / 106532 runs ( 0.01 ms per token, 199364.47 tokens per second) llama_perf_context_print: load time = 177215.16 ms llama_perf_context_print: prompt eval time = 5101404.01 ms / 104586 tokens ( 48.78 ms per token, 20.50 tokens per second) llama_perf_context_print: eval time = 500475.72 ms / 1946 runs ( 257.18 ms per token, 3.89 tokens per second) llama_perf_context_print: total time = 5603899.32 ms / 106532 tokens ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sampler (default values were used, DeepSeek recommends temp 0.6, but 0.8 was used):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Runescape: &lt;code&gt; sampler seed: 3756224448 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 110080 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -&amp;gt; logit-bias -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist &lt;/code&gt; Dipiloblop: &lt;code&gt; sampler seed: 1633590497 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 110080 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -&amp;gt; logit-bias -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The questions:&lt;/strong&gt; 1. Would 1x RTX PRO 6000 Blackwell or even 2x RTX PRO 6000 Blackwell significantly improve these metrics without any other hardware upgrade? (knowing that there would still be CPU offloading) 2. Would a different CPU, motherboard and RAM improve these metrics? 3. How to significantly improve prompt processing speed?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt; - Comparative results with Qwen3-235B-A22B-128K-UD-Q3_K_XL are here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l0m8r0/comment/mvg5ke9/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1l0m8r0/comment/mvg5ke9/&lt;/a&gt; - I've compiled the latest llama.cpp with Blackwell support (&lt;a href="https://github.com/Thireus/llama.cpp/releases/tag/b5565"&gt;https://github.com/Thireus/llama.cpp/releases/tag/b5565&lt;/a&gt;) and now get slightly better speeds than shared before: 21.71 tokens per second (pp) + 4.36 tokens per second, but uncertain about plausible quality degradation - I've been using the GGUF version from 2 days ago sha256: 0e2df082b88088470a761421d48a391085c238a66ea79f5f006df92f0d7d7193, see &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/commit/ff13ed80e2c95ebfbcf94a8d6682ed989fb6961b"&gt;https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/commit/ff13ed80e2c95ebfbcf94a8d6682ed989fb6961b&lt;/a&gt; - The newest GGUF version results may differ (which I have not tested)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0m8r0/104ktoken_prompt_in_a_110ktoken_context_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T11:00:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1l194gj</id>
    <title>What's next? Behemoth? Qwen VL/Coder? Mistral Large Reasoning/Vision?</title>
    <updated>2025-06-02T04:35:39+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;do you await any model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l194gj/whats_next_behemoth_qwen_vlcoder_mistral_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l194gj/whats_next_behemoth_qwen_vlcoder_mistral_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l194gj/whats_next_behemoth_qwen_vlcoder_mistral_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T04:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0y4ep</id>
    <title>A Privacy-Focused Perplexity That Runs Locally on all your devices - iPhone, Android, iPad!</title>
    <updated>2025-06-01T19:52:02+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt; community!&lt;/p&gt; &lt;p&gt;Following up on my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ku1444/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;previous post&lt;/a&gt;- the response has been incredible! Thank you to everyone who tried it out, left reviews, and provided feedback.&lt;/p&gt; &lt;p&gt;Based on your requests, I'm excited to announce that &lt;strong&gt;MyDeviceAI is now available on iPad and Android&lt;/strong&gt;!&lt;/p&gt; &lt;h1&gt;iPad Support&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Full native iPad experience with optimized UI&lt;/li&gt; &lt;li&gt;Same lightning-fast local processing with M-series chips&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Android Release&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Available as APK on GitHub releases (v1.2)&lt;/li&gt; &lt;li&gt;Download link: &lt;a href="https://github.com/navedmerchant/MyDeviceAI/releases"&gt;https://github.com/navedmerchant/MyDeviceAI/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Same core features: local AI, SearXNG integration, complete privacy&lt;/li&gt; &lt;li&gt;Works across a wide range of Android devices&lt;/li&gt; &lt;li&gt;Runs on CPU only for now, working on getting Adreno GPU support in llama.rn&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;I'm continuing to work on improvements based on your suggestions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ability to select a larger model for powerful supported devices (Qwen 3 4b)&lt;/li&gt; &lt;li&gt;Ability to add images and documents to the chat for supported devices (QwenVL support)&lt;/li&gt; &lt;li&gt;Advanced speech mode on device&lt;/li&gt; &lt;li&gt;Enhanced personalization features&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Download Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;iOS/iPad&lt;/strong&gt;: &lt;a href="https://apps.apple.com/us/app/mydeviceai/id6736578281?platform=ipad"&gt;MyDeviceAI on App Store&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: &lt;a href="https://github.com/navedmerchant/MyDeviceAI/releases"&gt;GitHub Releases v1.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Source Code&lt;/strong&gt;: &lt;a href="https://github.com/navedmerchant/MyDeviceAI"&gt;GitHub Repository&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you've been waiting for Android support or want to try it on iPad, now's your chance! As always, everything remains 100% free, open source, and completely private.&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts on the new platforms, and please consider leaving a review if MyDeviceAI has been useful for you. Your support helps tremendously with continued development!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y4ep/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y4ep/a_privacyfocused_perplexity_that_runs_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y4ep/a_privacyfocused_perplexity_that_runs_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0v8yq</id>
    <title>I made a simple tool to test/compare your local LLMs on AIME 2024</title>
    <updated>2025-06-01T17:54:01+00:00</updated>
    <author>
      <name>/u/EntropyMagnets</name>
      <uri>https://old.reddit.com/user/EntropyMagnets</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"&gt; &lt;img alt="I made a simple tool to test/compare your local LLMs on AIME 2024" src="https://external-preview.redd.it/-Bks8K2_TljN7hLY0DvxIu9Ncpa8BzunHNO4VODMSAA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e1cf6849b57a4c81ac6a807fbf541e56f6b4544" title="I made a simple tool to test/compare your local LLMs on AIME 2024" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made &lt;a href="https://github.com/Belluxx/LocalAIME"&gt;LocalAIME&lt;/a&gt; a simple tool that tests one or many LLMs locally or trough API (you can use any OpenAI-compatible API) on AIME 2024.&lt;/p&gt; &lt;p&gt;It is pretty useful for testing different quants of the same model or the same quant of different providers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0xk016htc4f1.png?width=4900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fbfc8a2d435ef0fe50a7ed0dab250cdc03e6f2c"&gt;Performance of some models i tested for each AIME 2024 problem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntropyMagnets"&gt; /u/EntropyMagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0v8yq/i_made_a_simple_tool_to_testcompare_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T17:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0n5ta</id>
    <title>Which is the best uncensored model?</title>
    <updated>2025-06-01T11:55:48+00:00</updated>
    <author>
      <name>/u/BoJackHorseMan53</name>
      <uri>https://old.reddit.com/user/BoJackHorseMan53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to learn ethical hacking. Tried dolphin-mistral-r1 it did answer but it's answers were bad.&lt;/p&gt; &lt;p&gt;Are there any good uncensored models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BoJackHorseMan53"&gt; /u/BoJackHorseMan53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0n5ta/which_is_the_best_uncensored_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0n5ta/which_is_the_best_uncensored_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0n5ta/which_is_the_best_uncensored_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T11:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1drru</id>
    <title>Best Video captioning model</title>
    <updated>2025-06-02T09:40:05+00:00</updated>
    <author>
      <name>/u/VihmaVillu</name>
      <uri>https://old.reddit.com/user/VihmaVillu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need to generate text captions from small video clips that later i can use to do semantic scene search. What are the best models for VRAM 12-32GB.&lt;/p&gt; &lt;p&gt;Maybe i can train/fine tune so i can do embeded search?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VihmaVillu"&gt; /u/VihmaVillu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1drru/best_video_captioning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1drru/best_video_captioning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1drru/best_video_captioning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T09:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l17m9g</id>
    <title>SAGA Update: Autonomous Novel Writing with Deep KG &amp; Semantic Context - Now Even More Advanced!</title>
    <updated>2025-06-02T03:12:03+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple of weeks ago, I shared an early version of SAGA (Semantic And Graph-enhanced Authoring), my project for autonomous novel generation. Thanks to some great initial feedback and a lot of focused development, I'm excited to share a significantly advanced version!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is SAGA?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SAGA, powered by its NANA (Next-gen Autonomous Narrative Architecture) engine, is designed to write entire novels. It's not just about stringing words together; it employs a team of specialized AI agents that handle planning, drafting, comprehensive evaluation, continuity checking, and intelligent revision. The core idea is to combine the creative power of local LLMs with the structured knowledge of a Neo4j graph database and the coherence provided by semantic embeddings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New &amp;amp; Improved Since Last Time?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SAGA has undergone substantial enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Deep Neo4j Integration:&lt;/strong&gt; Moved from a simpler DB to a full Neo4j backend. This allows for much richer tracking of characters, world-building, plot points, and dynamic relationships. It includes a robust schema with constraints and a vector index for semantic searches.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Hybrid Context Generation:&lt;/strong&gt; For each chapter, SAGA now generates a &amp;quot;hybrid context&amp;quot; by: &lt;ul&gt; &lt;li&gt; Performing &lt;strong&gt;semantic similarity searches&lt;/strong&gt; (via Ollama embeddings) on past chapter content stored in Neo4j to maintain narrative flow and tone.&lt;/li&gt; &lt;li&gt; Extracting &lt;strong&gt;key reliable facts&lt;/strong&gt; directly from the Neo4j knowledge graph to ensure the LLM adheres to established canon.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Advanced Revision Logic:&lt;/strong&gt; The revision process is now more sophisticated, capable of &lt;strong&gt;patch-based revisions&lt;/strong&gt; for targeted fixes or full chapter rewrites when necessary.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Sophisticated Evaluation &amp;amp; Continuity:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt; The &lt;code&gt;ComprehensiveEvaluatorAgent&lt;/code&gt; assesses drafts on multiple axes (plot, theme, depth, consistency).&lt;/li&gt; &lt;li&gt; A dedicated &lt;code&gt;WorldContinuityAgent&lt;/code&gt; performs focused checks against the KG and world-building data to catch inconsistencies.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Provisional Data Handling:&lt;/strong&gt; The system now explicitly tracks whether data is &amp;quot;provisional&amp;quot; (e.g., from an unrevised draft), allowing for better canon management.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Markdown for User Input:&lt;/strong&gt; You can now seed your story using a &lt;code&gt;user_story_elements.md&lt;/code&gt; file with &lt;code&gt;[Fill-in]&lt;/code&gt; placeholders, making initial setup more intuitive.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Text De-duplication:&lt;/strong&gt; Added a step to help reduce repetitive phrasing or content in generated drafts.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Performance &amp;amp; Stability:&lt;/strong&gt; Lots of under-the-hood improvements. SAGA can now generate a batch of 3 chapters (each ~13K+ tokens of narrative) in about 11 minutes on my setup, including all the planning, evaluation, and KG updates.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Architecture Still Intact:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The agentic pipeline remains central:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;strong&gt;Initial Setup:&lt;/strong&gt; Parses user markdown or generates plot, characters, and world-building; pre-populates Neo4j.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Chapter Loop:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Plan:&lt;/strong&gt; &lt;code&gt;PlannerAgent&lt;/code&gt; details scenes.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Context:&lt;/strong&gt; Hybrid semantic &amp;amp; KG context is built.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Draft:&lt;/strong&gt; &lt;code&gt;DraftingAgent&lt;/code&gt; writes the chapter.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Evaluate:&lt;/strong&gt; &lt;code&gt;ComprehensiveEvaluatorAgent&lt;/code&gt; &amp;amp; &lt;code&gt;WorldContinuityAgent&lt;/code&gt; scrutinize the draft.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Revise:&lt;/strong&gt; &lt;code&gt;ChapterRevisionLogic&lt;/code&gt; applies fixes.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Finalize &amp;amp; Update KG:&lt;/strong&gt; &lt;code&gt;KGMaintainerAgent&lt;/code&gt; summarizes, embeds, saves the chapter to Neo4j, and extracts/merges new knowledge back into the graph and agent state.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why This Approach?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The goal is to create narratives that are not only creative but also &lt;em&gt;coherent&lt;/em&gt; and &lt;em&gt;consistent&lt;/em&gt; over tens of thousands of tokens. The graph database acts as the story's long-term memory and source of truth, while semantic embeddings help maintain flow and relevance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current Performance Example:&lt;/strong&gt; Using local GGUF models (Qwen3 14B for narration/planning, smaller Qwen3s for other tasks), SAGA generates: * &lt;strong&gt;3 chapters&lt;/strong&gt; (each ~13,000+ tokens of narrative) * In approximately &lt;strong&gt;11 minutes&lt;/strong&gt; * This includes all planning, context generation, evaluation, and knowledge graph updates.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Check it out &amp;amp; Get Involved:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/Lanerra/saga"&gt;https://github.com/Lanerra/saga&lt;/a&gt; (The README has been updated with detailed setup instructions!)&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Setup:&lt;/strong&gt; You'll need Python, Ollama (for embeddings), an OpenAI-API compatible LLM server, and Neo4j (Docker setup provided).&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Reset Script:&lt;/strong&gt; &lt;code&gt;reset_neo4j.py&lt;/code&gt; is still there to easily clear the database and start fresh.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Inspect KG:&lt;/strong&gt; The &lt;code&gt;inspect_kg.py&lt;/code&gt; script mentioned previously has been replaced by direct Neo4j browser interaction (which is much more powerful for visualization).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm really proud of how far SAGA has come and believe it's pushing into some interesting territory for AI-assisted storytelling. I'd love for you all to try it out, see what kind of sagas NANA can spin up for you, and share your thoughts, feedback, or any issues you encounter.&lt;/p&gt; &lt;p&gt;What kind of stories will you create?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l17m9g/saga_update_autonomous_novel_writing_with_deep_kg/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l17m9g/saga_update_autonomous_novel_writing_with_deep_kg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l17m9g/saga_update_autonomous_novel_writing_with_deep_kg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T03:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0p3et</id>
    <title>Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop</title>
    <updated>2025-06-01T13:34:12+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt; &lt;img alt="Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop" src="https://external-preview.redd.it/3QugVQO6P_Q3v0881CbP7ispW7LV5z9hQhVFGV8ZV58.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64639bca07382b454fb4ec613939209217564782" title="Let's build a production level Small Language Model (SLM) from scratch | 3 hour workshop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/z6y0w621jb4f1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8491a9d59f58f3082afceaa6c737005c740ad38b"&gt;https://preview.redd.it/z6y0w621jb4f1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8491a9d59f58f3082afceaa6c737005c740ad38b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made a 3 hour workshop showing how to build an SLM from scratch. &lt;/p&gt; &lt;p&gt;Watch it here: &lt;a href="https://youtu.be/pOFcwcwtv3k?si=1UI4uCdw_HLbdQgX"&gt;https://youtu.be/pOFcwcwtv3k?si=1UI4uCdw_HLbdQgX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is what I cover in the workshop: &lt;/p&gt; &lt;p&gt;(a) Download a dataset with 1million+ samples&lt;/p&gt; &lt;p&gt;(b) Pre-process and tokenize the dataset&lt;/p&gt; &lt;p&gt;(c) Divide the dataset into input-target pairs&lt;/p&gt; &lt;p&gt;(d) Assemble the SLM architecture: tokenization layer, attention layer, transformer block, output layer and everything in between&lt;/p&gt; &lt;p&gt;(e) Pre-train the entire SLM&lt;/p&gt; &lt;p&gt;(f) Run inference and generate new text from your trained SLM!&lt;/p&gt; &lt;p&gt;This is not a toy project. &lt;/p&gt; &lt;p&gt;It's a production-level project with an extensive dataset. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0p3et/lets_build_a_production_level_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T13:34:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0q2zk</id>
    <title>DeepSeek-R1-0528-UD-Q6-K-XL on 10 Year Old Hardware</title>
    <updated>2025-06-01T14:19:51+00:00</updated>
    <author>
      <name>/u/Simusid</name>
      <uri>https://old.reddit.com/user/Simusid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't expect anything useful in this post. I did it just to see if it was possible. This was on a 10+ year old system with a 6th generation i5 with 12gb of RAM. My ssd is nearly full so I had to mount an external 8TB USB drive to store the 560GB model. At least it is USB-3.&lt;/p&gt; &lt;p&gt;I made an 800GB swap file and enabled it, then launched llama-cli with a simple prompt and went to bed. I half expected that the model might not even have fully loaded when I got up but it was already part way through the response.&lt;/p&gt; &lt;p&gt;With no GPU, it seems to be about seven minutes per token.&lt;/p&gt; &lt;p&gt;Edit - I've named this system TreeBeard&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simusid"&gt; /u/Simusid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0q2zk/deepseekr10528udq6kxl_on_10_year_old_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T14:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l13fqa</id>
    <title>How are people running dual GPU these days?</title>
    <updated>2025-06-01T23:42:00+00:00</updated>
    <author>
      <name>/u/admiralamott</name>
      <uri>https://old.reddit.com/user/admiralamott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 4080 but was considering getting a 3090 for LLM models. I've never ran a dual set up before because I read like 6 years ago that it isn't used anymore. But clearly people are doing it so is that still going on? How does it work? Will it only offload to 1 gpu and then to the RAM, or can it offload to one GPU and then to the second one if it needs more? How do I know if my PC can do it? It's down to the motherboard right? (Sorry I am so behind rn) I'm also using ollama with OpenWebUI if that helps.&lt;/p&gt; &lt;p&gt;Thank you for your time :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/admiralamott"&gt; /u/admiralamott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13fqa/how_are_people_running_dual_gpu_these_days/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13fqa/how_are_people_running_dual_gpu_these_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l13fqa/how_are_people_running_dual_gpu_these_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T23:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1bjhm</id>
    <title>System Prompt Learning: Teaching your local LLMs to learn problem-solving strategies from experience (optillm plugin)</title>
    <updated>2025-06-02T07:08:14+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I wanted to share something we've been working on that might interest folks running local LLMs - &lt;strong&gt;System Prompt Learning (SPL)&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;You know how ChatGPT, Claude, etc. perform so well partly because they have incredibly detailed system prompts with sophisticated reasoning strategies? Most of us running local models just use basic prompts and miss out on those performance gains.&lt;/p&gt; &lt;h1&gt;What is SPL?&lt;/h1&gt; &lt;p&gt;SPL implements what Andrej Karpathy called the &amp;quot;third paradigm&amp;quot; for LLM learning - instead of just pretraining and fine-tuning, models can now learn problem-solving strategies from their own experience.&lt;/p&gt; &lt;h1&gt;How it works:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Automatically classifies problems into 16 types (math, coding, word problems, etc.)&lt;/li&gt; &lt;li&gt;Builds a persistent database of effective solving strategies&lt;/li&gt; &lt;li&gt;Selects the best strategies for each query&lt;/li&gt; &lt;li&gt;Evaluates how well strategies worked and refines them over time&lt;/li&gt; &lt;li&gt;All strategies are human-readable JSON - you can inspect and edit them&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results:&lt;/h1&gt; &lt;p&gt;Tested with gemini-2.0-flash-lite across math benchmarks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Arena Hard&lt;/strong&gt;: 29% → 37.6% (+8.6%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AIME24&lt;/strong&gt;: 23.33% → 30% (+6.67%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OptiLLMBench&lt;/strong&gt;: 61% → 65% (+4%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MATH-500&lt;/strong&gt;: 85% → 85.6% (+0.6%)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After 500 queries, the system developed 129 strategies, refined 97 of them, and achieved much better problem-solving.&lt;/p&gt; &lt;h1&gt;For Local LLM Users:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Works with &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt; (so llama.cpp, Ollama, vLLM, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs completely locally&lt;/strong&gt; - strategies stored in local JSON files&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two modes&lt;/strong&gt;: inference-only (default) or learning mode&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Minimal overhead&lt;/strong&gt; - just augments your system prompt&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open source&lt;/strong&gt; and easy to inspect/modify&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Setup:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install optillm # Point to your local LLM endpoint python optillm.py --base_url http://localhost:8080/v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then just add &lt;code&gt;spl-&lt;/code&gt; prefix to your model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model=&amp;quot;spl-llama-3.2-3b&amp;quot; # or whatever your model is &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enable learning mode to create new strategies:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;extra_body={&amp;quot;spl_learning&amp;quot;: True} &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Example Strategy Learned:&lt;/h1&gt; &lt;p&gt;The system automatically learned this strategy for word problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Understand&lt;/strong&gt;: Read carefully, identify unknowns&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plan&lt;/strong&gt;: Define variables, write equations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Solve&lt;/strong&gt;: Step-by-step with units&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verify&lt;/strong&gt;: Check reasonableness&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All strategies are stored in &lt;code&gt;~/.optillm/spl/data/strategies.json&lt;/code&gt; so you can back them up, share them, or manually edit them.&lt;/p&gt; &lt;h1&gt;Why This Matters for Local LLMs:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Your model gets &lt;strong&gt;progressively better&lt;/strong&gt; at problem types you use frequently&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparent learning&lt;/strong&gt; - you can see exactly what strategies it develops&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No external dependencies&lt;/strong&gt; - everything runs locally&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transferable knowledge&lt;/strong&gt; - you can share strategy files between deployments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This feels like a step toward local models that actually improve through use, rather than being static after training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/codelion/optillm"&gt;https://github.com/codelion/optillm&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SPL Plugin: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/plugins/spl"&gt;https://github.com/codelion/optillm/tree/main/optillm/plugins/spl&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Technical article: &lt;a href="https://huggingface.co/blog/codelion/system-prompt-learning"&gt;https://huggingface.co/blog/codelion/system-prompt-learning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Andrej's original tweet: &lt;a href="https://x.com/karpathy/status/1921368644069765486"&gt;https://x.com/karpathy/status/1921368644069765486&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyone tried this yet? Would love to hear how it works with different local models!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Works great with reasoning models like DeepSeek-R1, QwQ, etc. The strategies help guide their thinking process.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1bjhm/system_prompt_learning_teaching_your_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1bjhm/system_prompt_learning_teaching_your_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1bjhm/system_prompt_learning_teaching_your_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T07:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1b801</id>
    <title>What LLM libraries/frameworks are worthwhile and what is better to roll your own from scratch?</title>
    <updated>2025-06-02T06:47:29+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe I'm suffering from NIH, but the core of systems can be quite simple to roll out using just python.&lt;/p&gt; &lt;p&gt;What libraries/frameworks do you find most valuable to use instead of rolling your own?&lt;/p&gt; &lt;p&gt;EDIT: Sorry. I was unclear. When implementing an application which calls on LLM functionality (via API) do you roll everything by hand or do you use frameworks such as Langchain, Pocket Flow or Burr etc. e.g. when you build pipelines/workflows for gathering data to put into context (RAG) or use multiple calls to generate context and have different flows/branches.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1b801/what_llm_librariesframeworks_are_worthwhile_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1b801/what_llm_librariesframeworks_are_worthwhile_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1b801/what_llm_librariesframeworks_are_worthwhile_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T06:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0zsv7</id>
    <title>25L Portable NV-linked Dual 3090 LLM Rig</title>
    <updated>2025-06-01T21:01:58+00:00</updated>
    <author>
      <name>/u/Special-Wolverine</name>
      <uri>https://old.reddit.com/user/Special-Wolverine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"&gt; &lt;img alt="25L Portable NV-linked Dual 3090 LLM Rig" src="https://b.thumbs.redditmedia.com/SCLEVQyCptjUTsrbRVb6jCIJUk1CuEOO-Ud355bse9Q.jpg" title="25L Portable NV-linked Dual 3090 LLM Rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Main point of portability is because The workplace of the coworker I built this for is truly offline, with no potential for LAN or wifi, so to download new models and update the system periodically I need to go pick it up from him and take it home. &lt;/p&gt; &lt;p&gt;WARNING - these components don't fit if you try to copy this build. The bottom GPU is resting on the Arctic p12 slim fans at the bottom of the case and pushing up on the GPU. Also the top arctic p14 Max fans don't have mounting points for half of their screw holes, and are in place by being very tightly wedged against the motherboard, case, and PSU. Also, there 's probably way too much pressure on the pcie cables coming off the gpus when you close the glass. Also I had to daisy chain the PCIE cables because the Corsair RM 1200e only has four available on the PSU side and these particular EVGA 3090s require 3x 8pin power. Allegedly it just enforces a hardware power limit to 300 w but you should make it a little bit more safe by also enforcing the 300W power limit in Nvidia -SMI To make sure that the cards don't try to pull 450W through 300W pipes. Could have fit a bigger PSU, but then I wouldn't get that front fan which is probably crucial.&lt;/p&gt; &lt;p&gt;All that being said, with a 300w power limit applied to both gpus in a silent fan profile, this rig has surprisingly good temperatures and noise levels considering how compact it is. &lt;/p&gt; &lt;p&gt;During Cinebench 24 with both gpus being 100% utilized, the CPU runs at 63 C and both gpus at 67 Celsius somehow with almost zero gap between them and the glass closed. All the while running at about 37 to 40 decibels from 1 meter away. &lt;/p&gt; &lt;p&gt;Prompt processing and inference - the gpus run at about 63 C, CPU at 55 C, and decibels at 34. &lt;/p&gt; &lt;p&gt;Again, I don't understand why the temperatures for both are almost the same, when logically the top GPU should be much hotter. The only gap between the two gpus is the size of one of those little silicone rubber DisplayPort caps wedged into the end, right between where the pcie power cables connect to force the GPUs apart a little.&lt;/p&gt; &lt;p&gt;Everything but the case, CPU cooler, and PSU was bought used on Facebook Marketplace&lt;/p&gt; &lt;p&gt;&lt;a href="https://pcpartpicker.com/list/nQXzgn"&gt;PCPartPicker Part List&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/qtvqqs/amd-ryzen-7-5800x-38-ghz-8-core-processor-100-100000063wof"&gt;AMD Ryzen 7 5800X 3.8 GHz 8-Core Processor&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$160.54 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/HbDQzy/id-cooling-frozn-a720-black-986-cfm-cpu-cooler-frozn-a720-black"&gt;ID-COOLING FROZN A720 BLACK 98.6 CFM CPU Cooler&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$69.98 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/CLkgXL/asus-rog-strix-x570-e-gaming-atx-am4-motherboard-rog-strix-x570-e-gaming"&gt;Asus ROG Strix X570-E Gaming ATX AM4 Motherboard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$559.00 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/6rrcCJ/corsair-memory-cmk32gx4m2b3200c16"&gt;Corsair Vengeance LPX 32 GB (2 x 16 GB) DDR4-3200 CL16 Memory&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$81.96 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/DDWBD3/samsung-980-pro-1-tb-m2-2280-nvme-solid-state-drive-mz-v8p1t0bam"&gt;Samsung 980 Pro 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$149.99 @ Amazon&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/PG848d/evga-geforce-rtx-3090-24-gb-ftw3-ultra-gaming-video-card-24g-p5-3987-kr"&gt;EVGA FTW3 ULTRA GAMING GeForce RTX 3090 24 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$750.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Video Card&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://pcpartpicker.com/product/PG848d/evga-geforce-rtx-3090-24-gb-ftw3-ultra-gaming-video-card-24g-p5-3987-kr"&gt;EVGA FTW3 ULTRA GAMING GeForce RTX 3090 24 GB Video Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$750.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;NVlink SLI bridge&lt;/td&gt; &lt;td align="left"&gt;$90.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Mechanic Master c34plus&lt;/td&gt; &lt;td align="left"&gt;$200.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Corsair RM1200e&lt;/td&gt; &lt;td align="left"&gt;$210.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Custom&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2x Arctic p14 max, 3x p12, 3x p12 slim&lt;/td&gt; &lt;td align="left"&gt;$60.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Prices include shipping, taxes, rebates, and discounts&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$3081.47&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Generated by &lt;a href="https://pcpartpicker.com"&gt;PCPartPicker&lt;/a&gt; 2025-06-01 16:48 EDT-0400&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Special-Wolverine"&gt; /u/Special-Wolverine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1l0zsv7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T21:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l0y0wp</id>
    <title>Allowing LLM to ponder in Open WebUI</title>
    <updated>2025-06-01T19:47:52+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"&gt; &lt;img alt="Allowing LLM to ponder in Open WebUI" src="https://external-preview.redd.it/dHd6NjY5c2JkZDRmMbDY_eAdKP8QUXyZwc-4j2cel9Olwb9ejqufCbXqijwB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d96e6747cff63170125fef17cdbcf53af47bbb3f" title="Allowing LLM to ponder in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A completely superficial way of letting LLM to ponder a bit before making its conversation turn. The process is streamed to an artifact within Open WebUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/ponder.py"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uoeptbsbdd4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l0y0wp/allowing_llm_to_ponder_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-01T19:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1l13tv3</id>
    <title>Who is getting paid to work doing this rather than just hobby dabbling..what was your path?</title>
    <updated>2025-06-02T00:00:47+00:00</updated>
    <author>
      <name>/u/bornfree4ever</name>
      <uri>https://old.reddit.com/user/bornfree4ever</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really enjoy hacking together LLM scripts and ideas. but how do I get paid doing it??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bornfree4ever"&gt; /u/bornfree4ever &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13tv3/who_is_getting_paid_to_work_doing_this_rather/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l13tv3/who_is_getting_paid_to_work_doing_this_rather/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l13tv3/who_is_getting_paid_to_work_doing_this_rather/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T00:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1a944</id>
    <title>Snapdragon 8 Elite gets 5.5 t/s on Qwen3 30B A3B</title>
    <updated>2025-06-02T05:44:39+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1a944/snapdragon_8_elite_gets_55_ts_on_qwen3_30b_a3b/"&gt; &lt;img alt="Snapdragon 8 Elite gets 5.5 t/s on Qwen3 30B A3B" src="https://preview.redd.it/jagac0yccg4f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6e0ede5a0ed01ba1b9b850ec415585886e5bad1" title="Snapdragon 8 Elite gets 5.5 t/s on Qwen3 30B A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Phone is a Razr Ultra 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jagac0yccg4f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1a944/snapdragon_8_elite_gets_55_ts_on_qwen3_30b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1a944/snapdragon_8_elite_gets_55_ts_on_qwen3_30b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T05:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1581z</id>
    <title>Which model are you using? June'25 edition</title>
    <updated>2025-06-02T01:09:13+00:00</updated>
    <author>
      <name>/u/Ok_Influence505</name>
      <uri>https://old.reddit.com/user/Ok_Influence505</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As proposed previously from this &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/we_should_have_a_monthly_which_models_are_you/"&gt;post&lt;/a&gt;, it's time for another monthly check-in on the latest models and their applications. The goal is to keep everyone updated on recent releases and discover hidden gems that might be flying under the radar.&lt;/p&gt; &lt;p&gt;With new models like DeepSeek-R1-0528, Claude 4 dropping recently, I'm curious to see how these stack up against established options. Have you tested any of the latest releases? How do they compare to what you were using before?&lt;/p&gt; &lt;p&gt;So, let start a discussion on what models (both proprietary and open-weights) are use using (or stop using ;) ) for different purposes (coding, writing, creative writing etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Influence505"&gt; /u/Ok_Influence505 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1581z/which_model_are_you_using_june25_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1581z/which_model_are_you_using_june25_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1581z/which_model_are_you_using_june25_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T01:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1e6ic</id>
    <title>Ignore the hype - AI companies still have no moat</title>
    <updated>2025-06-02T10:06:26+00:00</updated>
    <author>
      <name>/u/No_Tea2273</name>
      <uri>https://old.reddit.com/user/No_Tea2273</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e6ic/ignore_the_hype_ai_companies_still_have_no_moat/"&gt; &lt;img alt="Ignore the hype - AI companies still have no moat" src="https://external-preview.redd.it/TawOSRI4o3WDthoH5zp4cL7vlpQPtqKfMqXniUZMdX0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc884e61e4fbae7ba821197e1b5320440aaab413" title="Ignore the hype - AI companies still have no moat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An article I wrote a while back, I think &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; still wins&lt;/p&gt; &lt;p&gt;The basis of it is that Every single AI tool – has an open source alternative, every. single. one – so programming wise, for a new company to implement these features is not a matter of development complexity but a matter of getting the biggest audience &lt;/p&gt; &lt;p&gt;Everything has an open source versioned alternative right now&lt;/p&gt; &lt;p&gt;Take for example&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Tea2273"&gt; /u/No_Tea2273 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://river.berlin/blog/there-is-still-no-moat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e6ic/ignore_the_hype_ai_companies_still_have_no_moat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1e6ic/ignore_the_hype_ai_companies_still_have_no_moat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T10:06:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l19yud</id>
    <title>IQ1_Smol_Boi</title>
    <updated>2025-06-02T05:26:51+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l19yud/iq1_smol_boi/"&gt; &lt;img alt="IQ1_Smol_Boi" src="https://preview.redd.it/9u1teeqt4g4f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bab7030e08d75ad0063051230aa543c0b2a3ac7f" title="IQ1_Smol_Boi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some folks asked me for an R1-0528 quant that might fit on 128GiB RAM + 24GB VRAM. I didn't think it was possible, but turns out my new smol boi &lt;code&gt;IQ1_S_R4&lt;/code&gt; is 131GiB and actually runs okay (ik_llama.cpp fork only), and has perplexity lower &amp;quot;better&amp;quot; than &lt;code&gt;Qwen3-235B-A22B-Q8_0&lt;/code&gt; which is almost twice the size! Not sure that means it is better, but kinda surprising to me.&lt;/p&gt; &lt;p&gt;Unsloth's newest smol boi is an odd &lt;code&gt;UD-TQ1_0&lt;/code&gt; weighing in at 151GiB. The &lt;code&gt;TQ1_0&lt;/code&gt; quant is a 1.6875 bpw quant types for TriLMs and BitNet b1.58 models. However, if you open up the side-bar on the modelcard it doesn't actually have any TQ1_0 layers/tensors and is mostly a mix of IQN_S and such. So not sure what is going on there or if it was a mistake. It does at least run from what I can tell, though I didn't try inferencing with it. They do have an &lt;code&gt;IQ1_S&lt;/code&gt; as well, but it seems rather larger given their recipe though I've heard folks have had success with it.&lt;/p&gt; &lt;p&gt;Bartowski's smol boi &lt;code&gt;IQ1_M&lt;/code&gt; is the next smallest I've seen at about 138GiB and seems to work okay in my limited testing. Surprising how these quants can still run at such low bit rates!&lt;/p&gt; &lt;p&gt;Anyway, I wouldn't recommend these smol bois if you have enough RAM+VRAM to fit a more optimized larger quant, but if at least there are some options &amp;quot;For the desperate&amp;quot; haha...&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9u1teeqt4g4f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l19yud/iq1_smol_boi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l19yud/iq1_smol_boi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T05:26:51+00:00</published>
  </entry>
</feed>
