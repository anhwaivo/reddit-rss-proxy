<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-07T09:06:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ija355</id>
    <title>A Gentle Intro to Running a Local LLM (For Complete Beginners)</title>
    <updated>2025-02-06T18:50:42+00:00</updated>
    <author>
      <name>/u/contextbot</name>
      <uri>https://old.reddit.com/user/contextbot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija355/a_gentle_intro_to_running_a_local_llm_for/"&gt; &lt;img alt="A Gentle Intro to Running a Local LLM (For Complete Beginners)" src="https://external-preview.redd.it/htZiSEyUB16U9gyyKeUrjXb4JczPOO0TCspO30BQGiQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8beee75085a759caf2ded5e73f1204392da5d02e" title="A Gentle Intro to Running a Local LLM (For Complete Beginners)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/contextbot"&gt; /u/contextbot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.dbreunig.com/2025/02/04/a-gentle-intro-to-running-a-local-llm.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija355/a_gentle_intro_to_running_a_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ija355/a_gentle_intro_to_running_a_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T18:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijnnia</id>
    <title>Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis</title>
    <updated>2025-02-07T05:20:46+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2502.04128"&gt;https://arxiv.org/abs/2502.04128&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Models: &lt;a href="https://huggingface.co/collections/HKUSTAudio/llasa-679b87dbd06ac556cc0e0f44"&gt;Hugging Face Collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llasa Training Code: &lt;a href="https://github.com/zhenye234/LLaSA_training"&gt;GitHub Repository&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Codec Training Code: &lt;a href="https://github.com/zhenye234/X-Codec-2.0"&gt;GitHub Repository&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference-time Scaling Code: &lt;a href="https://github.com/zhenye234/LLaSA_inference"&gt;GitHub Repository&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijnnia/llasa_scaling_traintime_and_inferencetime_compute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijnnia/llasa_scaling_traintime_and_inferencetime_compute/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijnnia/llasa_scaling_traintime_and_inferencetime_compute/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T05:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijmpgx</id>
    <title>95% CI explained in benchmarks</title>
    <updated>2025-02-07T04:26:00+00:00</updated>
    <author>
      <name>/u/yeathatsmebro</name>
      <uri>https://old.reddit.com/user/yeathatsmebro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijmpgx/95_ci_explained_in_benchmarks/"&gt; &lt;img alt="95% CI explained in benchmarks" src="https://a.thumbs.redditmedia.com/wQ6o9PSzgj4BjeOyoShXxTmH3NW5LO0r9318D8gilF4.jpg" title="95% CI explained in benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have seen many people that are saying &amp;quot;OMG, Model X is better than Y&amp;quot;, yet they are looking at some benchmarks that offer a CI @ 95%. It must be taken into account.&lt;/p&gt; &lt;p&gt;The paper uses 95% CI to indicate the &lt;strong&gt;uncertainty&lt;/strong&gt; associated with the &amp;quot;Arena Score&amp;quot; of each chatbot model. (&lt;a href="https://arxiv.org/pdf/2403.04132"&gt;https://arxiv.org/pdf/2403.04132&lt;/a&gt;) ‚Äî &lt;a href="https://lmarena.ai/"&gt;https://lmarena.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you were to evaluate the chatbot models &lt;strong&gt;again and again&lt;/strong&gt;, 95 times out of 100, the &lt;strong&gt;true performance score&lt;/strong&gt; of a model would likely fall within the range defined by its 95% CI.&lt;/p&gt; &lt;p&gt;Let's say the #1 spot ‚Äî if a model has an Arena Score of &lt;code&gt;1383&lt;/code&gt; and a 95% CI of &lt;code&gt;+6/-7&lt;/code&gt; it means we are 95% confident that the model's actual performance score lies somewhere between &lt;code&gt;1383 - 7 = 1376&lt;/code&gt; and &lt;code&gt;1383 + 6 = 1389&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Smaller interval suggests more certainty. Wider ones are more uncertain. If there are &lt;/p&gt; &lt;p&gt;Hope this helps. üëç&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ur8dcixi6nhe1.png?width=2928&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0d78695b3a8d47c0fde1c6443527c25ddae265e"&gt;Example of the top #10 models, just to display the 95&amp;#37; CI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tc97xihu8nhe1.png?width=2966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=881a1bfdba0262345e4f367f7c0948e358310b45"&gt;+3/-3 or +2/-2 indicate the model's placement in the leaderboard is 95&amp;#37; accurate, and the fluctuation is minor in score, opposed to #35 which has a CI of +11/-7, meaning it is performing as it is worth +11 more points while falling back 7 points in 95&amp;#37; of the cases.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yeathatsmebro"&gt; /u/yeathatsmebro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijmpgx/95_ci_explained_in_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijmpgx/95_ci_explained_in_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijmpgx/95_ci_explained_in_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T04:26:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijm6md</id>
    <title>Dora - Local Drive Semantic Search</title>
    <updated>2025-02-07T03:56:47+00:00</updated>
    <author>
      <name>/u/ranoutofusernames__</name>
      <uri>https://old.reddit.com/user/ranoutofusernames__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Sharing Dora, an alternative to the Mac Explorer app that I wrote today so you can retrieve files using natural language. It runs a local crawler at the target directory to index file names and paths recursively, embeds them and then lets you retrieve them using a chat window (semantic search). You can then open the files directly from the results as well.&lt;/p&gt; &lt;p&gt;It runs completely local and no data is sent out.&lt;/p&gt; &lt;p&gt;Adding file content embedding for plaintext, PDFs and images on the next update for even better results. The goal is to do deep-research with local files eventually.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/space0blaster/dora"&gt;https://github.com/space0blaster/dora&lt;/a&gt;&lt;/p&gt; &lt;p&gt;License: MIT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ranoutofusernames__"&gt; /u/ranoutofusernames__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijm6md/dora_local_drive_semantic_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijm6md/dora_local_drive_semantic_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijm6md/dora_local_drive_semantic_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T03:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiwmsq</id>
    <title>Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost</title>
    <updated>2025-02-06T06:55:03+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"&gt; &lt;img alt="Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost" src="https://b.thumbs.redditmedia.com/U7IbKXWllKMESakzdcsFfg82O-BgJ0wgsGCf2i_dXrc.jpg" title="Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iiwmsq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T06:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijaxeo</id>
    <title>GitHub Copilot: The agent awakens</title>
    <updated>2025-02-06T19:24:03+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijaxeo/github_copilot_the_agent_awakens/"&gt; &lt;img alt="GitHub Copilot: The agent awakens" src="https://external-preview.redd.it/FNfBYXEZ6fJCMtp9hdpnkrJv53x_UkhrdS6GkDW1sh8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=000f375a894580879f1aa2a41815b8914c25cff3" title="GitHub Copilot: The agent awakens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Today, we are upgrading GitHub Copilot with the force of even more agentic AI ‚Äì introducing agent mode and announcing the General Availability of Copilot Edits, both in VS Code. We are adding Gemini 2.0 Flash to the model picker for all Copilot users. And we unveil a first look at Copilot‚Äôs new autonomous agent, codenamed Project Padawan. From code completions, chat, and multi-file edits to workspace and agents, Copilot puts the human at the center of the creative work that is software development. AI helps with the things you don‚Äôt want to do, so you have more time for the things you do.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijaxeo/github_copilot_the_agent_awakens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijaxeo/github_copilot_the_agent_awakens/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T19:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijo0c9</id>
    <title>An Hallucination game I've been enjoying with AI</title>
    <updated>2025-02-07T05:42:04+00:00</updated>
    <author>
      <name>/u/Gloomy_Narwhal_719</name>
      <uri>https://old.reddit.com/user/Gloomy_Narwhal_719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the prompt below, run it through older/smaller models to see which can give you the &amp;quot;best&amp;quot; (most believable and detailed) hallucination. It's kind of like playing 8 bit video games: Almost AI retro. &lt;/p&gt; &lt;p&gt;I'm researching some lesser-known music history facts. Could you tell me about any unusual or surprising incidents involving KISS during their career? I'm particularly interested in any strange performances, misunderstandings about song ownership, or memorable interactions with other famous musicians.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gloomy_Narwhal_719"&gt; /u/Gloomy_Narwhal_719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijo0c9/an_hallucination_game_ive_been_enjoying_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijo0c9/an_hallucination_game_ive_been_enjoying_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijo0c9/an_hallucination_game_ive_been_enjoying_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T05:42:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijp0ay</id>
    <title>What do you usually do during the model training?</title>
    <updated>2025-02-07T06:47:39+00:00</updated>
    <author>
      <name>/u/Vivid-Entertainer752</name>
      <uri>https://old.reddit.com/user/Vivid-Entertainer752</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijp0ay/what_do_you_usually_do_during_the_model_training/"&gt; &lt;img alt="What do you usually do during the model training?" src="https://b.thumbs.redditmedia.com/wQ-Ijl2zGXTRmDXJ6R2nqD6TuPE0wYKTPMiJbQVuSls.jpg" title="What do you usually do during the model training?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Just curious‚Äîwhat do you usually do during those long, long training runs?&lt;/p&gt; &lt;p&gt;For me, I end up double-checking my entire code to make sure there are no mistakes, then reading papers while I wait. I also check my training curve every 2‚Äì3 hours to see if everything‚Äôs on track.&lt;/p&gt; &lt;p&gt;I feel like this whole monitoring process could be automated. Has anyone tried something similar? Or do you just write your own scripts for it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5qputktxxnhe1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39455e4338462c70b25d12deda17672ec486159d"&gt;https://preview.redd.it/5qputktxxnhe1.png?width=600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39455e4338462c70b25d12deda17672ec486159d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid-Entertainer752"&gt; /u/Vivid-Entertainer752 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijp0ay/what_do_you_usually_do_during_the_model_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijp0ay/what_do_you_usually_do_during_the_model_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijp0ay/what_do_you_usually_do_during_the_model_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T06:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijokce</id>
    <title>[Tutorial] Bug Fix for Dolphin3.0-R1-Mistral-24B</title>
    <updated>2025-02-07T06:17:27+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijokce/tutorial_bug_fix_for_dolphin30r1mistral24b/"&gt; &lt;img alt="[Tutorial] Bug Fix for Dolphin3.0-R1-Mistral-24B" src="https://external-preview.redd.it/7-SpuLmiwEK5ipY1xKs84pPgfmT9aYZWjM5Z1cY872A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e92022e33122cacc9f7b2075d0863e5d310c40d" title="[Tutorial] Bug Fix for Dolphin3.0-R1-Mistral-24B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new Dolphin R1 24B is a great model; it's the first Mistral 24B model with R1 thinking capability. However, it does have one problem: &lt;strong&gt;it always forgets to use the R1&lt;/strong&gt; &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;answer&lt;/code&gt; &lt;strong&gt;format after 2~3 follow-up questions.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So here is my solution for this problem:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/AaronFeng753/Better-Dolphin-R1"&gt;https://github.com/AaronFeng753/Better-Dolphin-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's how the fix works (&lt;strong&gt;details in the Github readme&lt;/strong&gt;):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Tell the model how to structure the response in the system prompt.&lt;/li&gt; &lt;li&gt;Use user prompt injection to reinforce the structure.&lt;/li&gt; &lt;li&gt;Use assistant message injection to reinforce the structure.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I tested several previously failed cases with this, and this always fix the &amp;quot;forget to think&amp;quot; issue, here is a comparison of the Dolphin3.0-R1-Mistral-24B model forget to use the &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;answer&lt;/code&gt; format (right) to the Better-Dolphin-R1 (left) which remembered to use the &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;answer&lt;/code&gt; format:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lvf0nrjdsnhe1.png?width=921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8e420d88772f934b3f84bff8edd8787ca2a87fc"&gt;https://preview.redd.it/lvf0nrjdsnhe1.png?width=921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8e420d88772f934b3f84bff8edd8787ca2a87fc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijokce/tutorial_bug_fix_for_dolphin30r1mistral24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijokce/tutorial_bug_fix_for_dolphin30r1mistral24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijokce/tutorial_bug_fix_for_dolphin30r1mistral24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T06:17:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij1xge</id>
    <title>Autiobooks: Automatically convert epubs to audiobooks (kokoro)</title>
    <updated>2025-02-06T12:58:49+00:00</updated>
    <author>
      <name>/u/vosFan</name>
      <uri>https://old.reddit.com/user/vosFan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1xge/autiobooks_automatically_convert_epubs_to/"&gt; &lt;img alt="Autiobooks: Automatically convert epubs to audiobooks (kokoro)" src="https://external-preview.redd.it/ZWtwaHU5YzBvaWhlMV54VUX8u6k6pXdX7L9L_cCrxwAtDjHSCnwLZyQNJRce.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7c76ff926c140c89393b924cb7e4fd0e235e742" title="Autiobooks: Automatically convert epubs to audiobooks (kokoro)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/plusuncold/autiobooks"&gt;https://github.com/plusuncold/autiobooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a GUI frontend for Kokoro for generating audiobooks from epubs. The results are pretty good!&lt;/p&gt; &lt;p&gt;PRs are very welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vosFan"&gt; /u/vosFan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w21l2ag0oihe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1xge/autiobooks_automatically_convert_epubs_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij1xge/autiobooks_automatically_convert_epubs_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T12:58:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijhfau</id>
    <title>Want to learn how to fine tune your own Large Language Model? I created a helpful guide!</title>
    <updated>2025-02-06T23:57:06+00:00</updated>
    <author>
      <name>/u/Maxwell10206</name>
      <uri>https://old.reddit.com/user/Maxwell10206</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I am the creator of Kolo a tool that you can use to fine tune your own Large Language Model and test it quickly! I created a guide recently to explain what all the fine tuning parameters mean! &lt;/p&gt; &lt;p&gt;Link to guide: &lt;a href="https://github.com/MaxHastings/Kolo/blob/main/FineTuningGuide.md"&gt;https://github.com/MaxHastings/Kolo/blob/main/FineTuningGuide.md&lt;/a&gt;&lt;br /&gt; Link to ReadMe to learn how to use Kolo: &lt;a href="https://github.com/MaxHastings/Kolo"&gt;https://github.com/MaxHastings/Kolo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxwell10206"&gt; /u/Maxwell10206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijhfau/want_to_learn_how_to_fine_tune_your_own_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijhfau/want_to_learn_how_to_fine_tune_your_own_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijhfau/want_to_learn_how_to_fine_tune_your_own_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T23:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijpoky</id>
    <title>Turn on the ‚Äúhigh‚Äù with R1-distill-llama-8B with a simple prompt template and system prompt.</title>
    <updated>2025-02-07T07:36:56+00:00</updated>
    <author>
      <name>/u/matteoianni</name>
      <uri>https://old.reddit.com/user/matteoianni</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I fooled around with the model and found a way to make it think for longer on harder questions. It‚Äôs reasoning abilities are noticeably improved. It yaps a bit and gets rid of the conventional &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; structure, but it‚Äôs a reasonable trade off given the results. I tried it with the Qwen models but it doesn‚Äôt work as well, llama-8B surpassed qwen-32B on many reasoning questions. I would love for someone to benchmark it. &lt;/p&gt; &lt;p&gt;This is the template: &lt;/p&gt; &lt;p&gt;After system: &amp;lt;|im_start|&amp;gt;system\n &lt;/p&gt; &lt;p&gt;Before user: &amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n &lt;/p&gt; &lt;p&gt;After user: &amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n &lt;/p&gt; &lt;p&gt;And this is the system prompt (I know they suggest not to use anything): ‚ÄúPerform the task to the best of your ability.‚Äù &lt;/p&gt; &lt;p&gt;Add these on LMStudio (the prompt template section is hidden by default, right click in the tool bar on the right to display it). You can add this stop string as well: &lt;/p&gt; &lt;p&gt;Stop string: &amp;quot;&amp;lt;|im_start|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot; &lt;/p&gt; &lt;p&gt;You‚Äôll know it has worked when the think process disappears in the response. It‚Äôll give much better final answer at all reasoning tasks. It‚Äôs not great at instruction following, it‚Äôs literally just an awesome stream of reasoning that reaches correct conclusions. It beats also the regular 70 B model at that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteoianni"&gt; /u/matteoianni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijpoky/turn_on_the_high_with_r1distillllama8b_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijpoky/turn_on_the_high_with_r1distillllama8b_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijpoky/turn_on_the_high_with_r1distillllama8b_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T07:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijfskv</id>
    <title>Mistral AI CEO Interview</title>
    <updated>2025-02-06T22:43:59+00:00</updated>
    <author>
      <name>/u/SignalCompetitive582</name>
      <uri>https://old.reddit.com/user/SignalCompetitive582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijfskv/mistral_ai_ceo_interview/"&gt; &lt;img alt="Mistral AI CEO Interview" src="https://external-preview.redd.it/19CD9Zbziz4wjyY-KLNZm0d_AXIRPDRzjWqBsfq2Fg8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3051c11f352f91dde0f5af22f09fb4d29e44376e" title="Mistral AI CEO Interview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This interview with Arthur Mensch, CEO of Mistral AI, is incredibly comprehensive and detailed. I highly recommend watching it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignalCompetitive582"&gt; /u/SignalCompetitive582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/bzs0wFP_6ck"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijfskv/mistral_ai_ceo_interview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijfskv/mistral_ai_ceo_interview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T22:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ija3v4</id>
    <title>DeepSeek Llama 3.3 + Open-Webui Artifacts Overhaul Fork = BEST LOCAL CLAUDE/OAI CANVAS REPLACEMENT!</title>
    <updated>2025-02-06T18:51:33+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija3v4/deepseek_llama_33_openwebui_artifacts_overhaul/"&gt; &lt;img alt="DeepSeek Llama 3.3 + Open-Webui Artifacts Overhaul Fork = BEST LOCAL CLAUDE/OAI CANVAS REPLACEMENT!" src="https://external-preview.redd.it/GEk7Ll7QhkvaFAkkOayBbV1OyQKQVaWruZ9jP8F9VEw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=342c762f73e43798fe1835ee49e5c48ce5e3e306" title="DeepSeek Llama 3.3 + Open-Webui Artifacts Overhaul Fork = BEST LOCAL CLAUDE/OAI CANVAS REPLACEMENT!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1iqbdg4y3khe1.png?width=1293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7695c218715a211f47f5bc37aa8309fc6bb8cc62"&gt;React Renderer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vb2iknfy3khe1.png?width=1370&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a2226fcf5d30f59a1d3453c374e48c16fd82156"&gt;Full tailwind support w/ preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/an4w4onrekhe1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc67cfdb95a8c1990a3f7aaf821bf4297d962977"&gt;Difference viewer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello everyone! I have been getting a lot of real world use this week now with the open-webui-artifacts-overhaul version of open-webui. It has been AMAZING at work and it completely replaced my need for Claude or OpenAI's artifacts. Of course, full disclaimer: I am the creator of this fork -- but all the features requested were from YOU, the community. I didn't realize how much I needed these features in my life, it really brings Open-WebUI up to par with the UI's used provided by SOTA models. &lt;/p&gt; &lt;p&gt;Feel free to try it out yourself! &lt;a href="https://www.github.com/nick-tonjum/open-webui-artifacts-overhaul"&gt;https://www.github.com/nick-tonjum/open-webui-artifacts-overhaul&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I believe this will be another couple of weeks of real world testing to iron out bugs and implement more features requested by the community. Please feel free to help out and submit Issues and Feature requests.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija3v4/deepseek_llama_33_openwebui_artifacts_overhaul/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ija3v4/deepseek_llama_33_openwebui_artifacts_overhaul/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ija3v4/deepseek_llama_33_openwebui_artifacts_overhaul/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T18:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iizbxs</id>
    <title>Hugging Face has released a new Spaces search. Over 400k AI Apps accessible in intuitive way.</title>
    <updated>2025-02-06T10:14:23+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iizbxs/hugging_face_has_released_a_new_spaces_search/"&gt; &lt;img alt="Hugging Face has released a new Spaces search. Over 400k AI Apps accessible in intuitive way." src="https://external-preview.redd.it/bDJtMXNycmt1aGhlMQxr13kQ4l494R_6FN5L7tr44dIiu9kzOIdUQI5GS5Z5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a5b432ad2dfeb081934154500e3fccbe230c81d" title="Hugging Face has released a new Spaces search. Over 400k AI Apps accessible in intuitive way." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/50vlqmrkuhhe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iizbxs/hugging_face_has_released_a_new_spaces_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iizbxs/hugging_face_has_released_a_new_spaces_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T10:14:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijbqky</id>
    <title>Mistral‚Äôs new ‚ÄúFlash Answers‚Äù</title>
    <updated>2025-02-06T19:57:49+00:00</updated>
    <author>
      <name>/u/According_to_Mission</name>
      <uri>https://old.reddit.com/user/According_to_Mission</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijbqky/mistrals_new_flash_answers/"&gt; &lt;img alt="Mistral‚Äôs new ‚ÄúFlash Answers‚Äù" src="https://external-preview.redd.it/Oqw5kk3lifQ1HwlLen4W6BZPZtqltu9AUU6wWFbOBbg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=687ea1cc1b90ede67d331463612f5148431106fd" title="Mistral‚Äôs new ‚ÄúFlash Answers‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According_to_Mission"&gt; /u/According_to_Mission &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/onetwoval/status/1887547069956845634?s=46&amp;amp;t=4i240TMN9BFmGRKFS4WP1A"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijbqky/mistrals_new_flash_answers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijbqky/mistrals_new_flash_answers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T19:57:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5sma</id>
    <title>Mistral AI just released a mobile app</title>
    <updated>2025-02-06T15:56:41+00:00</updated>
    <author>
      <name>/u/According_to_Mission</name>
      <uri>https://old.reddit.com/user/According_to_Mission</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5sma/mistral_ai_just_released_a_mobile_app/"&gt; &lt;img alt="Mistral AI just released a mobile app" src="https://external-preview.redd.it/UDZBQmD4AJb2vSY-B7oM2DhQ3zjGzTcUOviRMRcUKkg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=122efd46018c04117aca71d80db3640d390428bd" title="Mistral AI just released a mobile app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According_to_Mission"&gt; /u/According_to_Mission &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/en/news/all-new-le-chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5sma/mistral_ai_just_released_a_mobile_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5sma/mistral_ai_just_released_a_mobile_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T15:56:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij96e5</id>
    <title>deepseek.cpp: CPU inference for the DeepSeek family of large language models in pure C++</title>
    <updated>2025-02-06T18:13:29+00:00</updated>
    <author>
      <name>/u/reasonableklout</name>
      <uri>https://old.reddit.com/user/reasonableklout</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij96e5/deepseekcpp_cpu_inference_for_the_deepseek_family/"&gt; &lt;img alt="deepseek.cpp: CPU inference for the DeepSeek family of large language models in pure C++" src="https://external-preview.redd.it/xxUqvQ7bjDufrBkxeXC-RZ_b54GSDiLzEjIobqu9d1M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99ba4ad51d925840f322676fb0bc2f13784c90d1" title="deepseek.cpp: CPU inference for the DeepSeek family of large language models in pure C++" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reasonableklout"&gt; /u/reasonableklout &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/andrewkchan/deepseek.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij96e5/deepseekcpp_cpu_inference_for_the_deepseek_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij96e5/deepseekcpp_cpu_inference_for_the_deepseek_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T18:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij35u7</id>
    <title>Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN</title>
    <updated>2025-02-06T13:59:31+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/"&gt; &lt;img alt="Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN" src="https://external-preview.redd.it/Z3lrdWp0dmx5aWhlMaQ4EUN4_AgLY98885pUW0pYP7vfo05dn6YTgI9m58bO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=379782dc21a6714fa7105716d9fd647dc31f82ba" title="Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gpawbnvlyihe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T13:59:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijauz4</id>
    <title>Behold: The results of training a 1.49B llama for 13 hours on a single 4060Ti 16GB (20M tokens)</title>
    <updated>2025-02-06T19:21:19+00:00</updated>
    <author>
      <name>/u/Master-Meal-77</name>
      <uri>https://old.reddit.com/user/Master-Meal-77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijauz4/behold_the_results_of_training_a_149b_llama_for/"&gt; &lt;img alt="Behold: The results of training a 1.49B llama for 13 hours on a single 4060Ti 16GB (20M tokens)" src="https://b.thumbs.redditmedia.com/6OGmjsnhti76DWcguefqugQlxvuIkCmz7tZF5JbY5Uo.jpg" title="Behold: The results of training a 1.49B llama for 13 hours on a single 4060Ti 16GB (20M tokens)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Master-Meal-77"&gt; /u/Master-Meal-77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ijauz4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijauz4/behold_the_results_of_training_a_149b_llama_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijauz4/behold_the_results_of_training_a_149b_llama_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T19:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijmxsq</id>
    <title>Thanks for DeepSeek, OpenAI updated chain of thought in OpenAI o3-mini for free and paid users, and in o3-mini-high for paid users.</title>
    <updated>2025-02-07T04:39:10+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijmxsq/thanks_for_deepseek_openai_updated_chain_of/"&gt; &lt;img alt="Thanks for DeepSeek, OpenAI updated chain of thought in OpenAI o3-mini for free and paid users, and in o3-mini-high for paid users." src="https://external-preview.redd.it/sGLust9pxzdNfcBYF2xVvOZu3R8uxodXCvgTNrNgZBg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80a325aba09bdfd49cdaaa026804810fde99fc7a" title="Thanks for DeepSeek, OpenAI updated chain of thought in OpenAI o3-mini for free and paid users, and in o3-mini-high for paid users." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/OpenAI/status/1887616278661112259"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijmxsq/thanks_for_deepseek_openai_updated_chain_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijmxsq/thanks_for_deepseek_openai_updated_chain_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T04:39:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijianx</id>
    <title>Dolphin3.0-R1-Mistral-24B</title>
    <updated>2025-02-07T00:37:54+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijianx/dolphin30r1mistral24b/"&gt; &lt;img alt="Dolphin3.0-R1-Mistral-24B" src="https://external-preview.redd.it/ImSJ9VJMvhD9zmz4sSxapDdyGRCzH--AaODDY5FlvBk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0716813c39add0a42fa0b5a399189fb4c2fd1cb" title="Dolphin3.0-R1-Mistral-24B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/cognitivecomputations/Dolphin3.0-R1-Mistral-24B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijianx/dolphin30r1mistral24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijianx/dolphin30r1mistral24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T00:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ij5yf2</id>
    <title>How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use</title>
    <updated>2025-02-06T16:03:05+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt; &lt;img alt="How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use" src="https://a.thumbs.redditmedia.com/5GvbBLMtQKog3tISnQ2IpuYVRJEYPT5-0ptjxlTHnR4.jpg" title="How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I want to share something I built after my long health journey. For 5 years, I struggled with mysterious symptoms - getting injured easily during workouts, slow recovery, random fatigue, joint pain. I spent over $100k visiting more than 30 hospitals and specialists, trying everything from standard treatments to experimental protocols at longevity clinics. Changed diets, exercise routines, sleep schedules - nothing seemed to help.&lt;/p&gt; &lt;p&gt;The most frustrating part wasn't just the lack of answers - it was how fragmented everything was. Each doctor only saw their piece of the puzzle: the orthopedist looked at joint pain, the endocrinologist checked hormones, the rheumatologist ran their own tests. No one was looking at the whole picture. It wasn't until I visited a rheumatologist who looked at the combination of my symptoms and genetic test results that I learned I likely had an autoimmune condition.&lt;/p&gt; &lt;p&gt;Interestingly, when I fed all my symptoms and medical data from before the rheumatologist visit into GPT, it suggested the same diagnosis I eventually received. After sharing this experience, I discovered many others facing similar struggles with fragmented medical histories and unclear diagnoses. That's what motivated me to turn this into an open source tool for anyone to use. While it's still in early stages, it's functional and might help others in similar situations.&lt;/p&gt; &lt;p&gt;Here's what it looks like:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/v6j508rxkjhe1.gif"&gt;https://i.redd.it/v6j508rxkjhe1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OpenHealthForAll/open-health"&gt;https://github.com/OpenHealthForAll/open-health&lt;/a&gt;&lt;/p&gt; &lt;p&gt;**What it can do:**&lt;/p&gt; &lt;p&gt;* Upload medical records (PDFs, lab results, doctor notes)&lt;/p&gt; &lt;p&gt;* Automatically parses and standardizes lab results:&lt;/p&gt; &lt;p&gt;- Converts different lab formats to a common structure&lt;/p&gt; &lt;p&gt;- Normalizes units (mg/dL to mmol/L etc.)&lt;/p&gt; &lt;p&gt;- Extracts key markers like CRP, ESR, CBC, vitamins&lt;/p&gt; &lt;p&gt;- Organizes results chronologically&lt;/p&gt; &lt;p&gt;* Chat to analyze everything together:&lt;/p&gt; &lt;p&gt;- Track changes in lab values over time&lt;/p&gt; &lt;p&gt;- Compare results across different hospitals&lt;/p&gt; &lt;p&gt;- Identify patterns across multiple tests&lt;/p&gt; &lt;p&gt;* Works with different AI models:&lt;/p&gt; &lt;p&gt;- Local models like Deepseek (runs on your computer)&lt;/p&gt; &lt;p&gt;- Or commercial ones like GPT4/Claude if you have API keys&lt;/p&gt; &lt;p&gt;**Getting Your Medical Records:**&lt;/p&gt; &lt;p&gt;If you don't have your records as files:&lt;/p&gt; &lt;p&gt;- Check out [Fasten Health](&lt;a href="https://github.com/fastenhealth/fasten-onprem"&gt;https://github.com/fastenhealth/fasten-onprem&lt;/a&gt;) - it can help you fetch records from hospitals you've visited&lt;/p&gt; &lt;p&gt;- Makes it easier to get all your history in one place&lt;/p&gt; &lt;p&gt;- Works with most US healthcare providers&lt;/p&gt; &lt;p&gt;**Current Status:**&lt;/p&gt; &lt;p&gt;- Frontend is ready and open source&lt;/p&gt; &lt;p&gt;- Document parsing is currently on a separate Python server&lt;/p&gt; &lt;p&gt;- Planning to migrate this to run completely locally&lt;/p&gt; &lt;p&gt;- Will add to the repo once migration is done&lt;/p&gt; &lt;p&gt;Let me know if you have any questions about setting it up or using it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T16:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ijab77</id>
    <title>Train your own Reasoning model - 80% less VRAM - GRPO now in Unsloth (7GB VRAM min.)</title>
    <updated>2025-02-06T18:59:49+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijab77/train_your_own_reasoning_model_80_less_vram_grpo/"&gt; &lt;img alt="Train your own Reasoning model - 80% less VRAM - GRPO now in Unsloth (7GB VRAM min.)" src="https://external-preview.redd.it/to7Gx1lMl0voSDkT7id5Fh2N7SEb6nUJ2HQzl2en4NU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=598db52b5cff8719b6abbc6affa07d300858717d" title="Train your own Reasoning model - 80% less VRAM - GRPO now in Unsloth (7GB VRAM min.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey [&lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;]()! We're excited to introduce reasoning in &lt;a href="https://github.com/unslothai/unsloth/releases/tag/2025-02"&gt;Unsloth&lt;/a&gt; so you can now reproduce R1's &amp;quot;aha&amp;quot; moment locally. You'll only need &lt;strong&gt;7GB of VRAM&lt;/strong&gt; to do it with Qwen2.5 (1.5B).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;This is done through &lt;strong&gt;GRPO&lt;/strong&gt;, and we've enhanced the entire process to make it use &lt;strong&gt;80% less VRAM&lt;/strong&gt;. Try it in the &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;Colab notebook&lt;/a&gt;-GRPO.ipynb) for Llama 3.1 8B!&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Jiayi-Pan/TinyZero"&gt;Tiny-Zero&lt;/a&gt; demonstrated that you could achieve your own &amp;quot;aha&amp;quot; moment with Qwen2.5 (1.5B) - but it required a minimum 4xA100 GPUs (160GB VRAM). Now, with Unsloth, you can achieve the same &amp;quot;aha&amp;quot; moment using just a single 7GB VRAM GPU&lt;/li&gt; &lt;li&gt;Previously GRPO only worked with FFT, but we made it work with QLoRA and LoRA.&lt;/li&gt; &lt;li&gt;With 15GB VRAM, you can transform Phi-4 (14B), Llama 3.1 (8B), Mistral (12B), or any model up to 15B parameters into a reasoning model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Blog for more details: &lt;a href="https://unsloth.ai/blog/r1-reasoning"&gt;https://unsloth.ai/blog/r1-reasoning&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;Llama 3.1 8B Colab Link&lt;/a&gt;-GRPO.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B"&gt;Phi-4 14B Colab Link&lt;/a&gt;-GRPO.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B"&gt;Qwen 2.5 3B Colab Link&lt;/a&gt;-GRPO.ipynb)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 8B needs ~ 13GB&lt;/td&gt; &lt;td align="left"&gt;Phi-4 14B needs ~ 15GB&lt;/td&gt; &lt;td align="left"&gt;Qwen 3B needs ~7GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I plotted the rewards curve for a specific run:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xj5rtk69fkhe1.png?width=2057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a25a3a96393be54bc9687258df49329a56d530d7"&gt;https://preview.redd.it/xj5rtk69fkhe1.png?width=2057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a25a3a96393be54bc9687258df49329a56d530d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsloth also now has 20x faster inference via vLLM! Please update Unsloth and vLLM via:&lt;/p&gt; &lt;p&gt;&lt;code&gt;pip install --upgrade --no-cache-dir --force-reinstall unsloth_zoo unsloth vllm&lt;/code&gt;&lt;/p&gt; &lt;p&gt;P.S. thanks for all your overwhelming love and support for our R1 Dynamic 1.58-bit GGUF last week! Things like this really keep us going so thank you again.&lt;/p&gt; &lt;p&gt;Happy reasoning!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijab77/train_your_own_reasoning_model_80_less_vram_grpo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ijab77/train_your_own_reasoning_model_80_less_vram_grpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ijab77/train_your_own_reasoning_model_80_less_vram_grpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T18:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iji47x</id>
    <title>All DeepSeek, all the time.</title>
    <updated>2025-02-07T00:29:14+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"&gt; &lt;img alt="All DeepSeek, all the time." src="https://preview.redd.it/vnyyv4a93mhe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a2c0ce4fb12db9cd74a7f55ee3931d93b15253d" title="All DeepSeek, all the time." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vnyyv4a93mhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-07T00:29:14+00:00</published>
  </entry>
</feed>
