<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-27T11:05:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ib4wkx</id>
    <title>Built a local Llama-powered chat for Apple Notes—quick to set up, would love feedback!</title>
    <updated>2025-01-27T09:59:53+00:00</updated>
    <author>
      <name>/u/arne226</name>
      <uri>https://old.reddit.com/user/arne226</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've heard some folks mention that they've built custom solutions to chat with their Apple Notes, so I decided to create one myself, but make it easy to setup for others.&lt;/p&gt; &lt;p&gt;I'm currently preparing it for launch and would love to hear your thoughts or feedback.&lt;/p&gt; &lt;p&gt;Best,&lt;br /&gt; Arne&lt;/p&gt; &lt;p&gt;Screenshot is in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arne226"&gt; /u/arne226 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4wkx/built_a_local_llamapowered_chat_for_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4wkx/built_a_local_llamapowered_chat_for_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4wkx/built_a_local_llamapowered_chat_for_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T09:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib3ry8</id>
    <title>Run LLM locally on Android</title>
    <updated>2025-01-27T08:47:31+00:00</updated>
    <author>
      <name>/u/sandoche</name>
      <uri>https://old.reddit.com/user/sandoche</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib3ry8/run_llm_locally_on_android/"&gt; &lt;img alt="Run LLM locally on Android" src="https://external-preview.redd.it/eHNudmRjaTIyaWZlMa2Abb99hOQyOQ71XUWNG6qkG6hM0CsQPGRGhgNwRDon.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edfe1eee2f5f558c6faa443b0e99865676349767" title="Run LLM locally on Android" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandoche"&gt; /u/sandoche &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/llp8vbi22ife1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib3ry8/run_llm_locally_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib3ry8/run_llm_locally_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T08:47:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iak7td</id>
    <title>Meet Qwen2.5-7B-Instruct-1M &amp; Qwen2.5-14B-Instruct-1M</title>
    <updated>2025-01-26T17:18:26+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1883557964759654608"&gt;https://x.com/Alibaba_Qwen/status/1883557964759654608&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're leveling up the game with our latest open-source models, Qwen2.5-1M ! Now supporting a 1 MILLION TOKEN CONTEXT LENGTH &lt;/p&gt; &lt;p&gt;Here's what’s new: &lt;/p&gt; &lt;p&gt;Open Models: Meet Qwen2.5-7B-Instruct-1M &amp;amp; Qwen2.5-14B-Instruct-1M —our first-ever models handling 1M-token contexts! &lt;/p&gt; &lt;p&gt;Lightning-Fast Inference Framework: We’ve fully open-sourced our inference framework based on vLLM , integrated with sparse attention methods. Experience 3x to 7x faster processing for 1M-token inputs! &lt;/p&gt; &lt;p&gt;Tech Deep Dive: Check out our detailed Technical Report for all the juicy details behind the Qwen2.5-1M series! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iak7td/meet_qwen257binstruct1m_qwen2514binstruct1m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iak7td/meet_qwen257binstruct1m_qwen2514binstruct1m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iak7td/meet_qwen257binstruct1m_qwen2514binstruct1m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T17:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib1jdl</id>
    <title>What's deepseek RL reward function?</title>
    <updated>2025-01-27T06:15:16+00:00</updated>
    <author>
      <name>/u/Fantastic_Climate_90</name>
      <uri>https://old.reddit.com/user/Fantastic_Climate_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I couldn't find on the paper.&lt;/p&gt; &lt;p&gt;Anyone knows how does the reward looks like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic_Climate_90"&gt; /u/Fantastic_Climate_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib1jdl/whats_deepseek_rl_reward_function/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib1jdl/whats_deepseek_rl_reward_function/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib1jdl/whats_deepseek_rl_reward_function/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T06:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1iakhai</id>
    <title>Confucius-o1-14B</title>
    <updated>2025-01-26T17:28:14+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iakhai/confuciuso114b/"&gt; &lt;img alt="Confucius-o1-14B" src="https://external-preview.redd.it/CtcKiRELpTFksl1HeUnxFZoytd4EOkb5O7UUupfTGWI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf53452cf6e0846b5bffa7ef2c25002c5c625413" title="Confucius-o1-14B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yus83af2idfe1.png?width=1379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfafdfe1b9f5e888515b26b26df2c905b12fabd5"&gt;https://preview.redd.it/yus83af2idfe1.png?width=1379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfafdfe1b9f5e888515b26b26df2c905b12fabd5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Confucius-o1-14B is a o1-like reasoning model developed by the NetEase Youdao Team, it can be easily deployed on a single GPU without quantization. This model is based on the Qwen2.5-14B-Instruct model and adopts a two-stage learning strategy, enabling the lightweight 14B model to possess thinking abilities similar to those of o1. What sets it apart is that after generating the chain of thought, it can summarize a step-by-step problem-solving process from the chain of thought on its own. This can prevent users from getting bogged down in the complex chain of thought and allows them to easily obtain the correct problem-solving ideas and answers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/netease-youdao/Confucius-o1-14B"&gt;Model Link &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://confucius-o1-demo.youdao.com/"&gt;Demo &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iakhai/confuciuso114b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iakhai/confuciuso114b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iakhai/confuciuso114b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T17:28:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaj0da</id>
    <title>AI models outperformed the champion of TUS (Medical Specialization Exam of Turkey)</title>
    <updated>2025-01-26T16:33:07+00:00</updated>
    <author>
      <name>/u/AloneCoffee4538</name>
      <uri>https://old.reddit.com/user/AloneCoffee4538</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaj0da/ai_models_outperformed_the_champion_of_tus/"&gt; &lt;img alt="AI models outperformed the champion of TUS (Medical Specialization Exam of Turkey)" src="https://preview.redd.it/x4xd7d7a8dfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d010b3576261b46f6f12443a4fd4ccbf4a63bf2d" title="AI models outperformed the champion of TUS (Medical Specialization Exam of Turkey)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So TUS is a really hard medical specialization exam consisting of two parts (each part 100 questions, so 200 in total). Never has a person answered all the questions correctly in its history. Doctors in Turkey must pass this exam to begin their desired residency in a hospital.&lt;/p&gt; &lt;p&gt;Credit: Ahmet Ay, founder of TUSBuddy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AloneCoffee4538"&gt; /u/AloneCoffee4538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x4xd7d7a8dfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaj0da/ai_models_outperformed_the_champion_of_tus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaj0da/ai_models_outperformed_the_champion_of_tus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T16:33:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib2fqy</id>
    <title>Could it be Qwen 3 / Qwen 2.5 72b Coder??!!</title>
    <updated>2025-01-27T07:18:34+00:00</updated>
    <author>
      <name>/u/notrdm</name>
      <uri>https://old.reddit.com/user/notrdm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2fqy/could_it_be_qwen_3_qwen_25_72b_coder/"&gt; &lt;img alt="Could it be Qwen 3 / Qwen 2.5 72b Coder??!!" src="https://a.thumbs.redditmedia.com/LxHYStboY0QZy26pIAj1LvV7kY0DGFXtBIr4q-j1Gj8.jpg" title="Could it be Qwen 3 / Qwen 2.5 72b Coder??!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ehwnv20klhfe1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b233335c2ad0fc9303b16dcaf81f66e3d692436"&gt;https://preview.redd.it/ehwnv20klhfe1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b233335c2ad0fc9303b16dcaf81f66e3d692436&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notrdm"&gt; /u/notrdm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2fqy/could_it_be_qwen_3_qwen_25_72b_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2fqy/could_it_be_qwen_3_qwen_25_72b_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2fqy/could_it_be_qwen_3_qwen_25_72b_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T07:18:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ia9iy1</id>
    <title>DeepSeekR1 3D game 100% from scratch</title>
    <updated>2025-01-26T08:36:26+00:00</updated>
    <author>
      <name>/u/Trick-Independent469</name>
      <uri>https://old.reddit.com/user/Trick-Independent469</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"&gt; &lt;img alt="DeepSeekR1 3D game 100% from scratch" src="https://preview.redd.it/qrdlt6i8vafe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8d13a97797fa31e558155d2f6738fd891080c24b" title="DeepSeekR1 3D game 100% from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've asked DeepSeek R1 to make me a game like kkrieger ( where most of the things are generated on run ) and it made me this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trick-Independent469"&gt; /u/Trick-Independent469 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrdlt6i8vafe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ia9iy1/deepseekr1_3d_game_100_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T08:36:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaufzj</id>
    <title>Is Deepseek R1 on Groq will make it think faster?</title>
    <updated>2025-01-27T00:00:20+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaufzj/is_deepseek_r1_on_groq_will_make_it_think_faster/"&gt; &lt;img alt="Is Deepseek R1 on Groq will make it think faster?" src="https://preview.redd.it/fwq4uvn2gffe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d7b8df15c914a6161d3fa722a23906a8470e1b9" title="Is Deepseek R1 on Groq will make it think faster?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fwq4uvn2gffe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaufzj/is_deepseek_r1_on_groq_will_make_it_think_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaufzj/is_deepseek_r1_on_groq_will_make_it_think_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T00:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib4qrg</id>
    <title>It was fun while it lasted.</title>
    <updated>2025-01-27T09:50:11+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4qrg/it_was_fun_while_it_lasted/"&gt; &lt;img alt="It was fun while it lasted." src="https://preview.redd.it/f4z3rtg5dife1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2e9e389c105a5198f86f71e16e2dc7186ad1daa" title="It was fun while it lasted." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f4z3rtg5dife1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4qrg/it_was_fun_while_it_lasted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4qrg/it_was_fun_while_it_lasted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T09:50:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iay2ik</id>
    <title>Wholesome interaction with deepseek v3</title>
    <updated>2025-01-27T03:01:55+00:00</updated>
    <author>
      <name>/u/ParadiseMaker69</name>
      <uri>https://old.reddit.com/user/ParadiseMaker69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iay2ik/wholesome_interaction_with_deepseek_v3/"&gt; &lt;img alt="Wholesome interaction with deepseek v3" src="https://b.thumbs.redditmedia.com/0t-ogYx1QGnshPztysHW1iUP_uuqACStRsV6ljgQECo.jpg" title="Wholesome interaction with deepseek v3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParadiseMaker69"&gt; /u/ParadiseMaker69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iay2ik"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iay2ik/wholesome_interaction_with_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iay2ik/wholesome_interaction_with_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T03:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iazi5b</id>
    <title>@emostaque : The future is local inference</title>
    <updated>2025-01-27T04:12:37+00:00</updated>
    <author>
      <name>/u/MrWidmoreHK</name>
      <uri>https://old.reddit.com/user/MrWidmoreHK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iazi5b/emostaque_the_future_is_local_inference/"&gt; &lt;img alt="@emostaque : The future is local inference" src="https://preview.redd.it/56cwsgs2pgfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68ee5a33606cf09deb184841e83242faf9fb6f01" title="@emostaque : The future is local inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrWidmoreHK"&gt; /u/MrWidmoreHK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/56cwsgs2pgfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iazi5b/emostaque_the_future_is_local_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iazi5b/emostaque_the_future_is_local_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T04:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaizfb</id>
    <title>Qwen2.5-1M Release on HuggingFace - The long-context version of Qwen2.5, supporting 1M-token context lengths!</title>
    <updated>2025-01-26T16:32:10+00:00</updated>
    <author>
      <name>/u/Silentoplayz</name>
      <uri>https://old.reddit.com/user/Silentoplayz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing to be the first to do it here.&lt;/p&gt; &lt;blockquote&gt; &lt;h1&gt;Qwen2.5-1M&lt;/h1&gt; &lt;p&gt;The long-context version of Qwen2.5, supporting 1M-token context lengths&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen25-1m-679325716327ec07860530ba"&gt;https://huggingface.co/collections/Qwen/qwen25-1m-679325716327ec07860530ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Related &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; post by another fellow regarding &amp;quot;Qwen 2.5 VL&amp;quot; models - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1iaciu9/qwen_25_vl_release_imminent/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Edit:&lt;/h1&gt; &lt;p&gt;Blogpost: &lt;a href="https://qwenlm.github.io/blog/qwen2.5-1m/"&gt;https://qwenlm.github.io/blog/qwen2.5-1m/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf"&gt;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you &lt;a href="/u/Balance-"&gt;u/Balance-&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silentoplayz"&gt; /u/Silentoplayz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizfb/qwen251m_release_on_huggingface_the_longcontext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizfb/qwen251m_release_on_huggingface_the_longcontext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaizfb/qwen251m_release_on_huggingface_the_longcontext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T16:32:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib4ksj</id>
    <title>How *exactly* is Deepseek so cheap?</title>
    <updated>2025-01-27T09:40:04+00:00</updated>
    <author>
      <name>/u/micamecava</name>
      <uri>https://old.reddit.com/user/micamecava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek's all the rage. I get it, 95-97% reduction in costs. &lt;/p&gt; &lt;p&gt;How *exactly*? &lt;/p&gt; &lt;p&gt;Aside from cheaper training (not doing RLHF), quantization, and caching (semantic input HTTP caching I guess?), where's the reduction coming from? &lt;/p&gt; &lt;p&gt;This can't be all, because supposedly R1 isn't quantized. Right?&lt;/p&gt; &lt;p&gt;Is it subsidized? Is OpenAI/Anthropic just...charging too much? What's the deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/micamecava"&gt; /u/micamecava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib4ksj/how_exactly_is_deepseek_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T09:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iayi0m</id>
    <title>I miss the days when ClosedAI was OpenAI</title>
    <updated>2025-01-27T03:23:39+00:00</updated>
    <author>
      <name>/u/nknnr</name>
      <uri>https://old.reddit.com/user/nknnr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since OpenAI became ClosedAI, they seem to have lost their innovativeness, under the delusion that they have created a moat that others cannot cross.&lt;/p&gt; &lt;p&gt;Maybe if they had continued to be OpenAI we would be seeing open source gpt5 and o5 by now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nknnr"&gt; /u/nknnr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iayi0m/i_miss_the_days_when_closedai_was_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iayi0m/i_miss_the_days_when_closedai_was_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iayi0m/i_miss_the_days_when_closedai_was_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T03:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaebwp</id>
    <title>Financial Times: "DeepSeek shocked Silicon Valley"</title>
    <updated>2025-01-26T13:19:03+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.md/b0M8i#selection-2491.0-2491.187"&gt;recent article&lt;/a&gt; in Financial Times says that US sanctions forced the AI companies in China to be more innovative &amp;quot;to maximise the computing power of a limited number of onshore chips&amp;quot;.&lt;/p&gt; &lt;p&gt;Most interesting to me was the claim that &amp;quot;DeepSeek’s singular focus on research makes it a dangerous competitor because it is willing to share its breakthroughs rather than protect them for commercial gains.&amp;quot;&lt;/p&gt; &lt;p&gt;What an Orwellian doublespeak! China, a supposedly closed country, leads the AI innovation and is willing to share its breakthroughs. And this makes them dangerous for ostensibly open countries where companies call themselves OpenAI but relentlessly hide information.&lt;/p&gt; &lt;p&gt;Here is the full link: &lt;a href="https://archive.md/b0M8i#selection-2491.0-2491.187"&gt;https://archive.md/b0M8i#selection-2491.0-2491.187&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaebwp/financial_times_deepseek_shocked_silicon_valley/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaebwp/financial_times_deepseek_shocked_silicon_valley/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaebwp/financial_times_deepseek_shocked_silicon_valley/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T13:19:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iarkra</id>
    <title>Major changes are coming this year. Buckle up.</title>
    <updated>2025-01-26T21:51:49+00:00</updated>
    <author>
      <name>/u/estebansaa</name>
      <uri>https://old.reddit.com/user/estebansaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If OpenAI can no longer demonstrate a significant lead over competitors in model development, securing necessary funding will become challenging. Investors are noting increased risk due to innovations from China, while OpenAI has lost several key researchers in recent months.&lt;/p&gt; &lt;p&gt;OpenAI faces mounting pressure. Sora's reception was underwhelming, DALL-E remains without updates, and their voice models lag behind ElevenLabs. Gemini offers competitive models at lower prices, while DeepSeek's pricing is highly competitive, and Open Source, including significant advances unique in the industry that optimize inference and improve results. Claude is better at coding, not to mention competition from LLama, and Elon gigantic compute farm. Further, Open Source Agentic models are coming that again push what people can do with an LLM.&lt;/p&gt; &lt;p&gt;o3 appears reactive to competitors' innovations, emerging after Anthropic demonstrated similar capabilities. OpenAI's position is precarious as competition intensifies rapidly. o3 is crucial for their future - if it shows only minimal improvements, investor funding will come at a premium, all while they attempt to transition to a for-profit model under scrutiny.&lt;/p&gt; &lt;p&gt;Major changes are coming this year. Buckle up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/estebansaa"&gt; /u/estebansaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iarkra/major_changes_are_coming_this_year_buckle_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iarkra/major_changes_are_coming_this_year_buckle_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iarkra/major_changes_are_coming_this_year_buckle_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T21:51:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaubfm</id>
    <title>Someone needs to create a "Can You Run It?" tool for open-source LLMs</title>
    <updated>2025-01-26T23:54:25+00:00</updated>
    <author>
      <name>/u/oromissed</name>
      <uri>https://old.reddit.com/user/oromissed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non-techie here! I’ve been itching to experiment with open-source LLMs (like Deepseek, LLaMA, Mistral, etc.), but every time I try, I hit the same wall: Will this model even run on my potato PC&lt;em&gt;?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Most guides assume you’re fluent in CUDA cores, VRAM, and quantization. Meanwhile, I’m just sitting here with my 8GB RAM laptop like 🥔.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We need a &amp;quot;Can You Run It?&amp;quot; equivalent for LLMs&lt;/strong&gt; — something like the &lt;a href="https://www.systemrequirementslab.com/cyri"&gt;System Requirements Lab&lt;/a&gt; tool for games. Imagine:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Select a model (e.g., &amp;quot;Llama3-8B&amp;quot; or &amp;quot;DeepSeek-R1&amp;quot;)&lt;/li&gt; &lt;li&gt;Upload your specs (CPU, RAM, GPU)&lt;/li&gt; &lt;li&gt;Get a simple ✅/❌ verdict: &lt;ul&gt; &lt;li&gt;&amp;quot;Yes, but expect 3 words per minute&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;No, your GPU will cry&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Try this quantized version instead&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Bonus points if it suggests optimizations (like Ollama flags or GGUF versions) for weaker hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oromissed"&gt; /u/oromissed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaubfm/someone_needs_to_create_a_can_you_run_it_tool_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaubfm/someone_needs_to_create_a_can_you_run_it_tool_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaubfm/someone_needs_to_create_a_can_you_run_it_tool_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T23:54:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaqajh</id>
    <title>deepseek is a side project pt. 2</title>
    <updated>2025-01-26T21:02:46+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaqajh/deepseek_is_a_side_project_pt_2/"&gt; &lt;img alt="deepseek is a side project pt. 2" src="https://preview.redd.it/bawhrb3ekefe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e236c7b8478b2b0b98ff8cb74b60fac011ead97e" title="deepseek is a side project pt. 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bawhrb3ekefe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaqajh/deepseek_is_a_side_project_pt_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaqajh/deepseek_is_a_side_project_pt_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T21:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib0ffq</id>
    <title>From this week's The Economist: "China’s AI industry has almost caught up with America’s"</title>
    <updated>2025-01-27T05:04:42+00:00</updated>
    <author>
      <name>/u/comfyui_user_999</name>
      <uri>https://old.reddit.com/user/comfyui_user_999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib0ffq/from_this_weeks_the_economist_chinas_ai_industry/"&gt; &lt;img alt="From this week's The Economist: &amp;quot;China’s AI industry has almost caught up with America’s&amp;quot;" src="https://external-preview.redd.it/C2yiTG1ri5iDyKLc-Vn_3V_ISkOymhbpYsu1RacQ-tE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1461e932e29591186828398f4de9362c5ad8106" title="From this week's The Economist: &amp;quot;China’s AI industry has almost caught up with America’s&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/comfyui_user_999"&gt; /u/comfyui_user_999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.economist.com/briefing/2025/01/23/chinas-ai-industry-has-almost-caught-up-with-americas"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib0ffq/from_this_weeks_the_economist_chinas_ai_industry/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib0ffq/from_this_weeks_the_economist_chinas_ai_industry/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iawl12</id>
    <title>Byee</title>
    <updated>2025-01-27T01:44:42+00:00</updated>
    <author>
      <name>/u/amirulnaim2000</name>
      <uri>https://old.reddit.com/user/amirulnaim2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iawl12/byee/"&gt; &lt;img alt="Byee" src="https://preview.redd.it/jauvd8soyffe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f98d8a0c2447bbee55852101602ad1ba740a3960" title="Byee" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amirulnaim2000"&gt; /u/amirulnaim2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jauvd8soyffe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iawl12/byee/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iawl12/byee/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T01:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib3igq</id>
    <title>I asked DeepSeek to comment on U.S. AI companies.</title>
    <updated>2025-01-27T08:26:21+00:00</updated>
    <author>
      <name>/u/Alternative-Duty-532</name>
      <uri>https://old.reddit.com/user/Alternative-Duty-532</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib3igq/i_asked_deepseek_to_comment_on_us_ai_companies/"&gt; &lt;img alt="I asked DeepSeek to comment on U.S. AI companies." src="https://preview.redd.it/g4gno1ubyhfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f61cf4623524611a6700d162f38168c64599aa7" title="I asked DeepSeek to comment on U.S. AI companies." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alternative-Duty-532"&gt; /u/Alternative-Duty-532 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g4gno1ubyhfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib3igq/i_asked_deepseek_to_comment_on_us_ai_companies/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib3igq/i_asked_deepseek_to_comment_on_us_ai_companies/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T08:26:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ib2uuz</id>
    <title>I created a "Can you run it" tool for open source LLMs</title>
    <updated>2025-01-27T07:46:52+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Raskoll2/LLMcalc"&gt;https://github.com/Raskoll2/LLMcalc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's extremly simple but tells you a tk/s estimate of all the quants, and how to run them e.g. 80% layer offload, KV offload, all on GPU. &lt;/p&gt; &lt;p&gt;I have no clue if it'll run on anyone else's systems. I've tried with with linux + 1x Nvidia GPU, if anyone on other systems or multi GPU systems could relay some error messages that would be great&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2uuz/i_created_a_can_you_run_it_tool_for_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2uuz/i_created_a_can_you_run_it_tool_for_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ib2uuz/i_created_a_can_you_run_it_tool_for_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T07:46:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iasyc3</id>
    <title>Deepseek is #1 on the U.S. App Store</title>
    <updated>2025-01-26T22:52:07+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"&gt; &lt;img alt="Deepseek is #1 on the U.S. App Store" src="https://preview.redd.it/sr4kvvnv3ffe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a82ab88b43a6f7f3f1aa6d284ecb8edff2e4630" title="Deepseek is #1 on the U.S. App Store" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sr4kvvnv3ffe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iasyc3/deepseek_is_1_on_the_us_app_store/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-26T22:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1iaz2or</id>
    <title>Deepseek like a boss</title>
    <updated>2025-01-27T03:52:40+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaz2or/deepseek_like_a_boss/"&gt; &lt;img alt="Deepseek like a boss" src="https://preview.redd.it/d6slqdvilgfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b3de28e4d080e64aa2341a87efd989cd1ede176" title="Deepseek like a boss" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d6slqdvilgfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iaz2or/deepseek_like_a_boss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iaz2or/deepseek_like_a_boss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-27T03:52:40+00:00</published>
  </entry>
</feed>
