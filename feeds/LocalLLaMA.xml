<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-08T23:48:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j6tz0s</id>
    <title>Can I Run This LLM ?</title>
    <updated>2025-03-08T23:25:01+00:00</updated>
    <author>
      <name>/u/a36</name>
      <uri>https://old.reddit.com/user/a36</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6tz0s/can_i_run_this_llm/"&gt; &lt;img alt="Can I Run This LLM ?" src="https://b.thumbs.redditmedia.com/BTUWfX3G271fvIx7pXlhBd0DxsQOmJr_tDqy7TrTH2A.jpg" title="Can I Run This LLM ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Can I Run This LLM locally ?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;One of the most frequent questions one faces while running LLMs locally is: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;I have &amp;lt;this&amp;gt; hardware, Can I run &amp;lt;that&amp;gt; LLM model ?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;I have vibe coded a simple application to help you with just that. &lt;/p&gt; &lt;p&gt;It lets you chose the important parameters like unified memory for Mac or GPU + RAM for PC and then lets you select the LLM you want to run (fetched from hugging-face)&lt;/p&gt; &lt;p&gt;In case you cannot run the chosen LLM comfortably, it will suggest a few options that will most likely work with your hardware.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://vercel.com/aruns-projects-0989ee08/can-i-run-this-llm"&gt;https://vercel.com/aruns-projects-0989ee08/can-i-run-this-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3fatnccdujne1.png?width=2038&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af0416739061ec1a1e270444e637dc9f604b7a04"&gt;https://preview.redd.it/3fatnccdujne1.png?width=2038&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af0416739061ec1a1e270444e637dc9f604b7a04&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a36"&gt; /u/a36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6tz0s/can_i_run_this_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6tz0s/can_i_run_this_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6tz0s/can_i_run_this_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T23:25:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6u731</id>
    <title>NVIDIA RTX PRO 6000 Blackwell leaked: 24064 cores, 96GB G7 memory and 600W Double Flow Through cooler</title>
    <updated>2025-03-08T23:35:33+00:00</updated>
    <author>
      <name>/u/mbolaris</name>
      <uri>https://old.reddit.com/user/mbolaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-pro-6000-blackwell-leaked-24064-cores-96gb-g7-memory-and-600w-double-flow-through-cooler"&gt;https://videocardz.com/newz/nvidia-rtx-pro-6000-blackwell-leaked-24064-cores-96gb-g7-memory-and-600w-double-flow-through-cooler&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mbolaris"&gt; /u/mbolaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6u731/nvidia_rtx_pro_6000_blackwell_leaked_24064_cores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6u731/nvidia_rtx_pro_6000_blackwell_leaked_24064_cores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6u731/nvidia_rtx_pro_6000_blackwell_leaked_24064_cores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T23:35:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6rebs</id>
    <title>Identifying Specific Words with Speech Recognition for IVR Systems</title>
    <updated>2025-03-08T21:25:34+00:00</updated>
    <author>
      <name>/u/yehuda1</name>
      <uri>https://old.reddit.com/user/yehuda1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best way to identify small chunks of speech against a predefined list of words. The list can range from 2 to 5000 options.&lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;p&gt;- Must be fast on CPU&lt;/p&gt; &lt;p&gt;- Must be open source&lt;/p&gt; &lt;p&gt;- Needs to handle occasional updates to the word lists&lt;/p&gt; &lt;p&gt;Whisper doesn't provide this specific functionality.&lt;/p&gt; &lt;p&gt;I'm considering wav2vec but can't decide on the best approach or a basic model to start with. I thought about custom training for each set of options, but this would require significant automation since the lists need regular updates.&lt;/p&gt; &lt;p&gt;The main use case is for a simple IVR system, for example: &amp;quot;What department are you looking for? [Sales, Support, Billing].&amp;quot; I understand this might be considered old-school nowadays, but it's a specific requirement.&lt;/p&gt; &lt;p&gt;Google and Azure have an option to send a list of words with higher rank, but you can't force it to use only the list. &lt;/p&gt; &lt;p&gt;It looks like all the big speech to text engines - especially Whisper - are built toward identifying full sentences and conversations. So my use case is not well covered&lt;/p&gt; &lt;p&gt;Any advice will be appreciated! &lt;/p&gt; &lt;p&gt;P.S. This is not for English content.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yehuda1"&gt; /u/yehuda1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rebs/identifying_specific_words_with_speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rebs/identifying_specific_words_with_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rebs/identifying_specific_words_with_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T21:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j681n3</id>
    <title>Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark</title>
    <updated>2025-03-08T03:22:26+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt; &lt;img alt="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" src="https://preview.redd.it/izw2ej1cwdne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73dec38ebaddd2720f9cf241a4ae7cf9de6d8481" title="Qwen QwQ-32B ranks right among GPT-4o models on Confabulations/Hallucinations benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/izw2ej1cwdne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j681n3/qwen_qwq32b_ranks_right_among_gpt4o_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T03:22:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6s34q</id>
    <title>What is the link between v/ram and context length ?</title>
    <updated>2025-03-08T21:56:56+00:00</updated>
    <author>
      <name>/u/Donde-esta-el</name>
      <uri>https://old.reddit.com/user/Donde-esta-el</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have 12GB of VRAM and 20GB of RAM in my system. I want to run Qwq or Mistral-Small using Ollama. How can I calculate the maximum context length for this setup? My system can accommodate up to 64GB of RAM in total.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Donde-esta-el"&gt; /u/Donde-esta-el &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6s34q/what_is_the_link_between_vram_and_context_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6s34q/what_is_the_link_between_vram_and_context_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6s34q/what_is_the_link_between_vram_and_context_length/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T21:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6sgez</id>
    <title>Introducing anemll-server: serve Anemll models locally with OpenAI API compatible endpoints</title>
    <updated>2025-03-08T22:13:46+00:00</updated>
    <author>
      <name>/u/BaysQuorv</name>
      <uri>https://old.reddit.com/user/BaysQuorv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Always wanted to connect a frontend to my Anemll models, and now you can too with hopefully any frontend! I have only tested with Open WebUI so far, but it shouldn't be an issue with other frontends unless they need meta endpoints other than /models which this server has.&lt;/p&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;OpenAI-compatible API&lt;/li&gt; &lt;li&gt;Streaming responses&lt;/li&gt; &lt;li&gt;System prompt, conversation history supported&lt;/li&gt; &lt;li&gt;Working with Open WebUI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you don't know what Anemll is, its a library that lets you run your llms on the ANE instead of the gpu. This draws like 1/5 the wattage on my base m4. But its still very early so there are limitations to it of course.&lt;/p&gt; &lt;p&gt;Thanks for the inspiration to &lt;a href="https://www.reddit.com/user/Anastasiosy/"&gt;https://www.reddit.com/user/Anastasiosy/&lt;/a&gt; with your Phi4 multimodal OpenAI api compatible server :)&lt;/p&gt; &lt;p&gt;And of course big thanks to the Anemll creators, I think this technology is truly amazing. Theres nothing more beautiful than checking macmon and seeing only 1.7W power draw for LLM inference&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/alexgusevski/anemll-server"&gt;https://github.com/alexgusevski/anemll-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Anemll/Anemll"&gt;https://github.com/Anemll/Anemll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaysQuorv"&gt; /u/BaysQuorv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6sgez/introducing_anemllserver_serve_anemll_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6sgez/introducing_anemllserver_serve_anemll_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6sgez/introducing_anemllserver_serve_anemll_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T22:13:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6mcrd</id>
    <title>Before I try... has someone already done it? (Ordered Retrieval Content - ORC)</title>
    <updated>2025-03-08T17:38:56+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Orcs aren't so smart... I learned this in Warcraft... but if you tell them to do a simple task they can get it done. So I thought... what if I broke down a giant context into digestible bits? Provide splitting characters(s) and a prompt and you immediately have multiple outputs run based on the splits. You could choose to provide the previous outputs along with the new section, or you could output each individually with no mixture. This idea is a rough approximation of RAG, but it's ordered and holistic.&lt;/p&gt; &lt;p&gt;For example, I want a book summary, but pasting the whole book crushes my system or I have to use a very degraded context precision and it results in extremely high level information about the entire context that doesn't consider each chapter, scene, paragraph or sentence individually. What if I could split the chapters by headings (hash tags), what if I split chapters into scenes (three asterisks ***), what if I split scenes into paragraphs, paragraphs into sentences or even sentences into words? Could I then have a list of prompts that were performed on each split! I could create a prompt that summarizes each chapter by a brief recap of scenes. I could have each paragraph in the book summarized into a sentence. I could have each sentence evaluated for passive voice. In theory I could transform an entire book sentence by sentence with the right prompt.&lt;/p&gt; &lt;p&gt;I was hoping to do something like this as an extension for Text Gen by Oobabooga. I might build off of the extension Twinbook by FartyPants. Before I even attempt it, I thought I'd ask. Has it been done before? Is there something similar like it? What would you change about this idea... what would you use it for? Love your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6mcrd/before_i_try_has_someone_already_done_it_ordered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6mcrd/before_i_try_has_someone_already_done_it_ordered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6mcrd/before_i_try_has_someone_already_done_it_ordered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T17:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6lrcg</id>
    <title>FULL Cursor AI Agent System Prompt</title>
    <updated>2025-03-08T17:11:57+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cursor AI (Agent, Sonnet 3.7 based) full System Prompt now published! &lt;/p&gt; &lt;p&gt;You can check it out here: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6lrcg/full_cursor_ai_agent_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6lrcg/full_cursor_ai_agent_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6lrcg/full_cursor_ai_agent_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T17:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5zzue</id>
    <title>QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!</title>
    <updated>2025-03-07T20:48:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt; &lt;img alt="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" src="https://preview.redd.it/gc42vz36ybne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a74298e2e3d4a3128892ea9834b44f8efd5e1a9" title="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gc42vz36ybne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T20:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6k84c</id>
    <title>Phi-4:mini didn't stop the inference till I force stopped it.</title>
    <updated>2025-03-08T16:02:38+00:00</updated>
    <author>
      <name>/u/Resident_Acadia_4798</name>
      <uri>https://old.reddit.com/user/Resident_Acadia_4798</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6k84c/phi4mini_didnt_stop_the_inference_till_i_force/"&gt; &lt;img alt="Phi-4:mini didn't stop the inference till I force stopped it." src="https://external-preview.redd.it/bml0YTY0dnhuaG5lMU2Zb_cWf0SBlyMf0Audt_A4sf0T3APMzRNk1uB8Pzdo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=047fccc87602f037b9eb8ebccfc452ed20d2c911" title="Phi-4:mini didn't stop the inference till I force stopped it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Resident_Acadia_4798"&gt; /u/Resident_Acadia_4798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ee76j1vxnhne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6k84c/phi4mini_didnt_stop_the_inference_till_i_force/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6k84c/phi4mini_didnt_stop_the_inference_till_i_force/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T16:02:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6pnpj</id>
    <title>Old dual xeon servers for llm inference?</title>
    <updated>2025-03-08T20:05:13+00:00</updated>
    <author>
      <name>/u/JohnnyLiverman</name>
      <uri>https://old.reddit.com/user/JohnnyLiverman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On ebay there are a lot of those used servers with two decent xeons and like 128gb 8 channel ddr4 ram for cheap (at least here in the UK). How much tokens per second could u expect with NUMA and llama.cpp on these systems?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JohnnyLiverman"&gt; /u/JohnnyLiverman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6pnpj/old_dual_xeon_servers_for_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6pnpj/old_dual_xeon_servers_for_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6pnpj/old_dual_xeon_servers_for_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T20:05:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6noh8</id>
    <title>Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1...</title>
    <updated>2025-03-08T18:36:50+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"&gt; &lt;img alt="Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1..." src="https://external-preview.redd.it/7RbGl7PqZ_sGLzEC-up6MH5b7zJrrofblGxxk0WxBC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=132e3f4ed63e0faee796888e40ee542ef9d2a07c" title="Flappy Bird Testing and comparison of local QwQ 32b VS O1 Pro, 4.5, o3 Mini High, Sonnet 3.7, Deepseek R1..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Deveraux-Parker/FlappyAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6noh8/flappy_bird_testing_and_comparison_of_local_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:36:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6ofw8</id>
    <title>Which reasoning model to use and for what to get most of your time and money?</title>
    <updated>2025-03-08T19:10:49+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the GPT-4.5 release, it seems CoT reasoning models are the only way forward for improved LLM performance. (Are the base models hitting the wall?) &lt;/p&gt; &lt;p&gt;And we now have 4-5 labs with frontier thinking/reasoning models: OpenAI with o3-mini-high, Anthropic with Claude 3.7 Sonnet thinking, XAI with Grok 3(think), and Deepseek with r1. (I didn't add Gemini because it's not good enough)&lt;/p&gt; &lt;p&gt;There are too many options, and though it’s a nice place to be, choosing which ones to get the most of your time and money might still be confusing. (I don't particularly like juggling between models) &lt;/p&gt; &lt;p&gt;So, I did a little test of all these frontier reasoning models on a personal set of questions across complex reasoning, math, coding, and writing to determine which one to go for which tasks. Here’s the overall findings&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Grok 3 Think&lt;/strong&gt; is the top choice for coding and mathematical validation. Also, the biggest plus is Grok is honest and does what he is asked. Least judgemental.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude 3.7 Sonnet&lt;/strong&gt; is the best &lt;strong&gt;all-rounder&lt;/strong&gt;, especially for writing and reasoning. You cannot go wrong with either Sonnet or Grok 3 for coding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek R1&lt;/strong&gt; is ideal for &lt;strong&gt;math-heavy tasks&lt;/strong&gt; but struggles elsewhere.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI o3-mini-high&lt;/strong&gt; is &lt;strong&gt;fast &amp;amp; efficient&lt;/strong&gt; but lacks fine-tuned reasoning in some areas.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a full breakdown of my analysis, check out this blog post: &lt;a href="https://composio.dev/blog/cot-reasoning-models-which-one-reigns-supreme-in-2025/"&gt;&lt;strong&gt;Comparative Analysis of CoT Reasoning Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I personally find myself using Claude 3.7 Sonnet more than anything. It writes good code (though it feels overfit for coding sometimes), writes decent prose, and reasons concisely.&lt;/p&gt; &lt;p&gt;I would love to know the reasoning models you find most beneficial for your tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ofw8/which_reasoning_model_to_use_and_for_what_to_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ofw8/which_reasoning_model_to_use_and_for_what_to_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ofw8/which_reasoning_model_to_use_and_for_what_to_get/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T19:10:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j68wr1</id>
    <title>Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great</title>
    <updated>2025-03-08T04:11:36+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt; &lt;img alt="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" src="https://a.thumbs.redditmedia.com/H0M55_ytNjyQLjluGxIhsq01_P1u9IVqCdRWbacevz8.jpg" title="Qwen team seems sure that their model is better than LiveBench ranks it and demand a rerun with more optimal settings, which is crazy because it already performed really great" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690"&gt;https://preview.redd.it/dtej53d65ene1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dd9f1517a1661a1a7533874cd88daca591e7690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In case you're wondering right now it scores about a 66 global average but Qwen advertised it scores around 73 so maybe with more optimal settings it will get closer to that range&lt;/p&gt; &lt;p&gt;This rerun with be posted on Monday&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j68wr1/qwen_team_seems_sure_that_their_model_is_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T04:11:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6ma8i</id>
    <title>Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!</title>
    <updated>2025-03-08T17:35:45+00:00</updated>
    <author>
      <name>/u/Competitive-Bake4602</name>
      <uri>https://old.reddit.com/user/Competitive-Bake4602</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt; &lt;img alt="Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!" src="https://external-preview.redd.it/OERDGiS518l9lA6nng9dhSyETZuedB7NMNyJJW94EgQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e69639f639d057ebece140432310ca7d2b192b1" title="Help Us Benchmark the Apple Neural Engine for the Open-Source ANEMLL Project!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oadtfpm06ine1.png?width=1874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cae2c489fa65f3fa2770991a81a54562560bd2ab"&gt;https://preview.redd.it/oadtfpm06ine1.png?width=1874&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cae2c489fa65f3fa2770991a81a54562560bd2ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re part of the open-source project &lt;a href="https://github.com/anemll/anemll"&gt;&lt;strong&gt;ANEMLL&lt;/strong&gt;&lt;/a&gt;, which is working to bring large language models (LLMs) to the Apple Neural Engine. This hardware has incredible potential, but there’s a catch—Apple hasn’t shared much about its inner workings, like memory speeds or detailed performance specs. That’s where you come in!&lt;/p&gt; &lt;p&gt;To help us understand the Neural Engine better, we’ve launched a new benchmark tool: &lt;a href="https://github.com/Anemll/anemll-bench"&gt;&lt;strong&gt;anemll-bench&lt;/strong&gt;&lt;/a&gt;. It measures the Neural Engine’s bandwidth, which is key for optimizing LLMs on Apple’s chips.&lt;/p&gt; &lt;p&gt;We’re especially eager to see results from &lt;strong&gt;Ultra&lt;/strong&gt; models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;M1 Ultra&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;M2 Ultra&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;And, if you’re one of the lucky few, &lt;strong&gt;M3 Ultra&lt;/strong&gt;!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(Max models like M2 Max, M3 Max, and M4 Max are also super helpful!)&lt;/p&gt; &lt;p&gt;If you’ve got one of these Macs, here’s how you can contribute:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Clone the repo&lt;/strong&gt;: &lt;a href="https://github.com/Anemll/anemll-bench"&gt;https://github.com/Anemll/anemll-bench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Run the benchmark&lt;/strong&gt;: Just follow the README—it’s straightforward!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Share your results&lt;/strong&gt;: Submit your JSON result via a &amp;quot;issues&amp;quot; or email&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why contribute?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You’ll help an open-source project make real progress.&lt;/li&gt; &lt;li&gt;You’ll get to see how your device stacks up.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious about the bigger picture? Check out the main ANEMLL project: &lt;a href="https://github.com/anemll/anemll"&gt;https://github.com/anemll/anemll&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thanks for considering this—every contribution helps us unlock the Neural Engine’s potential&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive-Bake4602"&gt; /u/Competitive-Bake4602 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6ma8i/help_us_benchmark_the_apple_neural_engine_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T17:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6a0s2</id>
    <title>Pov: when you overthink too much</title>
    <updated>2025-03-08T05:17:47+00:00</updated>
    <author>
      <name>/u/kernel348</name>
      <uri>https://old.reddit.com/user/kernel348</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt; &lt;img alt="Pov: when you overthink too much" src="https://preview.redd.it/m9paekz5hene1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b886e3b4eb343a109cd3fef74702179d30c3c20d" title="Pov: when you overthink too much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kernel348"&gt; /u/kernel348 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9paekz5hene1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6a0s2/pov_when_you_overthink_too_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T05:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6nct7</id>
    <title>Estimating how much the new NVIDIA RTX PRO 6000 Blackwell GPU should cost</title>
    <updated>2025-03-08T18:22:47+00:00</updated>
    <author>
      <name>/u/asssuber</name>
      <uri>https://old.reddit.com/user/asssuber</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No price released yet, so let's figure out how much that card should cost:&lt;/p&gt; &lt;p&gt;Extra GDDR6 costs less than $8 per GB for the end consumer &lt;a href="https://arstechnica.com/gadgets/2024/01/review-radeon-7600-xt-offers-peace-of-mind-via-lots-of-ram-remains-a-midrange-gpu/"&gt;when installed in a GPU clamshell style&lt;/a&gt; like Nvidia is using here. GDDR7 chips seems to carry a &lt;a href="https://www.trendforce.com/presscenter/news/20240627-12207.html"&gt;20-30% premium&lt;/a&gt; over GDDR6 which I'm going to generalize to all other costs and margins related to putting it in a card, so we get less than $10 per GB.&lt;/p&gt; &lt;p&gt;Using the $2000 MSRP of the 32GB RTX 5090 as basis, the NVIDIA RTX PRO 6000 Blackwell with 96GB &lt;strong&gt;should cost less than $2700&lt;/strong&gt; *(see EDIT2) to the end consumer. Oh, the wonders of a competitive capitalistic market, free of monopolistic practices!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; It seems my sarcasm above, the &amp;quot;Funny&amp;quot; flair and my comment bellow weren't sufficient, so I will try to repeat here:&lt;/p&gt; &lt;p&gt;I'm estimating how much it SHOULD cost, because everyone over here seems to be keen on normalizing the exorbitant prices for extra VRAM at the top end cards, and this is wrong. I know nvidia will price it much higher, but that was not the point of my post.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT2:&lt;/strong&gt; The RTX PRO 6000 Blackwell will reportedly feature an almost fully enabled GB202 chip, with a bit more than 10% more CUDA cores than the RTX 5090, so using the RTX 5090 MSRP is as base isn't sufficient.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asssuber"&gt; /u/asssuber &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nct7/estimating_how_much_the_new_nvidia_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6rngt</id>
    <title>Simple inference speed comparison of Deepseek-R1 between llama.cpp and ik_llama.cpp for CPU-only inference.</title>
    <updated>2025-03-08T21:36:58+00:00</updated>
    <author>
      <name>/u/U_A_beringianus</name>
      <uri>https://old.reddit.com/user/U_A_beringianus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a simple inference speed comparison of DeepSeek-R1 between &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;ik_llama.cpp&lt;/a&gt; for CPU-only inference. The latter is a fork of an old version of llama.cpp, but includes various recent optimizations and options that the original does not (yet?).&lt;br /&gt; Comparison is on linux, with a 16 core Ryzen 7 with 96GB RAM, using Q3 quants that are mem-mapped from nvme (~319GB). Initial context consists of merely one one-liner prompt.&lt;br /&gt; Options in &lt;strong&gt;bold&lt;/strong&gt; are exclusive to ik_llama.cpp, as of today.&lt;br /&gt; The quants in the mla/ directory are made with the fork, to support its use of the &amp;quot;-mla 1&amp;quot; command line option, which yields a significantly smaller requirement for KV-Cache space. &lt;/p&gt; &lt;p&gt;llama.cpp:&lt;br /&gt; llama-server -m DeepSeek-R1-Q3_K_M-00001-of-00007.gguf --host :: -fa -c 16384 -t 15 -ctk q8_0&lt;br /&gt; KV-Cache: 56120.00 MiB&lt;br /&gt; Token rate: 0.8 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m DeepSeek-R1-Q3_K_M-00001-of-00007.gguf --host :: -fa -c 16384 -t 15 -ctk q8_0&lt;br /&gt; KV-Cache: 56120.00 MiB&lt;br /&gt; Token rate: 1.1 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m DeepSeek-R1-Q3_K_M-00001-of-00007.gguf --host :: -fa -c 16384 -t 15 &lt;strong&gt;-fmoe&lt;/strong&gt; -ctk &lt;strong&gt;q8_KV&lt;/strong&gt;&lt;br /&gt; KV-Cache: 55632.00 MiB&lt;br /&gt; Token rate: 1.2 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m mla/DeepSeek-R1-Q3_K_M-00001-of-00030.gguf --host :: -fa -c 16384 -t 15 &lt;strong&gt;-mla 1&lt;/strong&gt; &lt;strong&gt;-fmoe&lt;/strong&gt; -ctk &lt;strong&gt;q8_KV&lt;/strong&gt;&lt;br /&gt; KV-Cache: 556.63 MiB (Yes, really, no typo. This would allow the use of much larger context.)&lt;br /&gt; Token rate: 1.6 t/s &lt;/p&gt; &lt;p&gt;ik_llama.cpp:&lt;br /&gt; llama-server -m mla/DeepSeek-R1-Q3_K_M-00001-of-00030.gguf --host :: -fa -c 16384 -t 15 &lt;strong&gt;-mla 1&lt;/strong&gt; &lt;strong&gt;-fmoe&lt;/strong&gt; (no KV cache quantization)&lt;br /&gt; KV-Cache: 1098.00 MiB&lt;br /&gt; Token rate: 1.6 t/s &lt;/p&gt; &lt;p&gt;Quants that work with MLA can be found there: &lt;a href="https://huggingface.co/daydream-org/DeepSeek-R1-GGUF-11446/tree/main/DeepSeek-R1-Q3_K_M"&gt;Q3&lt;/a&gt; &lt;a href="https://huggingface.co/gghfez/DeepSeek-R1-11446-Q2_K/tree/main"&gt;Q2&lt;/a&gt; &lt;a href="https://huggingface.co/gghfez/DeepSeek-R1-11446-Q4_K/tree/main"&gt;Q4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/U_A_beringianus"&gt; /u/U_A_beringianus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rngt/simple_inference_speed_comparison_of_deepseekr1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rngt/simple_inference_speed_comparison_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6rngt/simple_inference_speed_comparison_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T21:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dryj</id>
    <title>Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes</title>
    <updated>2025-03-08T09:41:19+00:00</updated>
    <author>
      <name>/u/2TierKeir</name>
      <uri>https://old.reddit.com/user/2TierKeir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt; &lt;img alt="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" src="https://external-preview.redd.it/aGlvZGVrdDZzZm5lMc_Az5p3qLdEN__5qSL7XTQoE-2LI7eWZo3yGOsqXnkB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7745bb1240348e2c2b8426f85b17a2fe6e2edeed" title="Mistral Small 24B did in 51 seconds what QwQ couldn't in 40 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2TierKeir"&gt; /u/2TierKeir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9xkdwav2sfne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dryj/mistral_small_24b_did_in_51_seconds_what_qwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6iuyf</id>
    <title>NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp; 600W TBP</title>
    <updated>2025-03-08T14:56:53+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp;amp; 600W TBP" src="https://external-preview.redd.it/ipqoihUxtH0AdjsoCf5u0QWlmwf7QkIL9jnTAAb3HTw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6333201abf11bb51d15493e0484c12b4cafa2d16" title="NVIDIA RTX PRO 6000 Blackwell GPU Packs 11% More Cores Than RTX 5090: 24,064 In Total With 96 GB GDDR7 Memory &amp;amp;amp; 600W TBP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-rtx-pro-6000-blackwell-gpu-more-cores-than-rtx-5090-24064-96-gb-gddr7-600w/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6iuyf/nvidia_rtx_pro_6000_blackwell_gpu_packs_11_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T14:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6f61q</id>
    <title>QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7</title>
    <updated>2025-03-08T11:24:32+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt; &lt;img alt="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" src="https://preview.redd.it/opow8do3agne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00cbc87c29904f9341ccf656e804f32edd07064a" title="QwQ-32B takes second place in EQ-Bench creative writing, above GPT 4.5 and Claude 3.7" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/opow8do3agne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6f61q/qwq32b_takes_second_place_in_eqbench_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T11:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j67bxt</id>
    <title>16x 3090s - It's alive!</title>
    <updated>2025-03-08T02:43:38+00:00</updated>
    <author>
      <name>/u/Conscious_Cut_6144</name>
      <uri>https://old.reddit.com/user/Conscious_Cut_6144</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt; &lt;img alt="16x 3090s - It's alive!" src="https://b.thumbs.redditmedia.com/VvyYO_xrL0vczMCglIvOXlchOAjzJG3mEsXsV_k93PQ.jpg" title="16x 3090s - It's alive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Cut_6144"&gt; /u/Conscious_Cut_6144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j67bxt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T02:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6i1ma</id>
    <title>Can't believe it, but the RTX 4090 actually exists and it runs!!!</title>
    <updated>2025-03-08T14:15:34+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt; &lt;img alt="Can't believe it, but the RTX 4090 actually exists and it runs!!!" src="https://b.thumbs.redditmedia.com/Jyu8XHnVjN10hhVg2LdYg8XX4xiDcuuwK_tm4Uimz_g.jpg" title="Can't believe it, but the RTX 4090 actually exists and it runs!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX 4090 96G version&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lqj0v5su4hne1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=595bfa005189d96dd1e6b8940b29dad9ae87cfc2"&gt;https://preview.redd.it/lqj0v5su4hne1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=595bfa005189d96dd1e6b8940b29dad9ae87cfc2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h10g0x915hne1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=aac7b20a79b477c60cac8c306a37e17b5034d4d1"&gt;https://preview.redd.it/h10g0x915hne1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=aac7b20a79b477c60cac8c306a37e17b5034d4d1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6i1ma/cant_believe_it_but_the_rtx_4090_actually_exists/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T14:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6nzrk</id>
    <title>New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s</title>
    <updated>2025-03-08T18:51:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt; &lt;img alt="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" src="https://preview.redd.it/wfkxh0q5iine1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f63ab35ce1aa6589c56196d048a5e4231e07749f" title="New GPU startup Bolt Graphics detailed their upcoming GPUs. The Bolt Zeus 4c26-256 looks like it could be really good for LLMs. 256GB @ 1.45TB/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfkxh0q5iine1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6nzrk/new_gpu_startup_bolt_graphics_detailed_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T18:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j6dzai</id>
    <title>Real-time token graph in Open WebUI</title>
    <updated>2025-03-08T09:56:58+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt; &lt;img alt="Real-time token graph in Open WebUI" src="https://external-preview.redd.it/dm1rY2E3dWl1Zm5lMeNo1g2VbIy6NNGx_1T_ctYYVLkaFt3bwpFyaChfDLc3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c66a72211f8ad427c81d09e427101d7638dfd38" title="Real-time token graph in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zscr76uiufne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j6dzai/realtime_token_graph_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-08T09:56:58+00:00</published>
  </entry>
</feed>
