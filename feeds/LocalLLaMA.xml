<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-20T02:59:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1isxyo9</id>
    <title>New LLM tech running on diffusion just dropped</title>
    <updated>2025-02-19T05:24:42+00:00</updated>
    <author>
      <name>/u/LorestForest</name>
      <uri>https://old.reddit.com/user/LorestForest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxyo9/new_llm_tech_running_on_diffusion_just_dropped/"&gt; &lt;img alt="New LLM tech running on diffusion just dropped" src="https://external-preview.redd.it/P8PjCGtAk7UmVMdaNZLIptoXPYYcXmgBOsABEcRwX6E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fe7f123e659e819d44d4238f56ef7e2825d7a43" title="New LLM tech running on diffusion just dropped" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claims to mitigate hallucinations unless you use it as a chat application.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LorestForest"&gt; /u/LorestForest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://timkellogg.me/blog/2025/02/17/diffusion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxyo9/new_llm_tech_running_on_diffusion_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isxyo9/new_llm_tech_running_on_diffusion_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T05:24:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1itcp0l</id>
    <title>Unleash the Power of Flux Schnell on Your Apple Silicon Mac!</title>
    <updated>2025-02-19T18:30:00+00:00</updated>
    <author>
      <name>/u/akashjss</name>
      <uri>https://old.reddit.com/user/akashjss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itcp0l/unleash_the_power_of_flux_schnell_on_your_apple/"&gt; &lt;img alt="Unleash the Power of Flux Schnell on Your Apple Silicon Mac!" src="https://external-preview.redd.it/kQEIp1ZGqdRuCsFn5xXQlcT97BnCom0g31VZ9kp1WDE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c39c12b582680c6e84396cff6c865ef645739e86" title="Unleash the Power of Flux Schnell on Your Apple Silicon Mac!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been blown away by the speed and quality of Flux Schnell for image generation. I wanted to bring that power to my local workflow using Open WebUI, but it needed a little something extra. So, I created Flux Generator! This tool, built for Apple Silicon, offers comparable performance to Open WebUI and integrates with it seamlessly. Imagine the possibilities! My blog post walks you through the integration process: &lt;a href="https://voipnuggets.com/2025/02/18/flux-generator-local-image-generation-on-apple-silicon-with-open-webui-integration-using-flux-llm/"&gt;https://voipnuggets.com/2025/02/18/flux-generator-local-image-generation-on-apple-silicon-with-open-webui-integration-using-flux-llm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Want to give it a try? Grab the code from my repo: &lt;a href="https://github.com/voipnuggets/flux-generator"&gt;https://github.com/voipnuggets/flux-generator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm excited to hear about your experience with this tool!&lt;/p&gt; &lt;h1&gt;AI #ImageGeneration #AppleSilicon #OpenWebUI #StableDiffusion #FluxSchnell&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akashjss"&gt; /u/akashjss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/voipnuggets/flux-generator"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itcp0l/unleash_the_power_of_flux_schnell_on_your_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itcp0l/unleash_the_power_of_flux_schnell_on_your_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T18:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1it46dv</id>
    <title>AMD mi300x deployment and tests.</title>
    <updated>2025-02-19T12:19:53+00:00</updated>
    <author>
      <name>/u/Shivacious</name>
      <uri>https://old.reddit.com/user/Shivacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with system configurations to optimize the deployment of DeepSeek R1, focusing on enhancing throughput and response times. By fine-tuning the GIMM (GPU Interconnect Memory Management), I've achieved significant performance improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Throughput increase&lt;/strong&gt;: 30-40 tokens per second&lt;/li&gt; &lt;li&gt;&lt;strong&gt;With caching&lt;/strong&gt;: Up to 90 tokens per second for 20 concurrent 10k prompt requests&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;System Specifications&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;2x AMD EPYC 9664 (96 cores/192 threads each)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;Approximately 2TB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;8x AMD Instinct MI300X (connected via Infinity Fabric)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;analysis of gpu: &lt;a href="https://github.com/ShivamB25/analysis/blob/main/README.md"&gt;https://github.com/ShivamB25/analysis/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you guys want me to deploy any other model or make the endpoint public ? open to running it for a month.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shivacious"&gt; /u/Shivacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it46dv/amd_mi300x_deployment_and_tests/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it46dv/amd_mi300x_deployment_and_tests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it46dv/amd_mi300x_deployment_and_tests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T12:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1it8m6k</id>
    <title>Hugging Face open sourced the first course on FINE-TUNING for AGENTS</title>
    <updated>2025-02-19T15:49:36+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you follow these two hugging face courses you get an end to end programming in fine-tuning models specifically for agents.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;New Supervised Fine-tuning unit in the NLP Course, for general sft knowledge.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;New Fine-tuning for agents bonus module in the Agents Course, for agent specific stuff.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links in this post &lt;a href="https://huggingface.co/posts/burtenshaw/189514834246661"&gt;https://huggingface.co/posts/burtenshaw/189514834246661&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it8m6k/hugging_face_open_sourced_the_first_course_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it8m6k/hugging_face_open_sourced_the_first_course_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it8m6k/hugging_face_open_sourced_the_first_course_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T15:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iskklo</id>
    <title>PerplexityAI releases R1-1776, a DeepSeek-R1 finetune that removes Chinese censorship while maintaining reasoning capabilities</title>
    <updated>2025-02-18T19:01:54+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskklo/perplexityai_releases_r11776_a_deepseekr1/"&gt; &lt;img alt="PerplexityAI releases R1-1776, a DeepSeek-R1 finetune that removes Chinese censorship while maintaining reasoning capabilities" src="https://external-preview.redd.it/fBZy2Z7nV_YeGpu-AjWKc9CfcCYUjRJwHSvwj2-4VbI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0eb7d3b3713b1cc2f605e5e87e0a44f39b650f01" title="PerplexityAI releases R1-1776, a DeepSeek-R1 finetune that removes Chinese censorship while maintaining reasoning capabilities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/perplexity-ai/r1-1776"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iskklo/perplexityai_releases_r11776_a_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iskklo/perplexityai_releases_r11776_a_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-18T19:01:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1itdmwn</id>
    <title>Revolution in Biology: Evo-2, the AI Model that Creates Genomes from Scratch</title>
    <updated>2025-02-19T19:06:58+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, the Arc Institute and NVIDIA introduced Evo-2, a groundbreaking artificial intelligence (AI) model trained on 9.3 trillion DNA base pairs, covering the entire tree of life. The most impressive aspect of this development is that Evo-2 doesn't just analyze genomes, it creates them from scratch, generating complete DNA sequences, including mitochondrial, prokaryotic, and eukaryotic genomes.&lt;/p&gt; &lt;p&gt;This AI model, which could be compared to a DNA-focused language model, has the ability to understand and generate genetic sequences, even those non-coding regions previously considered &amp;quot;junk&amp;quot; DNA. Moreover, Evo-2 is capable of predicting disease-causing mutations, including some that are not yet fully understood, opening up new possibilities for precision medicine.m&lt;/p&gt; &lt;p&gt;&lt;a href="https://arcinstitute.org/manuscripts/Evo2"&gt;https://arcinstitute.org/manuscripts/Evo2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcinstitute/evo2_40b"&gt;https://huggingface.co/arcinstitute/evo2_40b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itdmwn/revolution_in_biology_evo2_the_ai_model_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itdmwn/revolution_in_biology_evo2_the_ai_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itdmwn/revolution_in_biology_evo2_the_ai_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:06:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1iteb64</id>
    <title>MeetingBuddy - local meeting transcriptions and summaries or you can use an openAI key. (Link in comments)</title>
    <updated>2025-02-19T19:33:03+00:00</updated>
    <author>
      <name>/u/psdwizzard</name>
      <uri>https://old.reddit.com/user/psdwizzard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteb64/meetingbuddy_local_meeting_transcriptions_and/"&gt; &lt;img alt="MeetingBuddy - local meeting transcriptions and summaries or you can use an openAI key. (Link in comments)" src="https://preview.redd.it/rmjrfuszd5ke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c3b8d6841eef1e0444183aab81855bcd2698536" title="MeetingBuddy - local meeting transcriptions and summaries or you can use an openAI key. (Link in comments)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/psdwizzard"&gt; /u/psdwizzard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rmjrfuszd5ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteb64/meetingbuddy_local_meeting_transcriptions_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iteb64/meetingbuddy_local_meeting_transcriptions_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1it0ocl</id>
    <title>R1-1776 Dynamic GGUFs by Unsloth</title>
    <updated>2025-02-19T08:24:48+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, we uploaded 2bit to 16bit GGUFs for R1-1776, Perplexity's new DeepSeek-R1 finetune that removes all censorship while maintaining reasoning capabilities: &lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF"&gt;https://huggingface.co/unsloth/r1-1776-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also upload Dynamic 2-bit, 3 and 4-bit versions and standard 3, 4, etc bit versions. The Dynamic 4-bit is even smaller than the medium one and achieves even higher accuracy. 1.58-bit and 1-bit will have to be done later as it relies on imatrix quants, which take more time.&lt;/p&gt; &lt;p&gt;Instructions to run the model are in the model card we provided. Do not forget about &lt;code&gt;&amp;lt;｜User｜&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt; tokens! - Or use a chat template formatter. Also do not forget about &lt;code&gt;&amp;lt;think&amp;gt;\n&lt;/code&gt;! Prompt format: &lt;code&gt;&amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python.&amp;lt;｜Assistant｜&amp;gt;&amp;lt;think&amp;gt;\n&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can also refer to our previous blog for 1.58-bit R1 GGUF for hints and results: &lt;a href="https://unsloth.ai/blog/r1-reasoning"&gt;https://unsloth.ai/blog/r1-reasoning&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;2-bit Dynamic&lt;/td&gt; &lt;td align="left"&gt;UD-Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;211GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/UD-Q2_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3-bit Dynamic&lt;/td&gt; &lt;td align="left"&gt;UD-Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;298.8GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/UD-Q3_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4-bit Dynamic&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;377.1GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/UD-Q4_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2-bit extra small&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;206.1GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;405GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/r1-1776-GGUF/tree/main/Q4_K_M"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And you can find the rest like 6-bit, 8-bit etc on the model card. Happy running!&lt;/p&gt; &lt;p&gt;P.S. we have a new update coming very soon which you guys will absolutely love! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ocl/r11776_dynamic_ggufs_by_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ocl/r11776_dynamic_ggufs_by_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it0ocl/r11776_dynamic_ggufs_by_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T08:24:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1it1xd7</id>
    <title>Train a Little(39M) Language Model</title>
    <updated>2025-02-19T09:55:23+00:00</updated>
    <author>
      <name>/u/RoyalMaterial9614</name>
      <uri>https://old.reddit.com/user/RoyalMaterial9614</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've started getting more into LLMs this year, looking for resources has always been easy as we can find blogs organizing everything into one place but simply understanding the model architecture is not enough to fully grasp how these models are trained. &lt;/p&gt; &lt;p&gt;As I couldn't find any code with recent architecture's implementation in one place, I've made my own.&lt;/p&gt; &lt;p&gt;My aim with this project is to help anyone who has basic understanding of transformer architectures but wants to train their own model from scratch with recent architectural changes. (I include the resources + my own notes along the way)&lt;/p&gt; &lt;p&gt;So this project is my effort for training a small language model i.e 39M parameter model from scratch that can converse well.&lt;/p&gt; &lt;p&gt;It was trained on 2xA100 for approx. 2.5 hours on ~8B tokens.&lt;/p&gt; &lt;p&gt;I plan to include everything in this project!!!!&lt;/p&gt; &lt;p&gt;Right now it includes a basic Llama-like architecture.&lt;/p&gt; &lt;p&gt;- RMSNorm instead of LayerNorm&lt;/p&gt; &lt;p&gt;- Rotary Positional Embedding instead of Absolute Positional Embedding&lt;/p&gt; &lt;p&gt;- SwiGLU activations instead of ReLU&lt;/p&gt; &lt;p&gt;- Grouped Query Attention instead of Multi-head Attention&lt;/p&gt; &lt;p&gt;- Implementation of KV cache&lt;/p&gt; &lt;p&gt;TODO inclues&lt;/p&gt; &lt;p&gt;- Finetuning using SFT and DPO&lt;/p&gt; &lt;p&gt;- Adding Mixture of Experts (MoE) architecture&lt;/p&gt; &lt;p&gt;- And much more&lt;/p&gt; &lt;p&gt;It would be great if anyone's is willing to contribute to this project.&lt;/p&gt; &lt;p&gt;Please find the project here: &lt;a href="https://github.com/CohleM/lilLM"&gt;https://github.com/CohleM/lilLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoyalMaterial9614"&gt; /u/RoyalMaterial9614 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it1xd7/train_a_little39m_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it1xd7/train_a_little39m_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it1xd7/train_a_little39m_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T09:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1itf0t0</id>
    <title>Official TabbyAPI Installation Guide - Step by Step Video</title>
    <updated>2025-02-19T20:01:22+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itf0t0/official_tabbyapi_installation_guide_step_by_step/"&gt; &lt;img alt="Official TabbyAPI Installation Guide - Step by Step Video" src="https://external-preview.redd.it/Tg8UUeXL52PR4YsWEsyoAqlexjxwlOT2OVmfjX_Whk4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b3fc399790a768e6d6ec57f38789dd0d051e5c2" title="Official TabbyAPI Installation Guide - Step by Step Video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=03jYz0ijbUU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itf0t0/official_tabbyapi_installation_guide_step_by_step/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itf0t0/official_tabbyapi_installation_guide_step_by_step/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T20:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1it4aw9</id>
    <title>Audio chat model came out. Anyone tried it? One of the metrics is RP.</title>
    <updated>2025-02-19T12:26:57+00:00</updated>
    <author>
      <name>/u/a_beautiful_rhind</name>
      <uri>https://old.reddit.com/user/a_beautiful_rhind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it4aw9/audio_chat_model_came_out_anyone_tried_it_one_of/"&gt; &lt;img alt="Audio chat model came out. Anyone tried it? One of the metrics is RP." src="https://external-preview.redd.it/UqpDdVhhcFGOCCh36p2Wt1DEREyb0kimRstI82rsrWk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92d3600b91ab5534a05b7683e4e45f623c80e511" title="Audio chat model came out. Anyone tried it? One of the metrics is RP." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_beautiful_rhind"&gt; /u/a_beautiful_rhind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-Audio-Chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it4aw9/audio_chat_model_came_out_anyone_tried_it_one_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it4aw9/audio_chat_model_came_out_anyone_tried_it_one_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T12:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1itfg77</id>
    <title>[TEST] Prompt Processing VS Inferense Speed VS GPU layers</title>
    <updated>2025-02-19T20:18:48+00:00</updated>
    <author>
      <name>/u/NickNau</name>
      <uri>https://old.reddit.com/user/NickNau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itfg77/test_prompt_processing_vs_inferense_speed_vs_gpu/"&gt; &lt;img alt="[TEST] Prompt Processing VS Inferense Speed VS GPU layers" src="https://preview.redd.it/acrxsw0el5ke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d242ada08454709d5206ff1f83db21e0f9241413" title="[TEST] Prompt Processing VS Inferense Speed VS GPU layers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickNau"&gt; /u/NickNau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/acrxsw0el5ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itfg77/test_prompt_processing_vs_inferense_speed_vs_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itfg77/test_prompt_processing_vs_inferense_speed_vs_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T20:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ite7vw</id>
    <title>Large Language Diffusion Models</title>
    <updated>2025-02-19T19:29:31+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.09992"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ite7vw/large_language_diffusion_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ite7vw/large_language_diffusion_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1itb38c</id>
    <title>LM Studio 0.3.10 with Speculative Decoding released</title>
    <updated>2025-02-19T17:26:57+00:00</updated>
    <author>
      <name>/u/BaysQuorv</name>
      <uri>https://old.reddit.com/user/BaysQuorv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Allegedly you can increase t/s significantly at no impact to quality, if you can find two models that work well (main model + draft model that is much smaller).&lt;/p&gt; &lt;p&gt;So it takes slightly more ram because you need the smaller model aswell, but &amp;quot;can speed up token generation by up to 1.5x-3x in some cases.&amp;quot;&lt;/p&gt; &lt;p&gt;Personally I have not found 2 MLX models compatible for my needs. I'm trying to run an 8b non-instruct llama model with a 1 or 3b draft model, but for some reason chat models are suprisingly hard to find for MLX and the ones Ive found don't work well together (decreased t/s). Have you found any two models that work well with this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaysQuorv"&gt; /u/BaysQuorv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itb38c/lm_studio_0310_with_speculative_decoding_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itb38c/lm_studio_0310_with_speculative_decoding_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itb38c/lm_studio_0310_with_speculative_decoding_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T17:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1it912t</id>
    <title>SOCAMM is not a rumours anymore</title>
    <updated>2025-02-19T16:06:07+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kwak No-jung, CEO of SK Hynix, have confirmed that they are working on the next memory standard, that NVIDIA previously where rumoured to develop for DIGITS and their AI PC's:&lt;/p&gt; &lt;p&gt;President Kwak also mentioned SOCAMM, a next-generation memory that connects HBM and Compute Express Link (CXL). SOCAMM is drawing attention as Nvidia's new memory standard for AI PCs.&lt;/p&gt; &lt;p&gt;President Kwak said, &amp;quot;As semiconductor applications are diversifying, applications are also diversifying, not just in their past forms. (SOCAMM) is one of the trends of this change, and customers will comprehensively consider cost and performance.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.mk.co.kr/en/it/11245259"&gt;https://www.mk.co.kr/en/it/11245259&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The details that was leaked before, is that NVIDIA have teamed up with SK hynix, Micron and Samsung, to develop the new standard called System On Chip Advanced Memory Module (SOCAMM).&lt;/p&gt; &lt;p&gt;It is said to be more cost-effective when compared to traditional DRAM that uses the SO-DIMM form-factor, and that it may place LPDDR5X memory directly onto the substrate, offering further power efficiency.&lt;/p&gt; &lt;p&gt;It is reported to feature a significant number of I/O ports when compared to other standards. SOCAMM has up to 694 I/O ports, LPCAMM's have 644 and traditional DRAM's 260.&lt;/p&gt; &lt;p&gt;One reason for the lack of details is that it seems like NVIDIA isn't making the standard in collaboration with the Joint Electron Device Engineering council (JEDEC).&lt;/p&gt; &lt;p&gt;More information will probably come soon enough, since prototypes have already been made and it is said that they are likely to start production in the later part of this year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it912t/socamm_is_not_a_rumours_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it912t/socamm_is_not_a_rumours_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it912t/socamm_is_not_a_rumours_anymore/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T16:06:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1itjbcm</id>
    <title>LM Studio - Hugging Face Model Manager</title>
    <updated>2025-02-19T22:58:34+00:00</updated>
    <author>
      <name>/u/ifioravanti</name>
      <uri>https://old.reddit.com/user/ifioravanti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My personal gift and sign of love for &lt;a href="https://x.com/huggingface"&gt;u/huggingface&lt;/a&gt; and LM Studio&lt;/p&gt; &lt;p&gt;A simple script to import models from HF Cache to LM Studio without using additional space 😎 just using symbolic links! We don't need 4TB local disk anymore!&lt;/p&gt; &lt;p&gt;Here link to the repo: &lt;a href="https://github.com/ivanfioravanti/lmstudio_hf"&gt;https://github.com/ivanfioravanti/lmstudio_hf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ifioravanti"&gt; /u/ifioravanti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itjbcm/lm_studio_hugging_face_model_manager/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itjbcm/lm_studio_hugging_face_model_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itjbcm/lm_studio_hugging_face_model_manager/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T22:58:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1itbszy</id>
    <title>New Yolo model - YOLOv12</title>
    <updated>2025-02-19T17:54:59+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itbszy/new_yolo_model_yolov12/"&gt; &lt;img alt="New Yolo model - YOLOv12" src="https://b.thumbs.redditmedia.com/V_ObLi__u5vnX0U1PVP3J8G1KDTrXfFx9UUpKZo6Xqk.jpg" title="New Yolo model - YOLOv12" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.arxiv.org/abs/2502.12524"&gt;[2502.12524] YOLOv12: Attention-Centric Real-Time Object Detectors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4z8mb9bow4ke1.png?width=607&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0287cc06b6cd4aaaf1721592927b61e7f692d84a"&gt;https://preview.redd.it/4z8mb9bow4ke1.png?width=607&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0287cc06b6cd4aaaf1721592927b61e7f692d84a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itbszy/new_yolo_model_yolov12/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itbszy/new_yolo_model_yolov12/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itbszy/new_yolo_model_yolov12/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T17:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1itdy0k</id>
    <title>No system instructions for DeepSeek makes Jake oddly self aware. But anyway, got DeepSeek working locally with Unity</title>
    <updated>2025-02-19T19:18:54+00:00</updated>
    <author>
      <name>/u/Aikodex3D</name>
      <uri>https://old.reddit.com/user/Aikodex3D</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itdy0k/no_system_instructions_for_deepseek_makes_jake/"&gt; &lt;img alt="No system instructions for DeepSeek makes Jake oddly self aware. But anyway, got DeepSeek working locally with Unity" src="https://external-preview.redd.it/bnFnMDByNG5iNWtlMW12blNfQLF_g4OMKxQvzt-tAZaceY72pEZjInNM7LQL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=110e6a9d041b8ce5ff38c14d097b0dc020a3dc4f" title="No system instructions for DeepSeek makes Jake oddly self aware. But anyway, got DeepSeek working locally with Unity" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aikodex3D"&gt; /u/Aikodex3D &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/glxnes4nb5ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itdy0k/no_system_instructions_for_deepseek_makes_jake/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itdy0k/no_system_instructions_for_deepseek_makes_jake/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1isxhoy</id>
    <title>New laptops with AMD chips have 128 GB unified memory (up to 96 GB of which can be assigned as VRAM)</title>
    <updated>2025-02-19T04:58:16+00:00</updated>
    <author>
      <name>/u/zxyzyxz</name>
      <uri>https://old.reddit.com/user/zxyzyxz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxhoy/new_laptops_with_amd_chips_have_128_gb_unified/"&gt; &lt;img alt="New laptops with AMD chips have 128 GB unified memory (up to 96 GB of which can be assigned as VRAM)" src="https://external-preview.redd.it/rA82auRK9iN2YS0cZz7VCIDRx_3W3gxSA4bgLmUO0L8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f52aaef3d5bfe41e70571f92b4c1d36cf9abcc18" title="New laptops with AMD chips have 128 GB unified memory (up to 96 GB of which can be assigned as VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zxyzyxz"&gt; /u/zxyzyxz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=IVbm2a6lVBo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isxhoy/new_laptops_with_amd_chips_have_128_gb_unified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isxhoy/new_laptops_with_amd_chips_have_128_gb_unified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T04:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1itc4hp</id>
    <title>Defending Open Source AI Against the Monopolist, the Jingoist, the Doomer and the Idiot</title>
    <updated>2025-02-19T18:07:18+00:00</updated>
    <author>
      <name>/u/PataFunction</name>
      <uri>https://old.reddit.com/user/PataFunction</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc4hp/defending_open_source_ai_against_the_monopolist/"&gt; &lt;img alt="Defending Open Source AI Against the Monopolist, the Jingoist, the Doomer and the Idiot" src="https://external-preview.redd.it/8rOw0WWweYgxygKbe5fxUQ-d38ZXq4mjYXqeNVPHsPs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e804fb26cbbe5364fe905c12bdf9215a979580c1" title="Defending Open Source AI Against the Monopolist, the Jingoist, the Doomer and the Idiot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PataFunction"&gt; /u/PataFunction &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://danieljeffries.substack.com/p/defending-open-source-ai-against"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc4hp/defending_open_source_ai_against_the_monopolist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itc4hp/defending_open_source_ai_against_the_monopolist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T18:07:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1isu4un</id>
    <title>o3-mini won the poll! We did it guys!</title>
    <updated>2025-02-19T02:06:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"&gt; &lt;img alt="o3-mini won the poll! We did it guys!" src="https://preview.redd.it/ogpvvrth70ke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=441716ec57e99b756f365455cea717ed23f4f00b" title="o3-mini won the poll! We did it guys!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted a lot here yesterday to vote for the o3-mini. Thank you all!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ogpvvrth70ke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1isu4un/o3mini_won_the_poll_we_did_it_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T02:06:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1it36b0</id>
    <title>Gemini 2.0 is shockingly good at transcribing audio with Speaker labels, timestamps to the second;</title>
    <updated>2025-02-19T11:18:31+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it36b0/gemini_20_is_shockingly_good_at_transcribing/"&gt; &lt;img alt="Gemini 2.0 is shockingly good at transcribing audio with Speaker labels, timestamps to the second;" src="https://preview.redd.it/d3bl014yx2ke1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddda4efa5606005673aaca2845b18430aa309c24" title="Gemini 2.0 is shockingly good at transcribing audio with Speaker labels, timestamps to the second;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d3bl014yx2ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1it36b0/gemini_20_is_shockingly_good_at_transcribing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1it36b0/gemini_20_is_shockingly_good_at_transcribing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T11:18:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1iteeqv</id>
    <title>New Wayfarer Large Model: a brutally challenging roleplay model trained to let you fail and die, now with better data and a larger base.</title>
    <updated>2025-02-19T19:36:59+00:00</updated>
    <author>
      <name>/u/Nick_AIDungeon</name>
      <uri>https://old.reddit.com/user/Nick_AIDungeon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tired of AI models that coddle you with sunshine and rainbows? We heard you loud and clear. Last month, we shared Wayfarer (based on Nemo 12b), an open-source model that embraced death, danger, and gritty storytelling. The response was overwhelming—so we doubled down with Wayfarer Large.&lt;/p&gt; &lt;p&gt;Forged from Llama 3.3 70b Instruct, this model didn’t get the memo about being “nice.” We trained it to weave stories with teeth—danger, heartbreak, and the occasional untimely demise. While other AIs play it safe, Wayfarer Large thrives on risk, ruin, and epic stakes. We tested it on AI Dungeon a few weeks back, and players immediately became obsessed.&lt;/p&gt; &lt;p&gt;We’ve decided to open-source this model as well so anyone can experience unforgivingly brutal AI adventures!&lt;/p&gt; &lt;p&gt;Would love to hear your feedback as we plan to continue to improve and open source similar models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LatitudeGames/Wayfarer-Large-70B-Llama-3.3"&gt;https://huggingface.co/LatitudeGames/Wayfarer-Large-70B-Llama-3.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Or if you want to try this model without running it yourself, you can do so at &lt;a href="https://aidungeon.com"&gt;&lt;strong&gt;https://aidungeon.com&lt;/strong&gt;&lt;/a&gt; (Wayfarer Large requires a subscription while Wayfarer Small is free).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nick_AIDungeon"&gt; /u/Nick_AIDungeon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteeqv/new_wayfarer_large_model_a_brutally_challenging/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteeqv/new_wayfarer_large_model_a_brutally_challenging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iteeqv/new_wayfarer_large_model_a_brutally_challenging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:36:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iteaew</id>
    <title>Google releases PaliGemma 2 mix - a VLM for many tasks</title>
    <updated>2025-02-19T19:32:14+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Gemma tech lead over here :)&lt;/p&gt; &lt;p&gt;Today, we released a new model, PaliGemma 2 mix! It's the same architecture as PaliGemma 2, but these are some checkpoints that work well for a bunch of tasks without having to fine-tune it.&lt;/p&gt; &lt;p&gt;Some links first&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Official Google blog &lt;a href="https://developers.googleblog.com/en/introducing-paligemma-2-mix/?linkId=13028688"&gt;https://developers.googleblog.com/en/introducing-paligemma-2-mix/?linkId=13028688&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The Hugging Face blog &lt;a href="https://huggingface.co/blog/paligemma2mix"&gt;https://huggingface.co/blog/paligemma2mix&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Open models in &lt;a href="https://huggingface.co/collections/google/paligemma-2-mix-67ac6a251aaf3ee73679dcc4"&gt;https://huggingface.co/collections/google/paligemma-2-mix-67ac6a251aaf3ee73679dcc4&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Free demo to try out &lt;a href="https://huggingface.co/spaces/google/paligemma2-10b-mix"&gt;https://huggingface.co/spaces/google/paligemma2-10b-mix&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So what can this model do?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Image captioning (both short and long captions)&lt;/li&gt; &lt;li&gt;OCR&lt;/li&gt; &lt;li&gt;Question answering&lt;/li&gt; &lt;li&gt;Object detection&lt;/li&gt; &lt;li&gt;Image segmentation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So you can use the model for localization, image understanding, document understanding, and more! And as always, if you want even better results for your task, you can pick the base models and fine-tune them. The goal of this release was to showcase what can be done with PG2, which is a very good model for fine-tuning.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteaew/google_releases_paligemma_2_mix_a_vlm_for_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iteaew/google_releases_paligemma_2_mix_a_vlm_for_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iteaew/google_releases_paligemma_2_mix_a_vlm_for_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T19:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1itc3h7</id>
    <title>Training LLM on 1000s of GPUs made simple</title>
    <updated>2025-02-19T18:06:09+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc3h7/training_llm_on_1000s_of_gpus_made_simple/"&gt; &lt;img alt="Training LLM on 1000s of GPUs made simple" src="https://preview.redd.it/2wk7ntxpy4ke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04be9bf215a025ca522b0d41193331c0824a527c" title="Training LLM on 1000s of GPUs made simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2wk7ntxpy4ke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itc3h7/training_llm_on_1000s_of_gpus_made_simple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itc3h7/training_llm_on_1000s_of_gpus_made_simple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-19T18:06:09+00:00</published>
  </entry>
</feed>
