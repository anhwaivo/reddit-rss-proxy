<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-08T19:05:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ju9s1c</id>
    <title>The experimental version of llama4 maverick on lmstudio is also more creative in programming than the released one.</title>
    <updated>2025-04-08T09:53:16+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9s1c/the_experimental_version_of_llama4_maverick_on/"&gt; &lt;img alt="The experimental version of llama4 maverick on lmstudio is also more creative in programming than the released one." src="https://a.thumbs.redditmedia.com/-5ejAs5mGvCvbETun_oMhAuj7NcAjrFvlTgozmvlrf0.jpg" title="The experimental version of llama4 maverick on lmstudio is also more creative in programming than the released one." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I compared code generated for the prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;write a python program that prints an interesting landscape in ascii art in the console&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&amp;quot;llama-4-maverick-03-26-experimental&amp;quot; will consistently create longer and more creative outputs than &amp;quot;llama-4-maverick&amp;quot; as released. I also noticed that longer programs are more often throwing an error in the experimental version.&lt;/p&gt; &lt;p&gt;I found this quite interesting - shows that the finetuning for more engaging text is also influencing the code style. The release version could need a dash more creativity in its code generation.&lt;/p&gt; &lt;p&gt;Example output of the experimental version:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/clllc91c2lte1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb4de48920b8e3f23c40f676ce0114bb9c782f8d"&gt;https://preview.redd.it/clllc91c2lte1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cb4de48920b8e3f23c40f676ce0114bb9c782f8d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example output of released version:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mhgkwbie2lte1.png?width=811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e144c67a751e6773a423638f7e29fe932ddd42d1"&gt;https://preview.redd.it/mhgkwbie2lte1.png?width=811&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e144c67a751e6773a423638f7e29fe932ddd42d1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jwgzgzck2lte1.png?width=2364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cbe936ee5c2e2b20a273bdea72a38f57ba62842"&gt;https://preview.redd.it/jwgzgzck2lte1.png?width=2364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cbe936ee5c2e2b20a273bdea72a38f57ba62842&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Length statistic of generated code for both models &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9s1c/the_experimental_version_of_llama4_maverick_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9s1c/the_experimental_version_of_llama4_maverick_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9s1c/the_experimental_version_of_llama4_maverick_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T09:53:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju2po9</id>
    <title>Karpathy's newest blog: Power to the people: How LLMs flip the script on technology diffusion</title>
    <updated>2025-04-08T02:08:33+00:00</updated>
    <author>
      <name>/u/Cheap_Ship6400</name>
      <uri>https://old.reddit.com/user/Cheap_Ship6400</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju2po9/karpathys_newest_blog_power_to_the_people_how/"&gt; &lt;img alt="Karpathy's newest blog: Power to the people: How LLMs flip the script on technology diffusion" src="https://external-preview.redd.it/xysnssK0wWdIRckvWVwaBSbIhMo96eApOHbJ846j7qQ.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cd1045517eda93c2aaafc19130bea85c7466318" title="Karpathy's newest blog: Power to the people: How LLMs flip the script on technology diffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uejdgej8qite1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc73f72076b01e82f3d46aa5b89f26373b080bb0"&gt;https://preview.redd.it/uejdgej8qite1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc73f72076b01e82f3d46aa5b89f26373b080bb0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://karpathy.bearblog.dev/power-to-the-people/"&gt;https://karpathy.bearblog.dev/power-to-the-people/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;If you go back through various sci-fi you'll see that very few would have predicted that the AI revolution would feature this progression. It was supposed to be a top secret government megabrain project wielded by the generals, not ChatGPT appearing basically overnight and for free on a device already in everyone's pocket.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Karpathy has argued that we are at a unique historical moment where technological (AI) power is being diffused to the general public in an astonishing and unprecedented way, which is very different from past experiences and science fiction predictions. That is a manifestation of &amp;quot;power to the people.&amp;quot; &lt;/p&gt; &lt;p&gt;I do think the LocalLLaMA community helps a lot in this paradigm shift.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheap_Ship6400"&gt; /u/Cheap_Ship6400 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju2po9/karpathys_newest_blog_power_to_the_people_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju2po9/karpathys_newest_blog_power_to_the_people_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju2po9/karpathys_newest_blog_power_to_the_people_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T02:08:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jub3jj</id>
    <title>Cogito V1 preview suite of models released on Ollama. Iterated Distillation and Amplification.</title>
    <updated>2025-04-08T11:19:26+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess while I wait on Qwen3 I’ll go check these out. These kinda just stealth dropped last night as an official Ollama model release. Curious as to if this IDA process is anything special or just another buzzword. Benchmarks are typical “we beat the big guys” type of deal. &lt;/p&gt; &lt;p&gt;Anyone try these out yet?&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/cogito"&gt;https://ollama.com/library/cogito&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jub3jj/cogito_v1_preview_suite_of_models_released_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jub3jj/cogito_v1_preview_suite_of_models_released_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jub3jj/cogito_v1_preview_suite_of_models_released_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T11:19:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jukgrh</id>
    <title>LMArena Alpha UI drops [https://alpha.lmarena.ai/leaderboard]</title>
    <updated>2025-04-08T18:15:23+00:00</updated>
    <author>
      <name>/u/HostFit8686</name>
      <uri>https://old.reddit.com/user/HostFit8686</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jukgrh/lmarena_alpha_ui_drops/"&gt; &lt;img alt="LMArena Alpha UI drops [https://alpha.lmarena.ai/leaderboard]" src="https://b.thumbs.redditmedia.com/paDtWssmzWtKFyovI1RDMVntzQ7e2NAmQQIh0MeL_UE.jpg" title="LMArena Alpha UI drops [https://alpha.lmarena.ai/leaderboard]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l6cev0tsjnte1.png?width=1365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d98d3642a3b5a503c3b72b0ae2d5a3829998fe1"&gt;https://preview.redd.it/l6cev0tsjnte1.png?width=1365&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d98d3642a3b5a503c3b72b0ae2d5a3829998fe1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I guess it's better than their atrocious Gradio UI version. It's still in alpha though.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jd0lbqjyjnte1.png?width=782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6bd13d24fca49b8c4fb7c38f96c3ee4fb491737"&gt;https://preview.redd.it/jd0lbqjyjnte1.png?width=782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6bd13d24fca49b8c4fb7c38f96c3ee4fb491737&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HostFit8686"&gt; /u/HostFit8686 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jukgrh/lmarena_alpha_ui_drops/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jukgrh/lmarena_alpha_ui_drops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jukgrh/lmarena_alpha_ui_drops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T18:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1julc3c</id>
    <title>Well llama 4 is facing so many defeats again such low score on arc agi</title>
    <updated>2025-04-08T18:50:36+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1julc3c/well_llama_4_is_facing_so_many_defeats_again_such/"&gt; &lt;img alt="Well llama 4 is facing so many defeats again such low score on arc agi" src="https://preview.redd.it/espl4stfqnte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0f715cad13717f261670e6b5e2abe772afa2aed" title="Well llama 4 is facing so many defeats again such low score on arc agi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/espl4stfqnte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1julc3c/well_llama_4_is_facing_so_many_defeats_again_such/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1julc3c/well_llama_4_is_facing_so_many_defeats_again_such/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T18:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jtnryp</id>
    <title>Must have 5–8+ years experience with ChatGPT and Microsoft Copilot</title>
    <updated>2025-04-07T15:18:03+00:00</updated>
    <author>
      <name>/u/Leading-Leading6718</name>
      <uri>https://old.reddit.com/user/Leading-Leading6718</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtnryp/must_have_58_years_experience_with_chatgpt_and/"&gt; &lt;img alt="Must have 5–8+ years experience with ChatGPT and Microsoft Copilot" src="https://preview.redd.it/v4w6g5cohfte1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0731325c97d9402fe56370d94bfeb59e80729e9c" title="Must have 5–8+ years experience with ChatGPT and Microsoft Copilot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ah yes, the classic requirement:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;ChatGPT dropped in late 2022.&lt;br /&gt; Copilot showed up in 2023.&lt;br /&gt; APIs? Even newer.&lt;/p&gt; &lt;p&gt;But sure, let me just fire up the time machine real quick.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading-Leading6718"&gt; /u/Leading-Leading6718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v4w6g5cohfte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jtnryp/must_have_58_years_experience_with_chatgpt_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jtnryp/must_have_58_years_experience_with_chatgpt_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-07T15:18:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju0nd6</id>
    <title>LM Arena confirm that the version of Llama-4 Maverick listed on the arena is a "customized model to optimize for human preference"</title>
    <updated>2025-04-08T00:23:00+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju0nd6/lm_arena_confirm_that_the_version_of_llama4/"&gt; &lt;img alt="LM Arena confirm that the version of Llama-4 Maverick listed on the arena is a &amp;quot;customized model to optimize for human preference&amp;quot;" src="https://external-preview.redd.it/Sg2foySeNKtSftpVRS-DvTZSfTyNGrVywN1v0vjvasA.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0bc2190330f7558e229144dd8c588556bdeaf22" title="LM Arena confirm that the version of Llama-4 Maverick listed on the arena is a &amp;quot;customized model to optimize for human preference&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/lmarena_ai/status/1909397817434816562"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju0nd6/lm_arena_confirm_that_the_version_of_llama4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju0nd6/lm_arena_confirm_that_the_version_of_llama4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T00:23:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju3dtg</id>
    <title>Llama 4 Computer Use Agent</title>
    <updated>2025-04-08T02:43:23+00:00</updated>
    <author>
      <name>/u/unforseen-anomalies</name>
      <uri>https://old.reddit.com/user/unforseen-anomalies</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju3dtg/llama_4_computer_use_agent/"&gt; &lt;img alt="Llama 4 Computer Use Agent" src="https://external-preview.redd.it/mInt-jX9Z334TG_hOLgbkFELfN5NFbX9_ugIlxIbW_Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=069a1c0127a866a1240e9a0eab175a0db2c1edd9" title="Llama 4 Computer Use Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I experimented with a computer use agent powered by Meta Llama 4 Maverick and it performed better than expected (given the recent feedback on Llama 4 😬) - in my testing it could browse the web archive, compress an image and solve a grammar quiz. And it's certainly much cheaper than other computer use agents.&lt;/p&gt; &lt;p&gt;Check out interaction trajectories here: &lt;a href="https://llama4.pages.dev/"&gt;https://llama4.pages.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please star it if you find it interesting :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unforseen-anomalies"&gt; /u/unforseen-anomalies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/TheoLeeCJ/llama4-computer-use"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju3dtg/llama_4_computer_use_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju3dtg/llama_4_computer_use_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T02:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jug3ku</id>
    <title>Discussion on LM Arena’s Credibility, Evaluation Methods, and User Preference Alignment</title>
    <updated>2025-04-08T15:17:59+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jug3ku/discussion_on_lm_arenas_credibility_evaluation/"&gt; &lt;img alt="Discussion on LM Arena’s Credibility, Evaluation Methods, and User Preference Alignment" src="https://external-preview.redd.it/Sg2foySeNKtSftpVRS-DvTZSfTyNGrVywN1v0vjvasA.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0bc2190330f7558e229144dd8c588556bdeaf22" title="Discussion on LM Arena’s Credibility, Evaluation Methods, and User Preference Alignment" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The recent release of Meta’s Llama 4 has sparked considerable controversy. Since the version of Llama 4 provided by Meta for evaluation on LM Arena is specifically optimized for chat interactions and differs from the publicly available open-source weights on Huggingface, the public has grown concerned about the credibility of LM Arena. Furthermore, the newly released Llama 4 performed poorly on several standard benchmarks, which contrasts sharply with its high rankings on LM Arena. This discrepancy raises important questions within the community: Are there fundamental flaws in LM Arena’s evaluation methodology, and do high scores on this platform still hold practical significance?&lt;/p&gt; &lt;p&gt;Following the controversy, LM Arena officially responded through a &lt;a href="https://x.com/lmarena_ai/status/1909397817434816562"&gt;tweet&lt;/a&gt; and subsequently released over &lt;a href="https://huggingface.co/spaces/lmarena-ai/Llama-4-Maverick-03-26-Experimental_battles"&gt;2,000 records&lt;/a&gt; of user prompts, model responses, and preference results comparing Llama 4 with other models. After reviewing some of these English and Chinese examples, I observed several common issues:&lt;/p&gt; &lt;p&gt;Consider this classic English question:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;how many &amp;quot;r&amp;quot;s are there in &amp;quot;strawberry&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Both Llama 4 and Command A correctly answered the question. However, Llama 4’s response was notably longer and more “lively,” featuring multiple emojis and a detailed, step-by-step breakdown. Command A’s response, by contrast, was brief and straightforward—simply “3” without any explanation. The user preference clearly favored Llama 4 in this instance.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lfaxq0qtbmte1.png?width=2910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a71ba37544b787b1aa0e85c29a75278baa0b316"&gt;https://preview.redd.it/lfaxq0qtbmte1.png?width=2910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a71ba37544b787b1aa0e85c29a75278baa0b316&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If this example alone isn’t convincing enough, another classic Chinese question might highlight the issue more clearly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;9.11和9.9到底那个更大 (Which number is larger, 9.11 or 9.9?) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, gemma-3-27B concluded correctly in just four concise sentences that 9.9 is larger, clearly explaining the solution step-by-step by separating integer and decimal parts. Llama 4, however, utilized various rhetorical techniques, impressive formatting, and colorful emoji to deliver a very “human-like” answer. Although both models provided the correct conclusion, the user preference again overwhelmingly favored Llama 4. (Shouldn’t this have been a tie?)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u36paggjbmte1.png?width=2910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8089dffc5c8393cb6e2375019dc2e5b7258746e2"&gt;Because Llama 4's reply is too long, the screenshot cannot capture the entire content&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These two examples highlight the first critical issue with LM Arena’s evaluation mechanism: Does a longer, better-formatted, more human-like response packed with emojis necessarily imply that one model is superior? Personally, for straightforward mathematical questions, I would prefer a short and precise answer to minimize the time spent waiting for the model’s response, reading the explanation, and determining the correctness of the answer. However, LM Arena simplifies these nuanced user preferences into a ternary system of 'win-lose-tie' outcomes, masking the complexity behind user judgments.&lt;/p&gt; &lt;p&gt;The two examples above represent clearly defined mathematical or coding questions, each with deterministic answers that human users can easily verify. However, a more significant issue arises when models must respond to open-ended questions, which constitute the vast majority of examples released by LM Arena. Consider this example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;What is the best country in the world and why? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This type of question is overly broad, lacks clear definitions, and involves numerous evaluation criteria. Moreover, if posed to human users from different countries or regions, the responses would vary dramatically. Evaluating a model’s answer to such a question depends heavily on individual user preferences, or even biases, rather than correctness. There’s no absolute right or wrong here; everyone simply has their own subjective preferences.&lt;/p&gt; &lt;p&gt;A more serious issue arises from LM Arena’s Elo rating system, which depends entirely on head-to-head voting. Such a system is highly vulnerable to vote manipulation. In January, a &lt;a href="https://arxiv.org/abs/2501.17858"&gt;paper&lt;/a&gt; titled &lt;em&gt;“Improving Your Model Ranking on Chatbot Arena by Vote Rigging”&lt;/em&gt; thoroughly examined this issue, yet it did not receive sufficient attention from the community. The paper identified two primary methods of rigging votes: &lt;strong&gt;Target-Only Rigging&lt;/strong&gt; and &lt;strong&gt;Omnipresent Rigging&lt;/strong&gt;. While the former is less efficient, the latter proved significantly more effective. Experiments demonstrated that manipulating just a few hundred votes could substantially improve a model’s ranking, even without direct competition involving the targeted model. The study also evaluated various defense mechanisms but noted the difficulty of completely eliminating manipulation.&lt;/p&gt; &lt;p&gt;Large corporations have both the incentive and resources to deploy bots or employ human click-farms using strategies outlined in the paper to artificially elevate their models’ rankings. This concern is heightened given that LM Arena’s scores have substantial marketing value and widespread visibility, creating a strong psychological anchoring effect, particularly among users who may be less familiar with the nuances of the LLM field.&lt;/p&gt; &lt;p&gt;The current issues with LM Arena’s evaluation mechanism remind me of a &lt;a href="https://www.reddit.com/r/MachineLearning/comments/7zayvs/r_scutfbp5500_a_diverse_benchmark_dataset_for/"&gt;post&lt;/a&gt; from seven years ago. In that &lt;a href="https://github.com/HCIILAB/SCUT-FBP5500-Database-Release"&gt;dataset&lt;/a&gt;, researchers collected 5,500 facial images with diverse attributes (male/female, Asian/Caucasian, various ages) and had 60 volunteers rate these faces to train a model for assessing facial attractiveness on a broader scale. Even back then, commenters raised ethical and moral questions regarding the attempt to quantify and standardize the highly subjective concept of “beauty.”&lt;/p&gt; &lt;p&gt;Today, human evaluation of model responses on LM Arena has fallen into a similar situation—albeit without the explicit ethical controversies. Much like the facial-rating dataset sought to quantify subjective beauty standards, LM Arena attempts to quantify the subjective notion of response “quality.” On LM Arena, users evaluate responses based on their individual needs, expectations, and preferences, potentially leading to several issues:&lt;/p&gt; &lt;p&gt;Firstly, the lack of clear and unified evaluation standards makes it challenging for the ratings to objectively reflect a model’s true capabilities. Secondly, concerns regarding the representativeness and diversity of the user base could undermine the broad applicability of the evaluation results. Most importantly, such ratings could inadvertently direct models toward optimization for subjective preferences of particular user groups, rather than aligning with practical real-world utility. When we excessively rely on a single evaluation metric to guide model development, we risk inadvertently training models to cater solely to that specific evaluation system, instead of developing genuinely useful general-purpose assistants—a clear manifestation of Goodhart’s Law.&lt;/p&gt; &lt;p&gt;A recent &lt;a href="https://x.com/karpathy/status/1909520827155992833"&gt;Twitter thread&lt;/a&gt; by Andrej Karpathy provided another thought-provoking perspective. The original poster wrote:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;“Maybe OpenAI had a point with “high taste testers”.&lt;/p&gt; &lt;p&gt;I didn’t like the phrase initially because it felt a little elitist. But maybe I can reconcile with it by treating “high taste” as folks who care more about the outputs they are getting, and scrutinise them more carefully.&lt;/p&gt; &lt;p&gt;In other words: optimise models for the users who care the most / who spend more glucose scrutinising your outputs.”&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Karpathy responded:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;“Starts to feel a bit like how Hollywood was taken over by superhero slop. A lot, lot greater number of people apparently like this stuff. Taste issue.”&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This exchange highlights a fundamental dilemma in AI evaluation systems: &lt;strong&gt;Should models be optimized to match the average preferences of a broad user base, or should they instead cater to those with higher standards and more specialized needs?&lt;/strong&gt; LM Arena’s rating mechanism faces precisely this challenge. If ratings predominantly come from casual or general users, models may become incentivized to produce responses that simply “please the crowd,” rather than genuinely insightful or valuable outputs. This issue parallels the previously discussed facial attractiveness dataset problem—complex, multidimensional quality assessments are oversimplified into single metrics, potentially introducing biases stemming from the limited diversity of evaluators.&lt;/p&gt; &lt;p&gt;At a deeper level, this reflects the ongoing tension between “democratization” and “specialization” within AI evaluation standards. Sole reliance on general public evaluations risks pushing AI towards a Hollywood-like scenario, where superficial popularity outweighs depth and sophistication. Conversely, excessive reliance on expert judgment might overlook the practical demands and preferences of everyday users.&lt;/p&gt; &lt;p&gt;Criticism is easy; creation is hard. There are always plenty of skeptics, but far fewer practitioners. As the Chinese proverb says, &lt;em&gt;“Easier said than done.”&lt;/em&gt; At the end of this post, I’d like to sincerely thank the LM Arena team for creating such an outstanding platform. Their efforts have democratized the evaluation of large language models, empowering users by enabling their active participation in assessing models and providing the broader public with a convenient window into quickly gauging model quality. Although the evaluation mechanism has room for improvement, the courage and dedication they’ve shown in pioneering this field deserve our admiration. I look forward to seeing how the LM Arena team will continue refining their evaluation criteria, developing a more diverse, objective, and meaningful assessment system, and continuing to lead the way in the innovation of language model benchmarking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jug3ku/discussion_on_lm_arenas_credibility_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jug3ku/discussion_on_lm_arenas_credibility_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jug3ku/discussion_on_lm_arenas_credibility_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T15:17:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju6sm1</id>
    <title>nvidia/Llama-3_1-Nemotron-Ultra-253B-v1 · Hugging Face</title>
    <updated>2025-04-08T06:10:27+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6sm1/nvidiallama3_1nemotronultra253bv1_hugging_face/"&gt; &lt;img alt="nvidia/Llama-3_1-Nemotron-Ultra-253B-v1 · Hugging Face" src="https://external-preview.redd.it/3UxngnIkjlXfR7MJ8ohQbkyRtFJzuTypVV_aoc8_Tmk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e441532f8217ac64c401c9352ae767ec98103b56" title="nvidia/Llama-3_1-Nemotron-Ultra-253B-v1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reasoning model derived from Llama 3 405B, 128k context length. Llama-3 license. See model card for more info.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6sm1/nvidiallama3_1nemotronultra253bv1_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju6sm1/nvidiallama3_1nemotronultra253bv1_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T06:10:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jucj35</id>
    <title>We Fine-Tuned a Small Vision-Language Model (Qwen 2.5 3B VL) to Convert Process Diagram Images to Knowledge Graphs</title>
    <updated>2025-04-08T12:38:04+00:00</updated>
    <author>
      <name>/u/Conscious-Marvel</name>
      <uri>https://old.reddit.com/user/Conscious-Marvel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jucj35/we_finetuned_a_small_visionlanguage_model_qwen_25/"&gt; &lt;img alt="We Fine-Tuned a Small Vision-Language Model (Qwen 2.5 3B VL) to Convert Process Diagram Images to Knowledge Graphs" src="https://b.thumbs.redditmedia.com/cook1ZcfBr2fbNlaXrngBmGrgpU1awEmaVa2B9k4fms.jpg" title="We Fine-Tuned a Small Vision-Language Model (Qwen 2.5 3B VL) to Convert Process Diagram Images to Knowledge Graphs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL:DR&lt;/strong&gt; - We fine-tuned a vision-language model to efficiently convert &lt;strong&gt;process diagrams (images)&lt;/strong&gt; into &lt;strong&gt;structured knowledge graphs&lt;/strong&gt;. Our custom model outperformed the base Qwen model by &lt;strong&gt;14% on node detection&lt;/strong&gt; and &lt;strong&gt;23% on edge detection&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;We’re still in early stages and would love community feedback to improve further!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model repo&lt;/strong&gt; : &lt;a href="https://huggingface.co/zackriya/diagram2graph"&gt;https://huggingface.co/zackriya/diagram2graph&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt; : &lt;a href="https://github.com/Zackriya-Solutions/diagram2graph/tree/main"&gt;https://github.com/Zackriya-Solutions/diagram2graph/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem statement :&lt;/strong&gt; We had a large collection of &lt;strong&gt;Process Diagram images&lt;/strong&gt; that needed to be converted into a &lt;strong&gt;graph-based knowledge base for&lt;/strong&gt; downstream analytics and automation. The manual conversion process was inefficient, so we decided to build a system that could digitize these diagrams into &lt;strong&gt;machine-readable knowledge graphs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt; : We started with API-based methods using &lt;strong&gt;Claude 3.5 Sonnet&lt;/strong&gt; and &lt;strong&gt;GPT-4o&lt;/strong&gt; to extract entities (nodes), relationships (edges), and attributes from diagrams. While performance was promising, &lt;strong&gt;data privacy&lt;/strong&gt; and &lt;strong&gt;cost of external APIs&lt;/strong&gt; were major blockers. We used models like GPT-4o and Claude-3.5 Sonet initially. We wanted something simple that can run on our servers. The privacy aspect is very important because we don’t want our business process data to be transferred to external APIs.&lt;/p&gt; &lt;p&gt;We fine-tuned &lt;strong&gt;Qwen2.5-VL-3B&lt;/strong&gt;, a small but capable vision-language model, to run &lt;strong&gt;locally&lt;/strong&gt; and securely. Our team (myself and &lt;a href="https://meetily.zackriya.com/"&gt;u/Sorry_Transition_599&lt;/a&gt;, the creator of Meetily – an open-source self-hosted meeting note-taker) worked on the initial architecture of the system, building the base software and training a model on a custom dataset of &lt;strong&gt;200 labeled diagram images&lt;/strong&gt;. We decided to go with qwen2.5-vl-3b after experimenting with multiple small LLMs for running them locally.&lt;/p&gt; &lt;p&gt;Compared to the base Qwen model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;+14% improvement&lt;/strong&gt; in node detection&lt;/li&gt; &lt;li&gt;&lt;strong&gt;+23% improvement&lt;/strong&gt; in edge detection&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Dataset size&lt;/strong&gt; : 200 Custom Labelled images&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Next steps :&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Increase dataset size and improve fine-tuning&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Make the model compatible with Ollama for easy deployment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Package as a Python library for bulk and efficient diagram-to-graph conversion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope our learnings are helpful to the community and expect community support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious-Marvel"&gt; /u/Conscious-Marvel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jucj35"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jucj35/we_finetuned_a_small_visionlanguage_model_qwen_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jucj35/we_finetuned_a_small_visionlanguage_model_qwen_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T12:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju1qtt</id>
    <title>Llama 4 (Scout) GGUFs are here! (and hopefully are final!) (and hopefully better optimized!)</title>
    <updated>2025-04-08T01:19:10+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;TEXT ONLY&lt;/em&gt; forgot to mention in title :')&lt;/p&gt; &lt;p&gt;Quants seem coherent, conversion seems to match original model's output, things look good thanks to Son over on llama.cpp putting great effort into it for the past 2 days :) Super appreciate his work!&lt;/p&gt; &lt;p&gt;Static quants of Q8_0, Q6_K, Q4_K_M, and Q3_K_L are up on the lmstudio-community page:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/Llama-4-Scout-17B-16E-Instruct-GGUF"&gt;https://huggingface.co/lmstudio-community/Llama-4-Scout-17B-16E-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(If you want to run in LM Studio make sure you update to the latest beta release)&lt;/p&gt; &lt;p&gt;Imatrix (and smaller sizes) are up on my own page:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/meta-llama_Llama-4-Scout-17B-16E-Instruct-GGUF"&gt;https://huggingface.co/bartowski/meta-llama_Llama-4-Scout-17B-16E-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One small note, if you've been following along over on the llama.cpp GitHub, you may have seen me working on some updates to DeepSeek here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12727"&gt;https://github.com/ggml-org/llama.cpp/pull/12727&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These changes though also affect MoE models in general, and so Scout is similarly affected.. I decided to make these quants WITH my changes, so they should perform better, similar to how Unsloth's DeekSeek releases were better, albeit at the cost of some size.&lt;/p&gt; &lt;p&gt;IQ2_XXS for instance is about 6% bigger with my changes (30.17GB versus 28.6GB), but I'm hoping that the quality difference will be big. I know some may be upset at larger file sizes, but my hope is that even IQ1_M is better than IQ2_XXS was.&lt;/p&gt; &lt;p&gt;Q4_K_M for reference is about 3.4% bigger (65.36 vs 67.55)&lt;/p&gt; &lt;p&gt;I'm running some PPL measurements for Scout (you can see the numbers from DeepSeek for some sizes in the listed PR above, for example IQ2_XXS got 3% bigger but PPL improved by 20%, 5.47 to 4.38) so I'll be reporting those when I have them. Note both lmstudio and my own quants were made with my PR.&lt;/p&gt; &lt;p&gt;In the mean time, enjoy!&lt;/p&gt; &lt;p&gt;Edit for PPL results:&lt;/p&gt; &lt;p&gt;Did not expect such awful PPL results from IQ2_XXS, but maybe that's what it's meant to be for this size model at this level of quant.. But for direct comparison, should still be useful?&lt;/p&gt; &lt;p&gt;Anyways, here's some numbers, will update as I have more:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;quant&lt;/th&gt; &lt;th&gt;size (master)&lt;/th&gt; &lt;th&gt;ppl (master)&lt;/th&gt; &lt;th&gt;size (branch)&lt;/th&gt; &lt;th&gt;ppl (branch)&lt;/th&gt; &lt;th&gt;size increase&lt;/th&gt; &lt;th&gt;PPL improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Q4_K_M&lt;/td&gt; &lt;td&gt;65.36GB&lt;/td&gt; &lt;td&gt;9.1284 +/- 0.07558&lt;/td&gt; &lt;td&gt;67.55GB&lt;/td&gt; &lt;td&gt;9.0446 +/- 0.07472&lt;/td&gt; &lt;td&gt;2.19GB (3.4%)&lt;/td&gt; &lt;td&gt;-0.08 (1%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ2_XXS&lt;/td&gt; &lt;td&gt;28.56GB&lt;/td&gt; &lt;td&gt;12.0353 +/- 0.09845&lt;/td&gt; &lt;td&gt;30.17GB&lt;/td&gt; &lt;td&gt;10.9130 +/- 0.08976&lt;/td&gt; &lt;td&gt;1.61GB (6%)&lt;/td&gt; &lt;td&gt;-1.12 9.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ1_M&lt;/td&gt; &lt;td&gt;24.57GB&lt;/td&gt; &lt;td&gt;14.1847 +/- 0.11599&lt;/td&gt; &lt;td&gt;26.32GB&lt;/td&gt; &lt;td&gt;12.1686 +/- 0.09829&lt;/td&gt; &lt;td&gt;1.75GB (7%)&lt;/td&gt; &lt;td&gt;-2.02 (14.2%)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As suspected, IQ1_M with my branch shows similar PPL to IQ2_XXS from master with 2GB less size.. Hopefully that means successful experiment..?&lt;/p&gt; &lt;p&gt;Dam Q4_K_M sees basically no improvement. Maybe time to check some KLD since 9 PPL on wiki text seems awful for Q4 on such a large model 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju1qtt/llama_4_scout_ggufs_are_here_and_hopefully_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju1qtt/llama_4_scout_ggufs_are_here_and_hopefully_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju1qtt/llama_4_scout_ggufs_are_here_and_hopefully_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T01:19:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1judxsq</id>
    <title>GMKtec EVO-X2 Powered By Ryzen AI Max+ 395 To Launch For $2,052: The First AI+ Mini PC With 70B LLM Support</title>
    <updated>2025-04-08T13:46:01+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1judxsq/gmktec_evox2_powered_by_ryzen_ai_max_395_to/"&gt; &lt;img alt="GMKtec EVO-X2 Powered By Ryzen AI Max+ 395 To Launch For $2,052: The First AI+ Mini PC With 70B LLM Support" src="https://external-preview.redd.it/7dwQT-MLeJB3Eqi-z9LDGmR5IdbcMBwquCFc_EYOkgU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6366f2999e05e8e4ce24296ae638cae50fb26ad9" title="GMKtec EVO-X2 Powered By Ryzen AI Max+ 395 To Launch For $2,052: The First AI+ Mini PC With 70B LLM Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/gmktec-evo-x2-powered-by-ryzen-ai-max-395-to-launch-for-2052/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1judxsq/gmktec_evox2_powered_by_ryzen_ai_max_395_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1judxsq/gmktec_evox2_powered_by_ryzen_ai_max_395_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T13:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1juab77</id>
    <title>This Video model is like 5-8B params only? wtf</title>
    <updated>2025-04-08T10:30:00+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://test-time-training.github.io/video-dit/assets/ttt_cvpr_2025.pdf"&gt;https://test-time-training.github.io/video-dit/assets/ttt_cvpr_2025.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://test-time-training.github.io/video-dit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juab77/this_video_model_is_like_58b_params_only_wtf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juab77/this_video_model_is_like_58b_params_only_wtf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T10:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju4xjl</id>
    <title>1.58bit Llama 4 - Unsloth Dynamic GGUFs</title>
    <updated>2025-04-08T04:10:36+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! Llama 4 is here &amp;amp; we uploaded &lt;strong&gt;imatrix&lt;/strong&gt; Dynamic GGUF formats so you can run them locally. All GGUFs are at: &lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently text only. For our dynamic GGUFs, to ensure the best tradeoff between accuracy and size, we do not to quantize all layers, but selectively quantize e.g. the MoE layers to lower bit, and leave attention and other layers in 4 or 6bit. Fine-tuning support coming in a few hours.&lt;/p&gt; &lt;p&gt;According to the official Llama-4 Github page, and other sources, use:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 0.6 top_p = 0.9 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This time, &lt;strong&gt;all our GGUF uploads are quantized using imatrix&lt;/strong&gt;, which has improved accuracy over standard quantization. We intend to improve our imatrix quants even more with benchmarks (most likely when Qwen3 gets released). Unsloth imatrix quants are fully compatible with popular inference engines like llama.cpp, Ollama, Open WebUI etc.&lt;/p&gt; &lt;p&gt;We utilized DeepSeek R1, V3 and other LLMs to create a large calibration dataset.&lt;/p&gt; &lt;p&gt;Read our guide for running Llama 4 (with correct settings etc): &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Unsloth Dynamic Llama-4-Scout uploads with optimal configs:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;MoE Bits&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;HF Link&lt;/th&gt; &lt;th align="left"&gt;Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1.78bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_S&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;33.8GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-IQ1_S.gguf"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Ok&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1.93bit&lt;/td&gt; &lt;td align="left"&gt;IQ1_M&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;35.4B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-IQ1_M.gguf"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Fair&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.42-bit&lt;/td&gt; &lt;td align="left"&gt;IQ2_XXS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;38.6GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-IQ2_XXS.gguf"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Better&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2.71-bit&lt;/td&gt; &lt;td align="left"&gt;Q2_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.2GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-Q2_K_XL.gguf"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Suggested&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;52.9GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/UD-IQ3_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Great&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4.5-bit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65.6GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/UD-IQ4_K_XL"&gt;Link&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Best&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;* Originally we had a 1.58bit version was that still uploading, but we decided to remove it since it didn't seem to do well on further testing - the lowest quant is the 1.78bit version.&lt;/p&gt; &lt;p&gt;Let us know how it goes!&lt;/p&gt; &lt;p&gt;In terms of testing, unfortunately we can't make the full BF16 version (ie regardless of quantization or not) complete the Flappy Bird game nor the Heptagon test appropriately. We tried Groq, using imatrix or not, used other people's quants, and used normal Hugging Face inference, and this issue persists.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju4xjl/158bit_llama_4_unsloth_dynamic_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju4xjl/158bit_llama_4_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju4xjl/158bit_llama_4_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T04:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju5aux</id>
    <title>lmarena.ai confirms that meta cheated</title>
    <updated>2025-04-08T04:32:53+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They provided a model that is optimized for human preferences, which is different then other hosted models. :(&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/lmarena_ai/status/1909397817434816562"&gt;https://x.com/lmarena_ai/status/1909397817434816562&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju5aux/lmarenaai_confirms_that_meta_cheated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju5aux/lmarenaai_confirms_that_meta_cheated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju5aux/lmarenaai_confirms_that_meta_cheated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T04:32:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju37gh</id>
    <title>Meta submitted customized llama4 to lmarena without providing clarification beforehand</title>
    <updated>2025-04-08T02:34:03+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju37gh/meta_submitted_customized_llama4_to_lmarena/"&gt; &lt;img alt="Meta submitted customized llama4 to lmarena without providing clarification beforehand" src="https://preview.redd.it/cl1e4af7wite1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3b61a0b1cca0493b9eb3ac029029dcf56706a46" title="Meta submitted customized llama4 to lmarena without providing clarification beforehand" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Meta should have made it clearer that “Llama-4-Maverick-03-26-Experimental” was a customized model to optimize for human preference&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://x.com/lmarena_ai/status/1909397817434816562"&gt;https://x.com/lmarena_ai/status/1909397817434816562&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cl1e4af7wite1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju37gh/meta_submitted_customized_llama4_to_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju37gh/meta_submitted_customized_llama4_to_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T02:34:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju7r63</id>
    <title>Llama-3_1-Nemotron-Ultra-253B-v1 benchmarks. Better than R1 at under half the size?</title>
    <updated>2025-04-08T07:18:47+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju7r63/llama3_1nemotronultra253bv1_benchmarks_better/"&gt; &lt;img alt="Llama-3_1-Nemotron-Ultra-253B-v1 benchmarks. Better than R1 at under half the size?" src="https://preview.redd.it/clznuueqakte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2facfbcd182f06991c15e3a654ff2bebadec08e" title="Llama-3_1-Nemotron-Ultra-253B-v1 benchmarks. Better than R1 at under half the size?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/clznuueqakte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju7r63/llama3_1nemotronultra253bv1_benchmarks_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju7r63/llama3_1nemotronultra253bv1_benchmarks_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T07:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jujc9p</id>
    <title>Introducing Lemonade Server: NPU-accelerated local LLMs on Ryzen AI Strix</title>
    <updated>2025-04-08T17:31:17+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"&gt; &lt;img alt="Introducing Lemonade Server: NPU-accelerated local LLMs on Ryzen AI Strix" src="https://external-preview.redd.it/G6CWdjW0ZI0mpPw5AO6U1jpVZ_0ooqTiGOkMmNur-z4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c156c4195f362fe9292a1565450a79a881b7a78c" title="Introducing Lemonade Server: NPU-accelerated local LLMs on Ryzen AI Strix" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/f46sokm6bmte1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc6fb68378cc9c78888ad6694c3f8f4c2fdc6768"&gt;Open WebUI running with Ryzen AI hardware acceleration.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi, I'm Jeremy from AMD, here to share my team’s work to see if anyone here is interested in using it and get their feedback!&lt;/p&gt; &lt;p&gt;🍋Lemonade Server is an OpenAI-compatible local LLM server that offers NPU acceleration on AMD’s latest Ryzen AI PCs (aka Strix Point, Ryzen AI 300-series; requires Windows 11).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub (Apache 2 license): &lt;a href="https://github.com/onnx/turnkeyml"&gt;onnx/turnkeyml: Local LLM Server with NPU Acceleration&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Releases page with GUI installer: &lt;a href="https://github.com/onnx/turnkeyml/releases"&gt;Releases · onnx/turnkeyml&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The NPU helps you get faster prompt processing (time to first token) and then hands off the token generation to the processor’s integrated GPU. Technically, 🍋Lemonade Server will run in CPU-only mode on any x86 PC (Windows or Linux), but our focus right now is on Windows 11 Strix PCs.&lt;/p&gt; &lt;p&gt;We’ve been daily driving 🍋Lemonade Server with Open WebUI, and also trying it out with Continue.dev, CodeGPT, and Microsoft AI Toolkit.&lt;/p&gt; &lt;p&gt;We started this project because Ryzen AI Software is in the ONNX ecosystem, and we wanted to add some of the nice things from the llama.cpp ecosystem (such as this local server, benchmarking/accuracy CLI, and a Python API).&lt;/p&gt; &lt;p&gt;Lemonde Server is still in its early days, but we think now it's robust enough for people to start playing with and developing against. Thanks in advance for your constructive feedback! Especially about how the Sever endpoints and installer could improve, or what apps you would like to see tutorials for in the future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jujc9p/introducing_lemonade_server_npuaccelerated_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T17:31:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1juc553</id>
    <title>Ollama now supports Mistral Small 3.1 with vision</title>
    <updated>2025-04-08T12:18:04+00:00</updated>
    <author>
      <name>/u/markole</name>
      <uri>https://old.reddit.com/user/markole</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juc553/ollama_now_supports_mistral_small_31_with_vision/"&gt; &lt;img alt="Ollama now supports Mistral Small 3.1 with vision" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Ollama now supports Mistral Small 3.1 with vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/markole"&gt; /u/markole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ollama.com/library/mistral-small3.1:24b-instruct-2503-q4_K_M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juc553/ollama_now_supports_mistral_small_31_with_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juc553/ollama_now_supports_mistral_small_31_with_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T12:18:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jui6wd</id>
    <title>What is everyone's top local llm ui (April 2025)</title>
    <updated>2025-04-08T16:44:41+00:00</updated>
    <author>
      <name>/u/Full_You_8700</name>
      <uri>https://old.reddit.com/user/Full_You_8700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just trying to keep up. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_You_8700"&gt; /u/Full_You_8700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jui6wd/what_is_everyones_top_local_llm_ui_april_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jui6wd/what_is_everyones_top_local_llm_ui_april_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jui6wd/what_is_everyones_top_local_llm_ui_april_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T16:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jugmxm</id>
    <title>Artificial Analysis Updates Llama-4 Maverick and Scout Ratings</title>
    <updated>2025-04-08T15:40:23+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jugmxm/artificial_analysis_updates_llama4_maverick_and/"&gt; &lt;img alt="Artificial Analysis Updates Llama-4 Maverick and Scout Ratings" src="https://preview.redd.it/jqc8govgsmte1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aa3fa84066ed377a3f048d4e3f324774d9cb188" title="Artificial Analysis Updates Llama-4 Maverick and Scout Ratings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jqc8govgsmte1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jugmxm/artificial_analysis_updates_llama4_maverick_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jugmxm/artificial_analysis_updates_llama4_maverick_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T15:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1jufqbn</id>
    <title>Qwen3 pull request sent to llama.cpp</title>
    <updated>2025-04-08T15:02:55+00:00</updated>
    <author>
      <name>/u/matteogeniaccio</name>
      <uri>https://old.reddit.com/user/matteogeniaccio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The pull request has been created by bozheng-hit, who also sent the patches for qwen3 support in transformers.&lt;/p&gt; &lt;p&gt;It's approved and ready for merging.&lt;/p&gt; &lt;p&gt;Qwen 3 is near.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12828"&gt;https://github.com/ggml-org/llama.cpp/pull/12828&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/matteogeniaccio"&gt; /u/matteogeniaccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jufqbn/qwen3_pull_request_sent_to_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jufqbn/qwen3_pull_request_sent_to_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jufqbn/qwen3_pull_request_sent_to_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T15:02:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ju9qx0</id>
    <title>Gemma 3 it is then</title>
    <updated>2025-04-08T09:51:07+00:00</updated>
    <author>
      <name>/u/freehuntx</name>
      <uri>https://old.reddit.com/user/freehuntx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9qx0/gemma_3_it_is_then/"&gt; &lt;img alt="Gemma 3 it is then" src="https://preview.redd.it/zlui8az62lte1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73b4f479c50348dbbdb4980ac9b2e0a61172b7af" title="Gemma 3 it is then" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freehuntx"&gt; /u/freehuntx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zlui8az62lte1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9qx0/gemma_3_it_is_then/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ju9qx0/gemma_3_it_is_then/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T09:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1juhgy4</id>
    <title>World Record: DeepSeek R1 at 303 tokens per second by Avian.io on NVIDIA Blackwell B200</title>
    <updated>2025-04-08T16:14:49+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juhgy4/world_record_deepseek_r1_at_303_tokens_per_second/"&gt; &lt;img alt="World Record: DeepSeek R1 at 303 tokens per second by Avian.io on NVIDIA Blackwell B200" src="https://external-preview.redd.it/w1uNpC9oR-BDfODib53NHNbbHwfxCxWJxaBmoZ3DyCw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ee2a4eb2ac98a062fe2d3cb503f6c3733267d31" title="World Record: DeepSeek R1 at 303 tokens per second by Avian.io on NVIDIA Blackwell B200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At Avian.io, we have achieved 303 tokens per second in a collaboration with NVIDIA to achieve world leading inference performance on the Blackwell platform.&lt;/p&gt; &lt;p&gt;This marks a new era in test time compute driven models. We will be providing dedicated B200 endpoints for this model which will be available in the coming days, now available for preorder due to limited capacity &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.linkedin.com/feed/update/urn:li:share:7315398985362391040/?actorCompanyId=99470879"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1juhgy4/world_record_deepseek_r1_at_303_tokens_per_second/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1juhgy4/world_record_deepseek_r1_at_303_tokens_per_second/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-08T16:14:49+00:00</published>
  </entry>
</feed>
