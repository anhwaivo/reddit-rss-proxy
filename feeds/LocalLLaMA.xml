<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-05T18:07:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ihvrb8</id>
    <title>New (Evil) Thinking Model: Skynet-3B</title>
    <updated>2025-02-04T23:23:42+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Today, we are releasing a new experimental model: &lt;strong&gt;Art-Skynet-3B&lt;/strong&gt;, fine-tuned on &lt;strong&gt;LLaMa 3.2 3B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This experiment explores developing models capable of reasoning like &lt;strong&gt;DeepSeek-r1&lt;/strong&gt; and &lt;strong&gt;OpenAI-o3&lt;/strong&gt;, with a long-term goal of &lt;em&gt;world domination&lt;/em&gt; (as a test, of course üòâ).&lt;/p&gt; &lt;p&gt;üîπ &lt;strong&gt;Model card:&lt;/strong&gt; &lt;a href="https://huggingface.co/AGI-0/Art-Skynet-3B"&gt;https://huggingface.co/AGI-0/Art-Skynet-3B&lt;/a&gt; &lt;em&gt;(Leave a like on the repo if you enjoy this model!)&lt;/em&gt;&lt;br /&gt; üîπ &lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat"&gt;https://huggingface.co/spaces/freeCS-dot-org/Art3B-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;p&gt;Note: If the model doesn‚Äôt seem to work, try regenerating the answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihvrb8/new_evil_thinking_model_skynet3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihvrb8/new_evil_thinking_model_skynet3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihvrb8/new_evil_thinking_model_skynet3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T23:23:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii0i9b</id>
    <title>Llama-3.3 and Qwen2.5 speed comparisons on a 4-GPU / 120GB VRAM system</title>
    <updated>2025-02-05T03:11:22+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a couple of speed tests with Llama-3.3 70B Instruct, Qwen-2.5 72B Instruct, and Qwen2.5 Coder 32B Instruct where I asked each of them to &amp;quot;write a flappy bird game in Python that will run on a MacBook&amp;quot;. For this test I didn't care about code quality or results, I just wanted the model to output approximately 1k tokens of code, for which the task of writing flappy bird is perfect. &lt;/p&gt; &lt;p&gt;The only data I was really interested in comparing was raw prompt processing speed and inference/generation speed. I figured some of the folks round here might be curious about the numbers, so here they are.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supermicro M12SWA-TF motherboard&lt;/li&gt; &lt;li&gt;AMD Ryzen Threadripper Pro 3945WX&lt;/li&gt; &lt;li&gt;128GB DDR4 RAM&lt;/li&gt; &lt;li&gt;NVMe SSDs&lt;/li&gt; &lt;li&gt;1x EVGA RTX 3090 Ti 24GB&lt;/li&gt; &lt;li&gt;1x Pny RTX A6000 48GB&lt;/li&gt; &lt;li&gt;2x EVGA RTX 3090 FTW3 24GB&lt;/li&gt; &lt;li&gt;EVGA 2000W PSU running on dedicated 240V/20A 60Hz (USA)&lt;/li&gt; &lt;li&gt;All GPUs throttled at 250W&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Software setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ubuntu Linux&lt;/li&gt; &lt;li&gt;tabbyAPI / exllamav2 (8bpw exl2 quants unless otherwise noted)&lt;/li&gt; &lt;li&gt;tensor parallel enabled&lt;/li&gt; &lt;li&gt;speculative decoding (draft mode) enabled&lt;/li&gt; &lt;li&gt;context lengths for Llama and Qwen are empirically the ceiling of what I can fit in available VRAM (120GB)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LLama-3.3 70B Instruct with 3B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Llama-3.2-3B-Instruct-exl2_8.obpw&lt;/li&gt; &lt;li&gt;Main Model: Llama-3.3-70B-Instruct-exl2_8.0bpw&lt;/li&gt; &lt;li&gt;Context Size: 108,544 bytes&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 44.12 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 30.89 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen-2.5 72B Instruct with 3B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Qwen2.5-3B-Instruct-exl2_8.0bpw&lt;/li&gt; &lt;li&gt;Main Model: Qwen2.5-72B-Instruct-exl2_8.0bpw&lt;/li&gt; &lt;li&gt;Context Size: 128,512&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 97.83 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 37.93 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen-2.5 Coder 32B Instruct with 1.5B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Qwen2.5-Coder-1.5B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Main Model: Qwen2.5-Coder-32B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Context Size: 32,768&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 246.16 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 65.24 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen-2.5 Coder 32B Instruct with 3B draft model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Draft Model: Qwen2.5-Coder-3B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Main Model: Qwen2.5-Coder-32B-Instruct-exl2_8bpw (6 head bits)&lt;/li&gt; &lt;li&gt;Context Size: 32,768&lt;/li&gt; &lt;li&gt;Cache Mode: FP16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process&lt;/strong&gt;: 201.84 T/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate&lt;/strong&gt;: 57.08 T/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I find it interesting that Qwen 72B was faster than Llama 70B by a whopping 7 tokens/sec despite each model using a 3B draft model, Qwen being 2B parameters larger, and Qwen having an extra 20k bytes of context. My guess is that the output of the smaller Qwen model is more closely matched to its larger counterpart than the Llama models, which therefore boosts the speed of speculative decoding... but I'm just pulling guesses out of my butt. What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0i9b/llama33_and_qwen25_speed_comparisons_on_a_4gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0i9b/llama33_and_qwen25_speed_comparisons_on_a_4gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii0i9b/llama33_and_qwen25_speed_comparisons_on_a_4gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T03:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiev1g</id>
    <title>Looking for Local Open-Source AI Tools to Dub Videos in Different Languages (3080 10GB + 64GB RAM)</title>
    <updated>2025-02-05T17:03:03+00:00</updated>
    <author>
      <name>/u/nikprod</name>
      <uri>https://old.reddit.com/user/nikprod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I‚Äôm trying to find a local, open-source AI solution that can dub videos from one language to another (or vice versa). Specifically, I want to: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Dub non-English videos into English (e.g., Japanese ‚Üí English).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Dub English videos into other languages (e.g., Spanish, Mandarin, etc.).&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have a RTX 3080 (10GB VRAM) and 64GB RAM, so I‚Äôm hoping to run this locally for budget reasons.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are there any open-source projects (e.g., Whisper, Coqui, etc.) or workflows that handle speech-to-text ‚Üí translation ‚Üí text-to-speech + lip-sync?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Any recommendations for tools that work well with NVIDIA GPUs (like my 3080)?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Do I need to pre-process videos (e.g., separate audio/video streams) for best results?&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Tips for minimizing latency or optimizing for my hardware setup?&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance! üôè &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nikprod"&gt; /u/nikprod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiev1g/looking_for_local_opensource_ai_tools_to_dub/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiev1g/looking_for_local_opensource_ai_tools_to_dub/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiev1g/looking_for_local_opensource_ai_tools_to_dub/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T17:03:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii3yz2</id>
    <title>Prompt Targets now powered by LoRA of Arch-Function. 2M parameter model designed for intent detection and task routing.</title>
    <updated>2025-02-05T06:33:23+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3yz2/prompt_targets_now_powered_by_lora_of/"&gt; &lt;img alt="Prompt Targets now powered by LoRA of Arch-Function. 2M parameter model designed for intent detection and task routing." src="https://preview.redd.it/bxtskf7em9he1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=565ebb1694849b7e6d120b14c4949f34ca56a7e2" title="Prompt Targets now powered by LoRA of Arch-Function. 2M parameter model designed for intent detection and task routing." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The base model can be found here: &lt;a href="https://huggingface.co/katanemo/Arch-Function-3B"&gt;https://huggingface.co/katanemo/Arch-Function-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And we have yet to publish the LoRA because we beta-testing it and want more practical real world scenarios validated. But in our internal benchmarks we were able to show it‚Äôs comparable to SOTA while be negligee in cost and leaps and bounds faster. So we integrated in Arch - the intelligent edge and LLM proxy for agentic apps - and hosted it. Would for folks to try it so that we release it in a few weeks &lt;/p&gt; &lt;p&gt;More on prompt targets: &lt;a href="https://docs.archgw.com/concepts/prompt_target.html"&gt;https://docs.archgw.com/concepts/prompt_target.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More on the project: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bxtskf7em9he1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3yz2/prompt_targets_now_powered_by_lora_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3yz2/prompt_targets_now_powered_by_lora_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T06:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihpdjh</id>
    <title>What to expect from Mistral's upcoming reasoning models?</title>
    <updated>2025-02-04T18:59:05+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"&gt; &lt;img alt="What to expect from Mistral's upcoming reasoning models?" src="https://preview.redd.it/sa87uqtg66he1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd4e05d3454d29e6d5755deb360afc707a7f84aa" title="What to expect from Mistral's upcoming reasoning models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sa87uqtg66he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihpdjh/what_to_expect_from_mistrals_upcoming_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T18:59:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihv2lz</id>
    <title>AI systems with 'unacceptable risk' are now banned in the EU | TechCrunch</title>
    <updated>2025-02-04T22:53:21+00:00</updated>
    <author>
      <name>/u/Darkstar4125</name>
      <uri>https://old.reddit.com/user/Darkstar4125</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv2lz/ai_systems_with_unacceptable_risk_are_now_banned/"&gt; &lt;img alt="AI systems with 'unacceptable risk' are now banned in the EU | TechCrunch" src="https://external-preview.redd.it/3Nw1ySi8I9t9qNGxMrnWh6X9nJQqmR4w8i1-IlVJwSU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa55aebd954efb86ff5114a964670c3f21850a30" title="AI systems with 'unacceptable risk' are now banned in the EU | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Darkstar4125"&gt; /u/Darkstar4125 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/02/02/ai-systems-with-unacceptable-risk-are-now-banned-in-the-eu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv2lz/ai_systems_with_unacceptable_risk_are_now_banned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihv2lz/ai_systems_with_unacceptable_risk_are_now_banned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T22:53:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiey6e</id>
    <title>Hey so I am interested in creating a custom lightweight model for latin</title>
    <updated>2025-02-05T17:06:30+00:00</updated>
    <author>
      <name>/u/Fine_Salamander_8691</name>
      <uri>https://old.reddit.com/user/Fine_Salamander_8691</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to take a model with around 8 billion parameters and train it with latin translations, grammer, endings, etc to translate latin accurately. I don't mind manually training it to achieve the results I want. If you can help do that or advise if it is too ambitious for a rookie like myself. I'd like it to run on phones IF possible. Not necessary for it though&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fine_Salamander_8691"&gt; /u/Fine_Salamander_8691 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiey6e/hey_so_i_am_interested_in_creating_a_custom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiey6e/hey_so_i_am_interested_in_creating_a_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiey6e/hey_so_i_am_interested_in_creating_a_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T17:06:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihh15n</id>
    <title>Mistral boss says tech CEOs‚Äô obsession with AI outsmarting humans is a ‚Äòvery religious‚Äô fascination</title>
    <updated>2025-02-04T12:57:18+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt; &lt;img alt="Mistral boss says tech CEOs‚Äô obsession with AI outsmarting humans is a ‚Äòvery religious‚Äô fascination" src="https://b.thumbs.redditmedia.com/EnNfi2ijJYZ4t9ufiyUC2FCEhoi78PEnlcIcOHPzAtE.jpg" title="Mistral boss says tech CEOs‚Äô obsession with AI outsmarting humans is a ‚Äòvery religious‚Äô fascination" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/20usc7uld4he1.jpg?width=778&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fdc6226644169232c755f19ef8438c893b4ab3a0"&gt;https://preview.redd.it/20usc7uld4he1.jpg?width=778&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fdc6226644169232c755f19ef8438c893b4ab3a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://fortune.com/europe/article/mistral-boss-tech-ceos-obsession-ai-outsmarting-humans-very-religious-fascination/"&gt;https://fortune.com/europe/article/mistral-boss-tech-ceos-obsession-ai-outsmarting-humans-very-religious-fascination/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihh15n/mistral_boss_says_tech_ceos_obsession_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T12:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iic1ks</id>
    <title>Kokoro voice model extrapolation, blending, and experimenting python application</title>
    <updated>2025-02-05T15:06:21+00:00</updated>
    <author>
      <name>/u/rodbiren</name>
      <uri>https://old.reddit.com/user/rodbiren</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I have been playing around with blending the kokoro voice models (excellent&lt;a href="https://github.com/thewh1teagle/kokoro-onnx"&gt; text to speech library&lt;/a&gt; and &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;model&lt;/a&gt;) and determined I wanted more capability to create voices. I made an application that uses sqlite queries to select groups of voices based on the query. It then creates a linear model between the two voice groups which allows for easy blending of the voices, but also allows for &lt;strong&gt;extrapolation&lt;/strong&gt; of the voices.&lt;/p&gt; &lt;p&gt;For instance, if I make a group of British and a group of American voices I can model between and beyond them. This effectively allows you to make &amp;quot;extreme&amp;quot; versions of the difference in vocal traits between groups. You can make &lt;strong&gt;very&lt;/strong&gt; British and &lt;strong&gt;very&lt;/strong&gt; American accents. The code also allows for exporting voice models into other formats for use in other applications. Examples, codes, and instructions in the github.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RobViren/kokovoicelab"&gt;https://github.com/RobViren/kokovoicelab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rodbiren"&gt; /u/rodbiren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iic1ks/kokoro_voice_model_extrapolation_blending_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iic1ks/kokoro_voice_model_extrapolation_blending_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iic1ks/kokoro_voice_model_extrapolation_blending_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T15:06:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iic8yg</id>
    <title>Interest in a Visual workflow editor</title>
    <updated>2025-02-05T15:15:13+00:00</updated>
    <author>
      <name>/u/throwawayacc201711</name>
      <uri>https://old.reddit.com/user/throwawayacc201711</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iic8yg/interest_in_a_visual_workflow_editor/"&gt; &lt;img alt="Interest in a Visual workflow editor" src="https://b.thumbs.redditmedia.com/p4SdWJtH1UOtQwyYBw_LWGX2PMCWr3r1sda3a0xJTNQ.jpg" title="Interest in a Visual workflow editor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im a developer but when im trying to brainstorm workflows (with or without an LLM) - its a heavy investment to dive into coding something. I want to POC my ideas fast and so I started working on this visual editor.&lt;/p&gt; &lt;p&gt;It has various node types: input, output, read file, processing (out of the box math operations like double, square and custom mode - execute formulas or executed JavaScript code), transform which facilities using huggingfaces transformer.js library so it will do operations like summarize, sentient analysis or translation and finally an ai node which currently is based around interacting with ollama.&lt;/p&gt; &lt;p&gt;The screenshots above are from a demo flow I put together. It reads a csv file, sends the data to ollama and the prompt is to convert the csv to json, then the output branches off into two more nodes one that will find the oldest and one for the youngest. Then there are some processing nodes that essentially formats the data how I want it to be displayed. &lt;/p&gt; &lt;p&gt;The toolbar is fairly self explanatory. The data here is stored in json so it can be saved and loaded. A debug mode that includes adds all the inputs/outputs to the output panel.&lt;/p&gt; &lt;p&gt;They are screenshots so I couldn‚Äôt include it - but when the graph is running, you‚Äôll see a visual indicator (red border) around the current executing node.&lt;/p&gt; &lt;p&gt;Right now I‚Äôve been doing things fast and I haven‚Äôt focused on the UI appearance either. I wanted to see if a tool like this would be useful for people and if there‚Äôs interest in it. This will help me figure out which features to prioritize. &lt;/p&gt; &lt;p&gt;Some additional features I would like to add: 1. Way more node types such as iterators and decision nodes 2. I want to pair the editor with a server component. The server would expose a rest API so people can call their workflows. &lt;/p&gt; &lt;p&gt;If anyone has suggestions on additional features please let me know. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/throwawayacc201711"&gt; /u/throwawayacc201711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iic8yg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iic8yg/interest_in_a_visual_workflow_editor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iic8yg/interest_in_a_visual_workflow_editor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T15:15:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihyutf</id>
    <title>Open Euro LLM launches</title>
    <updated>2025-02-05T01:49:49+00:00</updated>
    <author>
      <name>/u/SuchSeries8760</name>
      <uri>https://old.reddit.com/user/SuchSeries8760</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuchSeries8760"&gt; /u/SuchSeries8760 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openeurollm.eu/launch-press-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihyutf/open_euro_llm_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihyutf/open_euro_llm_launches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T01:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii9tzt</id>
    <title>What do people use their AI for?</title>
    <updated>2025-02-05T13:24:08+00:00</updated>
    <author>
      <name>/u/salixfire</name>
      <uri>https://old.reddit.com/user/salixfire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just gotten started with LLMs and have Kobold CPP and Silly Tavern running on my PC to use for RP. I'm quite enjoying it, but I know the AI can be used for all sorts of things. Due to a disability, reading through pages and pages of stuff about it has left me more confused than ever. What sorts of things can it actually be used for nowadays? I've heard of everything from coding to it making money for people to videos and a whole lot more. What is the truth of the matter and how do you use it in your day to day lives. I am always looking to make things easier for myself due to my disabilities. Do you think there's specific things I could use it for that would help me? Please keep things simple in the explanation, not quite ELI5 level but probably not far off. I'm very out of the loop.&lt;/p&gt; &lt;p&gt;Thank you üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salixfire"&gt; /u/salixfire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9tzt/what_do_people_use_their_ai_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9tzt/what_do_people_use_their_ai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9tzt/what_do_people_use_their_ai_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihxbpe</id>
    <title>DeepSeek banned from Australian Government Devices</title>
    <updated>2025-02-05T00:36:23+00:00</updated>
    <author>
      <name>/u/sammcj</name>
      <uri>https://old.reddit.com/user/sammcj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxbpe/deepseek_banned_from_australian_government_devices/"&gt; &lt;img alt="DeepSeek banned from Australian Government Devices" src="https://external-preview.redd.it/UT58snSyDCf1en1PY0Usy8X0-HIt9R0OcuYrukJSr_0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f710938759a6b3631d87cc438df355ecc2726248" title="DeepSeek banned from Australian Government Devices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sammcj"&gt; /u/sammcj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.abc.net.au/news/2025-02-04/deepseek-banned-from-federal-government-devices/104896770"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxbpe/deepseek_banned_from_australian_government_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihxbpe/deepseek_banned_from_australian_government_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T00:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii5vi6</id>
    <title>Interview with Deepseek CEO Liang</title>
    <updated>2025-02-05T08:54:02+00:00</updated>
    <author>
      <name>/u/mesmerlord</name>
      <uri>https://old.reddit.com/user/mesmerlord</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii5vi6/interview_with_deepseek_ceo_liang/"&gt; &lt;img alt="Interview with Deepseek CEO Liang" src="https://external-preview.redd.it/65TzyKuMhkMpc_f2_BttXLzKse8GVSrynY3OE_u8jTY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5133312a80c92d85bc650060674afac9dd7c50f" title="Interview with Deepseek CEO Liang" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mesmerlord"&gt; /u/mesmerlord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/HRsVZuEMlvI?si=2kgt1IFtaib8vstb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii5vi6/interview_with_deepseek_ceo_liang/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii5vi6/interview_with_deepseek_ceo_liang/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T08:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihtpa2</id>
    <title>I just want to thank all organisations that did not stop open sourcing their results</title>
    <updated>2025-02-04T21:55:28+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a moment, I feared that entities like ClosedAI and Anthropic might alter the open-source paradigm in the realm of Machine Learning. Fortunately, it appears they have not succeeded, and the open-source community has emerged victorious. While the battle is far from over, and we may need to fight even harder, this initial triumph belongs to open source, to all of us.&lt;/p&gt; &lt;p&gt;Let's extend our gratitude to every organization, large and small, that has shared their models, papers, and code with the community. This collaborative spirit is essential for democratizing AI and achieving Artificial General Intelligence (AGI) collectively. By ensuring that the benefits of AI are accessible to all, rather than being monopolized by a few egomaniacs, we foster a more equitable future.&lt;/p&gt; &lt;p&gt;Let us continue to promote open-source initiatives and leave behind those who resist the democratization of AI. By embracing transparency and collaboration, we can build a future where AI serves the interests of all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihtpa2/i_just_want_to_thank_all_organisations_that_did/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihtpa2/i_just_want_to_thank_all_organisations_that_did/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihtpa2/i_just_want_to_thank_all_organisations_that_did/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T21:55:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqwnd</id>
    <title>OpenAI deep research but it's open source</title>
    <updated>2025-02-04T20:01:31+00:00</updated>
    <author>
      <name>/u/Thomjazz</name>
      <uri>https://old.reddit.com/user/Thomjazz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt; &lt;img alt="OpenAI deep research but it's open source" src="https://external-preview.redd.it/VhiwZJj7J5TUIfA6ujpiUeYD8CI4AKINeo7sLJZlD5Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e05cfb506bcac565f349480a4b0d9ba18f4768b1" title="OpenAI deep research but it's open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46"&gt;https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://huggingface.co/blog/open-deep-research"&gt;https://huggingface.co/blog/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thomjazz"&gt; /u/Thomjazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T20:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihph9f</id>
    <title>In case you thought your feedback was not being heard</title>
    <updated>2025-02-04T19:03:04+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt; &lt;img alt="In case you thought your feedback was not being heard" src="https://preview.redd.it/nvf2f1j876he1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c8871a5964dc8f2db918ba181e1157fc626b71" title="In case you thought your feedback was not being heard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nvf2f1j876he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7dfq</id>
    <title>Upgrading 3090's from 24GB to 48GB</title>
    <updated>2025-02-05T10:49:34+00:00</updated>
    <author>
      <name>/u/CertainlyBright</name>
      <uri>https://old.reddit.com/user/CertainlyBright</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm able to do this conversion but the raw parts cost is 550$ for all the new vram. &lt;/p&gt; &lt;p&gt;Plus labor, is that even worth it anymore for others to pay for the upgrade if I offer it as a service?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CertainlyBright"&gt; /u/CertainlyBright &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7dfq/upgrading_3090s_from_24gb_to_48gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7dfq/upgrading_3090s_from_24gb_to_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7dfq/upgrading_3090s_from_24gb_to_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T10:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiaa1r</id>
    <title>GRPO (the method used by Deepseek) will be worse than the original model if you make a mistake in the reward function.</title>
    <updated>2025-02-05T13:47:13+00:00</updated>
    <author>
      <name>/u/dahara111</name>
      <uri>https://old.reddit.com/user/dahara111</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiaa1r/grpo_the_method_used_by_deepseek_will_be_worse/"&gt; &lt;img alt="GRPO (the method used by Deepseek) will be worse than the original model if you make a mistake in the reward function." src="https://preview.redd.it/vt6h2sj2rbhe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10a4c4476b0b3303072dc9e8ed1aa00784dc242e" title="GRPO (the method used by Deepseek) will be worse than the original model if you make a mistake in the reward function." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dahara111"&gt; /u/dahara111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vt6h2sj2rbhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiaa1r/grpo_the_method_used_by_deepseek_will_be_worse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiaa1r/grpo_the_method_used_by_deepseek_will_be_worse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7qfy</id>
    <title>I created a website that tracks AI regulations around the world</title>
    <updated>2025-02-05T11:15:59+00:00</updated>
    <author>
      <name>/u/techie_ray</name>
      <uri>https://old.reddit.com/user/techie_ray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To help you stay on top of what governments are doing on AI, I created an interactive world map that tracks AI regulatory and policy developments around the world. Click on a region (or use the search bar) to view its profile. This website is updated regularly (including new regions to be added).&lt;/p&gt; &lt;p&gt;Free to access. No login required. This is for the community :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techieray.com/GlobalAIRegulationTracker"&gt;https://www.techieray.com/GlobalAIRegulationTracker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techie_ray"&gt; /u/techie_ray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii9lab</id>
    <title>2B model beats 72B model</title>
    <updated>2025-02-05T13:10:56+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9lab/2b_model_beats_72b_model/"&gt; &lt;img alt="2B model beats 72B model" src="https://preview.redd.it/nxx7b0kblbhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7a412b056534d115469db02236cac1fd22d5d1a" title="2B model beats 72B model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Deep-Agent/R1-V"&gt;https://github.com/Deep-Agent/R1-V&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 2B model outperforms the 72B model. &lt;/p&gt; &lt;p&gt;Only 100 training steps, costing less than $3.&lt;/p&gt; &lt;p&gt;The outperformance is in both effectiveness and out-of-distribution (OOD) robustness for vision language models.&lt;/p&gt; &lt;p&gt;in OOD tests within just 100 training steps. &lt;/p&gt; &lt;p&gt;R1-V is released, and fully open-sourced. &lt;/p&gt; &lt;p&gt;The project shows a 2B-parameter model surpassing a 72B-parameter counterpart in generalization tests. &lt;/p&gt; &lt;p&gt;With only 100 training steps (vs. thousands in conventional methods), 30 minutes on 8 A100 GPUs and $2.62 total cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxx7b0kblbhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9lab/2b_model_beats_72b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9lab/2b_model_beats_72b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii3qvv</id>
    <title>Google Lifts a Ban on Using Its AI for Weapons and Surveillance</title>
    <updated>2025-02-05T06:18:38+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"&gt; &lt;img alt="Google Lifts a Ban on Using Its AI for Weapons and Surveillance" src="https://external-preview.redd.it/NrA5s-vSHIcN9SaUL0ETUoJ_dGVpcnD0UsdffV5wGi8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afb92e2820ff849c88d693e6467404d7df633be8" title="Google Lifts a Ban on Using Its AI for Weapons and Surveillance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/google-responsible-ai-principles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T06:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiafzm</id>
    <title>We have to fight back now.</title>
    <updated>2025-02-05T13:55:35+00:00</updated>
    <author>
      <name>/u/mr_happy_nice</name>
      <uri>https://old.reddit.com/user/mr_happy_nice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-source innovation is the lifeblood of American progress, and any attempt to lock it down is a threat to our future. Banning open-source AI under harsh penalties will only stifle the creativity, transparency, and collaboration that have fueled our tech breakthroughs for decades. When anyone can build on and improve each other‚Äôs work, we all win‚Äîespecially in the race for a safer, smarter tomorrow.&lt;/p&gt; &lt;p&gt;We need to stand together for a future where ideas flow freely and innovation isn‚Äôt held hostage. Embracing open-source means a stronger, more competitive American tech ecosystem that benefits everyone, from citizens to startups to established giants. The open road is the best road‚Äîlet‚Äôs keep it that way.&lt;/p&gt; &lt;p&gt;The only thing that these people understand is money. So, follow the money. Here are some of Hawley‚Äôs contributors to get you started. You have a right to have your voice be heard. Let them hear. &lt;/p&gt; &lt;h1&gt;Smead Capital Management&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Addresses &amp;amp; Phone Numbers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Phoenix Office:&lt;/em&gt; 2502 E. Camelback Rd, Suite 210, Phoenix, AZ 85016 Phone: 602.889.3660&lt;/li&gt; &lt;li&gt;&lt;em&gt;Jersey City Office:&lt;/em&gt; 30 Montgomery St, Suite 920, Jersey City, NJ 07302 Phone: 484.535.5121&lt;/li&gt; &lt;li&gt;&lt;em&gt;London Office (UK):&lt;/em&gt; 18th Floor, 100 Bishopsgate, London EC2N 4AG Phone: +44 (0)20.8819.6490&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sales Desk (US):&lt;/strong&gt; 877.701.2883&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@smeadcap.com"&gt;info@smeadcap.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@smeadcap.com"&gt;info@smeadcap.com&lt;/a&gt;) &lt;em&gt;(Additional verified contact: Cole Smead can be reached at &lt;a href="mailto:cole@smeadcap.com"&gt;cole@smeadcap.com&lt;/a&gt;.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Indeck Energy Services&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;600 N. Buffalo Grove Road, Suite 300, Buffalo Grove, IL 60089&lt;/li&gt; &lt;li&gt;Phone: 847-520-3212&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@indeckenergy.com"&gt;info@indeckenergy.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@indeckenergy.com"&gt;info@indeckenergy.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Peck Enterprises, LLC&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number (as listed via Swagelok Alabama):&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;7290 Cahaba Valley Rd, Birmingham, AL 35242&lt;/li&gt; &lt;li&gt;Phone: 205.988.4812&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@ALFL.Swagelok.com"&gt;info@ALFL.Swagelok.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@ALFL.Swagelok.com"&gt;info@ALFL.Swagelok.com&lt;/a&gt;) &lt;em&gt;(Note: An alternate email ‚Äì Roderick Douglass at&lt;/em&gt; [&lt;em&gt;&lt;a href="mailto:rodd.douglass@gmail.com"&gt;rodd.douglass@gmail.com&lt;/a&gt;&lt;/em&gt;](mailto:&lt;a href="mailto:rodd.douglass@gmail.com"&gt;rodd.douglass@gmail.com&lt;/a&gt;) &lt;em&gt;‚Äì was found on a third‚Äëparty directory, but the verified contact on the official Swagelok page is used here.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Northwestern Mutual&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;3601 North Point Parkway, Glendale, WI 53217&lt;/li&gt; &lt;li&gt;Phone: 800-225-5945&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; &lt;em&gt;(None published ‚Äì inquiries are typically directed through the website‚Äôs contact form.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Prime Inc&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;4201 E. Kentucky Ave, Lincoln, NE 68504&lt;/li&gt; &lt;li&gt;Phone: 800-866-2747&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; &lt;em&gt;(No verified email found on the official website; please use the website contact form.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Veterans United Home Loans&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;1701 Wynnton Road, Suite 500, Columbia, MD 21046&lt;/li&gt; &lt;li&gt;Phone: 855-852-4189&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@veteransunited.com"&gt;info@veteransunited.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@veteransunited.com"&gt;info@veteransunited.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Diamond Pet Foods&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;1200 West Kemper Road, Eagan, MN 55122&lt;/li&gt; &lt;li&gt;Phone: 952-787-3400&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@diamondpet.com"&gt;info@diamondpet.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@diamondpet.com"&gt;info@diamondpet.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Leggett &amp;amp; Platt&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Numbers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;One Leggett Parkway, Carson, CA 90746&lt;/li&gt; &lt;li&gt;Customer Care: 800-232-8534; Corporate: (562) 467-2000&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; &lt;em&gt;(No verified email address was confirmed on their official site.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Opko Health&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Numbers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;One Opko Way, Miami, FL 33131&lt;/li&gt; &lt;li&gt;Phone: 800-543-4741 or (305) 300-1234&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@opko.com"&gt;info@opko.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@opko.com"&gt;info@opko.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Edward Jones&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phone Numbers&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Client Relations: (800) 441-2357 (7 a.m. ‚Äì 5:30 p.m. CT, Monday‚ÄìFriday)&lt;/li&gt; &lt;li&gt;Headquarters: (314) 515-2000 (7 a.m. ‚Äì 6 p.m. CT, Monday‚ÄìFriday)&lt;/li&gt; &lt;li&gt;Toll-Free: (800) 803-3333&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Address&lt;/strong&gt;: 12555 Manchester Road, St. Louis County, Missouri 63131, USA&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Edward Jones does not list a public email for customer service; inquiries are handled via phone or their online access portal.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Diamond Pet Foods&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phone Number&lt;/strong&gt;: (800) 442-0402&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Address&lt;/strong&gt;: PO Box 156, Meta, Missouri 65058, USA&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Diamond Pet Foods does not publicly provide a direct email but offers a contact form on their website for inquiries.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Hunter Engineering Company&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Corporate Headquarters Address&lt;/strong&gt;: 11250 Hunter Drive, Bridgeton, Missouri 63044, USA&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Phone Numbers&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Corporate Office: (314) 731-3020 or (800) 448-6848&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: &lt;a href="mailto:canadainfo@hunter.com"&gt;canadainfo@hunter.com&lt;/a&gt; (Canada-specific inquiries); &lt;a href="mailto:info@huntereng.de"&gt;info@huntereng.de&lt;/a&gt; (Germany-specific inquiries)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Hallmark Cards&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phone Numbers&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Toll-Free in the U.S.: (800) 425-5627&lt;/li&gt; &lt;li&gt;Customer Service: (816) 274-3613&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Hallmark does not list a direct customer service email but allows inquiries through a contact form on their website.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For further assistance with these companies, it is recommended to use the provided phone numbers or visit their official websites for additional contact options.&lt;/p&gt; &lt;h1&gt;Fisher Realty (North Carolina)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Belle Hart Schmidt LLC&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GJ Grewe Inc&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Holland Law Firm (Missouri)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found; please refer to a state bar directory for direct contact.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Wilson Logistics&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified email address was found; contact information is available via the company‚Äôs ‚ÄúContact Us‚Äù page.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;AGC Partners&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Warren David Properties LLC&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Durham Co&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact email was found. Public details are not available for inclusion.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Ozarks Coca‚ÄëCola Bottling&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in the public sources.)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_happy_nice"&gt; /u/mr_happy_nice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiafzm/we_have_to_fight_back_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiafzm/we_have_to_fight_back_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiafzm/we_have_to_fight_back_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iidv6u</id>
    <title>Gemini 2.0 is now available to everyone</title>
    <updated>2025-02-05T16:22:33+00:00</updated>
    <author>
      <name>/u/badgerfish2021</name>
      <uri>https://old.reddit.com/user/badgerfish2021</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iidv6u/gemini_20_is_now_available_to_everyone/"&gt; &lt;img alt="Gemini 2.0 is now available to everyone" src="https://external-preview.redd.it/o93Gv9_DhQvlI44kGBVGvb3sGB7HfG5Hch2mizSqwbM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8912d36a97a1fdaed885b0445f70238c8155ea29" title="Gemini 2.0 is now available to everyone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badgerfish2021"&gt; /u/badgerfish2021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iidv6u/gemini_20_is_now_available_to_everyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iidv6u/gemini_20_is_now_available_to_everyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T16:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii82yg</id>
    <title>DeepSeek just released an official demo for DeepSeek VL2 Small - It's really powerful at OCR, text extraction and chat use-cases (Hugging Face Space)</title>
    <updated>2025-02-05T11:40:12+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small"&gt;https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Vaibhav (VB) Srivastav on X: &lt;a href="https://x.com/reach_vb/status/1887094223469515121"&gt;https://x.com/reach_vb/status/1887094223469515121&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Zizheng Pan on X: Our official huggingface space demo for DeepSeek-VL2 Small is out! A 16B MoE model for various vision-language tasks: &lt;a href="https://x.com/zizhpan/status/1887110842711162900"&gt;https://x.com/zizhpan/status/1887110842711162900&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:40:12+00:00</published>
  </entry>
</feed>
