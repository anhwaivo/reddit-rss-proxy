<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-30T20:06:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n451ee</id>
    <title>Build an AI-Powered Image Search Engine Using Ollama and LangChain</title>
    <updated>2025-08-30T15:51:42+00:00</updated>
    <author>
      <name>/u/Flashy-Thought-5472</name>
      <uri>https://old.reddit.com/user/Flashy-Thought-5472</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ee/build_an_aipowered_image_search_engine_using/"&gt; &lt;img alt="Build an AI-Powered Image Search Engine Using Ollama and LangChain" src="https://external-preview.redd.it/1-TbC7xgICLdfvDtCoZXXwzT0BxWOljUGaLj15PAyT8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e429570269e4e3675c1cd2ce999a212302f0201" title="Build an AI-Powered Image Search Engine Using Ollama and LangChain" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy-Thought-5472"&gt; /u/Flashy-Thought-5472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/S9ugRzGjFtA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ee/build_an_aipowered_image_search_engine_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ee/build_an_aipowered_image_search_engine_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T15:51:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3vq5e</id>
    <title>Company Data While Using LLMs</title>
    <updated>2025-08-30T07:42:18+00:00</updated>
    <author>
      <name>/u/Imaginary_Context_32</name>
      <uri>https://old.reddit.com/user/Imaginary_Context_32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are a small startup, and our data is the most valuable asset we have. At the same time, we need to leverage LLMs to help us with formatting and processing this data.&lt;/p&gt; &lt;p&gt;particularly regarding privacy, security, and ensuring that none of our proprietary information is exposed or used for training without our consent?&lt;/p&gt; &lt;p&gt;Note&lt;/p&gt; &lt;p&gt;Open AI claims&lt;/p&gt; &lt;p&gt;&amp;quot;By default, API-submitted data is not used to train or improve OpenAI models.&amp;quot;&lt;/p&gt; &lt;p&gt;Google claims&lt;br /&gt; &amp;quot;Paid Services (e.g., Gemini API, AI Studio with billing active): When using paid versions, Google does not use prompts or responses for training, storing them only transiently for abuse detection or policy enforcement.&amp;quot;&lt;/p&gt; &lt;p&gt;But the catch is that we will not have the power to challenge those.&lt;/p&gt; &lt;p&gt;The local LLMs are not that powerful, is it?&lt;/p&gt; &lt;p&gt;The cloud compute provider is not that dependable either right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imaginary_Context_32"&gt; /u/Imaginary_Context_32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3vq5e/company_data_while_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3vq5e/company_data_while_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3vq5e/company_data_while_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T07:42:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n473r0</id>
    <title>KoboldCpp vs llama.cpp parameters</title>
    <updated>2025-08-30T17:16:10+00:00</updated>
    <author>
      <name>/u/kaisurniwurer</name>
      <uri>https://old.reddit.com/user/kaisurniwurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why are those two using different, or rather the same set of parameters, but store/write them in a different way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaisurniwurer"&gt; /u/kaisurniwurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n473r0/koboldcpp_vs_llamacpp_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n473r0/koboldcpp_vs_llamacpp_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n473r0/koboldcpp_vs_llamacpp_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T17:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3fcyf</id>
    <title>Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model</title>
    <updated>2025-08-29T18:31:54+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3fcyf/stepaudio_2_mini_an_8_billion_parameter_8b/"&gt; &lt;img alt="Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model" src="https://preview.redd.it/orq1ackg50mf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad378196d42b36b5ec5fb2a54a8db4c2b0c1d155" title="Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;StepFun AI recently released Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model. It outperforms GPT-4o-Audio and is Apache 2.0 licensed. The model was trained on over 8 million hours of real and synthesized audio data, supports over 50,000 voices, and excels in expressive and grounded speech benchmarks. Step-Audio 2 Mini employs advanced multi-modal large language model techniques, including reasoning-centric reinforcement learning and retrieval-augmented generation, enabling sophisticated audio understanding and natural speech conversation capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-Audio-2-mini?utm_source=perplexity"&gt;https://huggingface.co/stepfun-ai/Step-Audio-2-mini?utm_source=perplexity&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/orq1ackg50mf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3fcyf/stepaudio_2_mini_an_8_billion_parameter_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3fcyf/stepaudio_2_mini_an_8_billion_parameter_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T18:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3o55w</id>
    <title>Just found out the Ollama version of GPT-OSS has a much higher refusal rate.</title>
    <updated>2025-08-30T00:41:32+00:00</updated>
    <author>
      <name>/u/soup9999999999999999</name>
      <uri>https://old.reddit.com/user/soup9999999999999999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3o55w/just_found_out_the_ollama_version_of_gptoss_has_a/"&gt; &lt;img alt="Just found out the Ollama version of GPT-OSS has a much higher refusal rate." src="https://b.thumbs.redditmedia.com/Jm0QoUt5FCbJmeHJ_cTC82wwTx-rkSDAxEnuoO9kBqA.jpg" title="Just found out the Ollama version of GPT-OSS has a much higher refusal rate." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering why other people seemed to like the model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soup9999999999999999"&gt; /u/soup9999999999999999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n3o55w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3o55w/just_found_out_the_ollama_version_of_gptoss_has_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3o55w/just_found_out_the_ollama_version_of_gptoss_has_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T00:41:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n44xxi</id>
    <title>OpenWebUI lets you auto expand reasoning now!</title>
    <updated>2025-08-30T15:47:40+00:00</updated>
    <author>
      <name>/u/slpreme</name>
      <uri>https://old.reddit.com/user/slpreme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n44xxi/openwebui_lets_you_auto_expand_reasoning_now/"&gt; &lt;img alt="OpenWebUI lets you auto expand reasoning now!" src="https://preview.redd.it/ms5d125ng6mf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb790039d8346bafae55cda40cbc7c0b88f33de8" title="OpenWebUI lets you auto expand reasoning now!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not sure when they added this, but it was a pet peeve of mine so I wanted to share this is how you can turn on show reasoning content automatically. It's just in Settings &amp;gt; Interface &amp;gt; Always Expand Details. I'm guessing that also expands some other things but I don't use any tools so I don't know which.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slpreme"&gt; /u/slpreme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ms5d125ng6mf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n44xxi/openwebui_lets_you_auto_expand_reasoning_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n44xxi/openwebui_lets_you_auto_expand_reasoning_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T15:47:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4132x</id>
    <title>LM Studio on older CPUs &amp; Vulkan GPUs? Done!</title>
    <updated>2025-08-30T13:02:02+00:00</updated>
    <author>
      <name>/u/TheSpicyBoi123</name>
      <uri>https://old.reddit.com/user/TheSpicyBoi123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4132x/lm_studio_on_older_cpus_vulkan_gpus_done/"&gt; &lt;img alt="LM Studio on older CPUs &amp;amp; Vulkan GPUs? Done!" src="https://external-preview.redd.it/xDWIKqsM7gyhZ-1fiaj8ct2CzPw8KsFQKzwmZ9k0cg8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bea2b2a046ad01085a9fc976fd4899f56931e0da" title="LM Studio on older CPUs &amp;amp; Vulkan GPUs? Done!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LM Studio devs state it’s impossible to run on anything older than AVX2 CPUs… I say the MIT license and a bit of compiler magic make it run on anything 😂&lt;/p&gt; &lt;p&gt;Try the patched backends (AVX1) here and enjoy:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/theIvanR/lmstudio-unlocked-backend"&gt;https://github.com/theIvanR/lmstudio-unlocked-backend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w36wxlp5n5mf1.png?width=2419&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bc5e450fb1ea26afc43817fa2cba366b3cc61c1"&gt;https://preview.redd.it/w36wxlp5n5mf1.png?width=2419&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5bc5e450fb1ea26afc43817fa2cba366b3cc61c1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wk4a2cu6n5mf1.png?width=2031&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c89869be3b29561a04a79c9fd2fb482def4c6289"&gt;https://preview.redd.it/wk4a2cu6n5mf1.png?width=2031&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c89869be3b29561a04a79c9fd2fb482def4c6289&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Screenshots:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheSpicyBoi123"&gt; /u/TheSpicyBoi123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4132x/lm_studio_on_older_cpus_vulkan_gpus_done/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4132x/lm_studio_on_older_cpus_vulkan_gpus_done/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4132x/lm_studio_on_older_cpus_vulkan_gpus_done/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T13:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n48bt2</id>
    <title>Phantom Fragment: An ultra-fast, disposable sandbox for securely testing untrusted code.</title>
    <updated>2025-08-30T18:06:59+00:00</updated>
    <author>
      <name>/u/Ok_Horror_8567</name>
      <uri>https://old.reddit.com/user/Ok_Horror_8567</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;A while back, I posted an early version of a project I'm passionate about, Phantom Fragment. The feedback was clear: I needed to do a better job of explaining what it is, who it's for, and why it matters. Thank you for that honesty.&lt;/p&gt; &lt;p&gt;Today, I'm re-introducing the public beta of Phantom Fragment with a clearer focus.&lt;/p&gt; &lt;p&gt;What is Phantom Fragment? Phantom Fragment is a lightweight, high-speed sandboxing tool that lets you run untrusted or experimental code in a secure, isolated environment that starts in milliseconds and disappears without a trace.&lt;/p&gt; &lt;p&gt;Think of it as a disposable container, like Docker, but without the heavy daemons, slow startup times, and complex configuration. It's designed for one thing: running code now and throwing the environment away.&lt;/p&gt; &lt;p&gt;GitHub Repo: &lt;a href="https://github.com/Intro0siddiqui/Phantom-Fragment"&gt;https://github.com/Intro0siddiqui/Phantom-Fragment&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Who is this for? I'm building this for developers who are tired of the friction of traditional sandboxing tools:&lt;/p&gt; &lt;p&gt;AI Developers &amp;amp; Researchers: Safely run and test AI-generated code, models, or scripts without risking your host system.&lt;/p&gt; &lt;p&gt;Developers on Low-Spec Hardware: Get the benefits of containerization without the high memory and CPU overhead of tools like Docker.&lt;/p&gt; &lt;p&gt;Security Researchers: Quickly analyze potentially malicious code in a controlled, ephemeral environment.&lt;/p&gt; &lt;p&gt;Anyone who needs to rapidly test code: Perfect for CI/CD pipelines, benchmarking, or just trying out a new library without polluting your system.&lt;/p&gt; &lt;p&gt;How is it different from other tools like Bubblewrap? This question came up, and it's a great one.&lt;/p&gt; &lt;p&gt;Tools like Bubblewrap are fantastic low-level &amp;quot;toolkits.&amp;quot; They give you the raw parts (namespaces, seccomp, etc.) to build your own sandbox. Phantom Fragment is different. It's a complete, opinionated engine designed from the ground up for performance and ease of use.&lt;/p&gt; &lt;p&gt;Bubblewrap || Phantom Fragment Philosophy A flexible toolkit || A complete, high-speed engine Ease of Use Requires deep Linux knowledge || A single command to run Core Goal Flexibility || Speed and disposability You use Bubblewrap to build a car. Phantom Fragment is the car, tuned and ready to go.&lt;/p&gt; &lt;p&gt;Try it now The project is still in beta, but the core functionality is there. You can get started with a simple command:&lt;/p&gt; &lt;p&gt;phantom run --profile python-mini &amp;quot;print('Hello from inside the fragment!')&amp;quot;&lt;/p&gt; &lt;p&gt;Call for Feedback This is a solo project born from my own needs, but I want to build it for the community. I'm looking for feedback on the public beta.&lt;/p&gt; &lt;p&gt;Is the documentation clear?&lt;/p&gt; &lt;p&gt;What features are missing for your use case?&lt;/p&gt; &lt;p&gt;How can the user experience be improved?&lt;/p&gt; &lt;p&gt;Thank you for your time and for pushing me to present this better. I'm excited to hear what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Horror_8567"&gt; /u/Ok_Horror_8567 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48bt2/phantom_fragment_an_ultrafast_disposable_sandbox/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48bt2/phantom_fragment_an_ultrafast_disposable_sandbox/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n48bt2/phantom_fragment_an_ultrafast_disposable_sandbox/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T18:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3qcqn</id>
    <title>Patched P2P NVIDIA driver now works with multiple 5090s (and possibly blackwell 2.0 in general). Also works for 4090/3090.</title>
    <updated>2025-08-30T02:31:25+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you are having a good night.&lt;/p&gt; &lt;p&gt;I got informed that the P2P driver had a fork, which is this one: &lt;a href="https://github.com/aikitoria/open-gpu-kernel-modules"&gt;https://github.com/aikitoria/open-gpu-kernel-modules&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had some issues with multiple 5090s when using P2P on the latest tinygrad one (&lt;a href="https://github.com/tinygrad/open-gpu-kernel-modules/tree/570.148.08-p2p"&gt;https://github.com/tinygrad/open-gpu-kernel-modules/tree/570.148.08-p2p&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;So I went with the fork now and it works!&lt;/p&gt; &lt;p&gt;Here is a result of cuda-samples (p2pBandwidthLatencyTest). Each 5090 is running at X8/X8 5.0.&lt;/p&gt; &lt;p&gt;So then:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ ./p2pBandwidthLatencyTest [P2P (Peer-to-Peer) GPU Bandwidth Latency Test] Device: 0, NVIDIA GeForce RTX 5090, pciBusID: 1, pciDeviceID: 0, pciDomainID:0 Device: 1, NVIDIA GeForce RTX 5090, pciBusID: 3, pciDeviceID: 0, pciDomainID:0 Device=0 CAN Access Peer Device=1 Device=1 CAN Access Peer Device=0 ***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure. So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases. P2P Connectivity Matrix D\D 0 1 0 1 1 1 1 1 Unidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 0 1736.17 24.35 1 24.62 1771.60 Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s) D\D 0 1 0 1741.98 28.38 1 28.67 1755.68 Bidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 0 1737.98 30.20 1 30.47 1769.44 Bidirectional P2P=Enabled Bandwidth Matrix (GB/s) D\D 0 1 0 1751.59 52.19 1 55.94 1765.44 P2P=Disabled Latency Matrix (us) GPU 0 1 0 2.08 14.38 1 14.65 2.10 CPU 0 1 0 1.75 4.67 1 4.66 1.63 P2P=Enabled Latency (P2P Writes) Matrix (us) GPU 0 1 0 2.08 0.48 1 0.48 2.07 CPU 0 1 0 1.68 1.27 1 1.29 1.68 &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Unidirectional bandwidth goes from 24 GB/s to 28 GB/s&lt;/li&gt; &lt;li&gt;Bidirectional bandwidth goes from 30 GB/s to almost 56GB/s! (So i.e. if you have both at X16 5.0 on a threadipper, you would get about 112 GB/s)&lt;/li&gt; &lt;li&gt;Latency goes from 14 us to an insane 0.48us.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As an extra, I have 7 GPUs in my system (5090x2 at X8/X8 5.0, 4090x2+3090x2+A6000 at X4 4.0, consumer mobo) and P2P work between the 4090, and the 3090s/A6000.&lt;/p&gt; &lt;p&gt;Matrix looks like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6 pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ ./p2pBandwidthLatencyTest [P2P (Peer-to-Peer) GPU Bandwidth Latency Test] Device: 0, NVIDIA GeForce RTX 4090, pciBusID: 2, pciDeviceID: 0, pciDomainID:0 Device: 1, NVIDIA GeForce RTX 4090, pciBusID: 17, pciDeviceID: 0, pciDomainID:0 Device: 2, NVIDIA GeForce RTX 5090, pciBusID: 1, pciDeviceID: 0, pciDomainID:0 Device: 3, NVIDIA GeForce RTX 5090, pciBusID: 3, pciDeviceID: 0, pciDomainID:0 Device: 4, NVIDIA RTX A6000, pciBusID: 12, pciDeviceID: 0, pciDomainID:0 Device: 5, NVIDIA GeForce RTX 3090, pciBusID: 6, pciDeviceID: 0, pciDomainID:0 Device: 6, NVIDIA GeForce RTX 3090, pciBusID: d, pciDeviceID: 0, pciDomainID:0 Device=0 CAN Access Peer Device=1 Device=0 CANNOT Access Peer Device=2 Device=0 CANNOT Access Peer Device=3 Device=0 CANNOT Access Peer Device=4 Device=0 CANNOT Access Peer Device=5 Device=0 CANNOT Access Peer Device=6 Device=1 CAN Access Peer Device=0 Device=1 CANNOT Access Peer Device=2 Device=1 CANNOT Access Peer Device=3 Device=1 CANNOT Access Peer Device=4 Device=1 CANNOT Access Peer Device=5 Device=1 CANNOT Access Peer Device=6 Device=2 CANNOT Access Peer Device=0 Device=2 CANNOT Access Peer Device=1 Device=2 CAN Access Peer Device=3 Device=2 CANNOT Access Peer Device=4 Device=2 CANNOT Access Peer Device=5 Device=2 CANNOT Access Peer Device=6 Device=3 CANNOT Access Peer Device=0 Device=3 CANNOT Access Peer Device=1 Device=3 CAN Access Peer Device=2 Device=3 CANNOT Access Peer Device=4 Device=3 CANNOT Access Peer Device=5 Device=3 CANNOT Access Peer Device=6 Device=4 CANNOT Access Peer Device=0 Device=4 CANNOT Access Peer Device=1 Device=4 CANNOT Access Peer Device=2 Device=4 CANNOT Access Peer Device=3 Device=4 CAN Access Peer Device=5 Device=4 CAN Access Peer Device=6 Device=5 CANNOT Access Peer Device=0 Device=5 CANNOT Access Peer Device=1 Device=5 CANNOT Access Peer Device=2 Device=5 CANNOT Access Peer Device=3 Device=5 CAN Access Peer Device=4 Device=5 CAN Access Peer Device=6 Device=6 CANNOT Access Peer Device=0 Device=6 CANNOT Access Peer Device=1 Device=6 CANNOT Access Peer Device=2 Device=6 CANNOT Access Peer Device=3 Device=6 CAN Access Peer Device=4 Device=6 CAN Access Peer Device=5 ***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure. So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases. P2P Connectivity Matrix D\D 0 1 2 3 4 5 6 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 2 0 0 1 1 0 0 0 3 0 0 1 1 0 0 0 4 0 0 0 0 1 1 1 5 0 0 0 0 1 1 1 6 0 0 0 0 1 1 1 Unidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 992.67 6.34 6.53 6.53 6.07 3.11 3.09 1 6.34 1045.96 6.53 6.53 6.07 3.11 3.09 2 6.64 6.64 1763.54 24.56 6.23 4.92 4.90 3 6.64 6.64 24.66 1767.53 6.23 4.92 4.89 4 6.37 6.37 6.45 6.45 765.93 3.07 3.06 5 3.21 3.20 5.05 5.05 3.08 913.21 3.08 6 3.20 3.20 5.09 5.06 3.06 3.08 911.61 Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 991.26 6.60 6.53 6.53 6.07 3.11 3.09 1 6.60 1062.93 6.53 6.53 6.07 3.11 3.09 2 6.64 6.64 1761.00 28.62 6.23 4.93 4.90 3 6.64 6.64 28.68 1757.59 6.23 4.95 4.88 4 6.37 6.37 6.45 6.45 765.93 2.31 6.60 5 3.21 3.21 5.05 5.05 2.09 915.35 2.08 6 3.20 3.20 5.08 5.06 6.60 2.30 913.21 Bidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 998.39 8.66 8.88 8.89 8.21 4.64 4.61 1 8.67 1046.90 8.89 8.89 8.22 4.65 4.61 2 9.72 9.72 1758.21 30.68 8.34 7.27 6.77 3 9.72 9.72 30.58 1759.51 8.35 7.32 6.77 4 8.25 8.25 8.34 8.34 770.27 3.24 3.19 5 4.62 4.62 6.77 6.82 3.23 918.85 3.23 6 4.62 4.64 6.78 6.86 3.17 3.23 919.66 Bidirectional P2P=Enabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 994.30 12.88 8.88 8.89 8.15 4.65 4.60 1 12.88 1043.75 8.89 8.88 7.78 4.64 4.60 2 9.72 9.72 1760.16 56.11 8.28 7.30 6.79 3 9.72 9.72 55.93 1753.56 8.22 7.31 6.78 4 8.26 8.25 8.33 8.33 770.08 2.30 6.60 5 4.62 4.62 6.77 6.81 2.30 920.20 2.31 6 4.64 4.64 6.83 6.83 6.60 2.30 919.93 P2P=Disabled Latency Matrix (us) GPU 0 1 2 3 4 5 6 0 1.54 13.66 15.03 14.56 18.67 17.18 17.08 1 13.59 1.38 14.95 14.53 22.65 16.12 18.31 2 12.76 12.98 2.11 14.22 16.30 13.37 15.95 3 12.71 12.85 14.95 2.11 16.30 13.34 16.00 4 19.01 18.74 16.46 14.58 1.72 16.29 23.01 5 15.51 14.15 15.51 15.15 21.43 1.65 20.72 6 19.15 18.39 15.00 14.65 23.00 19.34 1.58 CPU 0 1 2 3 4 5 6 0 1.64 7.16 5.26 4.77 5.39 4.97 5.47 1 5.45 1.66 4.84 6.44 5.03 5.00 5.00 2 4.84 4.82 1.60 4.49 5.06 4.83 4.83 3 5.03 4.91 4.48 1.58 4.88 4.80 4.84 4 5.10 5.12 4.76 4.73 1.66 5.04 5.11 5 5.09 5.00 4.65 4.69 5.09 1.61 5.04 6 5.06 5.04 4.72 4.73 5.06 5.09 1.65 P2P=Enabled Latency (P2P Writes) Matrix (us) GPU 0 1 2 3 4 5 6 0 1.43 0.95 15.85 14.55 25.77 16.96 23.93 1 0.92 1.42 14.98 14.54 25.99 16.10 20.67 2 12.68 12.69 2.11 0.53 16.20 13.42 15.99 3 13.09 12.77 0.51 2.11 16.28 13.32 15.92 4 19.16 18.74 15.13 14.58 1.80 1.81 1.82 5 14.23 15.07 15.51 15.04 1.41 1.61 1.42 6 19.04 19.01 16.47 14.65 1.82 1.83 1.64 CPU 0 1 2 3 4 5 6 0 1.65 1.35 4.89 4.87 5.11 5.23 5.21 1 1.49 1.72 4.83 4.79 5.08 6.90 4.87 2 4.83 4.83 1.53 1.23 4.93 4.79 4.86 3 4.99 4.85 1.23 1.63 5.02 4.94 4.91 4 5.20 5.06 4.82 4.77 1.61 1.35 1.35 5 5.26 5.19 4.89 4.99 1.41 1.73 1.34 6 5.31 5.08 4.96 4.79 1.37 1.39 1.64 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So if you see carefully, even at those lower PCIe speeds you go i.e. 24 us latency to 5 us latency on 4090s and 3090s. Also 3090 work with P2P at the same time with the A6000.&lt;/p&gt; &lt;p&gt;Note the 3090s have a penalty here but it is I'm running them (and the A6000) on chipset lanes. So even when it says they run at X4 4.0, they share it themselves and also to the other chipset parts (usb, ethernet, etc). 5090s and 4090s are fully on CPU lanes.&lt;/p&gt; &lt;p&gt;Hope this helps!&lt;/p&gt; &lt;p&gt;EDIT: Some small speeds references on EXL3 + TP, via TabbyAPI.&lt;/p&gt; &lt;p&gt;Mistral Large 2411 3.5bpw (using just the 2 5090s), at 10K ctx, native and NCCL TP:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;TP disabled: 16 t/s&lt;/li&gt; &lt;li&gt;TP enabled, no P2P: 16 t/s&lt;/li&gt; &lt;li&gt;TP enabled (native), P2P: 20 t/s&lt;/li&gt; &lt;li&gt;TP enabled (NCCL), P2P: 21 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GLM 4.5 4bpw (using the 7 GPUs), at 32K ctx (NOTE: This runs pretty slow because it meets a PCIe bandwidth bottleneck, so base speeds themselves are slow), native TP:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;TP disabled: 16 t/s&lt;/li&gt; &lt;li&gt;TP enabled, no P2P: 11 t/s (so here it is a penalty)&lt;/li&gt; &lt;li&gt;TP enabled, P2P: 16 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So for GLM as being a model with few active params and having so many GPUs at X4 4.0, there is a demerit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3qcqn/patched_p2p_nvidia_driver_now_works_with_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3qcqn/patched_p2p_nvidia_driver_now_works_with_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3qcqn/patched_p2p_nvidia_driver_now_works_with_multiple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T02:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3wodi</id>
    <title>The Information reports that DeepSeek is using Huawei's Ascend chips to train and refine smaller versions of its R2 models but continues to use Nvidia chips for its largest models</title>
    <updated>2025-08-30T08:44:54+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/theinformation/status/1961417030436880773"&gt;The Information's description of the article on X&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek, one of China’s leading AI developers, will use Huawei’s AI chips to train some models, a sign it is starting to shift away from Nvidia.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The beginning of the article, copied from &lt;a href="https://www.theinformation.com/articles"&gt;https://www.theinformation.com/articles&lt;/a&gt; :&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek, one of China’s leading artificial intelligence developers, has decided to use Huawei Technologies’ AI chips to train some of its AI models, a sign it is reducing its reliance on Nvidia chips, according to three people with knowledge of the effort. The move follows pressure by the Chinese government on local tech companies to use...&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.techmeme.com/250829/p12#a250829p12"&gt;Techmeme's description of the article&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Sources: DeepSeek plans to use Huawei's Ascend AI chips to train smaller versions of its upcoming R2 models but will still use Nvidia chips for largest models (The Information)&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theinformation.com/articles/deepseek-opts-huawei-chips-train-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3wodi/the_information_reports_that_deepseek_is_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3wodi/the_information_reports_that_deepseek_is_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T08:44:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n48lcx</id>
    <title>10,000 $ Budget for a rig that will run ai (24/7)</title>
    <updated>2025-08-30T18:18:07+00:00</updated>
    <author>
      <name>/u/Holiday_Leg8427</name>
      <uri>https://old.reddit.com/user/Holiday_Leg8427</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says, I want to make myself at home a set-up that can run AI 24/7, i need it mainly to replace the general use of llms ( chatgpt, gemini etc...), was thinking and saw lots of posts and info aboout getting an macbook studio with maximum ram capacity, is that the best way?&lt;br /&gt; Thank you for your responses (in advance)!&lt;/p&gt; &lt;p&gt;Edit: Guys, i dont think i need a llm anymore, Imma just ask on reddit everything I need, and get result from you guys, thank for all the help and tips, and btw I have some sort of &amp;quot;credit&amp;quot; (and i can write it off basically entirely on my company) for a high end pc/pc parts, thats why I wanted to invest into something that can be as usefull as possible, and i work with many legal/medical teams in europe where gdpr is king&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Leg8427"&gt; /u/Holiday_Leg8427 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n48lcx/10000_budget_for_a_rig_that_will_run_ai_247/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T18:18:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3r26s</id>
    <title>NVIDIA-Nemotron-Nano-12B-v2</title>
    <updated>2025-08-30T03:08:36+00:00</updated>
    <author>
      <name>/u/bratao</name>
      <uri>https://old.reddit.com/user/bratao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3r26s/nvidianemotronnano12bv2/"&gt; &lt;img alt="NVIDIA-Nemotron-Nano-12B-v2" src="https://external-preview.redd.it/aiZgOrtrSP6Ci3NlUeoNSLXX-JmqAGN3OaJ3-7NGMyA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4804a6da3fb19d291a17b3e123ac0add54024c5d" title="NVIDIA-Nemotron-Nano-12B-v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bratao"&gt; /u/bratao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3r26s/nvidianemotronnano12bv2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3r26s/nvidianemotronnano12bv2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T03:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n451ka</id>
    <title>GPT OSS Fine-tuning QAT</title>
    <updated>2025-08-30T15:51:53+00:00</updated>
    <author>
      <name>/u/Short_Struggle7803</name>
      <uri>https://old.reddit.com/user/Short_Struggle7803</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read more about our (Nvidia) end to end example on GPT OSS fine tuning QAT + SGlang deployment 👉 &lt;a href="https://lmsys.org/blog/2025-08-28-gpt-oss-qat/"&gt;https://lmsys.org/blog/2025-08-28-gpt-oss-qat/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fine-tuning QAT helps keep the original MXFP4 quantization of GPT OSS while adapting to downstream task.&lt;/p&gt; &lt;p&gt;We have some example results (and comparisons to Nvidia’s NVFP4 format) here :&lt;/p&gt; &lt;p&gt;&lt;a href="https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/"&gt;https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do checkout 🙃!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Short_Struggle7803"&gt; /u/Short_Struggle7803 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n451ka/gpt_oss_finetuning_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T15:51:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3sdka</id>
    <title>Can 2 RTX 6000 Pros (2X98GB vram) rival Sonnet 4 or Opus 4?</title>
    <updated>2025-08-30T04:20:36+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Id rather pay $300 a month to own my hardware than pay $200 a month to rent. Anyone out there that has tried what can be achieved with 2 RTX 6000 pros?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3sdka/can_2_rtx_6000_pros_2x98gb_vram_rival_sonnet_4_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3sdka/can_2_rtx_6000_pros_2x98gb_vram_rival_sonnet_4_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3sdka/can_2_rtx_6000_pros_2x98gb_vram_rival_sonnet_4_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T04:20:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3b13b</id>
    <title>Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)</title>
    <updated>2025-08-29T15:47:53+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt; &lt;img alt="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" src="https://external-preview.redd.it/ZWZwemw0NXNiemxmMRIjC8ICuXshETDKyWbElsvvahdP8-tMtjXY4bwDOY1n.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f385cf95be0a591edf241b94d0612947ca571c1" title="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to models:&lt;br /&gt; - FastVLM: &lt;a href="https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e"&gt;https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e&lt;/a&gt;&lt;br /&gt; - MobileCLIP2: &lt;a href="https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47"&gt;https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo (+ source code): &lt;a href="https://huggingface.co/spaces/apple/fastvlm-webgpu"&gt;https://huggingface.co/spaces/apple/fastvlm-webgpu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ayma955sbzlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T15:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n407lv</id>
    <title>Training a 11M language model for Raspberry Pi Pico - progress</title>
    <updated>2025-08-30T12:19:39+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I have been training a Language Model for the Raspberry Pi Pico (fast &amp;amp; smart). My research showed me that 11M parameters is the sweet spot (about 2-3 token/s) to start.&lt;/p&gt; &lt;p&gt;What I use: &lt;/p&gt; &lt;p&gt;- For training: PicoLM&lt;/p&gt; &lt;p&gt;- GPU: 5090 for Prime Intellect (from grants), H100 on &lt;a href="http://Ori.co"&gt;Ori.co&lt;/a&gt; (free credits), AWS GPU (free credits)&lt;/p&gt; &lt;p&gt;Progress:&lt;/p&gt; &lt;p&gt;- I have been training it on 5M rows of Dolma + Olmo 7B tokenizers&lt;/p&gt; &lt;p&gt;Checkpoints:&lt;/p&gt; &lt;p&gt;17k:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### Prompt 7: &amp;quot;In the year 2050,&amp;quot; **Response**: ``` we have been doing that it was just for the same time, he said, and I have a lot of my father, and it was my own. We had a very much of a little as I wanted to do it. I'm not sure, but the people just like to me and I've never been a lot of my blog, so that I am not the most likely was that she's not to be my face. I wanted to get to me, ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;75k:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;### Prompt 1: &amp;quot;Hello, how are you?&amp;quot; **Response**: ``` I've been my own own name. I think I've got a couple of reasons. I think I was a lot of my mind and my own way I did not know my work. I got the way I took a little time with my first time. I am very sure I do. I like a good, I’m sure I did my car but I do have to be on my job. I've been with my friend and ``` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All benchmark results: &lt;a href="https://github.com/ThomasVuNguyen/Starmind-Zero/tree/main/benchmarks/results"&gt;https://github.com/ThomasVuNguyen/Starmind-Zero/tree/main/benchmarks/results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All datasets &amp;amp; code are available on my github:&lt;br /&gt; - Github: &lt;a href="https://github.com/ThomasVuNguyen/Starmind-Zero"&gt;https://github.com/ThomasVuNguyen/Starmind-Zero&lt;/a&gt;&lt;br /&gt; - Huggingface: &lt;a href="https://huggingface.co/ThomasTheMaker"&gt;https://huggingface.co/ThomasTheMaker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note:&lt;br /&gt; - I am not a ML scientist. Purely an AI startup founder with too much energy to just do normal engineering and be happy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n407lv/training_a_11m_language_model_for_raspberry_pi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n407lv/training_a_11m_language_model_for_raspberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n407lv/training_a_11m_language_model_for_raspberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T12:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3u7qf</id>
    <title>How’s your experience with the GPT OSS models? Which tasks do you find them good at—writing, coding, or something else</title>
    <updated>2025-08-30T06:06:58+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3u7qf/hows_your_experience_with_the_gpt_oss_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3u7qf/hows_your_experience_with_the_gpt_oss_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3u7qf/hows_your_experience_with_the_gpt_oss_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T06:06:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4329n</id>
    <title>How do you people run GLM 4.5 locally ?</title>
    <updated>2025-08-30T14:30:45+00:00</updated>
    <author>
      <name>/u/Skystunt</name>
      <uri>https://old.reddit.com/user/Skystunt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For context i have a dual rtx 3090 rig with 128gb of ddr5 ram and no matter what i try i get around 6 tokens per second...&lt;br /&gt; On CPU only inference i get between 5 and 6 tokens while on partial GPU offload i get between 5.5 and 6.8 tokens.&lt;br /&gt; I tried 2 different versions the one from unsloth Q4_K_S (&lt;a href="https://huggingface.co/unsloth/GLM-4.5-Air-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.5-Air-GGUF&lt;/a&gt;) and the one from LovedHeart MXFP4 (&lt;a href="https://huggingface.co/lovedheart/GLM-4.5-Air-GGUF-IQ1%5C_M"&gt;https://huggingface.co/lovedheart/GLM-4.5-Air-GGUF-IQ1\_M&lt;/a&gt;)&lt;br /&gt; The one from unsloth is 1 token per second slower but still no story change.&lt;br /&gt; I changed literally all settings from lmstudio, even managed to get it to load with the full 131k context but still nowhere near the speed other users get on a single 3090 with offloading.&lt;br /&gt; I tried installing vllm but i get too much errors and i gave up.&lt;br /&gt; Is there another program i should try ? Have i chose the wrong models ?&lt;br /&gt; It's really frustrating and it's taking me too much hours to solve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skystunt"&gt; /u/Skystunt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4329n/how_do_you_people_run_glm_45_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T14:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3xxm5</id>
    <title>🌟Introducing Art-0-8B: Reasoning the way you want it to with Adaptive Thinking🌟</title>
    <updated>2025-08-30T10:08:44+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3- &lt;strong&gt;Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to &amp;quot;think in rap lyrics&amp;quot; or &amp;quot;use bullet points to organize thoughts&amp;quot; and it will literally reason that way before giving you an answer.&lt;/p&gt; &lt;p&gt;You can check out the model on HuggingFace: &lt;a href="https://huggingface.co/AGI-0/Art-0-8B"&gt;https://huggingface.co/AGI-0/Art-0-8B&lt;/a&gt; (please leave a like in the repo if you like this model)&lt;/p&gt; &lt;p&gt;Let me know your thoughts!&lt;/p&gt; &lt;p&gt;P.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model card &lt;a href="https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab"&gt;https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T10:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1n45lx2</id>
    <title>GLM-4.5V model for Computer Use</title>
    <updated>2025-08-30T16:15:20+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"&gt; &lt;img alt="GLM-4.5V model for Computer Use" src="https://external-preview.redd.it/MjZwOTdqc3psNm1mMZtBXPQuBBghVYkEG23VKH2rdUK_y7uZuqgwTRJo1CZN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8f81993da62e75eef832974ce71abbca064f96f" title="GLM-4.5V model for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either locally via Hugging Face or Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eicije20m6mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n45lx2/glm45v_model_for_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3ldon</id>
    <title>Qwen3-coder is mind blowing on local hardware (tutorial linked)</title>
    <updated>2025-08-29T22:35:27+00:00</updated>
    <author>
      <name>/u/nick-baumann</name>
      <uri>https://old.reddit.com/user/nick-baumann</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt; &lt;img alt="Qwen3-coder is mind blowing on local hardware (tutorial linked)" src="https://external-preview.redd.it/MHAyYm12N3NjMW1mMWyTIaaq8py0BbLEXek7RrX8ohVlR1FrRoAdOlxuqQ67.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d298923517a79cfa7fc3e04c1533fbc4c70a8f3b" title="Qwen3-coder is mind blowing on local hardware (tutorial linked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello hello!&lt;/p&gt; &lt;p&gt;I'm honestly blown away by how far local models have gotten in the past 1-2 months. Six months ago, local models were completely useless in Cline, which tbf is pretty heavyweight in terms of context and tool-calling demands. And then a few months ago I found one of the qwen models to actually be somewhat usable, but not for any real coding.&lt;/p&gt; &lt;p&gt;However, qwen3-coder-30B is really impressive. 256k context and is actually able to complete tool calls and diff edits reliably in Cline. I'm using the 4-bit quantized version on my 36GB RAM Mac.&lt;/p&gt; &lt;p&gt;My machine does turn into a bit of a jet engine after a while, but the performance is genuinely useful. My setup is LM Studio + Qwen3 Coder 30B + Cline (VS Code extension). There are some critical config details that can break it (like disabling KV cache quantization in LM Studio), but once dialed in, it just works.&lt;/p&gt; &lt;p&gt;This feels like the first time local models have crossed the threshold from &amp;quot;interesting experiment&amp;quot; to &amp;quot;actually useful coding tool.&amp;quot; I wrote a full technical walkthrough and setup guide: &lt;a href="https://cline.bot/blog/local-models"&gt;https://cline.bot/blog/local-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nick-baumann"&gt; /u/nick-baumann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/75bfhw7sc1mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T22:35:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n40ngf</id>
    <title>What is the slowest Token/sec you can live with?</title>
    <updated>2025-08-30T12:41:26+00:00</updated>
    <author>
      <name>/u/OrganicApricot77</name>
      <uri>https://old.reddit.com/user/OrganicApricot77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me:&lt;/p&gt; &lt;p&gt;5tok/s is the slowest I’ll accept&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganicApricot77"&gt; /u/OrganicApricot77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n40ngf/what_is_the_slowest_tokensec_you_can_live_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T12:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46mk9</id>
    <title>LongCat-Flash-Chat is here, yet another Chinese open weight model</title>
    <updated>2025-08-30T16:56:47+00:00</updated>
    <author>
      <name>/u/MindlessScrambler</name>
      <uri>https://old.reddit.com/user/MindlessScrambler</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt; &lt;img alt="LongCat-Flash-Chat is here, yet another Chinese open weight model" src="https://external-preview.redd.it/SJG1eRbm3SwVjlU4Orqm4a5_PTL6rFKUpPzL-4SiNJI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d1f89904849c371c282657b5befc8d11c2c3998" title="LongCat-Flash-Chat is here, yet another Chinese open weight model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF: &lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat"&gt;https://huggingface.co/meituan-longcat/LongCat-Flash-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/meituan-longcat/LongCat-Flash-Chat"&gt;https://github.com/meituan-longcat/LongCat-Flash-Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Web: &lt;a href="https://longcat.ai"&gt;https://longcat.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmark:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9dqweyx9t6mf1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea84100d70303ada8bc3fccc2a4c0e5cb47fb08f"&gt;https://preview.redd.it/9dqweyx9t6mf1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea84100d70303ada8bc3fccc2a4c0e5cb47fb08f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MindlessScrambler"&gt; /u/MindlessScrambler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n46mk9/longcatflashchat_is_here_yet_another_chinese_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n49hcg</id>
    <title>New AMD unified memory product - 512 bit bus = ~512GB/s memory bandwidth</title>
    <updated>2025-08-30T18:54:22+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recent AMD leak hints at a new 512 bit memory bus for their unified memory systems. If so, a successor to the AI max would likely have 2x the memory bandwidth.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship"&gt;https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n49hcg/new_amd_unified_memory_product_512_bit_bus_512gbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T18:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46ify</id>
    <title>Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)</title>
    <updated>2025-08-30T16:52:00+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt; &lt;img alt="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" src="https://preview.redd.it/1wl79kpjs6mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967356c2a208d26c7d657a1b535e795c0332f304" title="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wl79kpjs6mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI — The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM – 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
