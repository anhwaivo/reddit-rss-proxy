<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-07T21:21:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j5s7cu</id>
    <title>Time to Level Up LLM Pruning – QwQ-32B Needs Some Love! imagine making smaller QWQ_14B !</title>
    <updated>2025-03-07T16:14:01+00:00</updated>
    <author>
      <name>/u/solomars3</name>
      <uri>https://old.reddit.com/user/solomars3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’ve been diving into the latest LLMs and one thing’s clear: while models like QwQ-32B are seriously impressive with their reasoning skills, their parameter count is still a bottleneck. Traditional pruning methods just don’t cut it here—every tiny weight can carry subtle but crucial info.&lt;/p&gt; &lt;p&gt;There are some cool new approaches like Wanda and LLM-Pruner showing promise, but we need to push harder on innovative pruning tech specifically tailored for these reasoning-heavy models. Making QwQ-32B leaner without sacrificing its smarts could seriously cut inference costs and open up more deployment options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solomars3"&gt; /u/solomars3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s7cu/time_to_level_up_llm_pruning_qwq32b_needs_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s7cu/time_to_level_up_llm_pruning_qwq32b_needs_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s7cu/time_to_level_up_llm_pruning_qwq32b_needs_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T16:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5fy49</id>
    <title>(Another) epub to audiobook converter (audiobook-generator)</title>
    <updated>2025-03-07T05:08:36+00:00</updated>
    <author>
      <name>/u/ibic</name>
      <uri>https://old.reddit.com/user/ibic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Folks, I'm just sharing this little python program I created for converting epub books to audiobooks here.&lt;/p&gt; &lt;h2&gt;tldr;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;With Python 3.10+, create a virtual environment and run &lt;code&gt;pip install audiobook-generator&lt;/code&gt; (Or just use &lt;code&gt;pipx&lt;/code&gt; instead which manages the python virtual environment for you automatically). Please be patient a bit as it downloads some large dependencies such as pytorch and the kokoro tts model. On Ubuntu, the dependencies take around 6GB.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run: &lt;code&gt;abg &amp;lt;epub path&amp;gt; &amp;lt;audio output directory&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;That's it.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Tested on Windows/Ubuntu/Mac.&lt;/li&gt; &lt;li&gt;Automatically selects cuda capable GPU if present.&lt;/li&gt; &lt;li&gt;Separate audio files (mp3) are generated per chapter according to the sequence and name of the chapter.&lt;/li&gt; &lt;li&gt;For the 2 books I tested, the total size is around 300 - 500MB, which is about 1/2 - 1/3 of the file size produced by other converters I tried.&lt;/li&gt; &lt;li&gt;The cover image, if present, is extracted to the same directory if present.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why another ebook to audiobook converter?&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;I tried 2 converters, but both took me quite some time to figure out how to get them running on cuda GPU (CPU is fine but slow), so in the end, I decided to create one myself, which includes everything in its dependencies so you don't need to install anything extra if you want to use a cuda GPU.&lt;/li&gt; &lt;li&gt;It's purely personal taste, but I perfer to have one mp3 with the filename being the chapter name instead of one big file for audiobooks. I just copy the whole directory to audiobookshelf then the file names are displayed as the chapter names.&lt;/li&gt; &lt;li&gt;I want to keep the cover image as well.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;GitHub repository&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/houtianze/audiobook-generator"&gt;https://github.com/houtianze/audiobook-generator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ibic"&gt; /u/ibic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5fy49/another_epub_to_audiobook_converter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5fy49/another_epub_to_audiobook_converter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5fy49/another_epub_to_audiobook_converter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T05:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5sikh</id>
    <title>DCLM dataset but better for smol models</title>
    <updated>2025-03-07T16:24:54+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5sikh/dclm_dataset_but_better_for_smol_models/"&gt; &lt;img alt="DCLM dataset but better for smol models" src="https://preview.redd.it/1bb7rwt2nane1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15ae37fc464e08e95a91d1cad47ac755075a057c" title="DCLM dataset but better for smol models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1bb7rwt2nane1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5sikh/dclm_dataset_but_better_for_smol_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5sikh/dclm_dataset_but_better_for_smol_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T16:24:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5xn6x</id>
    <title>Insert pauses into text file for kokoro</title>
    <updated>2025-03-07T19:25:05+00:00</updated>
    <author>
      <name>/u/dts-five</name>
      <uri>https://old.reddit.com/user/dts-five</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before discussions were taken offline, I had looked at this page:&lt;br /&gt; &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/discussions/61"&gt;Adding defined period pauses to the input text file&lt;/a&gt; &lt;/p&gt; &lt;p&gt;There was someone who made a GitHub that incorporated pauses:&lt;br /&gt; &lt;a href="https://github.com/vijay120/kokoro-tts"&gt;https://github.com/vijay120/kokoro-tts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;But the original thread included another way similar to inserting ...,...,... or something like that. Someone responded by saying that it makes a sighing noise, and someone posted a workaround that took away the sighing noise, but the pause was still there. &lt;/p&gt; &lt;p&gt;Does anyone know how to do that, or has anyone seen that old discussion thread and remember the method? &lt;/p&gt; &lt;p&gt;For testing I've been using this variant:&lt;br /&gt; &lt;a href="https://github.com/remsky/Kokoro-FastAPI"&gt;https://github.com/remsky/Kokoro-FastAPI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dts-five"&gt; /u/dts-five &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5xn6x/insert_pauses_into_text_file_for_kokoro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5xn6x/insert_pauses_into_text_file_for_kokoro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5xn6x/insert_pauses_into_text_file_for_kokoro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T19:25:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j57b06</id>
    <title>Deductive-Reasoning-Qwen-32B (used GRPO to surpass R1, o1, o3-mini, and almost Sonnet 3.7)</title>
    <updated>2025-03-06T21:56:26+00:00</updated>
    <author>
      <name>/u/_underlines_</name>
      <uri>https://old.reddit.com/user/_underlines_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j57b06/deductivereasoningqwen32b_used_grpo_to_surpass_r1/"&gt; &lt;img alt="Deductive-Reasoning-Qwen-32B (used GRPO to surpass R1, o1, o3-mini, and almost Sonnet 3.7)" src="https://external-preview.redd.it/yqkxDs5A63axozkmSyct8MXlpRrbuMOdYI9OOnHcIk0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5908068706fbbeeff49f0d24edaf236a083309b7" title="Deductive-Reasoning-Qwen-32B (used GRPO to surpass R1, o1, o3-mini, and almost Sonnet 3.7)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_underlines_"&gt; /u/_underlines_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/OpenPipe/Deductive-Reasoning-Qwen-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j57b06/deductivereasoningqwen32b_used_grpo_to_surpass_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j57b06/deductivereasoningqwen32b_used_grpo_to_surpass_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T21:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5p7mw</id>
    <title>Help Test YourStory! A New Interactive RPG on Twitch</title>
    <updated>2025-03-07T14:27:15+00:00</updated>
    <author>
      <name>/u/Affectionate-Leg8133</name>
      <uri>https://old.reddit.com/user/Affectionate-Leg8133</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Reddit,&lt;/p&gt; &lt;p&gt;I'm developing &lt;em&gt;YourStory&lt;/em&gt;, an interactive text-based RPG where viewers actively shape the adventure in real-time. This isn't just another text game—it's a fully narrated experience with visuals and music, and the story dynamically evolves based on your decisions.&lt;/p&gt; &lt;h1&gt;What makes it special?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Viewers directly influence the story&lt;/li&gt; &lt;li&gt;AI-driven narration, characters, and world-building&lt;/li&gt; &lt;li&gt;Dynamic music and visuals that adapt to the story&lt;/li&gt; &lt;li&gt;A multi-agent system designed for scalability&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;The game runs on a local architecture, capable of handling multiple &lt;strong&gt;Ollama&lt;/strong&gt; servers. Unfortunately, I currently only have one rig available for testing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current system setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Main agent rig (Storyteller, Memory Manager, Character Manager, Background Agent, Music Agent)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA RTX 3090 (24GB VRAM) + GTX 1080 Ti&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core i7-12700K&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB DDR4&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TTS and OBS rig&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; GeForce GTX 1050 Max-Q&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core i7-8750H @ 2.20GHz&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 32GB DDR4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TTS:&lt;/strong&gt; Kokoro (&lt;a href="https://huggingface.co/geneing/Kokoro"&gt;https://huggingface.co/geneing/Kokoro&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Planned Features&lt;/h1&gt; &lt;p&gt;Currently, &lt;em&gt;YourStory&lt;/em&gt; supports custom assets (images and music) that can be placed in designated folders. The agents autonomously select and use these assets to enhance the storytelling experience.&lt;/p&gt; &lt;p&gt;In the future, I plan to integrate AI-generated &lt;strong&gt;images&lt;/strong&gt; (or even short video sequences) and dynamically generated &lt;strong&gt;music&lt;/strong&gt; to create an even more immersive experience. This will allow the entire audiovisual presentation to be generated on the fly, adapting in real-time to the evolving narrative.&lt;/p&gt; &lt;h1&gt;Powered by:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLMs:&lt;/strong&gt; Cydonia-24B-v2, Mistral-Small-24B-Base-2501, Wayfarer-Large-70B-Llama-3.3&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Agents:&lt;/strong&gt; Storyteller, Memory Manager, Character Manager, Background Agent, and Music Agent&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm currently in the testing phase and need feedback to improve the system. If you're interested in interactive storytelling and want to see how AI-driven narration evolves in real-time, join the test session and help push the system to its limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Twitch Link:&lt;/strong&gt; &lt;a href="https://www.twitch.tv/thestarai"&gt;&lt;strong&gt;https://www.twitch.tv/thestarai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts and participation. See you there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate-Leg8133"&gt; /u/Affectionate-Leg8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5p7mw/help_test_yourstory_a_new_interactive_rpg_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5p7mw/help_test_yourstory_a_new_interactive_rpg_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5p7mw/help_test_yourstory_a_new_interactive_rpg_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T14:27:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5ao2j</id>
    <title>AIDER - As I suspected QwQ 32b is much smarter in coding than qwen 2.5 coder instruct 32b</title>
    <updated>2025-03-07T00:28:11+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ao2j/aider_as_i_suspected_qwq_32b_is_much_smarter_in/"&gt; &lt;img alt="AIDER - As I suspected QwQ 32b is much smarter in coding than qwen 2.5 coder instruct 32b" src="https://preview.redd.it/yfu1j9qhw5ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08e2bd1a7385f636f30fe8884c1fb096e06cdc40" title="AIDER - As I suspected QwQ 32b is much smarter in coding than qwen 2.5 coder instruct 32b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yfu1j9qhw5ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ao2j/aider_as_i_suspected_qwq_32b_is_much_smarter_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ao2j/aider_as_i_suspected_qwq_32b_is_much_smarter_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T00:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5u8h2</id>
    <title>OWL: The #1 Open-source agent on GAIA leaderboard</title>
    <updated>2025-03-07T17:24:01+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Processing img rbv952aqxane1...&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5u8h2/owl_the_1_opensource_agent_on_gaia_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5u8h2/owl_the_1_opensource_agent_on_gaia_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5u8h2/owl_the_1_opensource_agent_on_gaia_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T17:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5qofh</id>
    <title>HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs</title>
    <updated>2025-03-07T15:20:15+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.02003"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qofh/hot_highlighted_chain_of_thought_for_referencing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qofh/hot_highlighted_chain_of_thought_for_referencing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T15:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j60bc4</id>
    <title>AMD &amp; tinygrad cooperation happening</title>
    <updated>2025-03-07T21:08:39+00:00</updated>
    <author>
      <name>/u/Danmoreng</name>
      <uri>https://old.reddit.com/user/Danmoreng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"&gt; &lt;img alt="AMD &amp;amp; tinygrad cooperation happening" src="https://external-preview.redd.it/BbTfUmQNA0TFAIFilHM3Y4In9mUXZYXfhSxTgqgwTN8.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b210445be3d20e6a3900593f94809ea783979b0" title="AMD &amp;amp; tinygrad cooperation happening" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danmoreng"&gt; /u/Danmoreng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/AnushElangovan/status/1898101178728431637"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j60bc4/amd_tinygrad_cooperation_happening/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T21:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j59fue</id>
    <title>Meta drops AI bombshell: Latent tokens help to improve LLM reasoning</title>
    <updated>2025-03-06T23:29:50+00:00</updated>
    <author>
      <name>/u/Dense-Smf-6032</name>
      <uri>https://old.reddit.com/user/Dense-Smf-6032</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"&gt; &lt;img alt="Meta drops AI bombshell: Latent tokens help to improve LLM reasoning" src="https://a.thumbs.redditmedia.com/5Ri36_q4vpEcMh6tQDQuJzQoWFUwmI_Eb-w-OMLVXh8.jpg" title="Meta drops AI bombshell: Latent tokens help to improve LLM reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper link: &lt;a href="https://arxiv.org/abs/2502.03275"&gt;https://arxiv.org/abs/2502.03275&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: The researcher from Meta AI found compressing text with a vqvae into latent-tokens and then adding them onto the training helps to improve LLM reasoning capability.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7yhbmserm5ne1.png?width=2232&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17aa25843b32da965b7f92778e5a6e6bdaf1537b"&gt;https://preview.redd.it/7yhbmserm5ne1.png?width=2232&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17aa25843b32da965b7f92778e5a6e6bdaf1537b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dense-Smf-6032"&gt; /u/Dense-Smf-6032 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j59fue/meta_drops_ai_bombshell_latent_tokens_help_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T23:29:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1j53w92</id>
    <title>Intro to DeepSeek's open-source week and why it's a big deal</title>
    <updated>2025-03-06T19:32:52+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"&gt; &lt;img alt="Intro to DeepSeek's open-source week and why it's a big deal" src="https://preview.redd.it/3dvsybsvf4ne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9eddab159fdcfb7e8ea6d001d8fc521b45a52638" title="Intro to DeepSeek's open-source week and why it's a big deal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dvsybsvf4ne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j53w92/intro_to_deepseeks_opensource_week_and_why_its_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T19:32:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5hb4p</id>
    <title>Ensure you use the appropriate temperature of 0.6 with QwQ-32B</title>
    <updated>2025-03-07T06:35:39+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt; &lt;img alt="Ensure you use the appropriate temperature of 0.6 with QwQ-32B" src="https://external-preview.redd.it/6TRd04lcKHQEO7NFYroC88UsYfg6QAwSPoiUg0dROsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59db8a1b256d27e6f63efdf37ea7de63d8be02e2" title="Ensure you use the appropriate temperature of 0.6 with QwQ-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1j5hb4p/video/jjveqqjjo7ne1/player"&gt;ball bouncing inside of a hexagon as it rotates&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;can you give me a pygame script that allows me to have a ball bouncing inside of a hexagon as it rotates, makes sure to handle collisions and gravity&amp;quot;&lt;/p&gt; &lt;p&gt;Yesterday, I tried this prompt it failed. I was very disappointed since it spend 15 mins on it. &lt;/p&gt; &lt;p&gt;Today, I notice the Ollama setting have been updated to with the correct temperature.&lt;br /&gt; You can find the recommend settings here.&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json"&gt;https://huggingface.co/Qwen/QwQ-32B/blob/main/generation_config.json&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5hb4p/ensure_you_use_the_appropriate_temperature_of_06/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:35:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j55tnf</id>
    <title>Anthropic warns White House about R1 and suggests "equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention"</title>
    <updated>2025-03-06T20:53:47+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"&gt; &lt;img alt="Anthropic warns White House about R1 and suggests &amp;quot;equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention&amp;quot;" src="https://external-preview.redd.it/4jL_cfkf1eOugH8Cd6gazHmY0nNfDF0kn5G0AyNVMU4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ec84648eddc4b833c73bd77f2df99dd7c97d0bf" title="Anthropic warns White House about R1 and suggests &amp;quot;equipping the U.S. government with the capacity to rapidly evaluate whether future models—foreign or domestic—released onto the open internet internet possess security-relevant properties that merit national security attention&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j55tnf/anthropic_warns_white_house_about_r1_and_suggests/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-06T20:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5l0sv</id>
    <title>Mistral's New OCR Model (SaaS) - Best in Class</title>
    <updated>2025-03-07T11:04:24+00:00</updated>
    <author>
      <name>/u/Auslieferator</name>
      <uri>https://old.reddit.com/user/Auslieferator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral just announced a new model specialized on OCR, beating Azure OCR and Google Document AI. Sadly it is only offered as SaaS and not open weights as a lot of their models were. &lt;a href="https://mistral.ai/fr/news/mistral-ocr"&gt;https://mistral.ai/fr/news/mistral-ocr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you know about any other OCR specialized LLMs or stacks of LLMs and other computer vision software, achieving similar results? I am currently playing with Qwen2.5 VL 7B Instruct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Auslieferator"&gt; /u/Auslieferator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l0sv/mistrals_new_ocr_model_saas_best_in_class/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l0sv/mistrals_new_ocr_model_saas_best_in_class/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l0sv/mistrals_new_ocr_model_saas_best_in_class/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T11:04:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5ievy</id>
    <title>FT: Llama 4 w/ voice expected in coming weeks</title>
    <updated>2025-03-07T07:55:53+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like good self-hosted voice chat that isn't tacked on will soon be available from both Sesame and Meta! Can't wait!&lt;/p&gt; &lt;p&gt;P.S. Is anyone working on an iOS app with CarPlay that lets me talk to my private AI server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/a1014427-c2ce-4204-b41a-001277309cea"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ievy/ft_llama_4_w_voice_expected_in_coming_weeks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5ievy/ft_llama_4_w_voice_expected_in_coming_weeks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5lym7</id>
    <title>Lightweight Hallucination Detector for Local RAG Setups - No Extra LLM Calls Required</title>
    <updated>2025-03-07T12:05:37+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I've been working on solving a common problem many of us face when running RAG systems with our local models - hallucinations. While our locally-hosted LLMs are impressive, they still tend to make things up when using RAG, especially when running smaller models with limited context windows.&lt;/p&gt; &lt;p&gt;I've released an &lt;strong&gt;open-source hallucination detector&lt;/strong&gt; that's specifically designed to be efficient enough to run on consumer hardware alongside your local LLMs. Unlike other solutions that require additional LLM API calls (which add latency and often external dependencies), this is a lightweight transformer-based classifier.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Based on modernBERT architecture&lt;/li&gt; &lt;li&gt;Inference speed: ~1 example/second on CPU, ~10-20 examples/second on modest GPU&lt;/li&gt; &lt;li&gt;Zero external API dependencies - runs completely local&lt;/li&gt; &lt;li&gt;Works with any LLM output, including Llama-2, Llama-3, Mistral, Phi-3, etc.&lt;/li&gt; &lt;li&gt;Integrates easily with LlamaIndex, LangChain, or your custom RAG pipeline&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; The detector evaluates your LLM's response against the retrieved context to identify when the model generates information not present in the source material. It achieves 80.7% recall on the RAGTruth benchmark, with particularly strong performance on data-to-text tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example integration with your local setup:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from adaptive_classifier import AdaptiveClassifier # Load the hallucination detector (downloads once, runs locally after) detector = AdaptiveClassifier.from_pretrained(&amp;quot;adaptive-classifier/llm-hallucination-detector&amp;quot;) # Your existing RAG pipeline context = retriever.get_relevant_documents(query) response = your_local_llm.generate(context, query) # Format for the detector input_text = f&amp;quot;Context: {context}\nQuestion: {query}\nAnswer: {response}&amp;quot; # Check for hallucinations prediction = detector.predict(input_text) if prediction[0][0] == 'HALLUCINATED' and prediction[0][1] &amp;gt; 0.6: print(&amp;quot;⚠️ Warning: Response appears to contain information not in the context&amp;quot;) # Maybe re-generate or add a disclaimer &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The detector is part of the adaptive-classifier library which also has tools for routing between different local models based on query complexity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions for the community:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How have you been addressing hallucinations in your local RAG setups?&lt;/li&gt; &lt;li&gt;Would a token-level detector (highlighting exactly which parts are hallucinated) be useful?&lt;/li&gt; &lt;li&gt;What's your typical resource budget for this kind of auxiliary model in your stack?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/codelion/adaptive-classifier"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt;&lt;br /&gt; Docs: &lt;a href="https://github.com/codelion/adaptive-classifier#hallucination-detector"&gt;https://github.com/codelion/adaptive-classifier#hallucination-detector&lt;/a&gt;&lt;br /&gt; Installation: &lt;code&gt;pip install adaptive-classifier&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5lym7/lightweight_hallucination_detector_for_local_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T12:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5s629</id>
    <title>The Genius of DeepSeek’s 57X Efficiency Boost [MLA]</title>
    <updated>2025-03-07T16:12:48+00:00</updated>
    <author>
      <name>/u/Different-Olive-8745</name>
      <uri>https://old.reddit.com/user/Different-Olive-8745</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"&gt; &lt;img alt="The Genius of DeepSeek’s 57X Efficiency Boost [MLA]" src="https://external-preview.redd.it/cNA68CfuuchA-pKjC19aWS3fLxcXM9n1EmEaCjksBPI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e6d61fc0d91e37ef5748cd5797b5f122d3ab207" title="The Genius of DeepSeek’s 57X Efficiency Boost [MLA]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Olive-8745"&gt; /u/Different-Olive-8745 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0VLAoVGf_74?si=OSwMMKsz9EpLOISJ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5s629/the_genius_of_deepseeks_57x_efficiency_boost_mla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T16:12:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5l66b</id>
    <title>Flappy Bird game by QwQ 32B IQ4_XS GGUF</title>
    <updated>2025-03-07T11:14:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt; &lt;img alt="Flappy Bird game by QwQ 32B IQ4_XS GGUF" src="https://external-preview.redd.it/MDVpZGpwYnUzOW5lMU90rGzJ1hZd2Ko9NJiQB4OtIZYL8dNtOZuKS2VIGG38.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78b96d60c5871140edd7d5640114998de50d9192" title="Flappy Bird game by QwQ 32B IQ4_XS GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6usunobu39ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5l66b/flappy_bird_game_by_qwq_32b_iq4_xs_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T11:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5vjtr</id>
    <title>Cydonia 24B v2.1 - Bolder, better, brighter</title>
    <updated>2025-03-07T18:10:20+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"&gt; &lt;img alt="Cydonia 24B v2.1 - Bolder, better, brighter" src="https://external-preview.redd.it/Y53sGNZqFT7O2LPxR0ifOBO74G3g4p3oD2d89H5ZiiM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f99bb247d7af48f7df7184bd4ce33a8e1090f74f" title="Cydonia 24B v2.1 - Bolder, better, brighter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v2.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5vjtr/cydonia_24b_v21_bolder_better_brighter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T18:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5i8di</id>
    <title>QwQ Bouncing ball (it took 15 minutes of yapping)</title>
    <updated>2025-03-07T07:42:23+00:00</updated>
    <author>
      <name>/u/philschmid</name>
      <uri>https://old.reddit.com/user/philschmid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt; &lt;img alt="QwQ Bouncing ball (it took 15 minutes of yapping)" src="https://external-preview.redd.it/c3MxaHh0bHoxOG5lMVsIOv4dsf9lRkZrSYg6c4izCiXravlzRnamhYWv4oaG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51e2eb5a7001f62ab3db3f78e329b604eb60560a" title="QwQ Bouncing ball (it took 15 minutes of yapping)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philschmid"&gt; /u/philschmid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/c2tvpslz18ne1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5i8di/qwq_bouncing_ball_it_took_15_minutes_of_yapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T07:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5zzue</id>
    <title>QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!</title>
    <updated>2025-03-07T20:48:08+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt; &lt;img alt="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" src="https://preview.redd.it/gc42vz36ybne1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a74298e2e3d4a3128892ea9834b44f8efd5e1a9" title="QwQ on LiveBench - is better than Sonnet 3.7 (non thinking)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gc42vz36ybne1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5zzue/qwq_on_livebench_is_better_than_sonnet_37_non/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T20:48:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5h7k8</id>
    <title>QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags</title>
    <updated>2025-03-07T06:28:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt; &lt;img alt="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" src="https://preview.redd.it/efyqdgtwo7ne1.jpeg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c2f05b3315ef35bc7ca516d97097a37aff4994d" title="QwQ, one token after giving the most incredible R1-destroying correct answer in its think tags" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/efyqdgtwo7ne1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5h7k8/qwq_one_token_after_giving_the_most_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T06:28:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5wzea</id>
    <title>New AMD Driver Yields Up To 11% Performance Increase In koboldcpp</title>
    <updated>2025-03-07T19:02:32+00:00</updated>
    <author>
      <name>/u/WokeCapitalist</name>
      <uri>https://old.reddit.com/user/WokeCapitalist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-7000-series/amd-radeon-rx-7900-xt.html"&gt;AMD's Adrenalin 25.3.1 driver&lt;/a&gt; release mentioned &lt;strong&gt;&lt;em&gt;&amp;quot;AI Performance Improvements on AMD Radeon™ RX 7000 Series&amp;quot;&lt;/em&gt;&lt;/strong&gt; in the release notes along with some large percentage increases for applications like Adobe Lightroom Denoise or DaVinci Resolve. As I had their previous WHQL recommended driver already installed, I decided to test it out in koboldcpp. It turns out there was a nice performance bump there, too. Worth a download if you haven't done so already!&lt;/p&gt; &lt;h1&gt;Hardware Test Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Microsoft Windows 11 Professional (x64) Build 26100.3194 (24H2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel Core Ultra 7 265K&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; AMD Radeon RX 7900 XT (20GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; ASRock Z890I Nova WiFi (BIOS 2.22)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disk:&lt;/strong&gt; Lexar SSD NM800PRO 2TB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 64GB (2×32GB DDR5 6400 CL32)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Secure Boot, BitLocker &amp;amp; HVCI Enabled&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Software Test Setup:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; koboldcpp 1.83.1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; phi-4-q4,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Size&lt;/strong&gt;: 16384&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BLAS Batch Size&lt;/strong&gt;: 512&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: 8&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Layers&lt;/strong&gt;: 43/43&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Backend: Vulkan&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;30.312 s&lt;/td&gt; &lt;td align="left"&gt;27.607 s&lt;/td&gt; &lt;td align="left"&gt;8.95% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;537.21 T/s&lt;/td&gt; &lt;td align="left"&gt;589.85 T/s&lt;/td&gt; &lt;td align="left"&gt;9.84% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.203 s&lt;/td&gt; &lt;td align="left"&gt;5.301 s&lt;/td&gt; &lt;td align="left"&gt;1.87% slower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;19.22 T/s&lt;/td&gt; &lt;td align="left"&gt;18.86 T/s&lt;/td&gt; &lt;td align="left"&gt;1.88% lower&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;35.515 s&lt;/td&gt; &lt;td align="left"&gt;32.908 s&lt;/td&gt; &lt;td align="left"&gt;7.35% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Backend: ROCm&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 24.12.1&lt;/th&gt; &lt;th align="left"&gt;Adrenalin 25.3.1&lt;/th&gt; &lt;th align="left"&gt;% Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Time&lt;/td&gt; &lt;td align="left"&gt;24.861 s&lt;/td&gt; &lt;td align="left"&gt;22.370 s&lt;/td&gt; &lt;td align="left"&gt;10.06% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Processing Speed&lt;/td&gt; &lt;td align="left"&gt;655.00 T/s&lt;/td&gt; &lt;td align="left"&gt;727.94 T/s&lt;/td&gt; &lt;td align="left"&gt;11.15% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Time&lt;/td&gt; &lt;td align="left"&gt;5.831 s&lt;/td&gt; &lt;td align="left"&gt;5.586 s&lt;/td&gt; &lt;td align="left"&gt;4.20% faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation Speed&lt;/td&gt; &lt;td align="left"&gt;17.15 T/s&lt;/td&gt; &lt;td align="left"&gt;17.90 T/s&lt;/td&gt; &lt;td align="left"&gt;4.32% higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Time&lt;/td&gt; &lt;td align="left"&gt;30.692 s&lt;/td&gt; &lt;td align="left"&gt;27.956 s&lt;/td&gt; &lt;td align="left"&gt;8.97% faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WokeCapitalist"&gt; /u/WokeCapitalist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5wzea/new_amd_driver_yields_up_to_11_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T19:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j5qo7q</id>
    <title>QwQ-32B infinite generations fixes + best practices, bug fixes</title>
    <updated>2025-03-07T15:20:03+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt; &lt;img alt="QwQ-32B infinite generations fixes + best practices, bug fixes" src="https://external-preview.redd.it/C8aU2vS5rsrlIktUq8a_5r42ZGVY34rKstBbebj3EEA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09742bfb9b718b50a05ce6019bcbb8a232d8e890" title="QwQ-32B infinite generations fixes + best practices, bug fixes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! If you're having &lt;strong&gt;infinite repetitions with QwQ-32B&lt;/strong&gt;, you're not alone! I made a guide to help debug stuff! I also uploaded dynamic 4bit quants &amp;amp; other GGUFs! Link to guide: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When using &lt;strong&gt;repetition penalties&lt;/strong&gt; to counteract looping, it rather causes looping!&lt;/li&gt; &lt;li&gt;The Qwen team confirmed for long context (128K), you should use YaRN.&lt;/li&gt; &lt;li&gt;When using repetition penalties, add &lt;code&gt;--samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot;&lt;/code&gt; to stop infinite generations.&lt;/li&gt; &lt;li&gt;Using &lt;code&gt;min_p = 0.1&lt;/code&gt; helps remove low probability tokens.&lt;/li&gt; &lt;li&gt;Try using &lt;code&gt;--repeat-penalty 1.1 --dry-multiplier 0.5&lt;/code&gt; to reduce repetitions.&lt;/li&gt; &lt;li&gt;Please use &lt;code&gt;--temp 0.6 --top-k 40 --top-p 0.95&lt;/code&gt; as suggested by the Qwen team.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For example my settings in llama.cpp which work great - uses the DeepSeek R1 1.58bit Flappy Bird test I introduced back here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ibbloy/158bit_deepseek_r1_131gb_dynamic_gguf/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \ --threads 32 \ --ctx-size 16384 \ --n-gpu-layers 99 \ --seed 3407 \ --prio 2 \ --temp 0.6 \ --repeat-penalty 1.1 \ --dry-multiplier 0.5 \ --min-p 0.1 \ --top-k 40 \ --top-p 0.95 \ -no-cnv \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also uploaded dynamic 4bit quants for QwQ to &lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit&lt;/a&gt; which are directly vLLM compatible since 0.7.3&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w65lgkmh5ane1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f77f68e9639bbd8dccdb51c1314d084802b7b213"&gt;Quantization errors for QwQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links to models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-GGUF"&gt;QwQ-32B GGUFs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit"&gt;QwQ-32B dynamic 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B-bnb-4bit"&gt;QwQ-32B bitsandbytes 4bit&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/QwQ-32B"&gt;QwQ-32B 16bit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote more details on my findings, and made a guide here: &lt;a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively"&gt;https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j5qo7q/qwq32b_infinite_generations_fixes_best_practices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-07T15:20:03+00:00</published>
  </entry>
</feed>
