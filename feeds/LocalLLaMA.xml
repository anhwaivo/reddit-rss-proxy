<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-17T17:06:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lcrt1k</id>
    <title>Just finished recording 29 videos on "How to Build DeepSeek from Scratch"</title>
    <updated>2025-06-16T12:43:06+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playlist link: &lt;a href="https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms"&gt;https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are the 29 videos and their title:&lt;/p&gt; &lt;p&gt;(1) DeepSeek series introduction&lt;/p&gt; &lt;p&gt;(2) DeepSeek basics&lt;/p&gt; &lt;p&gt;(3) Journey of a token into the LLM architecture&lt;/p&gt; &lt;p&gt;(4) Attention mechanism explained in 1 hour&lt;/p&gt; &lt;p&gt;(5) Self Attention Mechanism - Handwritten from scratch&lt;/p&gt; &lt;p&gt;(6) Causal Attention Explained: Don't Peek into the Future&lt;/p&gt; &lt;p&gt;(7) Multi-Head Attention Visually Explained&lt;/p&gt; &lt;p&gt;(8) Multi-Head Attention Handwritten from Scratch&lt;/p&gt; &lt;p&gt;(9) Key Value Cache from Scratch&lt;/p&gt; &lt;p&gt;(10) Multi-Query Attention Explained&lt;/p&gt; &lt;p&gt;(11) Understand Grouped Query Attention (GQA)&lt;/p&gt; &lt;p&gt;(12) Multi-Head Latent Attention From Scratch&lt;/p&gt; &lt;p&gt;(13) Multi-Head Latent Attention Coded from Scratch in Python&lt;/p&gt; &lt;p&gt;(14) Integer and Binary Positional Encodings&lt;/p&gt; &lt;p&gt;(15) All about Sinusoidal Positional Encodings&lt;/p&gt; &lt;p&gt;(16) Rotary Positional Encodings&lt;/p&gt; &lt;p&gt;(17) How DeepSeek exactly implemented Latent Attention | MLA + RoPE&lt;/p&gt; &lt;p&gt;(18) Mixture of Experts (MoE) Introduction&lt;/p&gt; &lt;p&gt;(19) Mixture of Experts Hands on Demonstration&lt;/p&gt; &lt;p&gt;(20) Mixture of Experts Balancing Techniques&lt;/p&gt; &lt;p&gt;(21) How DeepSeek rewrote Mixture of Experts (MoE)?&lt;/p&gt; &lt;p&gt;(22) Code Mixture of Experts (MoE) from Scratch in Python&lt;/p&gt; &lt;p&gt;(23) Multi-Token Prediction Introduction&lt;/p&gt; &lt;p&gt;(24) How DeepSeek rewrote Multi-Token Prediction&lt;/p&gt; &lt;p&gt;(25) Multi-Token Prediction coded from scratch&lt;/p&gt; &lt;p&gt;(26) Introduction to LLM Quantization&lt;/p&gt; &lt;p&gt;(27) How DeepSeek rewrote Quantization Part 1&lt;/p&gt; &lt;p&gt;(28) How DeepSeek rewrote Quantization Part 2&lt;/p&gt; &lt;p&gt;(29) Build DeepSeek from Scratch 20 minute summary&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcrt1k/just_finished_recording_29_videos_on_how_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcrt1k/just_finished_recording_29_videos_on_how_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcrt1k/just_finished_recording_29_videos_on_how_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T12:43:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld8gs4</id>
    <title>Fine-tuning may be underestimated</title>
    <updated>2025-06-16T23:44:37+00:00</updated>
    <author>
      <name>/u/AgreeableCaptain1372</name>
      <uri>https://old.reddit.com/user/AgreeableCaptain1372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often see comments and posts online dismissing fine-tuning and saying that RAG is the way to go. While RAG is very powerful, what if i want to save both on tokens and compute? Fine tuning allows you to achieve the same results as RAG with smaller LLMs and fewer tokens. LORA won‚Äôt always be enough but you can get a model to memorize much of what a RAG knowledge base contains with a full fine tune. And the best part is you don‚Äôt need a huge model, the model can suck at everything else as long as it excels at your very specialized task. Even if you struggle to make the model memorize enough from your knowledge base and still need RAG, you will still save on compute by being able to rely on a smaller-sized LLM.&lt;/p&gt; &lt;p&gt;Now I think a big reason for this dismissal is many people seem to equate fine tuning to LORA and don't consider full tuning. Granted, full fine tuning is more expensive in the short run but it pays off in the long run.&lt;/p&gt; &lt;p&gt;Edit: when I say you can achieve the same results as RAG, this is mostly true for knowledge that does not require frequent updating. If your knowledge base changes every day, definitely agree RAG is more economical. In practice they can both be used together since a lot of domain knowledge can be either long term or short term.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgreeableCaptain1372"&gt; /u/AgreeableCaptain1372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld8gs4/finetuning_may_be_underestimated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld8gs4/finetuning_may_be_underestimated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld8gs4/finetuning_may_be_underestimated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T23:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lcw50r</id>
    <title>Kimi-Dev-72B</title>
    <updated>2025-06-16T15:40:31+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcw50r/kimidev72b/"&gt; &lt;img alt="Kimi-Dev-72B" src="https://external-preview.redd.it/1kvJDTWOvntivVoW834gDLI4V0P6WaqmrGfz5xyEWNU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b09e977edec166ad9c212551ee72f79018be5fa2" title="Kimi-Dev-72B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Dev-72B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lcw50r/kimidev72b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lcw50r/kimidev72b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T15:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldaco5</id>
    <title>üö∏Trained a Tiny Model(30 million parameter) to Tell Children's Stories!üö∏</title>
    <updated>2025-06-17T01:15:18+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldaco5/trained_a_tiny_model30_million_parameter_to_tell/"&gt; &lt;img alt="üö∏Trained a Tiny Model(30 million parameter) to Tell Children's Stories!üö∏" src="https://b.thumbs.redditmedia.com/1oEWEscJzgv9TuHB3scEvlbc3g52nDhz0QGOEo48yno.jpg" title="üö∏Trained a Tiny Model(30 million parameter) to Tell Children's Stories!üö∏" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wondered if a small language model, just 30 million parameters, could write meaningful, imaginative stories for kids? So I built one and it works.&lt;/p&gt; &lt;p&gt;Introducing Tiny-Children-Stories, a purpose-built, open-source model that specializes in generating short and creative stories.&lt;/p&gt; &lt;p&gt;üìå Why I Built It&lt;/p&gt; &lt;p&gt;Most large language models are incredibly powerful, but also incredibly resource-hungry. I wanted to explore:&lt;/p&gt; &lt;p&gt;‚úÖ Can a tiny model be fine-tuned for a specific task like storytelling?&lt;/p&gt; &lt;p&gt;‚úÖ Can models this small actually create engaging content?&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/25k6377v1e7f1.gif"&gt;https://i.redd.it/25k6377v1e7f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìå What‚Äôs Inside&lt;/p&gt; &lt;p&gt;I trained this model on a high-quality dataset of Children-Stories-Collection. The goal was to make the model understand not just language, but also intent, like writing an ‚Äúanimal friendship story‚Äù or a ‚Äúbedtime tale with a moral.‚Äù&lt;/p&gt; &lt;p&gt;‚ùì Why Build From Scratch?&lt;/p&gt; &lt;p&gt;You might wonder: why spend the extra effort training a brand-new model rather than simply fine-tuning an existing one? Building from scratch lets you tailor the architecture and training data specifically, so you only pay for the capacity you actually need. It gives you full control over behavior, keeps inference costs and environmental impact to a minimum, and most importantly, teaches you invaluable lessons about how model size, data quality, and tuning methods interact.&lt;/p&gt; &lt;p&gt;üìå If you're looking for a single tool to simplify your GenAI workflow and MCP integration, check out IdeaWeaver, your one-stop shop for Generative AI.Comprehensive documentation and examples&lt;/p&gt; &lt;p&gt;üîó Docs: &lt;a href="https://ideaweaver-ai-code.github.io/ideaweaver-docs/"&gt;https://ideaweaver-ai-code.github.io/ideaweaver-docs/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîó GitHub: &lt;a href="https://github.com/ideaweaver-ai-code/ideaweaver"&gt;https://github.com/ideaweaver-ai-code/ideaweaver&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ñ Try It Out or Build Your Own&lt;/p&gt; &lt;p&gt;üîó GitHub Repo: &lt;a href="https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model"&gt;https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚≠ê Star it if you think Tiny Models can do Big Things!&lt;/p&gt; &lt;p&gt;üôè Special thanks, this wouldn‚Äôt have been possible without these amazing folks:&lt;/p&gt; &lt;p&gt;1Ô∏è‚É£ &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7340544698115112960/#"&gt;Andrej Karpathy&lt;/a&gt; ‚Äì Your YouTube series on building an LLM from scratch made the whole process feel less intimidating and way more achievable. I must have watched those videos a dozen times.&lt;/p&gt; &lt;p&gt;2Ô∏è‚É£ &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7340544698115112960/#"&gt;Sebastian Raschka, PhD&lt;/a&gt;: Your book on building LLMs from scratch, honestly one of the best hands-on guides I‚Äôve come across. Clear, practical, and full of hard-won lessons.&lt;/p&gt; &lt;p&gt;3Ô∏è‚É£ The Vizura team: Your videos were a huge part of this journey.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldaco5/trained_a_tiny_model30_million_parameter_to_tell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldaco5/trained_a_tiny_model30_million_parameter_to_tell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldaco5/trained_a_tiny_model30_million_parameter_to_tell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T01:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld11x4</id>
    <title>Humanity's last library, which locally ran LLM would be best?</title>
    <updated>2025-06-16T18:43:42+00:00</updated>
    <author>
      <name>/u/TheCuriousBread</name>
      <uri>https://old.reddit.com/user/TheCuriousBread</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An apocalypse has come upon us. The internet is no more. Libraries are no more. The only things left are local networks and people with the electricity to run them. &lt;/p&gt; &lt;p&gt;If you were to create humanity's last library, a distilled LLM with the entirety of human knowledge. What would be a good model for that? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheCuriousBread"&gt; /u/TheCuriousBread &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:43:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldojsu</id>
    <title>What's your favorite desktop client?</title>
    <updated>2025-06-17T14:26:59+00:00</updated>
    <author>
      <name>/u/tuananh_org</name>
      <uri>https://old.reddit.com/user/tuananh_org</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I forgot to mention Linux. Prefer one with MCP support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tuananh_org"&gt; /u/tuananh_org &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldojsu/whats_your_favorite_desktop_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldojsu/whats_your_favorite_desktop_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldojsu/whats_your_favorite_desktop_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T14:26:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldsgf1</id>
    <title>Supercharge Your Coding Agent with Symbolic Tools</title>
    <updated>2025-06-17T16:57:31+00:00</updated>
    <author>
      <name>/u/Left-Orange2267</name>
      <uri>https://old.reddit.com/user/Left-Orange2267</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsgf1/supercharge_your_coding_agent_with_symbolic_tools/"&gt; &lt;img alt="Supercharge Your Coding Agent with Symbolic Tools" src="https://external-preview.redd.it/zBA49a0Cm-XBAD3DZH9SuQval19YIxsQsZErY_duL04.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1df560b5f346ab55b22136b14cd6a62eb30f133" title="Supercharge Your Coding Agent with Symbolic Tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How would you feel about writing code without proper IDE tooling? Your coding agent feels the same way! Some agents have symbolic tools to a degree (like cline, roo and so on), but many (like codex, opencoder and most others) don't and rely on just text matching, embeddings and file reading. Fortunately, it doesn't have to stay like this!&lt;/p&gt; &lt;p&gt;Include the open source (MIT) &lt;a href="https://github.com/oraios/serena"&gt;Serena MCP server&lt;/a&gt; into your project's toolbox and step into the light!&lt;/p&gt; &lt;p&gt;For example, for claude code it's just one shell command&lt;/p&gt; &lt;p&gt;&lt;code&gt;claude mcp add serena -- uvx --from git+https://github.com/oraios/serena serena-mcp-server --context ide-assistant --project $(pwd)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;If you enjoy this toolbox as much as I do, show some support by starring the repo and spreading the word ;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/skmgbriszh7f1.jpg?width=564&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cc0392ee94de6621f8d4158380ef6c39aad549e1"&gt;https://preview.redd.it/skmgbriszh7f1.jpg?width=564&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cc0392ee94de6621f8d4158380ef6c39aad549e1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Left-Orange2267"&gt; /u/Left-Orange2267 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsgf1/supercharge_your_coding_agent_with_symbolic_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsgf1/supercharge_your_coding_agent_with_symbolic_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsgf1/supercharge_your_coding_agent_with_symbolic_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T16:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldmui5</id>
    <title>Continuous LLM Loop for Real-Time Interaction</title>
    <updated>2025-06-17T13:15:43+00:00</updated>
    <author>
      <name>/u/skatardude10</name>
      <uri>https://old.reddit.com/user/skatardude10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuous inference is something I've been mulling over occasionally for a while (not referring to the usual run-on LLM output). It would be cool to break past the whole Query - Response paradigm and I think it's feasible. &lt;/p&gt; &lt;p&gt;Why: Steerable continuous stream of thought for, stories, conversation, assistant tasks, whatever.&lt;/p&gt; &lt;p&gt;The idea is pretty simple: &lt;/p&gt; &lt;p&gt;3 instances of Koboldcpp or llamacpp in a loop. Batch size of 1 for context / prompt processing latency. &lt;/p&gt; &lt;p&gt;Instance 1 is inferring tokens while instance 2 is processing instances 1's output token by token (context + instance 1 inference tokens). As soon as instance 1 stops inference, it continues prompt processing to stay caught up while instance 2 infers and feeds into instance 3. The cycle continues. &lt;/p&gt; &lt;p&gt;Options:&lt;br /&gt; - output length limited to one to a few tokens to take user input at any point during the loop. - explicitly stop generating whichever instance to take user input when sent to the loop - clever system prompting and timestamp injects for certain pad tokens during idle periods - tool calls/specific tokens or strings for adjusting inference speed / resource usage during idle periods (enable the loop to continue in the background, slowly,) - pad token output for idle times, regex to manage context on wake - additional system prompting for guiding the dynamics of the LLM loop (watch for timestamps, how many pad tokens, what is the conversation about, are we sitting here or actively brainstorming? Do you interrupt/bump your own speed up/clear pad tokens from your context and interject user freely?)&lt;/p&gt; &lt;p&gt;Anyways, I haven't thought down every single rabbit hole, but I feel like with small models these days on a 3090 this should be possible to get running in a basic form with a python script.&lt;/p&gt; &lt;p&gt;Has anyone else tried something like this yet? Either way, I think it would be cool to have a more dynamic framework beyond the basic query response that we could plug our own models into without having to train entirely new models meant for something like this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skatardude10"&gt; /u/skatardude10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldmui5/continuous_llm_loop_for_realtime_interaction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldmui5/continuous_llm_loop_for_realtime_interaction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldmui5/continuous_llm_loop_for_realtime_interaction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T13:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldqtwu</id>
    <title>Local Language Learning with Voice?</title>
    <updated>2025-06-17T15:55:37+00:00</updated>
    <author>
      <name>/u/Ok_Most9659</name>
      <uri>https://old.reddit.com/user/Ok_Most9659</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very interested in learning another language via speaking with a local LLM via voice. Speaking a language is much more helpful than only being able to communicate via writing. &lt;/p&gt; &lt;p&gt;Has anyone trialed this with any LLM model?&lt;br /&gt; If so what model do you recommend (including minimum parameter), any additional app/plug-in to enable voice? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Most9659"&gt; /u/Ok_Most9659 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqtwu/local_language_learning_with_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqtwu/local_language_learning_with_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqtwu/local_language_learning_with_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T15:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldjq1m</id>
    <title>I love the inference performances of QWEN3-30B-A3B but how do you use it in real world use case ? What prompts are you using ? What is your workflow ? How is it useful for you ?</title>
    <updated>2025-06-17T10:34:34+00:00</updated>
    <author>
      <name>/u/Whiplashorus</name>
      <uri>https://old.reddit.com/user/Whiplashorus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys I successful run on my old laptop QWEN3-30B-A3B-Q4-UD with 32K token window&lt;/p&gt; &lt;p&gt;I wanted to know how you use in real world use case this model.&lt;/p&gt; &lt;p&gt;And what are you best prompts for this specific model&lt;/p&gt; &lt;p&gt;Feel free to share your journey with me I need inspiration&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whiplashorus"&gt; /u/Whiplashorus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjq1m/i_love_the_inference_performances_of_qwen330ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjq1m/i_love_the_inference_performances_of_qwen330ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjq1m/i_love_the_inference_performances_of_qwen330ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T10:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldsez0</id>
    <title>Gemini 2.5 Pro and Flash are stable in AI Studio</title>
    <updated>2025-06-17T16:56:00+00:00</updated>
    <author>
      <name>/u/best_codes</name>
      <uri>https://old.reddit.com/user/best_codes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsez0/gemini_25_pro_and_flash_are_stable_in_ai_studio/"&gt; &lt;img alt="Gemini 2.5 Pro and Flash are stable in AI Studio" src="https://preview.redd.it/ng7glnbmpi7f1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19eafc0ab72b6a67d86e4a94088f6a7857bf8cfb" title="Gemini 2.5 Pro and Flash are stable in AI Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's also a new Gemini 2.5 flash preview model at the bottom there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/best_codes"&gt; /u/best_codes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ng7glnbmpi7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsez0/gemini_25_pro_and_flash_are_stable_in_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldsez0/gemini_25_pro_and_flash_are_stable_in_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T16:56:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld116d</id>
    <title>MiniMax latest open-sourcing LLM, MiniMax-M1 ‚Äî setting new standards in long-context reasoning,m</title>
    <updated>2025-06-16T18:42:52+00:00</updated>
    <author>
      <name>/u/srtng</name>
      <uri>https://old.reddit.com/user/srtng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"&gt; &lt;img alt="MiniMax latest open-sourcing LLM, MiniMax-M1 ‚Äî setting new standards in long-context reasoning,m" src="https://external-preview.redd.it/NmY1emg2N3kzYzdmMYrLLSKpxq16_nlRw_xdAcAPTlqNhk8r4UDdsUawD6kP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3275f690016b299979a56d72371c6133b5aa21d3" title="MiniMax latest open-sourcing LLM, MiniMax-M1 ‚Äî setting new standards in long-context reasoning,m" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The coding demo in video is so amazing!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;World‚Äôs longest context window: 1M-token input, 80k-token output&lt;/li&gt; &lt;li&gt;State-of-the-art agentic use among open-source models&lt;/li&gt; &lt;li&gt;&lt;p&gt;RL at unmatched efficiency: trained with just $534,700&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;40k: ‚Äã&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-40k"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M1-40k&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;80k: ‚Äã&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M1-80k"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M1-80k&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Space: ‚Äã&lt;a href="https://huggingface.co/spaces/MiniMaxAI/MiniMax-M1"&gt;https://huggingface.co/spaces/MiniMaxAI/MiniMax-M1&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/MiniMax-AI/MiniMax-M1"&gt;https://github.com/MiniMax-AI/MiniMax-M1&lt;/a&gt; &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tech Report: &lt;a href="https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf"&gt;https://github.com/MiniMax-AI/MiniMax-M1/blob/main/MiniMax_M1_tech_report.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Apache 2.0 license&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srtng"&gt; /u/srtng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t859utey3c7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld116d/minimax_latest_opensourcing_llm_minimaxm1_setting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T18:42:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldayo0</id>
    <title>Deepseek r1 0528 ties opus for #1 rank on webdev</title>
    <updated>2025-06-17T01:45:49+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;685 B params. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-0528&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/lmarena_ai/status/1934650635657367671"&gt;https://x.com/lmarena_ai/status/1934650635657367671&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldayo0/deepseek_r1_0528_ties_opus_for_1_rank_on_webdev/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldayo0/deepseek_r1_0528_ties_opus_for_1_rank_on_webdev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldayo0/deepseek_r1_0528_ties_opus_for_1_rank_on_webdev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T01:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lddrfu</id>
    <title>Quartet - a new algorithm for training LLMs in native FP4 on 5090s</title>
    <updated>2025-06-17T04:08:30+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across this paper while looking to see if training LLMs on Blackwell's new FP4 hardware was possible.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/papers/2505.14669"&gt;Quartet: Native FP4 Training Can Be Optimal for Large Language Models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and the associated code, with kernels you can use for your own training:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/IST-DASLab/Quartet"&gt;https://github.com/IST-DASLab/Quartet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to these researchers, training in FP4 is now a reasonable, and in many cases optimal, alternative to higher precision training!&lt;/p&gt; &lt;p&gt;DeepSeek was trained in FP8, which was cutting edge at the time. I can't wait to see the new frontiers FP4 unlocks.&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;I just tried to install it to start experimenting. Even though their README states &amp;quot;Kernels are 'Coming soon...'&amp;quot;, they created the python library for consumers to use a &lt;a href="https://github.com/IST-DASLab/Quartet/pull/3"&gt;couple weeks ago in a PR called &amp;quot;Kernels&amp;quot;&lt;/a&gt;, and included them in the initial release.&lt;/p&gt; &lt;p&gt;It seems that the actual cuda kernels are contained in a python package called &lt;code&gt;qutlass&lt;/code&gt;, however, and that does not appear to be published anywhere yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lddrfu/quartet_a_new_algorithm_for_training_llms_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lddrfu/quartet_a_new_algorithm_for_training_llms_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lddrfu/quartet_a_new_algorithm_for_training_llms_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T04:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldjd5t</id>
    <title>Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons</title>
    <updated>2025-06-17T10:12:14+00:00</updated>
    <author>
      <name>/u/jsonathan</name>
      <uri>https://old.reddit.com/user/jsonathan</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsonathan"&gt; /u/jsonathan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2506.01963"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjd5t/breaking_quadratic_barriers_a_nonattention_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjd5t/breaking_quadratic_barriers_a_nonattention_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T10:12:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldl4ii</id>
    <title>Latent Attention for Small Language Models</title>
    <updated>2025-06-17T11:53:39+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldl4ii/latent_attention_for_small_language_models/"&gt; &lt;img alt="Latent Attention for Small Language Models" src="https://b.thumbs.redditmedia.com/mwEsle7ZgYWmGBZGHOvoYZ1KFhy1Sk-MrECs6_POs7M.jpg" title="Latent Attention for Small Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h4pmsjrt7h7f1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1406dd1c4fe6260378cd828114ffaf2f1724b600"&gt;https://preview.redd.it/h4pmsjrt7h7f1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1406dd1c4fe6260378cd828114ffaf2f1724b600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Link to paper: &lt;a href="https://arxiv.org/pdf/2506.09342"&gt;https://arxiv.org/pdf/2506.09342&lt;/a&gt;&lt;/p&gt; &lt;p&gt;1) We trained 30M parameter Generative Pre-trained Transformer (GPT) models on 100,000 synthetic stories and benchmarked three architectural variants: standard multi-head attention (MHA), MLA, and MLA with rotary positional embeddings (MLA+RoPE).&lt;/p&gt; &lt;p&gt;(2) It led to a beautiful study in which we showed that MLA outperforms MHA: 45% memory reduction and 1.4 times inference speedup with minimal quality loss. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;This shows 2 things:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;(1) Small Language Models (SLMs) can become increasingly powerful when integrated with Multi-Head Latent Attention (MLA). &lt;/p&gt; &lt;p&gt;(2) All industries and startups building SLMs should replace MHA with MLA. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldl4ii/latent_attention_for_small_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldl4ii/latent_attention_for_small_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldl4ii/latent_attention_for_small_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T11:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldokl7</id>
    <title>Best frontend for vllm?</title>
    <updated>2025-06-17T14:27:51+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to optimise my inferences. &lt;/p&gt; &lt;p&gt;I use LM studio for an easy inference of llama.cpp but was wondering if there is a gui for more optimised inference. &lt;/p&gt; &lt;p&gt;Also is there anther gui for llama.cpp that lets you tweak inference settings a bit more? Like expert offloading etc? &lt;/p&gt; &lt;p&gt;Thanks!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldokl7/best_frontend_for_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldokl7/best_frontend_for_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldokl7/best_frontend_for_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T14:27:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ld66t0</id>
    <title>Fortune 500s Are Burning Millions on LLM APIs. Why Not Build Their Own?</title>
    <updated>2025-06-16T22:04:50+00:00</updated>
    <author>
      <name>/u/Neat-Knowledge5642</name>
      <uri>https://old.reddit.com/user/Neat-Knowledge5642</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You‚Äôre at a Fortune 500 company, spending millions annually on LLM APIs (OpenAI, Google, etc). Yet you‚Äôre limited by IP concerns, data control, and vendor constraints.&lt;/p&gt; &lt;p&gt;At what point does it make sense to build your own LLM in-house?&lt;/p&gt; &lt;p&gt;I work at a company behind one of the major LLMs, and the amount enterprises pay us is wild. Why aren‚Äôt more of them building their own models? Is it talent? Infra complexity? Risk aversion?&lt;/p&gt; &lt;p&gt;Curious where this logic breaks.&lt;/p&gt; &lt;p&gt;Edit: What about an acquisition? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neat-Knowledge5642"&gt; /u/Neat-Knowledge5642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld66t0/fortune_500s_are_burning_millions_on_llm_apis_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ld66t0/fortune_500s_are_burning_millions_on_llm_apis_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ld66t0/fortune_500s_are_burning_millions_on_llm_apis_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-16T22:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldfipl</id>
    <title>It seems as if the more you learn about AI, the less you trust it</title>
    <updated>2025-06-17T05:54:21+00:00</updated>
    <author>
      <name>/u/RhubarbSimilar1683</name>
      <uri>https://old.reddit.com/user/RhubarbSimilar1683</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is kind of a rant so sorry if not everything has to do with the title, For example, when the blog post on vibe coding was released on February 2025, I was surprised to see the writer talking about using it mostly for disposable projects and not for stuff that will go to production since that is what everyone seems to be using it for. That blog post was written by an OpenAI employee. Then Geoffrey Hinton and Yann LeCun occasionally talk about how AI can be dangerous if misused or how LLMs are not that useful currently because they don't really reason at an architectural level yet you see tons of people without the same level of education on AI selling snake oil based on LLMs. You then see people talking about how LLMs completely replace programmers even though senior programmers point out they seem to make subtle bugs all the time that people often can't find nor fix because they didn't learn programming since they thought it was obsolete.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RhubarbSimilar1683"&gt; /u/RhubarbSimilar1683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldfipl/it_seems_as_if_the_more_you_learn_about_ai_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldfipl/it_seems_as_if_the_more_you_learn_about_ai_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldfipl/it_seems_as_if_the_more_you_learn_about_ai_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T05:54:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldisw8</id>
    <title>nvidia/AceReason-Nemotron-1.1-7B ¬∑ Hugging Face</title>
    <updated>2025-06-17T09:35:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldisw8/nvidiaacereasonnemotron117b_hugging_face/"&gt; &lt;img alt="nvidia/AceReason-Nemotron-1.1-7B ¬∑ Hugging Face" src="https://external-preview.redd.it/W0uW5ur2PESMsYX8R36VZEsECrEgC1Wcshp3sPMT3JY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e3d6301fc6618942fb7fc855b933ef80b8bc864" title="nvidia/AceReason-Nemotron-1.1-7B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldisw8/nvidiaacereasonnemotron117b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldisw8/nvidiaacereasonnemotron117b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T09:35:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldhej3</id>
    <title>Who is ACTUALLY running local or open source model daily and mainly?</title>
    <updated>2025-06-17T07:59:50+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I've started to notice a lot of folk on here comment that they're using Claude or GPT, so: &lt;/p&gt; &lt;p&gt;Out of curiosity,&lt;br /&gt; - who is using local or open source models as their daily driver for any task: code, writing , agents?&lt;br /&gt; - what's you setup, are you serving remotely, sharing with friends, using local inference?&lt;br /&gt; - what kind if apps are you using? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldhej3/who_is_actually_running_local_or_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldhej3/who_is_actually_running_local_or_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldhej3/who_is_actually_running_local_or_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T07:59:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldr3ln</id>
    <title>Google launches Gemini 2.5 Flash Lite (API only)</title>
    <updated>2025-06-17T16:05:43+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldr3ln/google_launches_gemini_25_flash_lite_api_only/"&gt; &lt;img alt="Google launches Gemini 2.5 Flash Lite (API only)" src="https://preview.redd.it/93ekds1ugi7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a2557d0a1e69c350eb9aa80a0342ae2a8213d06" title="Google launches Gemini 2.5 Flash Lite (API only)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See &lt;a href="https://console.cloud.google.com/vertex-ai/studio/"&gt;https://console.cloud.google.com/vertex-ai/studio/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pricing not yet announced.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/93ekds1ugi7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldr3ln/google_launches_gemini_25_flash_lite_api_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldr3ln/google_launches_gemini_25_flash_lite_api_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T16:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldqroi</id>
    <title>A free goldmine of tutorials for the components you need to create production-level agents</title>
    <updated>2025-06-17T15:53:14+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I‚Äôve just launched a free resource with 25 detailed tutorials for building comprehensive production-level AI agents, as part of my Gen AI educational initiative.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The tutorials cover all the key components you need to create agents that are ready for real-world deployment. I plan to keep adding more tutorials over time and will make sure the content stays up to date.&lt;/p&gt; &lt;p&gt;The response so far has been incredible! (the repo got nearly 500 stars in just 8 hours from launch) This is part of my broader effort to create high-quality open source educational material. I already have over 100 code tutorials on GitHub with nearly 40,000 stars.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The link is in the first comment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The content is organized into these categories:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Orchestration&lt;/li&gt; &lt;li&gt;Tool integration&lt;/li&gt; &lt;li&gt;Observability&lt;/li&gt; &lt;li&gt;Deployment&lt;/li&gt; &lt;li&gt;Memory&lt;/li&gt; &lt;li&gt;UI &amp;amp; Frontend&lt;/li&gt; &lt;li&gt;Agent Frameworks&lt;/li&gt; &lt;li&gt;Model Customization&lt;/li&gt; &lt;li&gt;Multi-agent Coordination&lt;/li&gt; &lt;li&gt;Security&lt;/li&gt; &lt;li&gt;Evaluation&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqroi/a_free_goldmine_of_tutorials_for_the_components/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqroi/a_free_goldmine_of_tutorials_for_the_components/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldqroi/a_free_goldmine_of_tutorials_for_the_components/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T15:53:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldi5rs</id>
    <title>There are no plans for a Qwen3-72B</title>
    <updated>2025-06-17T08:52:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldi5rs/there_are_no_plans_for_a_qwen372b/"&gt; &lt;img alt="There are no plans for a Qwen3-72B" src="https://preview.redd.it/wwq0gc8bbg7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58bc7167b2a1d6c339112ec6468ce00e1eff9e6f" title="There are no plans for a Qwen3-72B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwq0gc8bbg7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldi5rs/there_are_no_plans_for_a_qwen372b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldi5rs/there_are_no_plans_for_a_qwen372b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T08:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ldjyhf</id>
    <title>Completed Local LLM Rig</title>
    <updated>2025-06-17T10:48:48+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/"&gt; &lt;img alt="Completed Local LLM Rig" src="https://external-preview.redd.it/HJkY2jxSg_GtjUMbmHI4EEBqY3YefZ9gwrvbaXuZONc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a5994778697f45b5780284b90c07b85daa01d2e" title="Completed Local LLM Rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So proud it's finally done!&lt;/p&gt; &lt;p&gt;GPU: 4 x RTX 3090 CPU: TR 3945wx 12c RAM: 256GB DDR4@3200MT/s SSD: PNY 3040 2TB MB: Asrock Creator WRX80 PSU: Seasonic Prime 2200W RAD: Heatkiller MoRa 420 Case: Silverstone RV-02&lt;/p&gt; &lt;p&gt;Was a long held dream to fit 4 x 3090 in an ATX form factor, all in my good old Silverstone Raven from 2011. An absolute classic. GPU temps at 57C.&lt;/p&gt; &lt;p&gt;Now waiting for the Fractal 180mm LED fans to put into the bottom. What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ldjyhf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ldjyhf/completed_local_llm_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-17T10:48:48+00:00</published>
  </entry>
</feed>
