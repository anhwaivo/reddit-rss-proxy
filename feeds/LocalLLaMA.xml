<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-23T13:49:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ksw070</id>
    <title>Genuine question: Why are the Unsloth GGUFs more preferred than the official ones?</title>
    <updated>2025-05-22T17:05:03+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's at least the case with the latest GLM, Gemma and Qwen models. Unlosh GGUFs are downloaded 5-10X more than the official ones.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksw070/genuine_question_why_are_the_unsloth_ggufs_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksw070/genuine_question_why_are_the_unsloth_ggufs_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksw070/genuine_question_why_are_the_unsloth_ggufs_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T17:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktgz28</id>
    <title>GUI RAG that can do an unlimited number of documents, or at least many</title>
    <updated>2025-05-23T11:17:55+00:00</updated>
    <author>
      <name>/u/Ponsky</name>
      <uri>https://old.reddit.com/user/Ponsky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most available LLM GUIs that can execute RAG can only handle 2 or 3 PDFs.&lt;/p&gt; &lt;p&gt;Are the any interfaces that can handle a bigger number ?&lt;/p&gt; &lt;p&gt;Sure, you can merge PDFs, but thatâ€™s a quite messy solution&lt;br /&gt; &lt;br /&gt; Thank You&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ponsky"&gt; /u/Ponsky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgz28/gui_rag_that_can_do_an_unlimited_number_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgz28/gui_rag_that_can_do_an_unlimited_number_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgz28/gui_rag_that_can_do_an_unlimited_number_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kthj8j</id>
    <title>Any drawbacks with putting a high end GPU together with a weak GPU on the same system?</title>
    <updated>2025-05-23T11:49:12+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Say one of them supports PCIe 5.0 x16 while the other is PCIe 5.0 x8 or even PCIe 4.0, and installed to appropriate PCIe slots that are not lower than the respective GPUs (in terms of PCIe support).&lt;/p&gt; &lt;p&gt;I vaguely recall we cannot mix memory sticks with different clock speeds, but not sure how this works for GPUs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kthj8j/any_drawbacks_with_putting_a_high_end_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kthj8j/any_drawbacks_with_putting_a_high_end_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kthj8j/any_drawbacks_with_putting_a_high_end_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt99hi</id>
    <title>Building a real-world LLM agent with open-source modelsâ€”structure &gt; prompt engineering</title>
    <updated>2025-05-23T02:59:41+00:00</updated>
    <author>
      <name>/u/Ecstatic-Cranberry90</name>
      <uri>https://old.reddit.com/user/Ecstatic-Cranberry90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on a production LLM agent the past couple months. Customer support use case with structured workflows like cancellations, refunds, and basic troubleshooting. After lots of playing with open models (Mistral, LLaMA, etc.), this is the first time it feels like the agent is reliable and not just a fancy demo.&lt;/p&gt; &lt;p&gt;Started out with a typical RAG + prompt stack (LangChain-style), but it wasnâ€™t cutting it. The agent would drift from instructions, invent things, or break tone consistency. Spent a ton of time tweaking prompts just to handle edge cases, and even then, things broke in weird ways.&lt;/p&gt; &lt;p&gt;What finally clicked was leaning into a more structured approach using a modeling framework called Parlant where I could define behavior in small, testable units instead of stuffing everything into a giant system prompt. That made it way easier to trace why things were going wrong and fix specific behaviors without destabilizing the rest.&lt;/p&gt; &lt;p&gt;Now the agent handles multi-turn flows cleanly, respects business rules, and behaves predictably even when users go off the happy path. Success rate across 80+ intents is north of 90%, with minimal hallucination.&lt;/p&gt; &lt;p&gt;This is only the beginning so wish me luck&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ecstatic-Cranberry90"&gt; /u/Ecstatic-Cranberry90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt99hi/building_a_realworld_llm_agent_with_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt99hi/building_a_realworld_llm_agent_with_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt99hi/building_a_realworld_llm_agent_with_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T02:59:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktgw6i</id>
    <title>AMD vs Nvidia LLM inference quality</title>
    <updated>2025-05-23T11:13:10+00:00</updated>
    <author>
      <name>/u/Ponsky</name>
      <uri>https://old.reddit.com/user/Ponsky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who have compared the same LLM using the same file with the same quant, fully loaded into VRAM.&lt;br /&gt; &lt;br /&gt; How do AMD and Nvidia compare ?&lt;br /&gt; &lt;br /&gt; Not asking about speed, but response quality.&lt;/p&gt; &lt;p&gt;Even if the response is not exactly the same, how is the response quality ?&lt;/p&gt; &lt;p&gt;Thank You &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ponsky"&gt; /u/Ponsky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgw6i/amd_vs_nvidia_llm_inference_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgw6i/amd_vs_nvidia_llm_inference_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgw6i/amd_vs_nvidia_llm_inference_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktabgk</id>
    <title>How to get the most out of my AMD 7900XT?</title>
    <updated>2025-05-23T03:57:34+00:00</updated>
    <author>
      <name>/u/crispyfrybits</name>
      <uri>https://old.reddit.com/user/crispyfrybits</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was forced to sell my Nvidia 4090 24GB this week to pay rent ðŸ˜­. I didn't know you could be so emotionally attached to a video card. &lt;/p&gt; &lt;p&gt;Anyway, my brother lent me his 7900XT until his rig is ready. I was just getting into local AI and want to continue. I've heard AMD is hard to support.&lt;/p&gt; &lt;p&gt;Can anyone help get me started on the right foot and advise what I need to get the most out this card?&lt;/p&gt; &lt;p&gt;Specs - Windows 11 Pro 64bit - AMD 7800X3D - AMD 7900XT 20GB - 32GB DDR5&lt;/p&gt; &lt;p&gt;Previously installed tools - Ollama - LM Studio&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crispyfrybits"&gt; /u/crispyfrybits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktabgk/how_to_get_the_most_out_of_my_amd_7900xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktabgk/how_to_get_the_most_out_of_my_amd_7900xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktabgk/how_to_get_the_most_out_of_my_amd_7900xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T03:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktdlqc</id>
    <title>Unfortunately, Claude 4 lags far behind O3 in the anti-fitting benchmark.</title>
    <updated>2025-05-23T07:28:50+00:00</updated>
    <author>
      <name>/u/flysnowbigbig</name>
      <uri>https://old.reddit.com/user/flysnowbigbig</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://llm-benchmark.github.io/"&gt;https://llm-benchmark.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;click the to expand all questions and answers for all models&lt;/p&gt; &lt;p&gt;I did not update the answers to CLAUDE 4 OPUS THINKING on the webpage. I only tried a few major questions (the rest were even more impossible to answer correctly). I only got 0.5 of the 8 questions right, which is not much different from the total errors in C3.7.ï¼ˆIf there is significant progress, I will update the page.ï¼‰&lt;/p&gt; &lt;p&gt;At present, O3 is still far ahead&lt;/p&gt; &lt;p&gt;I guess the secret is that there should be higher quality customized reasoning data sets, which need to be produced by hiring people. Maybe this is the biggest secret.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/flysnowbigbig"&gt; /u/flysnowbigbig &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktdlqc/unfortunately_claude_4_lags_far_behind_o3_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktdlqc/unfortunately_claude_4_lags_far_behind_o3_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktdlqc/unfortunately_claude_4_lags_far_behind_o3_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T07:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kta3re</id>
    <title>Is Claude 4 worse than 3.7 for anyone else?</title>
    <updated>2025-05-23T03:45:40+00:00</updated>
    <author>
      <name>/u/TrekkiMonstr</name>
      <uri>https://old.reddit.com/user/TrekkiMonstr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know, I know, whenever a model comes out you get people saying this, but it's on very concrete things for me, I'm not just biased against it. For reference, I'm comparing 4 Sonnet (concise) with 3.7 Sonnet (concise), no reasoning for either.&lt;/p&gt; &lt;p&gt;I asked it to calculate the total markup I paid at a gas station relative to the supermarket. I gave it quantities in a way I thought was clear (&amp;quot;I got three protein bars and three milks, one of the others each. What was the total markup I paid?&amp;quot;, but that's later in the conversation after it searched for prices). And indeed, 3.7 understands this without any issue (and I regenerated the message to make sure it wasn't a fluke). But with 4, even with much back and forth and several regenerations, it kept interpreting this as 3 milk, 1 protein bar, 1 [other item], 1 [other item], until I very explicitly laid it out as I just did.&lt;/p&gt; &lt;p&gt;And then, another conversation, I ask it, &amp;quot;Does this seem correct, or too much?&amp;quot; with a photo of food, and macro estimates for the meal in a screenshot. Again, 3.7 understands this fine, as asking whether the figures seem to be an accurate estimate. Whereas 4, again with a couple regenerations to test, seems to think I'm asking whether it's an appropriate meal (as in, not too much food for dinner or whatever). And in one instance, misreads the screenshot (thinking that the number of calories I will have cumulatively eaten after that meal is the number of calories &lt;em&gt;of&lt;/em&gt; that meal).&lt;/p&gt; &lt;p&gt;Is anyone else seeing any issues like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TrekkiMonstr"&gt; /u/TrekkiMonstr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kta3re/is_claude_4_worse_than_37_for_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kta3re/is_claude_4_worse_than_37_for_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kta3re/is_claude_4_worse_than_37_for_anyone_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T03:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kthdzn</id>
    <title>Stacking 2x3090s back to back for inference only - thermals</title>
    <updated>2025-05-23T11:41:05+00:00</updated>
    <author>
      <name>/u/YouAreRight007</name>
      <uri>https://old.reddit.com/user/YouAreRight007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone running 2x3090s stacked (no gap) for Llama 70B inference?&lt;br /&gt; If so, how are your temperatures looking when utilizing both cards for inference?&lt;/p&gt; &lt;p&gt;My single 3090 averages around 35-40% load (140 watts) for inference on 32GB 4bit models. Temperatures are around 60 degrees. &lt;/p&gt; &lt;p&gt;So it seems reasonable to me that I could stack 2x3090s right next to each, and have okay thermals provided the load on the cards remains close to or under 40%/140watts. &lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YouAreRight007"&gt; /u/YouAreRight007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kthdzn/stacking_2x3090s_back_to_back_for_inference_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kthdzn/stacking_2x3090s_back_to_back_for_inference_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kthdzn/stacking_2x3090s_back_to_back_for_inference_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:41:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiusw</id>
    <title>nanoVLM: The simplest repository to train your VLM in pure PyTorch</title>
    <updated>2025-05-23T12:54:55+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiusw/nanovlm_the_simplest_repository_to_train_your_vlm/"&gt; &lt;img alt="nanoVLM: The simplest repository to train your VLM in pure PyTorch" src="https://external-preview.redd.it/k3XI6YWGCxh9L4PoRExljDZTmAkbUgwnwQi71BtdC9A.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06978f4f95414bba1cfe00e253ce645b2a32d135" title="nanoVLM: The simplest repository to train your VLM in pure PyTorch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/nanovlm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiusw/nanovlm_the_simplest_repository_to_train_your_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiusw/nanovlm_the_simplest_repository_to_train_your_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:54:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt7u1n</id>
    <title>BTW: If you are getting a single GPU, VRAM is not the only thing that matters</title>
    <updated>2025-05-23T01:44:05+00:00</updated>
    <author>
      <name>/u/pneuny</name>
      <uri>https://old.reddit.com/user/pneuny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For example, if you have a 5060 Ti 16GB or an RX 9070 XT 16GB and use Qwen 3 30b-a3b q4_k_m with 16k context, you will likely overflow around 8.5GB to system memory. Assuming you do not do CPU offloading, that load now runs squarely on PCIE bandwidth and your system RAM speed. PCIE 5 x16 on the RX 9070 XT is going to help you a lot in feeding that GPU compared to the PCIE 5 x8 available on the 5060 Ti, resulting in much faster tokens per second for the 9070 XT, and making CPU offloading unnecessary in this scenario, whereas the 5060 Ti will become heavily bottlenecked.&lt;/p&gt; &lt;p&gt;While I returned my 5060 Ti for a 9070 XT and didn't get numbers for the former, I did see 42 t/s while the VRAM was overloaded to this degree on the Vulkan backend. Also, AMD does Vulkan way better then Nvidia, as Nvidia tends to crash when using Vulkan.&lt;/p&gt; &lt;p&gt;TL;DR: If you're buying a 16GB card and planning to use more than that, make sure you can leverage x16 PCIE 5 or you won't get the full performance from overflowing to DDR5 system RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pneuny"&gt; /u/pneuny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7u1n/btw_if_you_are_getting_a_single_gpu_vram_is_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7u1n/btw_if_you_are_getting_a_single_gpu_vram_is_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7u1n/btw_if_you_are_getting_a_single_gpu_vram_is_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T01:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kti9u1</id>
    <title>What's the most accurate way to convert arxiv papers to markdown?</title>
    <updated>2025-05-23T12:26:19+00:00</updated>
    <author>
      <name>/u/nextlevelhollerith</name>
      <uri>https://old.reddit.com/user/nextlevelhollerith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for the best method/library to convert arxiv papers to markdown. It could be from PDF conversion or using HTML like &lt;a href="http://ar5iv.labs.arxiv.org"&gt;ar5iv.labs.arxiv.org&lt;/a&gt; . &lt;/p&gt; &lt;p&gt;I tried &lt;a href="https://github.com/VikParuchuri/marker"&gt;marker&lt;/a&gt;, however, often it does not seem to handle well page breaks and footnotes. Also the section levels are often incorrect.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nextlevelhollerith"&gt; /u/nextlevelhollerith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kti9u1/whats_the_most_accurate_way_to_convert_arxiv/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kti9u1/whats_the_most_accurate_way_to_convert_arxiv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kti9u1/whats_the_most_accurate_way_to_convert_arxiv/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:26:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktban0</id>
    <title>Dans-PersonalityEngine V1.3.0 12b &amp; 24b</title>
    <updated>2025-05-23T04:55:30+00:00</updated>
    <author>
      <name>/u/PocketDocLabs</name>
      <uri>https://old.reddit.com/user/PocketDocLabs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The latest release in the Dans-PersonalityEngine series. With any luck you should find it to be an improvement on almost all fronts as compared to V1.2.0.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.3.0-12b"&gt;https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.3.0-12b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.3.0-24b"&gt;https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.3.0-24b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A blog post regarding its development can be found &lt;a href="https://pocketdoclabs.com/making-dans-personalityengine-v130/"&gt;here&lt;/a&gt; for those interested in some rough technical details on the project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PocketDocLabs"&gt; /u/PocketDocLabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktban0/danspersonalityengine_v130_12b_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktban0/danspersonalityengine_v130_12b_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktban0/danspersonalityengine_v130_12b_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T04:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiere</id>
    <title>A Demonstration of Cache-Augmented Generation (CAG) and its Performance Comparison to RAG</title>
    <updated>2025-05-23T12:33:08+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiere/a_demonstration_of_cacheaugmented_generation_cag/"&gt; &lt;img alt="A Demonstration of Cache-Augmented Generation (CAG) and its Performance Comparison to RAG" src="https://preview.redd.it/bn39fvozzi2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9702ce1baab0703350e9800e0619c24d489b70eb" title="A Demonstration of Cache-Augmented Generation (CAG) and its Performance Comparison to RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This project demonstrates how to implement Cache-Augmented Generation (CAG) in an LLM and shows its performance gains compared to RAG. &lt;/p&gt; &lt;p&gt;Project Link: &lt;a href="https://github.com/ronantakizawa/cacheaugmentedgeneration"&gt;https://github.com/ronantakizawa/cacheaugmentedgeneration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CAG preloads document content into an LLMâ€™s context as a precomputed key-value (KV) cache. &lt;/p&gt; &lt;p&gt;This caching eliminates the need for real-time retrieval during inference, reducing token usage by up to 76% while maintaining answer quality. &lt;/p&gt; &lt;p&gt;CAG is particularly effective for constrained knowledge bases like internal documentation, FAQs, and customer support systems, where all relevant information can fit within the model's extended context window.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bn39fvozzi2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiere/a_demonstration_of_cacheaugmented_generation_cag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiere/a_demonstration_of_cacheaugmented_generation_cag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt7cy7</id>
    <title>Did Anthropic drop Claude 3.7â€™s best GPQA score in the new chart?</title>
    <updated>2025-05-23T01:19:30+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7cy7/did_anthropic_drop_claude_37s_best_gpqa_score_in/"&gt; &lt;img alt="Did Anthropic drop Claude 3.7â€™s best GPQA score in the new chart?" src="https://b.thumbs.redditmedia.com/x1iuI2eR3-ZY90sV1Nfc2FrpK4YegdODuAcK3sr_NMA.jpg" title="Did Anthropic drop Claude 3.7â€™s best GPQA score in the new chart?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude 3.7 used to show &lt;strong&gt;84.8%&lt;/strong&gt; on GPQA with extended thinking.&lt;br /&gt; Now in the new chart, it only shows &lt;strong&gt;78.2%&lt;/strong&gt; â€” the non-extended score â€” while Claude 4 gets to show its extended scores (83.3%, 83.8%).&lt;/p&gt; &lt;p&gt;So... the 3.7 number went down, the 4 numbers went up. ðŸ¤”&lt;/p&gt; &lt;p&gt;Did they quietly change the comparison to make the upgrade look bigger?&lt;/p&gt; &lt;p&gt;Maybe I'm missing some detail from the announcement blog.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kt7cy7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7cy7/did_anthropic_drop_claude_37s_best_gpqa_score_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7cy7/did_anthropic_drop_claude_37s_best_gpqa_score_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T01:19:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt1hmk</id>
    <title>Tried Sonnet 4, not impressed</title>
    <updated>2025-05-22T20:46:01+00:00</updated>
    <author>
      <name>/u/Marriedwithgames</name>
      <uri>https://old.reddit.com/user/Marriedwithgames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt1hmk/tried_sonnet_4_not_impressed/"&gt; &lt;img alt="Tried Sonnet 4, not impressed" src="https://preview.redd.it/k68q6q65be2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7773c238e9148d1a369ca7c06ac85f64c5d87e5" title="Tried Sonnet 4, not impressed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A basic image prompt failed &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marriedwithgames"&gt; /u/Marriedwithgames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k68q6q65be2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt1hmk/tried_sonnet_4_not_impressed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt1hmk/tried_sonnet_4_not_impressed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T20:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktdisj</id>
    <title>GitHub - jacklishufan/LaViDa: Official Implementation of LaViDa: :A Large Diffusion Language Model for Multimodal Understanding</title>
    <updated>2025-05-23T07:23:08+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktdisj/github_jacklishufanlavida_official_implementation/"&gt; &lt;img alt="GitHub - jacklishufan/LaViDa: Official Implementation of LaViDa: :A Large Diffusion Language Model for Multimodal Understanding" src="https://external-preview.redd.it/_qyQ5Nb0aZ0pjIERMz0EBymLna5bhwRL3S2vTvBvqUQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=978b5b8d9f71176e70ad9f69cf874f5d01401ac0" title="GitHub - jacklishufan/LaViDa: Official Implementation of LaViDa: :A Large Diffusion Language Model for Multimodal Understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-Llama3-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models is available at &lt;a href="https://github.com/jacklishufan/LaViDa"&gt;https://github.com/jacklishufan/LaViDa&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jacklishufan/LaViDa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktdisj/github_jacklishufanlavida_official_implementation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktdisj/github_jacklishufanlavida_official_implementation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T07:23:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kszxmj</id>
    <title>Claude 4 Opus may contact press and regulators if you do something egregious (deleted Tweet from Sam Bowman)</title>
    <updated>2025-05-22T19:43:04+00:00</updated>
    <author>
      <name>/u/RuairiSpain</name>
      <uri>https://old.reddit.com/user/RuairiSpain</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kszxmj/claude_4_opus_may_contact_press_and_regulators_if/"&gt; &lt;img alt="Claude 4 Opus may contact press and regulators if you do something egregious (deleted Tweet from Sam Bowman)" src="https://preview.redd.it/g91uyr7tyd2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4631f915329d465f3cf27d7c20d9ddc5663b1465" title="Claude 4 Opus may contact press and regulators if you do something egregious (deleted Tweet from Sam Bowman)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RuairiSpain"&gt; /u/RuairiSpain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g91uyr7tyd2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kszxmj/claude_4_opus_may_contact_press_and_regulators_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kszxmj/claude_4_opus_may_contact_press_and_regulators_if/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T19:43:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt0zvd</id>
    <title>House passes budget bill that inexplicably bans state AI regulations for ten years</title>
    <updated>2025-05-22T20:26:06+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt0zvd/house_passes_budget_bill_that_inexplicably_bans/"&gt; &lt;img alt="House passes budget bill that inexplicably bans state AI regulations for ten years" src="https://external-preview.redd.it/is2Xb-bjmFmGSvp-crWowCGBhCXFlH_gdhrRUHNXU_I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bb497ae5922ecf83e5c0a152d97d9c4b33aa5a5" title="House passes budget bill that inexplicably bans state AI regulations for ten years" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tech.yahoo.com/articles/house-passes-budget-bill-inexplicably-184936484.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt0zvd/house_passes_budget_bill_that_inexplicably_bans/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt0zvd/house_passes_budget_bill_that_inexplicably_bans/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T20:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt7whv</id>
    <title>AGI Coming Soon... after we master 2nd grade math</title>
    <updated>2025-05-23T01:47:36+00:00</updated>
    <author>
      <name>/u/SingularitySoooon</name>
      <uri>https://old.reddit.com/user/SingularitySoooon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"&gt; &lt;img alt="AGI Coming Soon... after we master 2nd grade math" src="https://b.thumbs.redditmedia.com/eIAXh1BO-pSo8c3MXScDeH2kayk1IHs4BckFY-FL0QE.jpg" title="AGI Coming Soon... after we master 2nd grade math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pe2eeljssf2f1.png?width=580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f881b7ce4409013458c17fff08e8377a329cb9df"&gt;Claude 4 Sonnet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When will LLM master the classic &amp;quot;9.9 - 9.11&amp;quot; problem???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SingularitySoooon"&gt; /u/SingularitySoooon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T01:47:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt72ic</id>
    <title>Sonnet 4 droppedâ€¦ still feels like a 3.7.1 minor release</title>
    <updated>2025-05-23T01:04:09+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt72ic/sonnet_4_dropped_still_feels_like_a_371_minor/"&gt; &lt;img alt="Sonnet 4 droppedâ€¦ still feels like a 3.7.1 minor release" src="https://preview.redd.it/lambib8skf2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74a3f2740d0fc1f938c9b14e4bc5947bc0ce8931" title="Sonnet 4 droppedâ€¦ still feels like a 3.7.1 minor release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious if anyone's seen big improvements in edge cases or long-context tasks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lambib8skf2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt72ic/sonnet_4_dropped_still_feels_like_a_371_minor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt72ic/sonnet_4_dropped_still_feels_like_a_371_minor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T01:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktgxxa</id>
    <title>AceReason-Nemotron-14B: Advancing Math and Code Reasoning through Reinforcement Learning</title>
    <updated>2025-05-23T11:15:59+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgxxa/acereasonnemotron14b_advancing_math_and_code/"&gt; &lt;img alt="AceReason-Nemotron-14B: Advancing Math and Code Reasoning through Reinforcement Learning" src="https://external-preview.redd.it/_aQtUZTp2VBwp5MK35YBXI25HOZhHuEgT9O1MgXLN7I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb4068255f83d79055a6f21dccc859d949b32f54" title="AceReason-Nemotron-14B: Advancing Math and Code Reasoning through Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/AceReason-Nemotron-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgxxa/acereasonnemotron14b_advancing_math_and_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgxxa/acereasonnemotron14b_advancing_math_and_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktgvoe</id>
    <title>server audio input has been merged into llama.cpp</title>
    <updated>2025-05-23T11:12:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgvoe/server_audio_input_has_been_merged_into_llamacpp/"&gt; &lt;img alt="server audio input has been merged into llama.cpp" src="https://external-preview.redd.it/w-TAeYFPuT8QOBKphdcEnCVLkPeOPrOjKse263sRyos.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d30ef27dc1e8b5fa00168ba96a589759da20990b" title="server audio input has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13714"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgvoe/server_audio_input_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktgvoe/server_audio_input_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T11:12:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktiq99</id>
    <title>I accidentally too many P100</title>
    <updated>2025-05-23T12:48:51+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt; &lt;img alt="I accidentally too many P100" src="https://b.thumbs.redditmedia.com/IdF4SU4XHKp-_JI6o-Y6kol8-cLrv94jdBxKlq9CTYI.jpg" title="I accidentally too many P100" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I had quite positive results with a P100 last summer, so when R1 came out, I decided to try if I could put 16 of them in a single pc... and I could.&lt;/p&gt; &lt;p&gt;Not the fastest think in the universe, and I am not getting awesome PCIE speed (2@4x). But it works, is still cheaper than a 5090, and I hope I can run stuff with large contexts.&lt;/p&gt; &lt;p&gt;I hoped to run llama4 with large context sizes, and scout runs almost ok, but llama4 as a model is abysmal. I tried to run Qwen3-235B-A22B, but the performance with llama.cpp is pretty terrible, and I haven't been able to get it working with the vllm-pascal (ghcr.io/sasha0552/vllm:latest).&lt;/p&gt; &lt;p&gt;If you have any pointers on getting Qwen3-235B to run with any sort of parallelism, or want me to benchmark any model, just say so!&lt;/p&gt; &lt;p&gt;The MB is a 2014 intel S2600CW with dual 8-core xeons, so CPU performance is rather low. I also tried to use MB with an EPYC, but it doesn't manage to allocate the resources to all PCIe devices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ktiq99"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T12:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksyicp</id>
    <title>Introducing the world's most powerful model</title>
    <updated>2025-05-22T18:45:16+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"&gt; &lt;img alt="Introducing the world's most powerful model" src="https://preview.redd.it/hqx8fzosod2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96d0c448070aead295d21d9be7e8fd395520a72b" title="Introducing the world's most powerful model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hqx8fzosod2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T18:45:16+00:00</published>
  </entry>
</feed>
