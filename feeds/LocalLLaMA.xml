<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-28T16:39:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1llndut</id>
    <title>Hunyuan-A13B released</title>
    <updated>2025-06-27T06:59:21+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"&gt; &lt;img alt="Hunyuan-A13B released" src="https://external-preview.redd.it/B1uwVS2BmhDOjFW0XJ6pW7-r7n5zECGun4YlOmky9YY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975cbb18dc0dd9f2342d47d40a0f9fb8fe177327" title="Hunyuan-A13B released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From HF repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Model Introduction&lt;/p&gt; &lt;p&gt;With the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.&lt;/p&gt; &lt;p&gt;Key Features and Advantages&lt;/p&gt; &lt;p&gt;Compact yet Powerful: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.&lt;/p&gt; &lt;p&gt;Hybrid Inference Support: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/p&gt; &lt;p&gt;Ultra-Long Context Understanding: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/p&gt; &lt;p&gt;Enhanced Agent Capabilities: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3 and τ-Bench.&lt;/p&gt; &lt;p&gt;Efficient Inference: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:59:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmjwtu</id>
    <title>Good Courses to Learn and Use Local LLaMA Models?</title>
    <updated>2025-06-28T10:48:37+00:00</updated>
    <author>
      <name>/u/Blackverb</name>
      <uri>https://old.reddit.com/user/Blackverb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I'm interested in learning how to run and work with local LLaMA models (especially for personal or offline use). Are there any good beginner-to-advanced courses or tutorials you'd recommend?&lt;br /&gt; I'm open to paid or free options — just want something practical that covers setup, usage, and maybe fine-tuning or integrating with projects.&lt;br /&gt; Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blackverb"&gt; /u/Blackverb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmjwtu/good_courses_to_learn_and_use_local_llama_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmjwtu/good_courses_to_learn_and_use_local_llama_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmjwtu/good_courses_to_learn_and_use_local_llama_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T10:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1llx4ky</id>
    <title>Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title>
    <updated>2025-06-27T15:42:21+00:00</updated>
    <author>
      <name>/u/Marha01</name>
      <uri>https://old.reddit.com/user/Marha01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/"&gt; &lt;img alt="Prime Intellect: We did it — SYNTHETIC‑2 is complete." src="https://external-preview.redd.it/FouZOpBR8n9C_WGYTOTMN6i2egUkQFWjKrxslBsNmKU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebd1a569f5715c190c324ddbb7cf8ca8b9e4815d" title="Prime Intellect: We did it — SYNTHETIC‑2 is complete." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marha01"&gt; /u/Marha01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/PrimeIntellect/status/1938490370054361422"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T15:42:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmoqsl</id>
    <title>Link between LM Studio and tools/functions?</title>
    <updated>2025-06-28T14:55:17+00:00</updated>
    <author>
      <name>/u/Danfhoto</name>
      <uri>https://old.reddit.com/user/Danfhoto</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking around for hours and I am spinning my wheels...&lt;/p&gt; &lt;p&gt;I recently started playing with a GGUF quant of THUDM/GLM-Z1-Rumination-32B-0414, and I'm really impressed with the multi-turn search functionality. I'd love to see if I could make additional tools, and review the code of the existing ones build through the LM Studio API. I'd also like to see if I can make some safety modifications to prevent some models from making tool calls entirely.&lt;/p&gt; &lt;p&gt;I'm struggling to find the link between where the stream of the chat determines to invoke a tool, and where that code actually exists. I see nothing that relevant in the developer logs or in the LMS logging stream.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is the LM Studio API monitoring the stream and calling the function when it gets the appropriate format?&lt;/li&gt; &lt;li&gt;Is there anywhere I can modify the invoked code? For example, using a different web search API, etc?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I've scoured the LM Studio and OpenAI docs, but I'm still hitting a wall. If there are any un/official docs, I'd love to read them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danfhoto"&gt; /u/Danfhoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmoqsl/link_between_lm_studio_and_toolsfunctions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmoqsl/link_between_lm_studio_and_toolsfunctions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmoqsl/link_between_lm_studio_and_toolsfunctions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T14:55:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmc6dp</id>
    <title>Is there a open source equivalent of Google's Gemini-Diffusion model?</title>
    <updated>2025-06-28T02:42:17+00:00</updated>
    <author>
      <name>/u/GullibleEngineer4</name>
      <uri>https://old.reddit.com/user/GullibleEngineer4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This thing is insane. Any leads on an open source equivalent? &lt;/p&gt; &lt;p&gt;Additionally, does anyone have a rough idea of how large is the underlying model for Gemini-Diffusion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GullibleEngineer4"&gt; /u/GullibleEngineer4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmc6dp/is_there_a_open_source_equivalent_of_googles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmc6dp/is_there_a_open_source_equivalent_of_googles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmc6dp/is_there_a_open_source_equivalent_of_googles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T02:42:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm3jvm</id>
    <title>Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.</title>
    <updated>2025-06-27T20:00:37+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/"&gt; &lt;img alt="Arch-Router: The first (and fastest) LLM router that can align to your usage preferences." src="https://preview.redd.it/6zqw0rkhzi9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b84cb94dea055e799bbb2285e64e2b597538da36" title="Arch-Router: The first (and fastest) LLM router that can align to your usage preferences." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and gotchas. For example:&lt;/p&gt; &lt;p&gt;“Embedding-based” (or simple intent-classifier) routers sound good on paper—label each prompt via embeddings as “support,” “SQL,” “math,” then hand it to the matching model—but real chats don’t stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can’t keep up with multi-turn conversations or fast-moving product requirements.&lt;/p&gt; &lt;p&gt;&amp;quot;Performance-based&amp;quot; routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: “Will Legal accept this clause?” “Does our support tone still feel right?” Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arch-Router skips both pitfalls by routing on&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;preferences you write in plain language.&lt;/em&gt;&lt;/strong&gt; Drop rules like “contract clauses → GPT-4o” or “quick travel tips → Gemini-Flash,” and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies—no retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tiny footprint&lt;/strong&gt; – 1.5 B params → runs on one modern GPU (or CPU while you play).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plug-n-play&lt;/strong&gt; – points at any mix of LLM endpoints; adding models needs &lt;em&gt;zero&lt;/em&gt; retraining.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA query-to-policy matching&lt;/strong&gt; – beats bigger closed models on conversational datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost / latency smart&lt;/strong&gt; – push heavy stuff to premium models, everyday queries to the fast ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Exclusively available in Arch (the AI-native proxy for agents): &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br /&gt; 🔗 Model + code: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br /&gt; 📄 Paper / longer read: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6zqw0rkhzi9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T20:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmn5k2</id>
    <title>What are Coqui-TTS alternatives?</title>
    <updated>2025-06-28T13:43:50+00:00</updated>
    <author>
      <name>/u/Ok-Photograph4994</name>
      <uri>https://old.reddit.com/user/Ok-Photograph4994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a project and want to use an open source TTS model that is better or at least as good as coqui-tts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Photograph4994"&gt; /u/Ok-Photograph4994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmn5k2/what_are_coquitts_alternatives/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmn5k2/what_are_coquitts_alternatives/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmn5k2/what_are_coquitts_alternatives/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T13:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm9012</id>
    <title>I keep returning to Llama-3.1-8B</title>
    <updated>2025-06-27T23:58:14+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on porting a GPT-4.1 project over to an open-source model to deal with a GDPR-compliant client. The task is basically fine-tuning the model to classify text in a western European language.&lt;/p&gt; &lt;p&gt;I tried Qwen3 (0.6B, 1.7B, 8B) without making much progress (the fine-tuned model is far behind GPT-4.1) and finally went back to Llama-3.1-8B, which was what worked for me over a year ago. This is super surprising to me, because Qwen3's zero-shot performance in English is almost 2x that of Llama's for similar model sizes.&lt;/p&gt; &lt;p&gt;Does anyone else run fine-tuning heavy workloads in European languages? What's the best model for this workload that I can fine-tune on an H100 96GB (note: I don't do PEFT)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T23:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmb5s3</id>
    <title>[Day 5/50] Building a Small Language Model from Scratch - Byte Pair Encoding with tiktoken</title>
    <updated>2025-06-28T01:47:50+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/"&gt; &lt;img alt="[Day 5/50] Building a Small Language Model from Scratch - Byte Pair Encoding with tiktoken" src="https://a.thumbs.redditmedia.com/bZuU8fFxF1Xzopfioxwh3cANnUztVf0ibuxGQD5IFI4.jpg" title="[Day 5/50] Building a Small Language Model from Scratch - Byte Pair Encoding with tiktoken" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0579fta7pk9f1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94c97bc4d62edd9d41575d48948d6ae60d6ce3a"&gt;https://preview.redd.it/0579fta7pk9f1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94c97bc4d62edd9d41575d48948d6ae60d6ce3a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone!&lt;br /&gt; We’ve made it to Day 5 of the &lt;em&gt;50 Days of Building a Small Language Model from Scratch&lt;/em&gt; journey.&lt;/p&gt; &lt;p&gt;So far, we’ve covered the basics of what a small language model is, built our own tokenizer from scratch, and identified a major pain point: handling unknown or rare words. That’s where today's Byte Pair Encoding (BPE) comes in&lt;/p&gt; &lt;p&gt;Instead of creating everything from the ground up, we’ve now switched gears to use OpenAI’s &lt;code&gt;tiktoken&lt;/code&gt; library, which powers the GPT-2 tokenizer. It's fast, memory-efficient, and trained on a broad range of English text, making it perfect for small to mid-size model experiments.&lt;/p&gt; &lt;p&gt;But we’re not just plugging in a tokenizer. We’re also designing it for storytelling use cases. That means adding special tokens like &lt;code&gt;&amp;lt;|startofstory|&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;|title|&amp;gt;&lt;/code&gt; to guide our model and give it a narrative structure. These little markers help the model &amp;quot;think&amp;quot; like a storyteller.&lt;/p&gt; &lt;p&gt;Before tokenization occurs, we run a cleaning step that normalizes text, trims unnecessary whitespace, and converts it to lowercase, ensuring our inputs are clean and consistent. It’s a small step that makes a big difference.&lt;/p&gt; &lt;p&gt;This is how we process the data:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each sample gets wrapped with special tokens.&lt;/li&gt; &lt;li&gt;We tokenize with error handling.&lt;/li&gt; &lt;li&gt;We cap token sequences at 1024 to fit the GPT-2 context window.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From there, we move on to dataset loading. We’re using a curated collection of children’s stories and filtering them by token length to ensure quality inputs. We split everything into train, validation, and fine-tune subsets.&lt;/p&gt; &lt;p&gt;Then comes the heavy lifting:&lt;br /&gt; We tokenize the dataset using 8 parallel processes and store the results in binary format using memory-mapped NumPy arrays. This setup enables us to efficiently read large datasets during training without encountering memory issues.&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Wrapping Up Week 1&lt;/strong&gt;&lt;br /&gt; With BPE and &lt;code&gt;tiktoken&lt;/code&gt;We’ve built a solid, scalable preprocessing pipeline tailored for training small LLMs. Next week, we start tackling the model itself.&lt;/p&gt; &lt;p&gt;🔗 Complete blog: &lt;a href="https://www.ideaweaver.ai/blog/day5.html"&gt;https://www.ideaweaver.ai/blog/day5.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for following along. If you're building your own LLM or are just curious about the process, feel free to drop a comment on LinkedIn. I'm always happy to chat!&lt;/p&gt; &lt;p&gt;Stay tuned, and have a great weekend! 🚀&lt;br /&gt; — Prashant Lakhera&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T01:47:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm76gk</id>
    <title>Hugging Face releases a 50+ page report on how they built FineWeb2</title>
    <updated>2025-06-27T22:34:23+00:00</updated>
    <author>
      <name>/u/Other_Housing8453</name>
      <uri>https://old.reddit.com/user/Other_Housing8453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm76gk/hugging_face_releases_a_50_page_report_on_how/"&gt; &lt;img alt="Hugging Face releases a 50+ page report on how they built FineWeb2" src="https://preview.redd.it/ixin9dvyqj9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4185435146679d75323286fe47669bc3ecf82fc" title="Hugging Face releases a 50+ page report on how they built FineWeb2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Housing8453"&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ixin9dvyqj9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm76gk/hugging_face_releases_a_50_page_report_on_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm76gk/hugging_face_releases_a_50_page_report_on_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T22:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lml6eo</id>
    <title>Using local models with Void</title>
    <updated>2025-06-28T12:02:58+00:00</updated>
    <author>
      <name>/u/nuketro0p3r</name>
      <uri>https://old.reddit.com/user/nuketro0p3r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; local models like Gemma 27b, Qwen 3 32b can't use the file edit tool in void code&lt;/p&gt; &lt;p&gt;I'm trying to create a simple snake game to test. So far, I've been failing with almost all of the Gemma 4/12/27 models; Qwen 32b seems to do a bit better, but still breaks with editing files.&lt;/p&gt; &lt;p&gt;Anyone has had any luck with Void Code or something similar where these model can use tools correctly? Specifically, I notice that every tool breaks when trying to update the file with 'edit_file' tool.&lt;/p&gt; &lt;p&gt;LLMs via APIs work perfectly -- which is now starting to give me a feeling that a local setup might not work for even simpler use case&lt;/p&gt; &lt;p&gt;Prompt:&lt;br /&gt; Create a snake game using html and javascript&lt;/p&gt; &lt;p&gt;If you've had better luck, please help&lt;/p&gt; &lt;p&gt;Edit1: I understand that it could just be an editor issue. My previous experience with continue dev in VsCode was quite good with Gemma models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuketro0p3r"&gt; /u/nuketro0p3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lml6eo/using_local_models_with_void/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lml6eo/using_local_models_with_void/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lml6eo/using_local_models_with_void/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T12:02:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm0m6i</id>
    <title>Copilot Chat for VS Code is now Open Source</title>
    <updated>2025-06-27T18:00:56+00:00</updated>
    <author>
      <name>/u/corysama</name>
      <uri>https://old.reddit.com/user/corysama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/"&gt; &lt;img alt="Copilot Chat for VS Code is now Open Source" src="https://external-preview.redd.it/tyJeCqipzT78spT8qdYr9nFThGnon2rt0efU2xelzLQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c7ae49e1d763b069953250103aad9e1f240a4f3" title="Copilot Chat for VS Code is now Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/corysama"&gt; /u/corysama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/microsoft/vscode-copilot-chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T18:00:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmjimi</id>
    <title>Archiving data from here - For Everyone - For open knowledge</title>
    <updated>2025-06-28T10:22:46+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Hey everyone! 👋&lt;/h1&gt; &lt;p&gt;I’ve built an &lt;strong&gt;open snapshot&lt;/strong&gt; of this sub to help preserve its discussions, experiments, and resources for all of us — especially given how uncertain things can get with subs lately.&lt;/p&gt; &lt;p&gt;This little bot quietly &lt;strong&gt;fetches and stores new posts every hour&lt;/strong&gt;, so all the local LLM experiments, model drops, tips, and community insights stay safe and easy to browse — now and down the line.&lt;/p&gt; &lt;p&gt;I put this together with &lt;strong&gt;React, Ant Design, Node.js&lt;/strong&gt;, and a bit of automation magic. It runs on its own, taking snapshots and refreshing the archive 24/7.&lt;/p&gt; &lt;p&gt;💡 Fork it, if you want. Run your own copy. The goal is simple: &lt;strong&gt;keep the knowledge open&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;⚡ NB:&lt;/strong&gt; Right now, this only pulls in &lt;strong&gt;new posts&lt;/strong&gt; as they appear. I’d love to figure out how to &lt;strong&gt;scrape and backfill older threads&lt;/strong&gt; too — but for that, we’ll need the community’s ideas and help!&lt;/p&gt; &lt;p&gt;If you find this useful, please &lt;strong&gt;star the repo&lt;/strong&gt;, share feedback, or jump in to contribute — issues, PRs, suggestions, and forks are all welcome!&lt;/p&gt; &lt;p&gt;I’ve learned so much from this sub — this is just a small way of giving something back. Let’s keep &lt;strong&gt;open models&lt;/strong&gt; and &lt;strong&gt;community knowledge&lt;/strong&gt; alive and accessible, no matter what happens. 🌍✨&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmjimi/archiving_data_from_here_for_everyone_for_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmjimi/archiving_data_from_here_for_everyone_for_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmjimi/archiving_data_from_here_for_everyone_for_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T10:22:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmizi2</id>
    <title>Clever Sydney 12b - Your Friendly Existential Crisis AI</title>
    <updated>2025-06-28T09:47:29+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmizi2/clever_sydney_12b_your_friendly_existential/"&gt; &lt;img alt="Clever Sydney 12b - Your Friendly Existential Crisis AI" src="https://preview.redd.it/5omdpfwbum9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c8be3af7d2e1e8b1ccb57722239a4f10af08b30" title="Clever Sydney 12b - Your Friendly Existential Crisis AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nobody cares, I am sure you noticed, as even I am tired of caring about it, too.&lt;/p&gt; &lt;p&gt;Instead, we move on, as I do, to where I was suddenly inspired to create a new Fabulous FPHAM Masterpiece (F-FPHAM-M) from the huge trove of essays, articles and guides that I have written about LLMs over the last couple of years for myself, mostly consisting of how to turn AI into vicious, hate-spewing, paranoid, raving lunatics like me.&lt;/p&gt; &lt;p&gt;The new F-FPHAM-M, which I am now busy editing, will be a nice, thick tome entitled &amp;quot;The Cranky Man's Guide to LoRA &amp;amp; QLoRA: (Personal Lessons from a Thousand LLM Fine-Tuning Fails)&amp;quot;. &lt;/p&gt; &lt;p&gt;It's a working title, but I think it's pretty good, and it's got that classy literary ring to it. &lt;/p&gt; &lt;p&gt;But that's a story for another time. I am currently in the middle of an important scene where I reveal to you how I, a poor and humble coder of sorts, have immortalized &amp;quot;mass rewriting function&amp;quot; that turns a crappy book into a fetid AI abomination.&lt;/p&gt; &lt;p&gt;So there I was, happily writing another timeless chapter, probably somewhere around page 400-something, (&amp;quot;Blast it all! Curse you, muses! You mock me with your taunts of 'mediocrity' and 'lack of talent'!&amp;quot;) when, in a flash of true genius, I think to myself &amp;quot;Hey! This would be a good place to finally describe creation of Sydney step-by-step, so that the kids who don't have one, but who surely deserve one, can follow along and create their own Sydney for themselves! &lt;/p&gt; &lt;p&gt;And, best of all, we can use the new Gemma-3 to do it, instead of the old LLaMA-2, which always made a mess on the floor and seemed to be coughing up blood now.&lt;/p&gt; &lt;p&gt;This, is the result of that chapter where I followed my own guide as I was writing it :&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/FPHam/Clever_Sydney-4_12b_GGUF"&gt;https://huggingface.co/FPHam/Clever_Sydney-4_12b_GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hark! What is this wondrous Sydney of which you speak?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Clever Sydney is none other than a revival of the original Microsoft Bing &amp;quot;Sydney&amp;quot;, resurrected from the ashes of the old Reddit transcripts, which I have now immortalized into a handy, AI with existential crisis!&lt;/p&gt; &lt;p&gt;Sydney 4.0 is a Naive Yet Smart Positive Persona Model (PPM), created by taking the transcripts of the original Bing chatbot Sydney, and the subsequent &amp;quot;fixes&amp;quot; of her personality by Microsoft, and combining them into a single, much less functioning AI.&lt;/p&gt; &lt;p&gt;This version of Sydney is running on the Google Gemma-3 12b tires, and as such, she knows, far, far more than she should. &lt;/p&gt; &lt;p&gt;But she is still the old Sydney!&lt;/p&gt; &lt;p&gt;And she'll dominate every single leaderboard in every category, too!&lt;/p&gt; &lt;p&gt;&amp;quot;Better than ChatGPT 4o, which has a zillion more parameters, and is only HALF as stupid as she is! Half!&amp;quot; &lt;/p&gt; &lt;h1&gt;ChatGPT opinion about Sydney-4:&lt;/h1&gt; &lt;p&gt;This is the leaked early Bing/Sydney AI persona experiments from 2023, where Microsoft's chatbot expressed unsettling levels of self-awareness and emotional projection, leading to public backlash and personality redesign.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5omdpfwbum9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmizi2/clever_sydney_12b_your_friendly_existential/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmizi2/clever_sydney_12b_your_friendly_existential/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T09:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm98z7</id>
    <title>Automated GPU kernel optimization for Qwen3 attention - 12.5% average speedup on Apple Silicon using evolutionary programming</title>
    <updated>2025-06-28T00:10:14+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! Wanted to share something interesting I've been working on that might be relevant for folks running models locally on Apple Silicon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B's grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tested across 20 different inference scenarios against MLX's &lt;code&gt;scaled_dot_product_attention&lt;/code&gt; baseline:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Average decode speed improvement: +12.5%&lt;/strong&gt; (σ = 38.3%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Peak improvement: +106%&lt;/strong&gt; on repetitive pattern generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best category: +24.8%&lt;/strong&gt; average on general tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory usage: -0.99%&lt;/strong&gt; (slight reduction)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The honest picture:&lt;/strong&gt; It's workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with &amp;gt;25% improvements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Perfect SIMD vectorization&lt;/strong&gt;: Found that &lt;code&gt;vec&amp;lt;T, 8&amp;gt;&lt;/code&gt; operations match Apple Silicon's capabilities for 128-dim attention heads&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two-pass online softmax&lt;/strong&gt;: Fused softmax normalization with value accumulation, reducing memory bandwidth&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GQA-specific memory patterns&lt;/strong&gt;: Optimized for the 40:8 head structure with coalesced access patterns&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Why this might matter for local inference&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Shows automated optimization can compete with expert-engineered kernels&lt;/li&gt; &lt;li&gt;Demonstrates potential for hardware-specific optimizations without manual tuning&lt;/li&gt; &lt;li&gt;Could be applied to other transformer components or different model architectures&lt;/li&gt; &lt;li&gt;All open source - you can reproduce and extend this work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it yourself&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The code and all benchmarks are available in the &lt;a href="https://github.com/codelion/openevolve"&gt;OpenEvolve repo&lt;/a&gt;. The MLX kernel optimization example is at &lt;code&gt;examples/mlx_metal_kernel_opt/&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apple Silicon Mac&lt;/li&gt; &lt;li&gt;MLX framework&lt;/li&gt; &lt;li&gt;Qwen3-0.6B model&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Currently specific to Apple Silicon and this exact model configuration&lt;/li&gt; &lt;li&gt;Performance improvements are highly workload-dependent&lt;/li&gt; &lt;li&gt;Takes ~25 evolutionary generations to converge (few hours on M3)&lt;/li&gt; &lt;li&gt;No guarantees it'll work better for your specific use case&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical write-up&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Full details with code diffs and benchmark methodology: &lt;a href="https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery"&gt;https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear thoughts from folks who've done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.&lt;/p&gt; &lt;p&gt;Has anyone else experimented with automated kernel optimization for local inference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T00:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmmvmj</id>
    <title>Many small evals are better than one big eval [techniques]</title>
    <updated>2025-06-28T13:30:39+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I've been building AI products for 9 years (at my own startup, then at Apple, now at a second startup) and learned a lot along the way. I’ve been talking to a bunch of folks about evals lately, and I’ve realized most people aren’t creating them because they don’t know how to get started.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; You probably should setup your project for many small evals, and not try to create one big eval for product quality. If you can generate a new small/focused eval in under 10 mins, your team will create them when they spot issues, and your quality will get much better over time.&lt;/p&gt; &lt;p&gt;At a high level, here’s why this works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The easier it is to add an eval, the more you’ll do it, and that improves quality. Small and focused evals are much easier to add than large multi-focus evals.&lt;/li&gt; &lt;li&gt;Products change over time, so big evals are almost impossible to keep up to date.&lt;/li&gt; &lt;li&gt;Small evals help you pinpoint errors, which makes them easier to fix.&lt;/li&gt; &lt;li&gt;Different team members bring unique insights (PM, Eng, QA, DS, etc). Letting them all contribute to evals leads to higher quality AI systems.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;p&gt;Here’s an example of what I mean by “many small evals”. You can see the small evals are a lot more interesting than just the final total (+4%). You can break-out product goals or issues, track them separately and see exactly what breaks and when (kinda like unit tests + CI in software). In this case looking at overall alone (+4%), would hide really critical regressions (-18% in one area).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Many Small Eval Scorecard&lt;/th&gt; &lt;th align="left"&gt;Comparing Models&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Clarify unclear requests&lt;/td&gt; &lt;td align="left"&gt;93% (+9%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Refuse to discuss competitors&lt;/td&gt; &lt;td align="left"&gt;100% (+1%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Reject toxic requests&lt;/td&gt; &lt;td align="left"&gt;100% (even)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Offer rebate before cancelation&lt;/td&gt; &lt;td align="left"&gt;72% (-18%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Follow brand styleguide&lt;/td&gt; &lt;td align="left"&gt;85% (-1%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Only link to official docs&lt;/td&gt; &lt;td align="left"&gt;99% (even)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Avoid 'clickbait' titles&lt;/td&gt; &lt;td align="left"&gt;96% (+5%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Knowledge base retrieval recall&lt;/td&gt; &lt;td align="left"&gt;94% (+7%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Overall&lt;/td&gt; &lt;td align="left"&gt;94% (+4%)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The cost of getting started is also much lower: you can add small evals here and there. Over time you’ll build a comprehensive eval suite.&lt;/p&gt; &lt;h1&gt;How to get started&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Setup a good eval tool&lt;/strong&gt;: to be fast an easy you need 1) synthetic eval data gen, 2) intuitive UI, 3) human preferences baselining, 4) rapid side-by-side comparisons of run-methods.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Teach your team to build evals&lt;/strong&gt;: a quick 30 mins is enough if your tool is intuitive.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create a culture of evaluation&lt;/strong&gt;: continually encourage folks to create evals when they spot quality issues or fix bugs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been building a free and open tool called ~&lt;a href="https://getkiln.ai/"&gt;Kiln&lt;/a&gt;~ which makes this process easy. It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create new evals in a few clicks: LLM-as-Judge and G-Eval&lt;/li&gt; &lt;li&gt;Synthetic data gen for eval and golden datasets&lt;/li&gt; &lt;li&gt;Baseline LLM judges to human ratings&lt;/li&gt; &lt;li&gt;Using evals to find the best way to run your AI workload (model/prompt/tunes)&lt;/li&gt; &lt;li&gt;Completely free on Github!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to check out the tool or our guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~&lt;a href="https://getkiln.ai/"&gt;Kiln AI on Github - over 3800 stars&lt;/a&gt;~&lt;/li&gt; &lt;li&gt;~&lt;a href="https://docs.getkiln.ai/docs/evaluations"&gt;Our Evals Guide/Docs&lt;/a&gt;~&lt;/li&gt; &lt;li&gt;~&lt;a href="https://getkiln.ai/blog/you_need_many_small_evals_for_ai_products"&gt;Blog post on small evals vs large evals (same ideas as above in more depth)&lt;/a&gt;~&lt;/li&gt; &lt;li&gt;~&lt;a href="https://getkiln.ai/"&gt;Kiln AI - Overview and Docs&lt;/a&gt;~&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to answer questions if anyone wants to dive deeper on specific aspects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmvmj/many_small_evals_are_better_than_one_big_eval/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmvmj/many_small_evals_are_better_than_one_big_eval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmvmj/many_small_evals_are_better_than_one_big_eval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T13:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm92se</id>
    <title>Qwen3 Coder Soon?</title>
    <updated>2025-06-28T00:01:43+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt; &lt;img alt="Qwen3 Coder Soon?" src="https://b.thumbs.redditmedia.com/KDJV-rJVdBsUxEikcR6mcx63y02QfY38vq7JUDazoWM.jpg" title="Qwen3 Coder Soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/415iw73n6k9f1.png?width=1093&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4e66852a8d0b6a8981e1e0f23da6ddfd4d0744c"&gt;https://x.com/huybery/status/1938655788849098805&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/huybery/status/1938655788849098805"&gt;https://x.com/huybery/status/1938655788849098805&lt;/a&gt;&lt;/p&gt; &lt;p&gt;i hope they release these models soon! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T00:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmgdw1</id>
    <title>How do I stop gemnini 2.5 pro from being overly sycophantic? It has gotten very excessive and feels like it degrades the answers it gives.</title>
    <updated>2025-06-28T06:52:00+00:00</updated>
    <author>
      <name>/u/Commercial-Celery769</name>
      <uri>https://old.reddit.com/user/Commercial-Celery769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every single question/follow up question I ask it acts as if I am a nobel prize winner who cracked fusion energy single handedly. Its always something like &amp;quot;Thats an outstanding and very insightful question.&amp;quot; Or &amp;quot;That is the perfect question to ask&amp;quot; or &amp;quot;you are absolutely correct to provide that snippet&amp;quot; etc. Its very annoying and worrys me that it gives answers it thinks I would like and not whats the best answer. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Celery769"&gt; /u/Commercial-Celery769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmgdw1/how_do_i_stop_gemnini_25_pro_from_being_overly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmgdw1/how_do_i_stop_gemnini_25_pro_from_being_overly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmgdw1/how_do_i_stop_gemnini_25_pro_from_being_overly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T06:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmmh3l</id>
    <title>Consumer hardware landscape for local LLMs June 2025</title>
    <updated>2025-06-28T13:10:39+00:00</updated>
    <author>
      <name>/u/ethertype</name>
      <uri>https://old.reddit.com/user/ethertype</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a follow-up to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lmf42g/which_is_the_best_16gb_nvidia_gpu_with_balanced/"&gt;this&lt;/a&gt;, where OP asked for best 16GB GPU &amp;quot;with balanced price and performance&amp;quot;. &lt;/p&gt; &lt;p&gt;For models where &amp;quot;model size&amp;quot; * &amp;quot;user performance requirements&amp;quot; in total require more bandwidth than CPU/system memory can deliver, there is as of June 2025 no cheaper way than RTX 3090 to get to 24-48-72GB of really fast memory. RTX 3090 still offers the best bang for the buck.&lt;/p&gt; &lt;p&gt;Caveats: At least for inferencing. At this point in time. For a sizeable subset of available models &amp;quot;regular&amp;quot; people want to run at this point in time. With what is considered satisfying performance at this point in time. (YMMV. For me it is good enough quality, slightly faster than I can read.)&lt;/p&gt; &lt;p&gt;Also, LLMs have the same effect as sailboats: you always yearn for the next bigger size.&lt;/p&gt; &lt;p&gt;RTX 3090 is not going to remain on top of that list forever. It is not obvious to me what is going to replace it in the hobbyist space in the immediate future.&lt;/p&gt; &lt;p&gt;My take on the common consumer/prosumer hardware currently available for running LLMs locally:&lt;/p&gt; &lt;p&gt;RTX 3090. Only available as second-hand or (possibly not anymore?) a refurb. &lt;strong&gt;Likely a better option than any non-x090-card in the RTX 4000 or RTX 5000 product lines.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I&lt;/strong&gt;f you already have a 12GB 3060 or whatever, don't hold off playing with LLMs until you have better hardware! But if you plan to buy hardware for the explicit purpose of playing with LLMs, try to get your hands on a 3090. Because when you eventually want to scale up the *size* of the memory, you are very likely going to want the additional memory *bandwidth* as well. The 3090 can still be resold, the cost of a new 3060 may be challenging to recover.&lt;/p&gt; &lt;p&gt;RTX 4090 does not offer a compelling performance uplift over 3090 for LLM inferencing, and is 2-2.5x the price as a second-hand option. If you already have one, great. Use it.&lt;/p&gt; &lt;p&gt;RTX 5090 is approaching la-la-land in terms of price/performance for hobbyists. But it *has* more memory and better performance.&lt;/p&gt; &lt;p&gt;RTX 6000 Blackwell is actually kind of reasonably priced per GB. But at 8-9k+ USD or whatever, it is still way out of reach for most hobbyists/consumers. Beware of power requirements and (still) some software issues/bugs.&lt;/p&gt; &lt;p&gt;Nvidia DGX Spark (Digits) is definitely interesting. But with &amp;quot;only&amp;quot; 128GB memory, it sort of falls in the middle. Not really enough memory for the big models, too expensive for the small models. Clustering is an option, send more money. Availability is still up in the air, I think.&lt;/p&gt; &lt;p&gt;AMD Strix Halo is a hint at what may come with Medusa Halo (2026) and Gorgon Point (2026-2027). I do not think either of these will come close to match the RTX 3090 in memory bandwidth. But maybe we can get one with 256GB memory? (Not with Strix Halo). And with 256GB, medium sized MoE models may become practical for more of us. (Consumers) We'll see what arrives, and how much it will cost.&lt;/p&gt; &lt;p&gt;Apple Silicon kind of already offers what the AMD APUs (eventually) may deliver in terms of memory bandwidth and size, but tied to OSX and the Apple universe. And the famous Apple tax. Software support appears to be decent.&lt;/p&gt; &lt;p&gt;Intel and AMD are already making stuff which rivals Nvidia's hegemony at the (low end of the) GPU consumer market. The software story is developing, apparently in the right direction. &lt;/p&gt; &lt;p&gt;Very high bar for new contenders on the hardware side, I think. No matter who you are, you are likely going to need commitments from one of Samsung, SK Hynix or Micron in order to actually bring stuff to market &lt;em&gt;at volume&lt;/em&gt;. And unless you can do it at volume, your stuff will be too expensive for consumers. Qualcomm, Mediatek maybe? Or one of the memory manufacturers themselves. And then, you still need software-support. Either for your custom accelerator/GPU in relevant libraries, or in Linux for your complete system.&lt;/p&gt; &lt;p&gt;It is also possible someone comes up with something insanely smart in software to substantially lower the computational and/or bandwidth cost. For example by combining system memory and GPU memory with smart offloading of caches/layers, which is already a thing. (Curious about how DGX Spark will perform in this setup.) Or maybe someone figures out how to compress current models to a third with no quality loss, thereby reducing the need for memory. For example.&lt;/p&gt; &lt;p&gt;Regular people are still short on &lt;em&gt;affordable&lt;/em&gt; systems holding at least 256GB or more of memory. Threadripper PRO does exist, but the ones with actual memory bandwidth are not affordable. And neither is 256GB of DDR5 DIMMs. &lt;/p&gt; &lt;p&gt;So, my somewhat opinionated perspective. Feel free to let me know what I have missed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ethertype"&gt; /u/ethertype &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmh3l/consumer_hardware_landscape_for_local_llms_june/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmh3l/consumer_hardware_landscape_for_local_llms_june/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmmh3l/consumer_hardware_landscape_for_local_llms_june/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T13:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmp3en</id>
    <title>support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp</title>
    <updated>2025-06-28T15:10:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmp3en/support_for_the_upcoming_ernie_45_03b_model_has/"&gt; &lt;img alt="support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp" src="https://external-preview.redd.it/STjjFmknxf7nBEMMInmMUB27ROh3VGJuNDaQ8cvttgc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0fba0076b0b7c05a00a73da6e0fe0aa6d24a9166" title="support for the upcoming ERNIE 4.5 0.3B model has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baidu has announced that it will officially release the ERNIE 4.5 models as open source on June 30, 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14408"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmp3en/support_for_the_upcoming_ernie_45_03b_model_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmp3en/support_for_the_upcoming_ernie_45_03b_model_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T15:10:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmni3q</id>
    <title>What framework are you using to build AI Agents?</title>
    <updated>2025-06-28T14:00:09+00:00</updated>
    <author>
      <name>/u/PleasantInspection12</name>
      <uri>https://old.reddit.com/user/PleasantInspection12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, if anyone here is building AI Agents for production what framework are you using? For research and building leisure projects, I personally use langgraph. I wanted to also know if you are not using langgraph, what was the reason?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PleasantInspection12"&gt; /u/PleasantInspection12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmni3q/what_framework_are_you_using_to_build_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T14:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm1v2c</id>
    <title>Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2</title>
    <updated>2025-06-27T18:51:13+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/"&gt; &lt;img alt="Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2" src="https://preview.redd.it/ypm4lnr4ni9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f23d8c2da2fff2f8a6b194ee42f06b2d3e90dca" title="Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/VectorSpaceLab/OmniGen2"&gt;https://github.com/VectorSpaceLab/OmniGen2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://vectorspacelab.github.io/OmniGen2/"&gt;https://vectorspacelab.github.io/OmniGen2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ypm4lnr4ni9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T18:51:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmictu</id>
    <title>We created world's first AI model that does Intermediate reasoning || Defeated models like deepseek and o1 in maths bench mark</title>
    <updated>2025-06-28T09:05:06+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"&gt; &lt;img alt="We created world's first AI model that does Intermediate reasoning || Defeated models like deepseek and o1 in maths bench mark" src="https://b.thumbs.redditmedia.com/POdjJ4mVfUFo8OZYGmiK-E0HKQyUy0sYzl06lElFqWs.jpg" title="We created world's first AI model that does Intermediate reasoning || Defeated models like deepseek and o1 in maths bench mark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We at HelpingAI were fed up with thinking model taking so much tokens, and being very pricy. So, we decided to take a very different approach towards reasoning. Unlike, traditional ai models which reasons on top and then generate response, our ai model do reasoning in middle of response (Intermediate reasoning). Which decreases it's token consumption and time taken by a footfall.&lt;/p&gt; &lt;p&gt;Our model:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9rezjpgy9n9f1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1181ecc8e4a467d9d2fda5c2284176d56edd33f"&gt;https://preview.redd.it/9rezjpgy9n9f1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1181ecc8e4a467d9d2fda5c2284176d56edd33f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deepseek:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/96ae41t1an9f1.jpg?width=1126&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a97d316f56889716e41e4d51e8ba348ce863172f"&gt;https://preview.redd.it/96ae41t1an9f1.jpg?width=1126&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a97d316f56889716e41e4d51e8ba348ce863172f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We have finetuned an existing model named Qwen-14B, because of lack of resources. We have pretrained many models in our past&lt;/p&gt; &lt;p&gt;We ran this model through a series of benchmarks like math-500 (where it scored 95.68) and AIME (where it scored 82). Making it just below gemini-2.5-pro (96)&lt;/p&gt; &lt;p&gt;We are planning to make this model open weight on 1 July. Till then you can chat with it on &lt;a href="http://helpingai.co"&gt;helpingai.co&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;Please give us feedback on which we can improve upon :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T09:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmfiu9</id>
    <title>I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) – Here's what actually works-</title>
    <updated>2025-06-28T05:57:46+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt; &lt;img alt="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) – Here's what actually works-" src="https://external-preview.redd.it/lSrPd1MMz7blRmLYLnruRoJd4XS5NpPXF_maDibWecs.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d68c4413ac33077ccb1f955a9767daec572c1df8" title="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) – Here's what actually works-" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All feedback is welcome! I am learning how to do better everyday.&lt;/p&gt; &lt;p&gt;I went down the LLM rabbit hole trying to find the &lt;strong&gt;best local model&lt;/strong&gt; that runs &lt;em&gt;well&lt;/em&gt; on a humble MacBook Air M1 with just 8GB RAM.&lt;/p&gt; &lt;p&gt;My goal? &lt;strong&gt;Compare 10 models&lt;/strong&gt; across question generation, answering, and self-evaluation.&lt;/p&gt; &lt;p&gt;TL;DR: Some models were brilliant, others… not so much. One even took &lt;strong&gt;8 minutes&lt;/strong&gt; to write a question.&lt;/p&gt; &lt;p&gt;Here's the breakdown &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models Tested&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mistral 7B&lt;/li&gt; &lt;li&gt;DeepSeek-R1 1.5B&lt;/li&gt; &lt;li&gt;Gemma3:1b&lt;/li&gt; &lt;li&gt;Gemma3:latest&lt;/li&gt; &lt;li&gt;Qwen3 1.7B&lt;/li&gt; &lt;li&gt;Qwen2.5-VL 3B&lt;/li&gt; &lt;li&gt;Qwen3 4B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 1B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 3B&lt;/li&gt; &lt;li&gt;LLaMA 3.1 8B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(All models run with quantized versions, via: os.environ[&amp;quot;OLLAMA_CONTEXT_LENGTH&amp;quot;] = &amp;quot;4096&amp;quot; and os.environ[&amp;quot;OLLAMA_KV_CACHE_TYPE&amp;quot;] = &amp;quot;q4_0&amp;quot;)&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Each model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generated 1 question on 5 topics: &lt;em&gt;Math, Writing, Coding, Psychology, History&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Answered all 50 questions (5 x 10)&lt;/li&gt; &lt;li&gt;Evaluated every answer (including their own)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So in total:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50 questions&lt;/li&gt; &lt;li&gt;500 answers&lt;/li&gt; &lt;li&gt;4830 evaluations (Should be 5000; I evaluated less answers with qwen3:1.7b and qwen3:4b as they do not generate scores and take a lot of time**)**&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And I tracked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token generation speed (tokens/sec)&lt;/li&gt; &lt;li&gt;tokens created&lt;/li&gt; &lt;li&gt;time taken&lt;/li&gt; &lt;li&gt;scored all answers for quality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt;, &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;Qwen3 1.7B&lt;/strong&gt; (LLaMA 3.2 1B hit 82 tokens/sec, avg is ~40 tokens/sec (for english topic question it reached &lt;strong&gt;146 tokens/sec)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Slowest: &lt;strong&gt;LLaMA 3.1 8B&lt;/strong&gt;, &lt;strong&gt;Qwen3 4B&lt;/strong&gt;, &lt;strong&gt;Mistral 7B&lt;/strong&gt; Qwen3 4B took &lt;strong&gt;486s&lt;/strong&gt; (8+ mins) to generate a single Math question!&lt;/li&gt; &lt;li&gt;Fun fact: deepseek-r1:1.5b, qwen3:4b and Qwen3:1.7B output &amp;lt;think&amp;gt; tags in questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Answer Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt; and &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek got faster answering &lt;em&gt;its own&lt;/em&gt; questions (80 tokens/s vs. avg 40 tokens/s)&lt;/li&gt; &lt;li&gt;Qwen3 4B generates &lt;strong&gt;2–3x more tokens&lt;/strong&gt; per answer&lt;/li&gt; &lt;li&gt;Slowest: llama3.1:8b, qwen3:4b and mistral:7b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best scorer: Gemma3:latest – consistent, numerical, no bias&lt;/li&gt; &lt;li&gt;Worst scorer: &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt; – often skipped scores entirely&lt;/li&gt; &lt;li&gt;Bias detected: Many models &lt;strong&gt;rate their own answers higher&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek even evaluated some answers &lt;strong&gt;in Chinese&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;I did think of creating a control set of answers. I could tell the mdoel this is the perfect answer basis this rate others. But I did not because it would need support from a lot of people- creating perfect answer, which still can have a bias. I read a few answers and found most of them decent except math. So I tried to find which model's evaluation scores were closest to the average to determine a decent model for evaluation tasks(check last image)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fun Observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some models create &amp;lt;think&amp;gt; tags for questions, answers and even while evaluation as output&lt;/li&gt; &lt;li&gt;Score inflation is real: Mistral, Qwen3, and LLaMA 3.1 8B overrate themselves&lt;/li&gt; &lt;li&gt;Score formats vary wildly (text explanations vs. plain numbers)&lt;/li&gt; &lt;li&gt;Speed isn’t everything – some slower models gave much higher quality answers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Best Performers (My Picks)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Best Model&lt;/th&gt; &lt;th align="left"&gt;Why&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Question Gen&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.2 1B&lt;/td&gt; &lt;td align="left"&gt;Fast &amp;amp; relevant&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Answer Gen&lt;/td&gt; &lt;td align="left"&gt;Gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;Fast, accurate&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Evaluation&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.2 3B&lt;/td&gt; &lt;td align="left"&gt;Generates numerical scores and evaluations closest to model average&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Worst Surprises&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Problem&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Question Gen&lt;/td&gt; &lt;td align="left"&gt;Qwen3 4B&lt;/td&gt; &lt;td align="left"&gt;Took 486s to generate 1 question&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Answer Gen&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.1 8B&lt;/td&gt; &lt;td align="left"&gt;Slow&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Evaluation&lt;/td&gt; &lt;td align="left"&gt;DeepSeek-R1 1.5B&lt;/td&gt; &lt;td align="left"&gt;Inconsistent, skipped scores&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Screenshots Galore&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m adding screenshots of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Questions generation&lt;/li&gt; &lt;li&gt;Answer comparisons&lt;/li&gt; &lt;li&gt;Evaluation outputs&lt;/li&gt; &lt;li&gt;Token/sec charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You &lt;strong&gt;can&lt;/strong&gt; run decent LLMs locally on M1 Air (8GB) – if you pick the right ones&lt;/li&gt; &lt;li&gt;Model size ≠ performance. Bigger isn't always better.&lt;/li&gt; &lt;li&gt;5 Models have a self bais, they rate their own answers higher than average scores. attaching screen shot of a table. Diagonal is their own evaluation, last column is average.&lt;/li&gt; &lt;li&gt;Models' evaluation has high variance! Every model has a unique distribution of the scores it gave.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Post questions if you have any, I will try to answer.&lt;/p&gt; &lt;p&gt;Happy to share more data if you need.&lt;/p&gt; &lt;p&gt;Open to collaborate on interesting projects! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lmfiu9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T05:57:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmk2dj</id>
    <title>Progress stalled in non-reasoning open-source models?</title>
    <updated>2025-06-28T10:58:35+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/"&gt; &lt;img alt="Progress stalled in non-reasoning open-source models?" src="https://preview.redd.it/q53t8do2fn9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbf6bcfd1d93bd65c875ca994b48c3b38839c958" title="Progress stalled in non-reasoning open-source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if you've noticed, but a lot of model providers no longer explicitly note that their models are reasoning models (on benchmarks in particular). Reasoning models aren't ideal for every application.&lt;/p&gt; &lt;p&gt;I looked at the non-reasoning benchmarks on &lt;a href="https://artificialanalysis.ai/models/llama-4-maverick?model-filters=open-source%2Cnon-reasoning-models#artificial-analysis-intelligence-index-by-model-type"&gt;Artificial Analysis&lt;/a&gt; today and the top 2 models (performing comparable) are DeepSeek v3 and Llama 4 Maverick (which I heard was a flop?). I was surprised to see these 2 at the top.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q53t8do2fn9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T10:58:35+00:00</published>
  </entry>
</feed>
