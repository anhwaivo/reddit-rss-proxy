<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-01T12:51:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lobyx5</id>
    <title>Upcoming Coding Models?</title>
    <updated>2025-06-30T16:25:45+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on past threads from this sub, I see that below coding models are coming.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3 Coder - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt;Recent thread&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Deep Cogito - Preview models there&lt;/li&gt; &lt;li&gt;Polaris - Preview models there&lt;/li&gt; &lt;li&gt;Granite releasing any new coding models? Preview (General) models there for upcoming Version 4. How good is their existing coding models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What other coding models coming apart from above ones?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp0o3i</id>
    <title>Resources to learn about samplers?</title>
    <updated>2025-07-01T12:43:05+00:00</updated>
    <author>
      <name>/u/Black-Mack</name>
      <uri>https://old.reddit.com/user/Black-Mack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could you share how to learn more about samplers?&lt;/p&gt; &lt;p&gt;Anything is fine: blogs, articles, videos, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Black-Mack"&gt; /u/Black-Mack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0o3i/resources_to_learn_about_samplers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0o3i/resources_to_learn_about_samplers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0o3i/resources_to_learn_about_samplers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lox9c4</id>
    <title>Need help finding educational datasets and model suggestions for offline LLM on phone</title>
    <updated>2025-07-01T09:30:45+00:00</updated>
    <author>
      <name>/u/Phantomx_77</name>
      <uri>https://old.reddit.com/user/Phantomx_77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I’m trying to build a local LLM that can work offline on a phone, mainly for educational purposes — like helping students with concepts, solving problems step by step, and answering basic academic questions (school or early college level).&lt;/p&gt; &lt;p&gt;I’m planning to fine-tune a smaller model like Phi-2, Mistral 7B, or maybe Qwen 1.5 (4B or 7B). My final goal is to run this model &lt;strong&gt;completely offline&lt;/strong&gt; on a phone using something like llama.cpp.&lt;/p&gt; &lt;p&gt;So I need help with two things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Good educational datasets&lt;/strong&gt; – any open datasets you know of for instruction-style Q&amp;amp;A or tutoring? Preferably stuff that’s already in a good format for fine-tuning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model suggestions + mobile performance&lt;/strong&gt; – I want to use a model that won’t make my phone overheat or lag too much. I’ve heard about 4-bit quantized models (GGUF) — but which ones actually run well on phones?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Also, are there any common things to watch out for to avoid performance issues? Like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which quantization type is best for smooth performance (e.g., Q4_K_M or Q6_K)?&lt;/li&gt; &lt;li&gt;What thread settings or tweaks help reduce heat or battery drain?&lt;/li&gt; &lt;li&gt;Should I go with 3B models instead of 7B for better efficiency?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would really appreciate any tips or your own experience if you’ve tried this already. I’m still figuring it out so anything helps.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Phantomx_77"&gt; /u/Phantomx_77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lox9c4/need_help_finding_educational_datasets_and_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lox9c4/need_help_finding_educational_datasets_and_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lox9c4/need_help_finding_educational_datasets_and_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T09:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lompd5</id>
    <title>I've built a spec for LLM-to-LLM comms by combining semantic patterns with structured syntax</title>
    <updated>2025-06-30T23:31:00+00:00</updated>
    <author>
      <name>/u/sbuswell</name>
      <uri>https://old.reddit.com/user/sbuswell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I'm a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I've found something that's pretty effective (well it has been for me anyway).&lt;/p&gt; &lt;p&gt;Long story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, → for causality and progression, etc) and I've combined these two layers to create a DSL that's more token-efficient but also richer and more logically sound.&lt;/p&gt; &lt;p&gt;This isn't a library you need to install; it's just a spec. Any LLM I've tested it on can understand it out of the box. I've documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.&lt;/p&gt; &lt;p&gt;I'm sharing this because I think it's a genuinely useful technique, and I'd love to get your feedback to help improve it. Or even someone tell me it already exists and I'll use the proper version!&lt;/p&gt; &lt;p&gt;Link to the repo: &lt;a href="https://github.com/elevanaltd/octave"&gt;https://github.com/elevanaltd/octave&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbuswell"&gt; /u/sbuswell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lox1f7</id>
    <title>Local models not following instructions</title>
    <updated>2025-07-01T09:16:05+00:00</updated>
    <author>
      <name>/u/thecookingsenpai</name>
      <uri>https://old.reddit.com/user/thecookingsenpai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have some problems on applying local LLMs to structured workflows.&lt;/p&gt; &lt;p&gt;I use 8b to 24b models on my 16GB 4070 Super TI&lt;/p&gt; &lt;p&gt;I have no problems in chatting or doing web rag with my models, either using open webui or AnythingLLM or custom solutions in python or nodejs. What I am unable to do is doing some more structured work. &lt;/p&gt; &lt;p&gt;Specifically, but this is just an example, I am trying to have my models output a specific JSON format. &lt;/p&gt; &lt;p&gt;I am trying almost everything in the system prompt and even in forcing json responses from ollama, but 70% of the times the models just produce wrong outputs. &lt;/p&gt; &lt;p&gt;Now, my question is more generic than having this specific json so I am not sure about posting the prompt etc. &lt;/p&gt; &lt;p&gt;My question is: are there models that are more suited to follow instructions than others? &lt;/p&gt; &lt;p&gt;Mistral 3.2 is almost always a failure in producing a decent json, so is Gemma 12b&lt;/p&gt; &lt;p&gt;Any specific tips and tricks or models to test? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecookingsenpai"&gt; /u/thecookingsenpai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T09:16:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lozhqc</id>
    <title>Cannot Load any GGUF model using tools like LM Studio or Jan Ai etc</title>
    <updated>2025-07-01T11:44:42+00:00</updated>
    <author>
      <name>/u/Physical-Citron5153</name>
      <uri>https://old.reddit.com/user/Physical-Citron5153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So everything was okay until I upgraded from Windows 10 to 11 and suddenly I couldn’t load any local model through these GUI interfaces. I don’t see any error; it just loads indefinitely, no VRAM will also get occupied. &lt;/p&gt; &lt;p&gt;I checked with llama cpp and it worked fine, no errors.&lt;/p&gt; &lt;p&gt;I have 2x RTX 3090 and I am just confused why this is happening. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical-Citron5153"&gt; /u/Physical-Citron5153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lozhqc/cannot_load_any_gguf_model_using_tools_like_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lozhqc/cannot_load_any_gguf_model_using_tools_like_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lozhqc/cannot_load_any_gguf_model_using_tools_like_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T11:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lomilz</id>
    <title>[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀</title>
    <updated>2025-06-30T23:22:23+00:00</updated>
    <author>
      <name>/u/Awkward-Dare-1127</name>
      <uri>https://old.reddit.com/user/Awkward-Dare-1127</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt; &lt;img alt="[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀" src="https://external-preview.redd.it/4LxPZe7g9FzOelbXz3aOGVDeWd4mgBA3OWlLsXGi_Bc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a61aa76d902ab96a1963a6d4338aa8b21a38657e" title="[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Copy one portable&lt;/em&gt; &lt;code&gt;.exe&lt;/code&gt; &lt;em&gt;+ a&lt;/em&gt; &lt;code&gt;.gguf&lt;/code&gt; &lt;em&gt;model to a flash drive → double-click on any Windows PC → start chatting offline in seconds.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;GitHub ▶︎ &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad"&gt;&lt;strong&gt;https://github.com/runzhouye/Local_LLM_Notepad&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8"&gt;https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/lz6e4zmpd5af1.gif"&gt;https://i.redd.it/lz6e4zmpd5af1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;30-second Quick-Start&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Grab &lt;strong&gt;Local_LLM_Notepad-portable.exe&lt;/strong&gt; from the &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad/releases"&gt;latest release&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download a small CPU model like &lt;strong&gt;gemma-3-1b-it-Q4_K_M.gguf&lt;/strong&gt; (≈0.8 GB) from &lt;a href="https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/tree/main"&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Copy both files onto a USB stick.&lt;/li&gt; &lt;li&gt;Double-click the EXE on any Windows box → first run loads the model.&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;✅&lt;/th&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;What it means&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Plug-and-play&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Single 45 MB EXE runs without admin rights&lt;/td&gt; &lt;td align="left"&gt;Run on any computer—no install needed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Source-word highlighting&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Bold-underlines every word/number from your prompt&lt;/td&gt; &lt;td align="left"&gt;Ctrl-click to trace facts &amp;amp; tables for quick fact-checking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Hotkeys&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;Ctrl + SCtrl + ZCtrl + FCtrl + X&lt;/code&gt; send, stop, search, clear, etc.&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Portable chat logs&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;One-click JSON export&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward-Dare-1127"&gt; /u/Awkward-Dare-1127 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:22:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1loxw8f</id>
    <title>What is night forge?</title>
    <updated>2025-07-01T10:12:17+00:00</updated>
    <author>
      <name>/u/Professional-Ad-4376</name>
      <uri>https://old.reddit.com/user/Professional-Ad-4376</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt; &lt;img alt="What is night forge?" src="https://a.thumbs.redditmedia.com/93U5v10Vycvd1XA2AyyAUDnfoGNgsP5NzRsHUeD4F_0.jpg" title="What is night forge?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/l4xe14k6m8af1.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e263c0717a28e4d5a85e0664b5e7bc8d144aec"&gt;https://preview.redd.it/l4xe14k6m8af1.png?width=2920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4e263c0717a28e4d5a85e0664b5e7bc8d144aec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did a webdev arena, and one was very distinct in its style but I preferred it.&lt;/p&gt; &lt;p&gt;after voting for it, it said it was nightforge? I tried googling but couldn't find anything. Am I on the moon or whats going on?&lt;/p&gt; &lt;p&gt;Does anyone know what this is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Ad-4376"&gt; /u/Professional-Ad-4376 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loxw8f/what_is_night_forge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T10:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp0j7f</id>
    <title>Best open source Arabic tts</title>
    <updated>2025-07-01T12:36:32+00:00</updated>
    <author>
      <name>/u/Spiritual_Button827</name>
      <uri>https://old.reddit.com/user/Spiritual_Button827</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I’ve been trying to find the best TTS options to fine tune for Arabic and I’ve kinda hit a wall with Fish audio after their release of the new S1 model, as they’ve removed the fine tuning code for older models like v1.5.&lt;/p&gt; &lt;p&gt;I tried coqui’s XTTS fork by Idap: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;https://github.com/idiap/coqui-ai-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And got good results, but I would like to try other good options.&lt;/p&gt; &lt;p&gt;I looked at &lt;a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena"&gt;https://huggingface.co/spaces/TTS-AGI/TTS-Arena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And I see that not many options support Arabic.&lt;/p&gt; &lt;p&gt;I’m kinda new to TTS and would appreciate any help/advice.&lt;/p&gt; &lt;p&gt;I have a good server in hand with lots of compute to test anything so any open source model with fine tuning code available and can support Arabic is welcome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Button827"&gt; /u/Spiritual_Button827 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp0j7f/best_open_source_arabic_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lotzy4</id>
    <title>Video Cards &amp; GPUs SPARKLE intros new Arc Pro B60 cards: one is a dual-GPU workstation card with 48GB of VRAM</title>
    <updated>2025-07-01T05:52:37+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tweaktown.com/news/106121/sparkle-intros-new-arc-pro-b60-cards-one-is-dual-gpu-workstation-card-with-48gb-of-vram/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotzy4/video_cards_gpus_sparkle_intros_new_arc_pro_b60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lotzy4/video_cards_gpus_sparkle_intros_new_arc_pro_b60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T05:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lovqjc</id>
    <title>Best Local Model for Vision?</title>
    <updated>2025-07-01T07:46:16+00:00</updated>
    <author>
      <name>/u/xukecheng</name>
      <uri>https://old.reddit.com/user/xukecheng</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe Gemma3 is the best model for vision tasks? Each image uses only 256 tokens. In my own hardware tests, it was the only model capable of processing 60 images simultaneously.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xukecheng"&gt; /u/xukecheng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovqjc/best_local_model_for_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovqjc/best_local_model_for_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lovqjc/best_local_model_for_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T07:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokkpc</id>
    <title>A Meta-Framework for Self-Improving LLMs with Transparent Reasoning</title>
    <updated>2025-06-30T21:59:34+00:00</updated>
    <author>
      <name>/u/henryb213</name>
      <uri>https://old.reddit.com/user/henryb213</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt; &lt;img alt="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" src="https://external-preview.redd.it/GF7LOLNV1EkT3j_WQj3wN6pKRBc62ktaNGoxeqmHjug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31f4c15b33f9e40cd80aee5e1468225b045437e8" title="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Framework overview:&lt;/strong&gt; LLMs iteratively refine their own outputs—typically through a three‑phase cycle &lt;strong&gt;draft → critique → revision&lt;/strong&gt;, repeat until convergence (all phases &amp;amp; stop rules are configurable). I started coding three weeks ago after an eight‑year break and zero professional dev experience.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;The classes work as Python callables with built in observability: instances are callable -&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Python,tabs=4 from recursive_companion.base import MarketingCompanion agent = MarketingCompanion() answer = agent(&amp;quot;question or problem…&amp;quot;) # final refined output print(answer) print(agent.run_log) # list[dict] of every draft, critique &amp;amp; revision &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Why it stays clean &amp;amp; modular&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Templates are plain text files (system prompts, user prompts, protocol). &lt;em&gt;Swap harsh critiques for creative ones by swapping files.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;build_templates()&lt;/code&gt; lets you compose any combination.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Protocol injection&lt;/strong&gt; cleanly separates reasoning patterns from implementation.&lt;/li&gt; &lt;li&gt;New agents in &lt;strong&gt;3 lines&lt;/strong&gt;—just inherit from &lt;code&gt;BaseCompanion&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Convergence uses &lt;strong&gt;embedding‑based cosine similarity&lt;/strong&gt; by default, but the metric is fully pluggable.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;How it came together&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The design emerged from recursive dialogues with multiple LLMs—the same iterative process the framework now automates. No legacy assumptions meant every piece became independent: swap models, add phases, change convergence logic—no rewiring required.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Extras&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Streamlit app&lt;/strong&gt; shows the thinking live as it happens.&lt;/li&gt; &lt;li&gt;Demos cover raw orchestration &lt;em&gt;and&lt;/em&gt; LangGraph integration (agents as graph nodes).&lt;/li&gt; &lt;li&gt;Full architecture docs, comprehensive docstrings, commenting, and worked examples included.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Repo (MIT)&lt;/strong&gt; &lt;a href="https://github.com/hankbesser/recursive-companion"&gt;https://github.com/hankbesser/recursive-companion&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Built by questioning everything. Learning by building, built for learning.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Thanks for reading and really looking for any feedback and open to contributors, no question or discussion is too big or small.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryb213"&gt; /u/henryb213 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/hankbesser/recursive-companion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lol3na</id>
    <title>[Dataset] 4,000 hours of full-body, in-person, human face-to-face interaction videos</title>
    <updated>2025-06-30T22:20:44+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dataset on Huggingface: &lt;a href="https://huggingface.co/datasets/facebook/seamless-interaction"&gt;https://huggingface.co/datasets/facebook/seamless-interaction&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.aidemos.meta.com/seamless_interaction_dataset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lodmc6</id>
    <title>ERNIE 4.5 Collection from Baidu</title>
    <updated>2025-06-30T17:27:55+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T17:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1loo2u3</id>
    <title>Struggling with vLLM. The instructions make it sound so simple to run, but it’s like my Kryptonite. I give up.</title>
    <updated>2025-07-01T00:35:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m normally the guy they call in to fix the IT stuff nobody else can fix. I’ll laser focus on whatever it is and figure it out probably 99% of the time. I’ve been in IT for over 28+ years. I’ve been messing with AI stuff for nearly 2 years now. Getting my Masters in AI right now. All that being said, I’ve never encountered a more difficult software package to run than trying to get vLLM working in Docker. I can run nearly anything else in Docker except for vLLM. I feel like I’m really close, but every time I think it’s going to run, BAM! some new error that i find very little information on. - I’m running Ubuntu 24.04 - I have a 4090, 3090, and 64GB of RAM on AERO-D TRX50 motherboard. - Yes I have the Nvidia runtime container working - Yes I have the hugginface token generated is there an easy button somewhere that I’m missing? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T00:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lococc</id>
    <title>Open Source AI Editor: First Milestone</title>
    <updated>2025-06-30T16:52:52+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt; &lt;img alt="Open Source AI Editor: First Milestone" src="https://external-preview.redd.it/7W6FU5na7gC1vKxg3pWb3QkD0a8T5GyzeaLh8U3roNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d188c22d72aa036de764ff96aa9d951cba5ae6b3" title="Open Source AI Editor: First Milestone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know if you have any questions about open sourcing. Happy to answer. &lt;/p&gt; &lt;p&gt;vscode pm here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lovuxp</id>
    <title>Current state of Intel A770 16GB GPU for Inference?</title>
    <updated>2025-07-01T07:55:04+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I could only find old posts regarding how the Intel A770 fares with LLMs, specifically people notice the high idle power consumption and difficult setup depending on what framework you use. At least a year ago, it was supposed to be a pain to use with Ollama.&lt;/p&gt; &lt;p&gt;Here in Germany, it is by far the cheapest 16GB card, in summary:&lt;br /&gt; - Intel A770, prices starting at 280-300€&lt;br /&gt; - AMD 9060 XT starting at 370€ (+32%)&lt;br /&gt; - Nvidia RTX 5060 Ti starting at 440€ (+57%)&lt;/p&gt; &lt;p&gt;Price-wise the A770 is a no-brainer, but what is your current experience? Currently using an RTX 4060 8GB and LMStudio on Windows 11 (+32GB DDR5).&lt;/p&gt; &lt;p&gt;Thanks for any insights&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lovuxp/current_state_of_intel_a770_16gb_gpu_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T07:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1loswvr</id>
    <title>New to the scene. Yesterday, got 4 t/s on R1 671b q4. Today, I'm getting about 0.15 t/s... What did I break lol</title>
    <updated>2025-07-01T04:46:12+00:00</updated>
    <author>
      <name>/u/sourpatchgrownadults</name>
      <uri>https://old.reddit.com/user/sourpatchgrownadults</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5975wx, 512gb DDR4 3200, dual 3090s. Ollama + OpenWebUI. Running on LMDE.&lt;/p&gt; &lt;p&gt;Idk what went wrong now but I'm struggling to get it back to 4 t/s... I can work with 4 t/s, but 0.15 t/s is just terrible.&lt;/p&gt; &lt;p&gt;Any ideas? Happy to provide information upon request.&lt;/p&gt; &lt;p&gt;Total noob here, just built this a few days ago and very little terminal experience lol but have an open mind and a will to learn.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sourpatchgrownadults"&gt; /u/sourpatchgrownadults &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T04:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lok3r2</id>
    <title>[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News</title>
    <updated>2025-06-30T21:40:05+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt; &lt;img alt="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" src="https://external-preview.redd.it/7cRnC2dFTB8VTd7qs9tim3BVul_HOXlhVu97BYC8mXw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfea0e06944005f53398ccc99f53814a8c4923f4" title="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trendforce.com/news/2024/10/31/news-datacenter-gpus-may-have-an-astonishingly-short-lifespan-of-only-1-to-3-years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lp01c7</id>
    <title>Deepseek R1 at 6,5 tk/s on an Nvidia Tesla P40</title>
    <updated>2025-07-01T12:12:19+00:00</updated>
    <author>
      <name>/u/dc740</name>
      <uri>https://old.reddit.com/user/dc740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I figured I'd post my final setup since many people asked about the P40 and assumed you couldn't do much with it (but you can!).&lt;/p&gt; &lt;pre&gt;&lt;code&gt;numactl --cpunodebind=0 -- ./ik_llama.cpp/build/bin/llama-cli \ --numa numactl \ --model models/unsloth/DeepSeek-R1-0528-GGUF/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf \ --threads 40 \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --top-p 0.95 \ --temp 0.6 \ --ctx-size 32768 \ --seed 3407 \ --n-gpu-layers 62 \ -ot &amp;quot;exps=CPU&amp;quot; \ --mlock \ --no-mmap \ -mla 2 -fa -fmoe \ -ser 5,1 \ -amb 512 \ --prompt &amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python.&amp;lt;｜Assistant｜&amp;gt;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result at the end of the run is 6.5tk/s&lt;/p&gt; &lt;p&gt;I'm open to ideas on how to improve it.&lt;/p&gt; &lt;p&gt;Hardware: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully populated Dell R740 (in performance profile)&lt;/li&gt; &lt;li&gt;Nvidia Tesla P40 (24GB vram)&lt;/li&gt; &lt;li&gt;Xeon Gold 6138&lt;/li&gt; &lt;li&gt;1.5TB of ram (all ram slots populated)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For other models, like Mistral or QwQ I get around 10tk/s&lt;/p&gt; &lt;p&gt;These are my QwQ settings (I use the regular llama.cpp for this one)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;numactl --cpunodebind=0 -- ./llama.cpp/build/bin/llama-cli \ --numa numactl \ --model models/unsloth/unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \ --threads 40 \ --ctx-size 16384 \ --n-gpu-layers 99 \ --seed 3407 \ --temp 0.6 \ --repeat-penalty 1.1 \ --min-p 0.01 \ --top-k 40 \ --top-p 0.95 \ --dry-multiplier 0.5 \ --mlock \ --no-mmap \ --prio 3 \ -no-cnv \ -fa \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; # --prompt &amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python.&amp;lt;｜Assistant｜&amp;gt;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The details on the selected quants are in the model path. Surprisingly, using ik_llama.cpp optimized models from &lt;em&gt;ubergarm&lt;/em&gt; did not speed up Deepseek, but it slowed it down greatly.&lt;/p&gt; &lt;p&gt;Feel free to suggest improvements. For models different than deepseek, ik_llama.cpp was giving me a lot of gibberish output if I enabled fast attention. And some models I couldn't even run on it, so that's why I still use the regular llama.cpp for some of them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dc740"&gt; /u/dc740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T12:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lom2r9</id>
    <title>With the OpenAI employees that Meta hired, do you think this will be positive for local models?</title>
    <updated>2025-06-30T23:02:37+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt; &lt;img alt="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" src="https://preview.redd.it/ymsyhfb2b5af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6adc725dda988a88523c2dd76383f72148e4d67a" title="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, if these people hired were so important to developing powerful and important OpenAI models. Hopefully the next Llama models will be much better than Llama 4... and raise the bar like Llama did before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymsyhfb2b5af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokp88</id>
    <title>Intel Arc Pro B60 Dual 48G Turbo Maxsun GPU Pricing Revealed</title>
    <updated>2025-06-30T22:04:32+00:00</updated>
    <author>
      <name>/u/Airwalker19</name>
      <uri>https://old.reddit.com/user/Airwalker19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many others, I was hyped for the dual GPU Intel Arc Pro B60, so I emailed Maxsun for a quote. Their US distributor hit me back with $5k per unit for 3 GPUs, or $4.5k each for 5+.&lt;/p&gt; &lt;p&gt;Sure, dual GPUs should cost more, but this is &lt;em&gt;10x&lt;/em&gt; the rumored MSRP of the 24GB card. Space savings are nice, but not &lt;em&gt;that&lt;/em&gt; nice.&lt;/p&gt; &lt;p&gt;RIP my hopes for an (affordable) AI desktop win.&lt;/p&gt; &lt;p&gt;Anyone else think this pricing is delusional, or just me?&lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;Here's a screenshot of the email &lt;a href="https://imgur.com/a/Qh1nYb1"&gt;https://imgur.com/a/Qh1nYb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also talked on the phone with a rep and talked him down to $3,800 for 4 units. 5+ units down to $3,000. Still not worth it if the $500 price point for the 24GB cards are to be believed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Airwalker19"&gt; /u/Airwalker19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lotza5</id>
    <title>KrunchWrapper - a LLM compression proxy (beta)</title>
    <updated>2025-07-01T05:51:25+00:00</updated>
    <author>
      <name>/u/LA_rent_Aficionado</name>
      <uri>https://old.reddit.com/user/LA_rent_Aficionado</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"&gt; &lt;img alt="KrunchWrapper - a LLM compression proxy (beta)" src="https://preview.redd.it/c4bjroisb7af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b39a5201c024ced3ca9aba3ebe3b3090ade2d9" title="KrunchWrapper - a LLM compression proxy (beta)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With context limits being the way there are I wanted to experiment with creating a standalone middleman API server that &amp;quot;compresses&amp;quot; requests sent to models as a proof of concept. I've seen other methods employed that use a seperate model for compression but, Krunchwrapper completely avoids the need for running a model as an intermediary - which I find particularly in VRAM constrained environments. With KrunchWrapper I wanted to avoid this dependency and instead rely on local processing to identify areas for compression and pass a &amp;quot;decoder&amp;quot; to the LLM via a system prompt.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Github Link&lt;/strong&gt;: &lt;a href="https://github.com/thad0ctor/KrunchWrapper"&gt;https://github.com/thad0ctor/KrunchWrapper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The server runs on Python 3.12 from its own venv and curently works on both Linux and Windows (mostly tested on linux but I did a few runs on windows). Currently, I have tested it to work on its own embedded WebUI (thank you llama.cpp), SillyTavern and with Cline interfacing with a locally hosted OpenAI compatible server. I also have support for using Cline with the Anthropic API.&lt;/p&gt; &lt;p&gt;Between compression and (optional) comment stripping, &lt;strong&gt;I have been able to acheive &amp;gt;40% compression when passing code files to the LLM that contain lots of repetition.&lt;/strong&gt; So far I haven't had any issues with fairly smart models like Qwen3 (14B, 32B, 235B) and Gemma3 understanding and adhering to the compression instructions.&lt;/p&gt; &lt;p&gt;At its core, what KrunchWrapper essentially does is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Receive:&lt;/strong&gt; Establishes a proxy server that &amp;quot;intercepts&amp;quot; prompts going to a LLM server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyze:&lt;/strong&gt; Analyzes those prompts for common patterns of text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assign:&lt;/strong&gt; Maps a unicode symbol (known to use fewer tokens) to that pattern of text &lt;ol&gt; &lt;li&gt;Analyzes whether savings &amp;gt; system prompt overhead&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compress:&lt;/strong&gt; Replaces all identified patterns of text with the selected symbol(s) &lt;ol&gt; &lt;li&gt; Preserves JSON, markdown, tool calls&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intercept:&lt;/strong&gt; Passes a system prompt with the compression decoder to the LLM along with the compressed message&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruct:&lt;/strong&gt; Instucts the LLM to use the compressed symbols in any response&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Decompress:&lt;/strong&gt; Decodes any responses received from the LLM that contain the compressed symbols&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repeat:&lt;/strong&gt; Intilligently adds to and re-uses any compression dictionaries in follow-on messages&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Beyond the basic functionality there is a wide range of customization and documentation to explain the settings to fine tune compression to your individual needs. For example: users can defer compression to subsequent messages if they intended to provide other files and not &amp;quot;waste&amp;quot; compression tokens on minimal impact compression opportunities.&lt;/p&gt; &lt;p&gt;Looking ahead, I would like to expand this for other popular tools like Roo, Aider, etc. and other APIs. I beleive this could really help save on API costs once expanded.I also did some initial testing with Cursor but given it is proprietary nature and that its requests are encrypted with SSL a lot more work needs to be done to properly intercept its traffic to apply compression for non-local API requests.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Disclaimers:&lt;/strong&gt; I am not a programmer by trade. I refuse to use the v-word I so often see on here but let's just say I could have never even attempted this without agentic coding and API invoice payments flying out the door. This is reflected in the code. I have done my best to employ best practices and not have this be some spaghetti code quagmire but to say this tool is production ready would be an insult to every living software engineer - I would like to stress how Beta this is - like Tarkov 2016, not Tarkov 2025.&lt;/p&gt; &lt;p&gt;This type of compression does not come without latency. Be sure to change the thread settings in the configs to maximize throughput. That said, there is a cost to using less context by means of an added processing delay. Lastly, I highly recommend not turning on DEBUG and verbose logging in your terminal output... seriously.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LA_rent_Aficionado"&gt; /u/LA_rent_Aficionado &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c4bjroisb7af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T05:51:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojlrw</id>
    <title>[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team</title>
    <updated>2025-06-30T21:19:51+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt; &lt;img alt="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team" src="https://external-preview.redd.it/hHtdtWWuX05qlxJIIZFgrRzaMxdrmlIQ8OiqTPog1_w.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e97f33d6160ce6f067a79278cab0942d295e3325" title="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lorbc5</id>
    <title>Is the rumours true about Apple abandoning MLX?</title>
    <updated>2025-07-01T03:17:23+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some folks on X are saying&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T03:17:23+00:00</published>
  </entry>
</feed>
