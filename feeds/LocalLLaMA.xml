<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-12T09:06:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1imy754</id>
    <title>[Update] Building a Fully Open-Source Local LLM-Based Ai for Meeting Minutes Recording and Analysis : Meeting note taker / Ai meeting minutes generator</title>
    <updated>2025-02-11T13:09:57+00:00</updated>
    <author>
      <name>/u/Sorry_Transition_599</name>
      <uri>https://old.reddit.com/user/Sorry_Transition_599</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy754/update_building_a_fully_opensource_local_llmbased/"&gt; &lt;img alt="[Update] Building a Fully Open-Source Local LLM-Based Ai for Meeting Minutes Recording and Analysis : Meeting note taker / Ai meeting minutes generator" src="https://b.thumbs.redditmedia.com/idyw0pmCSC4s13toxY2DsBIQKn8nJnwDHDXfYi-skNE.jpg" title="[Update] Building a Fully Open-Source Local LLM-Based Ai for Meeting Minutes Recording and Analysis : Meeting note taker / Ai meeting minutes generator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Transition_599"&gt; /u/Sorry_Transition_599 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1imy754"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy754/update_building_a_fully_opensource_local_llmbased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imy754/update_building_a_fully_opensource_local_llmbased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1inefmz</id>
    <title>Dive: An OpenSource MCP Client and Host for Desktop</title>
    <updated>2025-02-12T00:44:55+00:00</updated>
    <author>
      <name>/u/BigGo_official</name>
      <uri>https://old.reddit.com/user/BigGo_official</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our team has developed an open-source platform called Dive. Dive is an open-source AI Agent desktop that seamlessly integrates any Tools Call-supported LLM with Anthropic's MCP.&lt;/p&gt; &lt;p&gt;• Universal LLM Support - Works with Claude, GPT, Ollama and other Tool Call-capable LLM&lt;/p&gt; &lt;p&gt;• Open Source &amp;amp; Free - MIT License&lt;/p&gt; &lt;p&gt;• Desktop Native - Built for Windows/Mac/Linux&lt;/p&gt; &lt;p&gt;• MCP Protocol - Full support for Model Context Protocol&lt;/p&gt; &lt;p&gt;• Extensible - Add your own tools and capabilities&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/OpenAgentPlatform/Dive"&gt;https://github.com/OpenAgentPlatform/Dive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download: &lt;a href="https://github.com/OpenAgentPlatform/Dive/releases/tag/v0.1.1"&gt;https://github.com/OpenAgentPlatform/Dive/releases/tag/v0.1.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’d love to hear your feedback, ideas, and use cases&lt;/p&gt; &lt;p&gt;If you like it, please give us a thumbs up&lt;/p&gt; &lt;p&gt;NOTE: This is just a proof-of-concept system and is only at the usable stage.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BigGo_official"&gt; /u/BigGo_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inefmz/dive_an_opensource_mcp_client_and_host_for_desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inefmz/dive_an_opensource_mcp_client_and_host_for_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inefmz/dive_an_opensource_mcp_client_and_host_for_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T00:44:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1inbt4r</id>
    <title>Local PR reviews WITHIN VSCode and Cursor</title>
    <updated>2025-02-11T22:45:07+00:00</updated>
    <author>
      <name>/u/EntelligenceAI</name>
      <uri>https://old.reddit.com/user/EntelligenceAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"&gt; &lt;img alt="Local PR reviews WITHIN VSCode and Cursor" src="https://b.thumbs.redditmedia.com/Tukr5pK_f7zfxTUHMe9BsvOSaLkFcz7OCWYAQrOTrxo.jpg" title="Local PR reviews WITHIN VSCode and Cursor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw Cursor is charging $36(!!) for their new &amp;quot;Bug Fixes&amp;quot; feature - crazy. I just want a PR reviewer to catch my bugs before I push code so people and PR bots don't cover it with comments!&lt;/p&gt; &lt;p&gt;So I built something different: Review your code BEFORE pushing, right in your editor CURSOR or VSCode!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/byj5ebps8lie1.png?width=1960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b24d5068bc0a0a8faf0aa1ace602ff88fd07a40"&gt;https://preview.redd.it/byj5ebps8lie1.png?width=1960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b24d5068bc0a0a8faf0aa1ace602ff88fd07a40&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Super simple:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install the bot in VSCode or Cursor&lt;/li&gt; &lt;li&gt;Make your changes&lt;/li&gt; &lt;li&gt;Type /reviewDiff&lt;/li&gt; &lt;li&gt;Get instant line-by-line feedback&lt;/li&gt; &lt;li&gt;Fix issues before anyone sees them&lt;/li&gt; &lt;li&gt;Push clean code and get that LGTMNo more bot comments cluttering your PRs or embarrassing feedback in front of the team. Just real-time reviews while you're still coding, pulling your full file context for accurate feedback.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Check it out here: &lt;a href="https://marketplace.visualstudio.com/items?itemName=EntelligenceAI.EntelligenceAI"&gt;https://marketplace.visualstudio.com/items?itemName=EntelligenceAI.EntelligenceAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What else would make your pre-PR workflow better? Please share how we can make this better!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntelligenceAI"&gt; /u/EntelligenceAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inbt4r/local_pr_reviews_within_vscode_and_cursor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T22:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1inm9z7</id>
    <title>Running a Local LLM on Langflow – Best Ways to Embed PDFs, CSVs, JSON, Text, Images &amp; Videos?</title>
    <updated>2025-02-12T08:17:03+00:00</updated>
    <author>
      <name>/u/HaDeSxD</name>
      <uri>https://old.reddit.com/user/HaDeSxD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I’m setting up a local LLM using Ollama and Langflow to create a personal AI assistant that can store and reason over my data. My goal is to retrieve notes, summarize documents, and get insights from various types of personal data. &lt;/p&gt; &lt;p&gt;I have a mix of: &lt;/p&gt; &lt;p&gt;Text files (.txt) &lt;/p&gt; &lt;p&gt;Documents (PDFs) &lt;/p&gt; &lt;p&gt;Structured data (CSVs, JSON) &lt;/p&gt; &lt;p&gt;Images (screenshots, scanned notes) &lt;/p&gt; &lt;p&gt;Videos (recorded explanations, tutorials) &lt;/p&gt; &lt;p&gt;I know FAISS and ChromaDB are good for vector storage, but I’m still figuring out: &lt;/p&gt; &lt;p&gt;How to properly embed each data type for retrieval? (text is easy, but what about images/videos?) &lt;/p&gt; &lt;p&gt;What’s the best way to chunk PDFs and long text files for embedding? &lt;/p&gt; &lt;p&gt;How should I handle structured data like CSVs/JSON—convert them to text first, or is there a better approach? &lt;/p&gt; &lt;p&gt;Are there good local embedding models that work well for multimodal data without cloud services? &lt;/p&gt; &lt;p&gt;Any best practices for integrating these embeddings into Langflow? &lt;/p&gt; &lt;p&gt;Would love to hear from others who have tackled similar problems!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HaDeSxD"&gt; /u/HaDeSxD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inm9z7/running_a_local_llm_on_langflow_best_ways_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inm9z7/running_a_local_llm_on_langflow_best_ways_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inm9z7/running_a_local_llm_on_langflow_best_ways_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T08:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1imnaj2</id>
    <title>Elon's bid for OpenAI is about making the for-profit transition as painful as possible for Altman, not about actually purchasing it (explanation in comments).</title>
    <updated>2025-02-11T01:55:14+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From @ phill__1 on twitter:&lt;/p&gt; &lt;p&gt;OpenAI Inc. (the non-profit) wants to convert to a for-profit company. But you &lt;strong&gt;cannot&lt;/strong&gt; just turn a non-profit into a for-profit – that would be an incredible tax loophole. Instead, the new for-profit OpenAI company would need to &lt;strong&gt;pay out&lt;/strong&gt; OpenAI Inc.'s technology and IP (likely in equity in the new for-profit company). &lt;/p&gt; &lt;p&gt;The valuation is tricky since OpenAI Inc. is theoretically the sole controlling shareholder of the capped-profit subsidiary, OpenAI LP. But there have been some numbers floating around. Since the rumored SoftBank investment at a $260B valuation is dependent on the for-profit move, we're using the current ~$150B valuation. &lt;/p&gt; &lt;p&gt;&lt;em&gt;Control premiums&lt;/em&gt; in market transactions typically range between 20-30% of enterprise value; experts have predicted something around $30B-$40B. &lt;strong&gt;The key is&lt;/strong&gt;, this valuation is ultimately signed off on by the California and Delaware Attorneys General. &lt;/p&gt; &lt;p&gt;Now, if you want to &lt;strong&gt;block&lt;/strong&gt; OpenAI from the for-profit transition, but have yet to be successful in court, what do you do? &lt;em&gt;Make it as painful as possible.&lt;/em&gt; Elon Musk just gave regulators a &lt;strong&gt;perfect&lt;/strong&gt; argument for why the non-profit should get $97B for selling their technology and IP. This would instantly make the non-profit the majority stakeholder at &lt;em&gt;62%&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;It's a clever move that throws a major wrench into the for-profit transition, potentially even stopping it dead in its tracks. Whether OpenAI accepts the offer or not (they won't), the mere existence of this valuation benchmark will be hard for regulators to ignore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imnaj2/elons_bid_for_openai_is_about_making_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T01:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1imy7gs</id>
    <title>Android NPU prompt processing ~16k tokens using llama 8B!</title>
    <updated>2025-02-11T13:10:22+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy7gs/android_npu_prompt_processing_16k_tokens_using/"&gt; &lt;img alt="Android NPU prompt processing ~16k tokens using llama 8B!" src="https://external-preview.redd.it/bHR4Mzh3eHBjaWllMcHEeWWDxXtRsAlZuyHnXPHYA8F-o65beS5TU-PcpoQ-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=271efc0a7cf13784adc6a1a4b5f128c1609db55d" title="Android NPU prompt processing ~16k tokens using llama 8B!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/83sbjwxpciie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imy7gs/android_npu_prompt_processing_16k_tokens_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imy7gs/android_npu_prompt_processing_16k_tokens_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9qsg</id>
    <title>Boosting Unsloth 1.58 Quant of Deepseek R1 671B Performance with Faster Storage – 3x Speedup!</title>
    <updated>2025-02-11T21:19:12+00:00</updated>
    <author>
      <name>/u/akumaburn</name>
      <uri>https://old.reddit.com/user/akumaburn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a test to see if I could improve the performance of &lt;strong&gt;Unsloth 1.58-bit-quantized DeepSeek R1 671B&lt;/strong&gt; by upgrading my storage setup. &lt;strong&gt;Spoiler: It worked!&lt;/strong&gt; Nearly &lt;strong&gt;tripled&lt;/strong&gt; my token generation rate, and I learned a lot along the way.&lt;/p&gt; &lt;p&gt;Hardware Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: Ryzen 5900X (4.5GHz, 12 cores)&lt;/li&gt; &lt;li&gt;GPU: XFX AMD Radeon 7900 XTX Black (24GB GDDR6)&lt;/li&gt; &lt;li&gt;RAM: 96GB DDR4 3600MHz (mismatched 4 sticks, not ideal)&lt;/li&gt; &lt;li&gt;Motherboard: MSI X570 Tomahawk MAX WIFI&lt;/li&gt; &lt;li&gt;OS: EndeavourOS (Arch Linux)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Storage:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single NVMe (BTRFS, on motherboard): XPG 4TB GAMMIX S70 Blade PCIe Gen4&lt;/li&gt; &lt;li&gt;Quad NVMe RAID 0 (XFS, via ASUS Hyper M.2 x16 Gen5 card): 4× 2TB Silicon Power US75&lt;/li&gt; &lt;li&gt;Key Optimisations: &lt;ul&gt; &lt;li&gt;Scheduler: Set to kyber&lt;/li&gt; &lt;li&gt;read_ahead_kb: Set to 128 for better random read performance&lt;/li&gt; &lt;li&gt;File System Tests: Tried F2FS, BTRFS, and XFS – XFS performed the best on the RAID array&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Findings &amp;amp; Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;This result is only valid for low context sizes&lt;/strong&gt; (~2048). Higher contexts dramatically increase memory &amp;amp; VRAM usage. (I'm planning on running some more tests for higher context sizes, but suspect I will run out of RAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Couldn’t fully utilise the RAID 0 speeds&lt;/strong&gt; – capped at 16GB/s on Linux, likely due to PCIe lane limitations (both on-board NVMe slots are filled + the 7900 XTX eats up bandwidth).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Biggest impact? read_ahead_kb&lt;/strong&gt; had the most noticeable effect. mmap relies heavily on random read throughput, which is greatly affected by this setting. (lower seems better to a degree)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;If I did it again?&lt;/strong&gt; (or if was doing it from scratch and not just upgrading my main PC) I'd go Threadripper for more PCIe lanes and I'd try to get faster memory.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Stats:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4TB NVME Single Drive:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(base) [akumaburn@a-pc ~]$ ionice -c 1 -n 0 /usr/bin/taskset -c 0-11 /home/akumaburn/Desktop/Projects/llama.cpp/build/bin/llama-bench -m /home/akumaburn/Desktop/Projects/LLaMA/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf -p 512 -n 128 -b 512 -ub 512 -ctk q4_0 -t 12 -ngl 70 -fa 1 -r 5 -o md --progress ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | warp size: 64 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | n_batch | type_k | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -: | ------------: | -------------------: | llama-bench: benchmark 1/2: starting ggml_vulkan: Compiling shaders.............................................Done! llama-bench: benchmark 1/2: warmup prompt run llama-bench: benchmark 1/2: prompt run 1/5 llama-bench: benchmark 1/2: prompt run 2/5 llama-bench: benchmark 1/2: prompt run 3/5 llama-bench: benchmark 1/2: prompt run 4/5 llama-bench: benchmark 1/2: prompt run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | pp512 | 5.11 ± 0.01 | llama-bench: benchmark 2/2: starting llama-bench: benchmark 2/2: warmup generation run llama-bench: benchmark 2/2: generation run 1/5 llama-bench: benchmark 2/2: generation run 2/5 llama-bench: benchmark 2/2: generation run 3/5 llama-bench: benchmark 2/2: generation run 4/5 llama-bench: benchmark 2/2: generation run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | tg128 | 1.29 ± 0.09 | build: 80d0d6b4 (4519) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;4x2TB NVME Raid-0:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(base) [akumaburn@a-pc ~]$ ionice -c 1 -n 0 /usr/bin/taskset -c 0-11 /home/akumaburn/Desktop/Projects/llama.cpp/build/bin/llama-bench -m /mnt/xfs_raid0/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf -p 512 -n 128 -b 512 -ub 512 -ctk q4_0 -t 12 -ngl 70 -fa 1 -r 5 -o md --progress ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | warp size: 64 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | n_batch | type_k | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -: | ------------: | -------------------: | llama-bench: benchmark 1/2: starting ggml_vulkan: Compiling shaders.............................................Done! llama-bench: benchmark 1/2: warmup prompt run llama-bench: benchmark 1/2: prompt run 1/5 llama-bench: benchmark 1/2: prompt run 2/5 llama-bench: benchmark 1/2: prompt run 3/5 llama-bench: benchmark 1/2: prompt run 4/5 llama-bench: benchmark 1/2: prompt run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | pp512 | 6.01 ± 0.05 | llama-bench: benchmark 2/2: starting llama-bench: benchmark 2/2: warmup generation run llama-bench: benchmark 2/2: generation run 1/5 llama-bench: benchmark 2/2: generation run 2/5 llama-bench: benchmark 2/2: generation run 3/5 llama-bench: benchmark 2/2: generation run 4/5 llama-bench: benchmark 2/2: generation run 5/5 | deepseek2 671B IQ1_S - 1.5625 bpw | 130.60 GiB | 671.03 B | Vulkan | 70 | 512 | q4_0 | 1 | tg128 | 3.30 ± 0.15 | build: 80d0d6b4 (4519) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akumaburn"&gt; /u/akumaburn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9qsg/boosting_unsloth_158_quant_of_deepseek_r1_671b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9qsg/boosting_unsloth_158_quant_of_deepseek_r1_671b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9qsg/boosting_unsloth_158_quant_of_deepseek_r1_671b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1imxthq</id>
    <title>I built and open-sourced a model-agnostic architecture that applies R1-inspired reasoning onto (in theory) any LLM. (More details in the comments.)</title>
    <updated>2025-02-11T12:49:40+00:00</updated>
    <author>
      <name>/u/JakeAndAI</name>
      <uri>https://old.reddit.com/user/JakeAndAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imxthq/i_built_and_opensourced_a_modelagnostic/"&gt; &lt;img alt="I built and open-sourced a model-agnostic architecture that applies R1-inspired reasoning onto (in theory) any LLM. (More details in the comments.)" src="https://external-preview.redd.it/Y2NpeDU4eXVhaWllMXfHHmD6JzwUZ6Bm0WYsf1XL3J4yUspRtDfHF7Qk2Xoa.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4df278042d4308f102ad830d111c5117d2eea27" title="I built and open-sourced a model-agnostic architecture that applies R1-inspired reasoning onto (in theory) any LLM. (More details in the comments.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JakeAndAI"&gt; /u/JakeAndAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9howo9yuaiie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imxthq/i_built_and_opensourced_a_modelagnostic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imxthq/i_built_and_opensourced_a_modelagnostic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T12:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1indxb7</id>
    <title>Cool new TTS</title>
    <updated>2025-02-12T00:20:57+00:00</updated>
    <author>
      <name>/u/throwawayacc201711</name>
      <uri>https://old.reddit.com/user/throwawayacc201711</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1indxb7/cool_new_tts/"&gt; &lt;img alt="Cool new TTS" src="https://external-preview.redd.it/ckDm43nxsYs8mLGkY2iPQxL86a_8AnLyS95I0lJP1I8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=264273fc9f005840ca55e0bad8426aadecc23da2" title="Cool new TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled across this new TTS model (open source and open weights) call zonos. &lt;/p&gt; &lt;p&gt;Saw this post with a demo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/zyphraai/status/1888996367923888341?s=46&amp;amp;t=EKmVivcLrgePIH2fyc-IfA"&gt;https://x.com/zyphraai/status/1888996367923888341?s=46&amp;amp;t=EKmVivcLrgePIH2fyc-IfA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gonna try and get some time to test it out. Has anyone used it before?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/throwawayacc201711"&gt; /u/throwawayacc201711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Zyphra/Zonos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1indxb7/cool_new_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1indxb7/cool_new_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T00:20:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1injegi</id>
    <title>LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!</title>
    <updated>2025-02-12T05:05:27+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.07374"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1injegi/llms_can_easily_learn_to_reason_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1injegi/llms_can_easily_learn_to_reason_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T05:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9zth</id>
    <title>Thomson Reuters Wins First Major AI Copyright Case in the US</title>
    <updated>2025-02-11T21:29:31+00:00</updated>
    <author>
      <name>/u/tofous</name>
      <uri>https://old.reddit.com/user/tofous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9zth/thomson_reuters_wins_first_major_ai_copyright/"&gt; &lt;img alt="Thomson Reuters Wins First Major AI Copyright Case in the US" src="https://external-preview.redd.it/-0DUUc5f5aqTMh8-ZeH6fmczh-Ih5wc4UWlpTHp3exU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f55fcc6771896d54eec682ba109deef9ce2630d" title="Thomson Reuters Wins First Major AI Copyright Case in the US" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tofous"&gt; /u/tofous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9zth/thomson_reuters_wins_first_major_ai_copyright/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9zth/thomson_reuters_wins_first_major_ai_copyright/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1inf39f</id>
    <title>Updates: UK and US only two countries not to sign AI safety agreement at Paris AI Summit</title>
    <updated>2025-02-12T01:16:26+00:00</updated>
    <author>
      <name>/u/MerePotato</name>
      <uri>https://old.reddit.com/user/MerePotato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inf39f/updates_uk_and_us_only_two_countries_not_to_sign/"&gt; &lt;img alt="Updates: UK and US only two countries not to sign AI safety agreement at Paris AI Summit" src="https://external-preview.redd.it/6TXevgYpufzYpfTjezpGg52UERA8ZIyKmvoa8ynszgs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1ee4f4645a585d0b344f74ef45c2a3af79f5ac" title="Updates: UK and US only two countries not to sign AI safety agreement at Paris AI Summit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MerePotato"&gt; /u/MerePotato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.sky.com/story/politics-latest-immigration-labour-starmer-badenoch-farage-live-news-12593360?postid=9087865#liveblog-body"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inf39f/updates_uk_and_us_only_two_countries_not_to_sign/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inf39f/updates_uk_and_us_only_two_countries_not_to_sign/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T01:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1inahmj</id>
    <title>AI-RP GUI, thoughts?</title>
    <updated>2025-02-11T21:50:23+00:00</updated>
    <author>
      <name>/u/Diligent-Builder7762</name>
      <uri>https://old.reddit.com/user/Diligent-Builder7762</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"&gt; &lt;img alt="AI-RP GUI, thoughts?" src="https://external-preview.redd.it/anUxbWh5MXF3a2llMZDcyaEte4yY5OTnn_MraO3a1mLbSyQuEc8JUukRHPMy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ffabf7e7e829026793cb0c9fe2931fd6dab7c685" title="AI-RP GUI, thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Builder7762"&gt; /u/Diligent-Builder7762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9ej41y1qwkie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inahmj/airp_gui_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:50:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1in4b81</id>
    <title>Why AMD or Intel doesn't sell card with huge amount of Vram ?</title>
    <updated>2025-02-11T17:39:14+00:00</updated>
    <author>
      <name>/u/Euphoric_Tutor_5054</name>
      <uri>https://old.reddit.com/user/Euphoric_Tutor_5054</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, we saw that even with an epyc processor and 512 gb of ram you can run deepseek pretty fast, but compared to a graphic card it's pretty slow. But the problem is that you need a lot of vram on your graphic card so why AMD and intel doesn't sell such card with enormous amount of vram ? especially since 8gb of gddr6 is super cheap now ! like 3$ I believe, look here : &lt;a href="https://www.dramexchange.com/"&gt;https://www.dramexchange.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Would be a killer for inference &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Euphoric_Tutor_5054"&gt; /u/Euphoric_Tutor_5054 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in4b81/why_amd_or_intel_doesnt_sell_card_with_huge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T17:39:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1inieoe</id>
    <title>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</title>
    <updated>2025-02-12T04:07:22+00:00</updated>
    <author>
      <name>/u/ekaesmem</name>
      <uri>https://old.reddit.com/user/ekaesmem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt; &lt;img alt="Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling" src="https://b.thumbs.redditmedia.com/xPf7D_ti2_9HmqPFloVt-vKCywazKffYdOe7zEWXDAg.jpg" title="Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2vt00wzoumie1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fef3cc679cc29305e31e0eb234f1b990c0f2fca"&gt;https://preview.redd.it/2vt00wzoumie1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fef3cc679cc29305e31e0eb234f1b990c0f2fca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.06703"&gt;[2502.06703] Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ekaesmem"&gt; /u/ekaesmem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T04:07:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1inbili</id>
    <title>UK and US refuse to sign international AI declaration</title>
    <updated>2025-02-11T22:32:41+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"&gt; &lt;img alt="UK and US refuse to sign international AI declaration" src="https://external-preview.redd.it/4F7TAGCOz8Rfg3WqXnrQP8MWV7RA9T-cLoTH3Je-EA8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbfa17295cbd542f368737b59db21a9f78ea0823" title="UK and US refuse to sign international AI declaration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bbc.com/news/articles/c8edn0n58gwo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T22:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1in0938</id>
    <title>I made Iris: A fully-local realtime voice chatbot!</title>
    <updated>2025-02-11T14:49:16+00:00</updated>
    <author>
      <name>/u/Born_Search2534</name>
      <uri>https://old.reddit.com/user/Born_Search2534</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt; &lt;img alt="I made Iris: A fully-local realtime voice chatbot!" src="https://external-preview.redd.it/stLjrcu85AgTrwW4zDhH8loF7eayJD3_hD2XyXmIgUw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65bd986bcf0e0f5ab0f244100b8cb83d8e1266a9" title="I made Iris: A fully-local realtime voice chatbot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Born_Search2534"&gt; /u/Born_Search2534 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=XK-37m-p11k&amp;amp;feature=shared"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T14:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmkbc</id>
    <title>agentica-org/DeepScaleR-1.5B-Preview</title>
    <updated>2025-02-12T08:39:47+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt; &lt;img alt="agentica-org/DeepScaleR-1.5B-Preview" src="https://preview.redd.it/3fm88arb7oie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=094bb1f83ed48f6b26b3ca5b52f7cdfb742b34e0" title="agentica-org/DeepScaleR-1.5B-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3fm88arb7oie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T08:39:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1in83vw</id>
    <title>Chonky Boi has arrived</title>
    <updated>2025-02-11T20:12:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt; &lt;img alt="Chonky Boi has arrived" src="https://external-preview.redd.it/YKjiYoZG5DJKHF8InRyOIM7iTEJQimQj1eCDwySpqqE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69fcf80dcc8f1be1b6b900fa7ba68bf7b62ee469" title="Chonky Boi has arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/kh64WJy.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1in69s3</id>
    <title>4x3090 in a 4U case, don't recommend it</title>
    <updated>2025-02-11T18:58:29+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt; &lt;img alt="4x3090 in a 4U case, don't recommend it" src="https://a.thumbs.redditmedia.com/78a1YQ1sn0cHJ3q5f1VyWRySQ3DMLohcp9folBrA-j0.jpg" title="4x3090 in a 4U case, don't recommend it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1in69s3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T18:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9vsb</id>
    <title>NYT: Vance speech at EU AI summit</title>
    <updated>2025-02-11T21:24:49+00:00</updated>
    <author>
      <name>/u/Mediocre_Tree_5690</name>
      <uri>https://old.reddit.com/user/Mediocre_Tree_5690</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt; &lt;img alt="NYT: Vance speech at EU AI summit" src="https://preview.redd.it/vjltv4twukie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6ab17775dd480a87f8fde7e76f52035c4a8b6cc" title="NYT: Vance speech at EU AI summit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://archive.is/eWNry"&gt;https://archive.is/eWNry&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's an archive link in case anyone wants to read the article. Macron spoke about lighter regulation at the AI summit as well. Are we thinking safetyism is finally on its way out? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Tree_5690"&gt; /u/Mediocre_Tree_5690 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vjltv4twukie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1imyp19</id>
    <title>If you want my IT department to block HF, just say so.</title>
    <updated>2025-02-11T13:35:20+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt; &lt;img alt="If you want my IT department to block HF, just say so." src="https://preview.redd.it/h1dbbwhxiiie1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1bafacbc3514f10c5b939ade16c607722c1d9b0" title="If you want my IT department to block HF, just say so." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1dbbwhxiiie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:35:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1in7nka</id>
    <title>ChatGPT 4o feels straight up stupid after using o1 and DeepSeek for awhile</title>
    <updated>2025-02-11T19:54:09+00:00</updated>
    <author>
      <name>/u/Getabock_</name>
      <uri>https://old.reddit.com/user/Getabock_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And to think I used to be really impressed with 4o. Crazy. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Getabock_"&gt; /u/Getabock_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T19:54:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1in8vya</id>
    <title>EU mobilizes $200 billion in AI race against US and China</title>
    <updated>2025-02-11T20:44:31+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt; &lt;img alt="EU mobilizes $200 billion in AI race against US and China" src="https://external-preview.redd.it/X_db72AfOkvUPacuwPMCLwkrfkqSycSFXdfcaogRMnw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6dff9e983d7903f62f20823b94ae4fd34bac0f8" title="EU mobilizes $200 billion in AI race against US and China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/news/609930/eu-200-billion-investment-ai-development"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1inch7r</id>
    <title>A new paper demonstrates that LLMs could "think" in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows.</title>
    <updated>2025-02-11T23:14:51+00:00</updated>
    <author>
      <name>/u/tehbangere</name>
      <uri>https://old.reddit.com/user/tehbangere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt; &lt;img alt="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." src="https://external-preview.redd.it/lsXw1VKNR0EoTFYgDUro5o8By4n9gHC7i_cxDktIeuo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7f243d34bc596be68af0031b70b22b21c475830" title="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tehbangere"&gt; /u/tehbangere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2502.05171"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T23:14:51+00:00</published>
  </entry>
</feed>
