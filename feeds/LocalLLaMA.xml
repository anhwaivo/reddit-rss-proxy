<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-15T16:52:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kmi3ra</id>
    <title>AMD Strix Halo (Ryzen AI Max+ 395) GPU LLM Performance</title>
    <updated>2025-05-14T15:29:44+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been doing some (ongoing) testing on a Strix Halo system recently and with a bunch of desktop systems coming out, and very few advanced/serious GPU-based LLM performance reviews out there, I figured it might be worth sharing a few notes I've made on the current performance and state of software.&lt;/p&gt; &lt;p&gt;This post will primarily focus on LLM inference with the Strix Halo GPU on Linux (but the llama.cpp testing should be pretty relevant for Windows as well).&lt;/p&gt; &lt;p&gt;This post gets rejected with too many links so I'll just leave a single link for those that want to dive deeper: &lt;a href="https://llm-tracker.info/_TOORG/Strix-Halo"&gt;https://llm-tracker.info/_TOORG/Strix-Halo&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Raw Performance&lt;/h1&gt; &lt;p&gt;In terms of raw compute specs, the Ryzen AI Max 395's Radeon 8060S has 40 RDNA3.5 CUs. At a max clock of 2.9GHz this should have a peak of &lt;strong&gt;59.4 FP16/BF16 TFLOPS&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;code&gt; 512 ops/clock/CU * 40 CU * 2.9e9 clock / 1e12 = 59.392 FP16 TFLOPS &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This peak value requires either WMMA or wave32 VOPD otherwise the max is halved.&lt;/p&gt; &lt;p&gt;Using mamf-finder to test, without hipBLASLt, it takes about 35 hours to test and only gets to &lt;strong&gt;5.1 BF16 TFLOPS&lt;/strong&gt; (&lt;strong&gt;&amp;lt;9%&lt;/strong&gt; max theoretical).&lt;/p&gt; &lt;p&gt;However, when run with hipBLASLt, this goes up to &lt;strong&gt;36.9 TFLOPS&lt;/strong&gt; (&lt;strong&gt;&amp;gt;60%&lt;/strong&gt; max theoretical) which is comparable to MI300X efficiency numbers.&lt;/p&gt; &lt;p&gt;On the memory bandwidth (MBW) front, &lt;code&gt;rocm_bandwidth_test&lt;/code&gt; gives about &lt;strong&gt;212 GB/s&lt;/strong&gt; peak bandwidth (DDR5-8000 on a 256-bit bus gives a theoretical peak MBW of &lt;strong&gt;256 GB/s&lt;/strong&gt;). This is roughly in line with the max MBW tested by ThePhawx, jack stone, and others on various Strix Halo systems.&lt;/p&gt; &lt;p&gt;One thing &lt;code&gt;rocm_bandwidth_test&lt;/code&gt; gives you is also CPU to GPU speed, which is &lt;strong&gt;~84 GB/s&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The system I am using is set to almost all of its memory dedicated to GPU - 8GB GART and 110 GB GTT and has a very high PL (&amp;gt;100W TDP).&lt;/p&gt; &lt;h1&gt;llama.cpp&lt;/h1&gt; &lt;p&gt;What most people probably want to know is how these chips perform with llama.cpp for bs=1 inference. &lt;/p&gt; &lt;p&gt;First I'll test with the standard TheBloke/Llama-2-7B-GGUF Q4_0 so you can easily compare to other tests like my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/llamacpp_compute_and_memory_bandwidth_efficiency/"&gt;previous compute and memory bandwidth efficiency tests across architectures&lt;/a&gt; or the official llama.cpp Apple Silicon M-series performance thread.&lt;/p&gt; &lt;p&gt;I ran with a number of different backends, and the results were actually pretty surprising:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp512 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg128 (t/s)&lt;/th&gt; &lt;th align="left"&gt;Max Mem (MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;294.64 ± 0.58&lt;/td&gt; &lt;td align="left"&gt;28.94 ± 0.04&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU + FA&lt;/td&gt; &lt;td align="left"&gt;294.36 ± 3.13&lt;/td&gt; &lt;td align="left"&gt;29.42 ± 0.03&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;348.96 ± 0.31&lt;/td&gt; &lt;td align="left"&gt;48.72 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;4219&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + FA&lt;/td&gt; &lt;td align="left"&gt;331.96 ± 0.41&lt;/td&gt; &lt;td align="left"&gt;45.78 ± 0.02&lt;/td&gt; &lt;td align="left"&gt;4245&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA&lt;/td&gt; &lt;td align="left"&gt;322.63 ± 1.34&lt;/td&gt; &lt;td align="left"&gt;48.40 ± 0.02&lt;/td&gt; &lt;td align="left"&gt;4218&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA + FA&lt;/td&gt; &lt;td align="left"&gt;343.91 ± 0.60&lt;/td&gt; &lt;td align="left"&gt;50.88 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;4218&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;881.71 ± 1.71&lt;/td&gt; &lt;td align="left"&gt;52.22 ± 0.05&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3923&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan + FA&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;884.20 ± 6.23&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;52.73 ± 0.07&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3923&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The HIP version performs &lt;strong&gt;far&lt;/strong&gt; below what you'd expect in terms of tok/TFLOP efficiency for prompt processing even vs other RDNA3 architectures:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gfx1103&lt;/code&gt; Radeon 780M iGPU gets 14.51 tok/TFLOP. At that efficiency you'd expect the about 850 tok/s that the Vulkan backend delivers.&lt;/li&gt; &lt;li&gt;&lt;code&gt;gfx1100&lt;/code&gt; Radeon 7900 XTX gets 25.12 tok/TFLOP. At that efficiency you'd expect almost 1500 tok/s, almost double what the Vulkan backend delivers, and &amp;gt;4X what the current HIP backend delivers.&lt;/li&gt; &lt;li&gt;HIP pp512 barely beats out CPU backend numbers. I don't have an explanation for this.&lt;/li&gt; &lt;li&gt;Just for a reference of how bad the HIP performance is, an 18CU M3 Pro has ~12.8 FP16 TFLOPS (4.6X less compute than Strix Halo) and delivers about the same pp512. Lunar Lake Arc 140V has 32 FP16 TFLOPS (almost 1/2 Strix Halo) and has a pp512 of 657 tok/s (1.9X faster)&lt;/li&gt; &lt;li&gt;With the Vulkan backend pp512 is about the same as an M4 Max and tg128 is about equivalent to an M4 Pro&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Testing a similar system with Linux 6.14 vs 6.15 showed a 15% performance difference so it's possible future driver/platform updates will improve/fix Strix Halo's ROCm/HIP compute efficiency problems.&lt;/p&gt; &lt;p&gt;So that's a bit grim, but I did want to point out one silver lining. With the recent fixes for Flash Attention with the llama.cpp Vulkan backend, I did some higher context testing, and here, the HIP + rocWMMA backend actually shows some strength. It has basically &lt;strong&gt;no decrease in either pp or tg performance at 8K context&lt;/strong&gt; and uses the least memory to boot:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp8192 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg8192 (t/s)&lt;/th&gt; &lt;th align="left"&gt;Max Mem (MiB)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;245.59 ± 0.10&lt;/td&gt; &lt;td align="left"&gt;12.43 ± 0.00&lt;/td&gt; &lt;td align="left"&gt;6+10591&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + FA&lt;/td&gt; &lt;td align="left"&gt;190.86 ± 0.49&lt;/td&gt; &lt;td align="left"&gt;30.01 ± 0.00&lt;/td&gt; &lt;td align="left"&gt;7+8089&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA&lt;/td&gt; &lt;td align="left"&gt;230.10 ± 0.70&lt;/td&gt; &lt;td align="left"&gt;12.37 ± 0.00&lt;/td&gt; &lt;td align="left"&gt;6+10590&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP + WMMA + FA&lt;/td&gt; &lt;td align="left"&gt;368.77 ± 1.22&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;50.97 ± 0.00&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;7+8062&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;487.69 ± 0.83&lt;/td&gt; &lt;td align="left"&gt;7.54 ± 0.02&lt;/td&gt; &lt;td align="left"&gt;7761+1180&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan + FA&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;490.18 ± 4.89&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;32.03 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;7767+1180&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;You need to have &lt;code&gt;rocmwmma&lt;/code&gt; installed - many distros have packages but you need gfx1151 support is very new (#PR 538) from last week) so you will probably need to build your own rocWMMA from source&lt;/li&gt; &lt;li&gt;You should then rebuild llama.cpp with &lt;code&gt;-DGGML_HIP_ROCWMMA_FATTN=ON&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you mostly do 1-shot inference, then the Vulkan + FA backend is actually probably the best and is the most cross-platform/easy option. If you frequently have longer conversations then HIP + WMMA + FA is probalby the way to go, even if prompt processing is much slower than it should be right now.&lt;/p&gt; &lt;p&gt;I also ran some tests with Qwen3-30B-A3B UD-Q4_K_XL. Larger MoEs is where these large unified memory APUs really shine. &lt;/p&gt; &lt;p&gt;Here are Vulkan results. One thing worth noting, and this is particular to the Qwen3 MoE and Vulkan backend, but using &lt;code&gt;-b 256&lt;/code&gt; significantly improves the pp512 performance:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp512 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg128 (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;70.03 ± 0.18&lt;/td&gt; &lt;td align="left"&gt;75.32 ± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan b256&lt;/td&gt; &lt;td align="left"&gt;118.78 ± 0.64&lt;/td&gt; &lt;td align="left"&gt;74.76 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While the pp512 is slow, tg128 is as speedy as you'd expect for 3B activations.&lt;/p&gt; &lt;p&gt;This is still only a 16.5 GB model though, so let's go bigger. Llama 4 Scout is 109B parameters and 17B activations and the UD-Q4_K_XL is 57.93 GiB.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Run&lt;/th&gt; &lt;th align="left"&gt;pp512 (t/s)&lt;/th&gt; &lt;th align="left"&gt;tg128 (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;102.61 ± 1.02&lt;/td&gt; &lt;td align="left"&gt;20.23 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HIP&lt;/td&gt; &lt;td align="left"&gt;GPU Hang&lt;/td&gt; &lt;td align="left"&gt;GPU Hang&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While Llama 4 has had a rocky launch, this is a model that performs about as well as Llama 3.3 70B, but tg is 4X faster, and has SOTA vision as well, so having this speed for tg is a real win.&lt;/p&gt; &lt;p&gt;I've also been able to successfully RPC llama.cpp to test some truly massive (Llama 4 Maverick, Qwen 235B-A22B models, but I'll leave that for a future followup).&lt;/p&gt; &lt;p&gt;Besides romWMMA, I was able to build a ROCm 6.4 image for Strix Halo (gfx1151) using &lt;a href="/u/scottt"&gt;u/scottt&lt;/a&gt;'s dockerfiles. These docker images have hipBLASLt built with gfx1151 support.&lt;/p&gt; &lt;p&gt;I was also able to build AOTriton without too much hassle (it takes about 1h wall time on Strix Halo if you restrict to just the gfx1151 GPU_TARGET).&lt;/p&gt; &lt;p&gt;Composable Kernel (CK) has gfx1151 support now as well and builds in about 15 minutes.&lt;/p&gt; &lt;p&gt;PyTorch was a huge PITA to build, but with a fair amount of elbow grease, I was able to get HEAD (2.8.0a0) compiling, however it still has problems with Flash Attention not working even with &lt;code&gt;TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL&lt;/code&gt; set.&lt;/p&gt; &lt;p&gt;There's a lot of active work ongoing for PyTorch. For those interested, I'd recommend checking out my linked docs.&lt;/p&gt; &lt;p&gt;I won't bother testing training or batch inference engines until at least PyTorch FA is sorted. Current testing shows fwd/bwd pass to be in the &lt;strong&gt;~1 TFLOPS&lt;/strong&gt; ballpark (very bad)...&lt;/p&gt; &lt;p&gt;This testing obviously isn't very comprehensive, but since there's very little out there, I figure I'd at least share some of the results, especially with the various Chinese Strix Halo mini PCs beginning to ship and with Computex around the corner.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmnsol</id>
    <title>AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms</title>
    <updated>2025-05-14T19:14:49+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmnsol/alphaevolve_a_geminipowered_coding_agent_for/"&gt; &lt;img alt="AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms" src="https://preview.redd.it/pj1r83skrs0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7100be7b0bc1cceb9f30d390f47de6dcbfbabcec" title="AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Google announced AlphaEvolve, an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization. AlphaEvolve pairs the creative problem-solving capabilities of our Gemini models with automated evaluators that verify answers, and uses an evolutionary framework to improve upon the most promising ideas.&lt;/p&gt; &lt;p&gt;AlphaEvolve enhanced the efficiency of Google's data centers, chip design and AI training processes — including training the large language models underlying &lt;strong&gt;AlphaEvolve itself&lt;/strong&gt;. It has also helped design faster matrix multiplication algorithms and find new solutions to open mathematical problems, showing incredible promise for application across many areas.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/"&gt;https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf"&gt;https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pj1r83skrs0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmnsol/alphaevolve_a_geminipowered_coding_agent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmnsol/alphaevolve_a_geminipowered_coding_agent_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T19:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1knbo80</id>
    <title>HanaVerse - Chat with AI through an interactive anime character! 🌸</title>
    <updated>2025-05-15T15:52:31+00:00</updated>
    <author>
      <name>/u/OrganicTelevision652</name>
      <uri>https://old.reddit.com/user/OrganicTelevision652</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbo80/hanaverse_chat_with_ai_through_an_interactive/"&gt; &lt;img alt="HanaVerse - Chat with AI through an interactive anime character! 🌸" src="https://external-preview.redd.it/VMExyAyOE_4W1BYj5ZE65UYho8s1S8iYWLFddyI6R88.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a56e1472206dfd96091c85f5438f8e274f3dd61" title="HanaVerse - Chat with AI through an interactive anime character! 🌸" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on something I think you'll love - HanaVerse, an interactive web UI for Ollama that brings your AI conversations to life through a charming 2D anime character named Hana!&lt;/p&gt; &lt;p&gt;What is &lt;strong&gt;HanaVerse&lt;/strong&gt;? 🤔&lt;/p&gt; &lt;p&gt;HanaVerse transforms how you interact with Ollama's language models by adding a visual, animated companion to your conversations. Instead of just text on a screen, you chat with Hana - a responsive anime character who reacts to your interactions in real-time!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features that make HanaVerse special&lt;/strong&gt;: ✨&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Talks Back:&lt;/strong&gt; Answers with voice&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Streaming Responses:&lt;/strong&gt; See answers form in real-time as they're generated&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Markdown Support:&lt;/strong&gt; Beautiful formatting with syntax highlighting&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LaTeX Math Rendering:&lt;/strong&gt; Perfect for equations and scientific content&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Customizable:&lt;/strong&gt; Choose any Ollama model and configure system prompts&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Responsive Design:&lt;/strong&gt; Works on both desktop(preferred) and mobile&lt;/p&gt; &lt;p&gt;Why I built this 🛠️&lt;/p&gt; &lt;p&gt;I wanted to make AI interactions more engaging and personal while leveraging the power of self-hosted Ollama models. The result is an interface that makes AI conversations feel more natural and enjoyable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1knbo80/video/uczc6t9cwy0f1/player"&gt;Hanaverse demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're looking for a more engaging way to interact with your Ollama models, give HanaVerse a try and let me know what you think!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Ashish-Patnaik/HanaVerse"&gt;https://github.com/Ashish-Patnaik/HanaVerse&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Skeleton Demo = &lt;a href="https://hanaverse.vercel.app/"&gt;https://hanaverse.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love your feedback and contributions - stars ⭐ are always appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganicTelevision652"&gt; /u/OrganicTelevision652 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbo80/hanaverse_chat_with_ai_through_an_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbo80/hanaverse_chat_with_ai_through_an_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knbo80/hanaverse_chat_with_ai_through_an_interactive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T15:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn810l</id>
    <title>LLM based Personally identifiable information detection tool</title>
    <updated>2025-05-15T13:19:47+00:00</updated>
    <author>
      <name>/u/geeganage</name>
      <uri>https://old.reddit.com/user/geeganage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub repo: &lt;a href="https://github.com/rpgeeganage/pII-guard"&gt;https://github.com/rpgeeganage/pII-guard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;br /&gt; I recently built a small open-source tool called PII (personally identifiable information) to detect personally identifiable information (PII) in logs using AI. It’s self-hosted and designed for privacy-conscious developers or teams.&lt;/p&gt; &lt;p&gt;Features: - HTTP endpoint for log ingestion with buffered processing&lt;br /&gt; - PII detection using local AI models via Ollama (e.g., gemma:3b)&lt;br /&gt; - PostgreSQL + Elasticsearch for storage&lt;br /&gt; - Web UI to review flagged logs&lt;br /&gt; - Docker Compose for easy setup&lt;/p&gt; &lt;p&gt;It’s still a work in progress, and any suggestions or feedback would be appreciated. Thanks for checking it out!&lt;/p&gt; &lt;p&gt;My apologies if this post is not relevant to this group &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geeganage"&gt; /u/geeganage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn810l/llm_based_personally_identifiable_information/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn810l/llm_based_personally_identifiable_information/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn810l/llm_based_personally_identifiable_information/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T13:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2weg</id>
    <title>LLM for Translation locally</title>
    <updated>2025-05-15T08:12:32+00:00</updated>
    <author>
      <name>/u/yayita2500</name>
      <uri>https://old.reddit.com/user/yayita2500</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi ! I need to translate some texts..I have been doint Gcloud Trasnlate V3 and also Vertex, but the cost is absolutely high..I do have a 4070 with 12Gb. which model you suggest using Ollama to use a translator that support asian and western languages? &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yayita2500"&gt; /u/yayita2500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2weg/llm_for_translation_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2weg/llm_for_translation_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2weg/llm_for_translation_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T08:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn86oz</id>
    <title>Suggestion for TTS Models</title>
    <updated>2025-05-15T13:27:01+00:00</updated>
    <author>
      <name>/u/Heavy_Ad_4912</name>
      <uri>https://old.reddit.com/user/Heavy_Ad_4912</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m building a fun little custom speech-to-speech app. For speech-to-text, I’m using &lt;code&gt;parakeet-0.6B&lt;/code&gt; (latest on HuggingFace), and for the LLM part, I’m currently experimenting with &lt;code&gt;gemma3:4b&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now I’m looking for a suitable &lt;strong&gt;text-to-speech (TTS)&lt;/strong&gt; model from the open-source HuggingFace community. My main constraints are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Max model size:&lt;/strong&gt; 2–3 GB (due to 8GB VRAM and 32GB RAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual support:&lt;/strong&gt; Primarily &lt;strong&gt;English, Hindi, and French&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve looked into a few models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;kokoro-82M&lt;/strong&gt; – seems promising&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zonos&lt;/strong&gt; and &lt;strong&gt;Nari-labs/Dia&lt;/strong&gt; – both ~6GB, too heavy for my setup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cesame-1B&lt;/strong&gt; – tried it, but the performance was underwhelming&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given these constraints, which TTS models would you recommend? Bonus points for ones that work out-of-the-box or require minimal finetuning.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heavy_Ad_4912"&gt; /u/Heavy_Ad_4912 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn86oz/suggestion_for_tts_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn86oz/suggestion_for_tts_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn86oz/suggestion_for_tts_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T13:27:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn69mp</id>
    <title>How do SOTA LLMs Process PDFs: Native Understanding, OCR, or RAG?</title>
    <updated>2025-05-15T11:54:06+00:00</updated>
    <author>
      <name>/u/coconautico</name>
      <uri>https://old.reddit.com/user/coconautico</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"&gt; &lt;img alt="How do SOTA LLMs Process PDFs: Native Understanding, OCR, or RAG?" src="https://b.thumbs.redditmedia.com/rqzDSRVqRbCQDZ3SFaKzcnoeDqw5K2FyJ-Re4mj9eho.jpg" title="How do SOTA LLMs Process PDFs: Native Understanding, OCR, or RAG?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm trying to build a solution to &lt;strong&gt;analyze a set of PDF files&lt;/strong&gt; (5-10) using an LLM.&lt;/p&gt; &lt;p&gt;My current approach is to perform a &lt;strong&gt;high-quality OCR&lt;/strong&gt; (using Docling) and then, dump all this information as the &lt;strong&gt;context for my prompt&lt;/strong&gt;. However, I doubt this is the best strategy nowadays.&lt;/p&gt; &lt;p&gt;Playing around with Gemini, I've noticed it handles PDF files extremely well*, even showing the &lt;strong&gt;tokens it contains&lt;/strong&gt;. So I was wondering if the model is &amp;quot;&lt;strong&gt;reading&lt;/strong&gt;&amp;quot; the PDF file &lt;strong&gt;directly&lt;/strong&gt; (native vision), or is there a preliminary step where it converts the PDF to pure text using &lt;strong&gt;OCR before processing&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;I'm also wondering if a &lt;strong&gt;Retrieval Augmented Generation (RAG) strategy&lt;/strong&gt; is involved in how it interacts with the document content once uploaded.&lt;/p&gt; &lt;p&gt;If anyone knows more about this process, it would be interesting to hear.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;p&gt;*It was able to perfectly process a PDF of images with handwritten text and equations&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Additional information:&lt;/strong&gt;&lt;br /&gt; I've noticed that Gemini sometimes appends labels like `--- PAGE 1 ---`, `--- PAGE 2 ---`, etc., when processing PDFs. When I ask the model what tool it's using, it replies with something like “an internal tool to transcribe PDFs.” I've tried replicating the results using Google's public Vision APIs, but none of them produce the same output. So I assume they're using some internal system (maybe a custom-built tool) to reliably convert anything into plain text.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What seems to be happening under the hood&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As &lt;a href="/u/highergraphic"&gt;u/highergraphic&lt;/a&gt; suggested, I tried to pin down whether Gemini first turns each PDF page into an image and then processes natively using its multimodal capabilities on that rasterized page. Result? Every experiment seems to point to &amp;quot;yes.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Original PDF:&lt;/strong&gt; Mixed text, images, and tables. → Perfect extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flat image of the same page:&lt;/strong&gt; Exported the page as a single PNG/JPG. → Same perfect extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid PDF:&lt;/strong&gt; Re-created the page but replaced some paragraphs and tables with screenshots of themselves (same size). → Still perfect.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tiny-font PDF:&lt;/strong&gt; Shrunk the text until it was almost unreadable. → Worked until the characters were too small.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tiny-font PDF (from images):&lt;/strong&gt; Same experiement as the previous one, but this time, I shrunk the images of the text until it was almost unreadable. → Same. It worked until the characters were too small.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Gemini (and, I suspect, other modern multimodal LLMs) appears to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Rasterize&lt;/strong&gt; each PDF page into an image.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process it using the multimodal LLM&lt;/strong&gt; to produce plain text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repeat.\&lt;/strong&gt;*&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;*Each new image processing adds a markers like &lt;code&gt;--- PAGE X ---&lt;/code&gt; to help with the context.&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example of the PDF with textual parts of it replaced by images of the same size:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vjygb9rzly0f1.jpg?width=2479&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=26934052347e813429bd24acf12953167c945b7d"&gt;Example of the PDF page with text parts replaced by images of the same size&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coconautico"&gt; /u/coconautico &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T11:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmrfoo</id>
    <title>MLA optimization with flashattention for llama.cpp,MLA + FA now only uses K-cache - 47% saving on KV-cache size</title>
    <updated>2025-05-14T21:42:55+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13529"&gt;MLA + FA now only uses K-cache - 47% saving on KV-cache size (only for use with #13435 for now) by jukofyork · Pull Request #13529 · ggml-org/llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_kv_cache_unified: kv_size = 163840, type_k = 'f16', type_v = 'f16', n_layer = 61, can_shift = 0, padding = 256&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_kv_cache_unified: CUDA0 KV buffer size = 10980.00 MiB&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_kv_cache_unified: KV self size = 10980.00 MiB, K (f16): 10980.00 MiB, V (f16): 0.00 MiB&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The full context of 160k tokens now takes up less than 11GB without kquants&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T21:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmi6vl</id>
    <title>I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU.</title>
    <updated>2025-05-14T15:33:15+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"&gt; &lt;img alt="I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU." src="https://external-preview.redd.it/Z3l2NXpmczhucjBmMUwcvEt1gWTYtmZHqUwsIc9aRH3JKfTLJ5UHo4J1H4An.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61d172895901d0b35dab0f76eb10b4c4648b8f5c" title="I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/&lt;/a&gt;, I decided to update the llama.cpp server demo so that it runs 100% locally in-browser on WebGPU, using Transformers.js. This means you can simply visit the link and run the demo, without needing to install anything locally. &lt;/p&gt; &lt;p&gt;I hope you like it! &lt;a href="https://huggingface.co/spaces/webml-community/smolvlm-realtime-webgpu"&gt;https://huggingface.co/spaces/webml-community/smolvlm-realtime-webgpu&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PS: The source code is a single index.html file you can find in the &amp;quot;Files&amp;quot; section on the demo page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/or5b3ks8nr0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmyr7h</id>
    <title>Qwen3-235B-A22B not measuring up to DeepseekV3-0324</title>
    <updated>2025-05-15T03:45:07+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep trying to get it to behave, but q8 is not keeping up with my deepseekv3_q3_k_xl. what gives? am I doing something wrong or is it just all hype? it's a capable model and I'm sure for those that have not been able to run big models, this is a shock and great, but for those of us who have been able to run huge models, it's feel like a waste of bandwidth and time. it's not a disaster like llama-4 yet I'm having a hard time getting it into rotation of my models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmyr7h/qwen3235ba22b_not_measuring_up_to_deepseekv30324/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmyr7h/qwen3235ba22b_not_measuring_up_to_deepseekv30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmyr7h/qwen3235ba22b_not_measuring_up_to_deepseekv30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T03:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1knb2kq</id>
    <title>qSpeak - A Cross platform alternative for WisprFlow supporting local LLMs and Linux</title>
    <updated>2025-05-15T15:28:13+00:00</updated>
    <author>
      <name>/u/fajfas3</name>
      <uri>https://old.reddit.com/user/fajfas3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, together with my colleagues, we've created &lt;a href="http://qSpeak.app"&gt;qSpeak.app&lt;/a&gt; 🎉 &lt;/p&gt; &lt;p&gt;qSpeak is an alternative to tools like SuperWhisper or WisprFlow but works on all platforms including Linux. 🚀&lt;/p&gt; &lt;p&gt;Also we're working on integrating LLMs more deeply into it to include more sophisticated interactions like multi step conversations (essentially assistants) and in the near future MCP integration. &lt;/p&gt; &lt;p&gt;The app is currently completely free so please try it out! 🎁&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fajfas3"&gt; /u/fajfas3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qspeak.app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knb2kq/qspeak_a_cross_platform_alternative_for_wisprflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knb2kq/qspeak_a_cross_platform_alternative_for_wisprflow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T15:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmlu2y</id>
    <title>Qwen3-30B-A6B-16-Extreme is fantastic</title>
    <updated>2025-05-14T17:57:00+00:00</updated>
    <author>
      <name>/u/DocWolle</name>
      <uri>https://old.reddit.com/user/DocWolle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen3-30B-A6B-16-Extreme"&gt;https://huggingface.co/DavidAU/Qwen3-30B-A6B-16-Extreme&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quants:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A6B-16-Extreme-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-30B-A6B-16-Extreme-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Someone recently mentioned this model here on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; and I gave it a try. For me it is the best model I can run locally with my 36GB CPU only setup. In my view it is a lot smarter than the original A3B model. &lt;/p&gt; &lt;p&gt;It uses 16 experts instead of 8 and when watching it thinking I can see that it thinks a step further/deeper than the original model. Speed is still great. &lt;/p&gt; &lt;p&gt;I wonder if anyone else has tried it. A 128k context version is also available.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocWolle"&gt; /u/DocWolle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmlu2y/qwen330ba6b16extreme_is_fantastic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmlu2y/qwen330ba6b16extreme_is_fantastic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmlu2y/qwen330ba6b16extreme_is_fantastic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T17:57:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn6mic</id>
    <title>Qwen 2.5 vs Qwen 3 vs Gemma 3: Real world base model comparison?</title>
    <updated>2025-05-15T12:12:37+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6mic/qwen_25_vs_qwen_3_vs_gemma_3_real_world_base/"&gt; &lt;img alt="Qwen 2.5 vs Qwen 3 vs Gemma 3: Real world base model comparison?" src="https://preview.redd.it/kq34jkwvsx0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26b2276e1df77e5648f6b562bf60fe6c8a922ea4" title="Qwen 2.5 vs Qwen 3 vs Gemma 3: Real world base model comparison?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been digging into the latest base models and wanted to get some practical opinions beyond just benchmark numbers.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;For those who have actually used both Qwen 2.5 and Qwen 3 base models&lt;/strong&gt;: Did you notice a truly big jump in general usage (reasoning, instruction following, robustness), or is the improvement mostly confined to coding and math tasks? I’m not talking about fine-tuned chat versions, just the raw base models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma 3 vs Qwen&lt;/strong&gt;: Is Gemma 3 genuinely that far behind, or is there some possible benchmark leakage or overfitting with Qwen? A few benchmark charts make me suspicious. Would love to hear hands-on perspectives if anyone has experimented with both.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why I’m asking:&lt;/strong&gt;&lt;br /&gt; I want to build a highly &lt;em&gt;steerable&lt;/em&gt; model for my research and product work. I only have budget for one serious base model to work from, so I want to select the absolute best starting point. I’m focusing on openness, quality, and steerability, not just raw benchmark wins.&lt;/p&gt; &lt;p&gt;Any honest feedback, experiments, or even failures you’ve had with these models would help me massively. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kq34jkwvsx0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6mic/qwen_25_vs_qwen_3_vs_gemma_3_real_world_base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6mic/qwen_25_vs_qwen_3_vs_gemma_3_real_world_base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T12:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2gsa</id>
    <title>Is neural engine on mac a wasted opportunity?</title>
    <updated>2025-05-15T07:41:38+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the point of having a 32-core neural engine on the new mac studio if you can’t use it for LLM or image/video generation tasks ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2gsa/is_neural_engine_on_mac_a_wasted_opportunity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2gsa/is_neural_engine_on_mac_a_wasted_opportunity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2gsa/is_neural_engine_on_mac_a_wasted_opportunity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:41:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1knbdd3</id>
    <title>Hugging Face free and open source MCP course</title>
    <updated>2025-05-15T15:40:16+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're thrilled to announce the launch of our comprehensive Model Context Protocol (MCP) Course! This free program is designed to take learners from foundational understanding to practical application of MCP in AI.&lt;/p&gt; &lt;p&gt;Join the course on the hub:&lt;a href="https://huggingface.co/mcp-course"&gt;https://huggingface.co/mcp-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this course, you will: 📖 Study Model Context Protocol in theory, design, and practice. 🧑‍💻 Learn to use established MCP SDKs and frameworks. 💾 Share your projects and explore applications created by the community. 🏆 Participate in challenges and evaluate your MCP implementations. 🎓 Earn a certificate of completion.&lt;/p&gt; &lt;p&gt;At the end, you'll understand how MCP works and how to build your own AI applications that leverage external data and tools using the latest MCP standards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbdd3/hugging_face_free_and_open_source_mcp_course/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbdd3/hugging_face_free_and_open_source_mcp_course/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knbdd3/hugging_face_free_and_open_source_mcp_course/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T15:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1knca48</id>
    <title>Quick Qwen3-30B-A6B-16-Extreme vs Qwen3-30B A3B Benchmark</title>
    <updated>2025-05-15T16:17:15+00:00</updated>
    <author>
      <name>/u/terhechte</name>
      <uri>https://old.reddit.com/user/terhechte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I have a Benchmark suite of 110 tasks across multiple programming languages. The focus really is on more complex problems and not Javascript one-shot problems. I was interested in comparing the above two models. &lt;/p&gt; &lt;p&gt;Setup&lt;/p&gt; &lt;p&gt;- Qwen3-30B-A6B-16-Extreme Q4_K_M running in LMStudio&lt;br /&gt; - Qwen3-30B A3B on OpenRouter&lt;/p&gt; &lt;p&gt;I understand that this is not a fair fight because the A6B is heavily quantized, but running this benchmark on my Macbook takes almost 12 hours with reasoning models, so a better comparison will take a bit longer. &lt;/p&gt; &lt;p&gt;Here are the results:&lt;/p&gt; &lt;p&gt;| lmstudio/qwen3-30b-a6b-16-extreme | correct: 56 | wrong: 54 |&lt;/p&gt; &lt;p&gt;| openrouter/qwen/qwen3-30b-a3b | correct: 68 | wrong: 42 |&lt;/p&gt; &lt;p&gt;I will try to report back in a couple of days with more comparisons. &lt;/p&gt; &lt;p&gt;You can learn more about the benchmark here (&lt;a href="https://ben.terhech.de/posts/2025-01-31-llms-vs-programming-languages.html"&gt;https://ben.terhech.de/posts/2025-01-31-llms-vs-programming-languages.html&lt;/a&gt;) but I've since also added support for more models and languages. However I haven't really released the results in some time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terhechte"&gt; /u/terhechte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knca48/quick_qwen330ba6b16extreme_vs_qwen330b_a3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knca48/quick_qwen330ba6b16extreme_vs_qwen330b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knca48/quick_qwen330ba6b16extreme_vs_qwen330b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T16:17:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn6427</id>
    <title>Llamafile 0.9.3 Brings Support For Qwen3 &amp; Phi4</title>
    <updated>2025-05-15T11:45:30+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6427/llamafile_093_brings_support_for_qwen3_phi4/"&gt; &lt;img alt="Llamafile 0.9.3 Brings Support For Qwen3 &amp;amp; Phi4" src="https://external-preview.redd.it/Cj4HZCrFxF1ZWikVE2EGwsOPpKF5ST6n_sC3VWnurnI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ee9e6c0ea1c9f1b1be02252a698b00e32a60cbe" title="Llamafile 0.9.3 Brings Support For Qwen3 &amp;amp; Phi4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Llamafile-0.9.3-Released"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6427/llamafile_093_brings_support_for_qwen3_phi4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6427/llamafile_093_brings_support_for_qwen3_phi4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T11:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn94oi</id>
    <title>Update: We fit 50+ LLMs on 2 GPUs — and now we’re inviting you to try it.</title>
    <updated>2025-05-15T14:08:22+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week’s post on cold starts and snapshotting hit a nerve. Turns out many of you are also trying to juggle multiple models, deal with bloated memory, or squeeze more out of a single GPU.&lt;/p&gt; &lt;p&gt;We’re making our snapshot-based runtime available to a limited number of builders — especially if you’re running agents, RAG pipelines, or multi-model workloads locally.&lt;/p&gt; &lt;p&gt;It’s still early, and we’re limited in support, but the tech is real:&lt;/p&gt; &lt;p&gt;• 50+ models on 2× A4000s • Cold starts under 2s • 90%+ GPU utilization • No bloating, no prewarming&lt;/p&gt; &lt;p&gt;If you’re experimenting with multiple models and want to deploy more on fewer GPUs, this might help.&lt;/p&gt; &lt;p&gt;We’d love your feedback . reach out and we’ll get you access.&lt;/p&gt; &lt;p&gt;Please feel free to ask any questions &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn94oi/update_we_fit_50_llms_on_2_gpus_and_now_were/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn94oi/update_we_fit_50_llms_on_2_gpus_and_now_were/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn94oi/update_we_fit_50_llms_on_2_gpus_and_now_were/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T14:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn9882</id>
    <title>LLaDA-8B-Tools: A diffusion language model fine-tuned for tool use</title>
    <updated>2025-05-15T14:12:37+00:00</updated>
    <author>
      <name>/u/ProximileLLC</name>
      <uri>https://old.reddit.com/user/ProximileLLC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Instead of generating token-by-token, this architecture refines the whole output by replacing mask tokens across the sequence.&lt;/p&gt; &lt;p&gt;The bidirectional attention seems to help with structured outputs, though this is just a rough first attempt with some issues (e.g. extra text after a message, because of this architecture's preset generation length).&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Proximile/LLaDA-8B-Tools"&gt;https://huggingface.co/Proximile/LLaDA-8B-Tools&lt;/a&gt;&lt;br /&gt; Dataset: &lt;a href="https://huggingface.co/datasets/Proximile/LLaDA-8B-Tools"&gt;https://huggingface.co/datasets/Proximile/LLaDA-8B-Tools&lt;/a&gt;&lt;br /&gt; Format mostly follows Llama 3.1: &lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/"&gt;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on a variant tuned for more general tool use using a range of i/o formats.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProximileLLC"&gt; /u/ProximileLLC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn9882/llada8btools_a_diffusion_language_model_finetuned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn9882/llada8btools_a_diffusion_language_model_finetuned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn9882/llada8btools_a_diffusion_language_model_finetuned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T14:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2aay</id>
    <title>Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures</title>
    <updated>2025-05-15T07:28:36+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"&gt; &lt;img alt="Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures" src="https://preview.redd.it/ww4aygc1ew0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18baa07396402b906dd387ccabc4f5bab873fba3" title="Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.09343"&gt;https://arxiv.org/abs/2505.09343&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww4aygc1ew0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:28:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kna53n</id>
    <title>Qwen3-32B hallucinates more than QwQ-32B</title>
    <updated>2025-05-15T14:50:45+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"&gt; &lt;img alt="Qwen3-32B hallucinates more than QwQ-32B" src="https://b.thumbs.redditmedia.com/BKpqaFBECcC520jrmC7_8NwZgTpcy4cdtN47rrRQrVU.jpg" title="Qwen3-32B hallucinates more than QwQ-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been seeing some people complaining about Qwen3's hallucination issues. Personally, I have never run into such issue, but I recently came across some Chinese benchmarks of Qwen3 and QwQ, so I might as well share them here.&lt;/p&gt; &lt;p&gt;I translated these to English; the sources are in the images.&lt;/p&gt; &lt;p&gt;TLDR:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3-32B has a lower SimpleQA score than QwQ (5.87% vs 8.07%)&lt;/li&gt; &lt;li&gt;Qwen3-32B has a higher hallucination rate than QwQ in reasoning mode (30.15% vs 22.7%)&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;SuperCLUE-Faith is designed to evaluate Chinese language performance, so it obviously gives Chinese models an advantage over American ones, but should be useful for comparing Qwen models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nrjfzhl2ky0f1.jpg?width=3388&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c2021c8da8fb21fc46cefb8539130e97ce20dee"&gt;https://preview.redd.it/nrjfzhl2ky0f1.jpg?width=3388&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c2021c8da8fb21fc46cefb8539130e97ce20dee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5rh9qe4cky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=218051f6ddbc88ff99a584ed0c2877f7e97f8132"&gt;https://preview.redd.it/5rh9qe4cky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=218051f6ddbc88ff99a584ed0c2877f7e97f8132&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jwi0mphyky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=57dbad3cead06c339f4cabf16f39bb211925aa22"&gt;https://preview.redd.it/jwi0mphyky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=57dbad3cead06c339f4cabf16f39bb211925aa22&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7gy8ebvyky0f1.jpg?width=2156&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=30da9915523db714b599bf88b1925d85a40f545f"&gt;https://preview.redd.it/7gy8ebvyky0f1.jpg?width=2156&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=30da9915523db714b599bf88b1925d85a40f545f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T14:50:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn8m8t</id>
    <title>Open-source general purpose agent with built-in MCPToolkit support</title>
    <updated>2025-05-15T13:46:21+00:00</updated>
    <author>
      <name>/u/Fluffy_Sheepherder76</name>
      <uri>https://old.reddit.com/user/Fluffy_Sheepherder76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn8m8t/opensource_general_purpose_agent_with_builtin/"&gt; &lt;img alt="Open-source general purpose agent with built-in MCPToolkit support" src="https://preview.redd.it/h6y4hb7s9y0f1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee3d023b25d2e2f99165aa457441e34896b8d16c" title="Open-source general purpose agent with built-in MCPToolkit support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The open-source OWL agent now comes with built-in MCPToolkit support, just drop in your MCP servers (Playwright, desktop-commander, custom Python tools, etc.) and OWL will automatically discover and call them in its multi-agent workflows.&lt;/p&gt; &lt;p&gt;OWL: &lt;a href="https://github.com/camel-ai/owl"&gt;https://github.com/camel-ai/owl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Sheepherder76"&gt; /u/Fluffy_Sheepherder76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h6y4hb7s9y0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn8m8t/opensource_general_purpose_agent_with_builtin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn8m8t/opensource_general_purpose_agent_with_builtin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T13:46:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn75q8</id>
    <title>PDF input merged into llama.cpp</title>
    <updated>2025-05-15T12:39:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13562"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75q8/pdf_input_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75q8/pdf_input_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T12:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2mv9</id>
    <title>LLMs Get Lost In Multi-Turn Conversation</title>
    <updated>2025-05-15T07:53:58+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt; &lt;img alt="LLMs Get Lost In Multi-Turn Conversation" src="https://b.thumbs.redditmedia.com/MIMwMQ4O4HnoFjzXbBTjShTxVfai2B_u3_lcuHpfKVk.jpg" title="LLMs Get Lost In Multi-Turn Conversation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://arxiv.org/abs/2505.06120"&gt;paper&lt;/a&gt; found that the performance of open and closed LLMs drops significantly in multi-turn conversations. Most benchmarks focus on single-turn, fully-specified instruction settings. They found that LLMs often make (incorrect) assumptions in early turns, on which they rely going forward and never recover from.&lt;/p&gt; &lt;p&gt;They concluded that when a multi-turn conversation doesn't yield the desired results, it might help to restart with a fresh conversation, putting all the relevant information from the multi-turn conversation into the first turn.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ltlt4zbiiw0f1.png?width=1515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4de01b7a2339658690b3492899e107bd4af9836"&gt;https://preview.redd.it/ltlt4zbiiw0f1.png?width=1515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4de01b7a2339658690b3492899e107bd4af9836&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Sharded&amp;quot; means they split an original fully-specified single-turn instruction into multiple tidbits of information that they then fed the LLM turn by turn. &amp;quot;Concat&amp;quot; is a comparison as a baseline where they fed all the generated information pieces in the same turn. Here are examples on how they did the splitting:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y40aremjiw0f1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe81a4a2be778437bf7134933863ebbd88e5ef2"&gt;https://preview.redd.it/y40aremjiw0f1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe81a4a2be778437bf7134933863ebbd88e5ef2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:53:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn542r</id>
    <title>Introducing A.I.T.E Ball</title>
    <updated>2025-05-15T10:45:28+00:00</updated>
    <author>
      <name>/u/tonywestonuk</name>
      <uri>https://old.reddit.com/user/tonywestonuk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"&gt; &lt;img alt="Introducing A.I.T.E Ball" src="https://external-preview.redd.it/NXllMTcxNDFkeDBmMcTQf63cMAAIN-71fn86oCbnKUR2tA_D5RmS947R5l7-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf3613bb545a67e6ba0ae442a8d9fddc761c89a7" title="Introducing A.I.T.E Ball" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a totally self contained (no internet) AI powered 8ball.&lt;/p&gt; &lt;p&gt;Its running on an Orange pi zero 2w, with whisper.cpp to do the text-2-speach, and llama.cpp to do the llm thing, Its running Gemma 3 1b. About as much as I can do on this hardware. But even so.... :-) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonywestonuk"&gt; /u/tonywestonuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/scyofz31dx0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T10:45:28+00:00</published>
  </entry>
</feed>
