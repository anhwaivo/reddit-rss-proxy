<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-25T16:54:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m8ozb0</id>
    <title>China's Bytedance releases Seed LiveInterpret simultaneous interpretation model</title>
    <updated>2025-07-25T03:43:35+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://seed.bytedance.com/en/seed_liveinterpret"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ozb0/chinas_bytedance_releases_seed_liveinterpret/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T03:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m83644</id>
    <title>China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp; Slightly Behind the RTX 5060 in New Benchmarks</title>
    <updated>2025-07-24T12:33:36+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"&gt; &lt;img alt="China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp;amp; Slightly Behind the RTX 5060 in New Benchmarks" src="https://external-preview.redd.it/lkz-MfcF29exvP3pe2apdSH9SVJIH63YmzcEuEfzEgU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6aa69848c81b950052de8eb2024c390e13024272" title="China’s First High-End Gaming GPU, the Lisuan G100, Reportedly Outperforms NVIDIA’s GeForce RTX 4060 &amp;amp; Slightly Behind the RTX 5060 in New Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/china-first-high-end-gaming-gpu-lisuan-g100-outperforms-nvidia-geforce-rtx-4060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m83644/chinas_first_highend_gaming_gpu_the_lisuan_g100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T12:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8wi62</id>
    <title>Do models make fun of other models?</title>
    <updated>2025-07-25T11:20:47+00:00</updated>
    <author>
      <name>/u/Fussy-Fur3608</name>
      <uri>https://old.reddit.com/user/Fussy-Fur3608</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8wi62/do_models_make_fun_of_other_models/"&gt; &lt;img alt="Do models make fun of other models?" src="https://preview.redd.it/8sdpzbq280ff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb549a0f9db72b902ee2c8fba005b948a7894058" title="Do models make fun of other models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was just chatting with Claude about my experiments with Aider and qwen2.5-coder (7b &amp;amp; 14b).&lt;/p&gt; &lt;p&gt;i wasn't ready for Claudes response. so good.&lt;/p&gt; &lt;p&gt;FWIW i'm trying codellama:13b next.&lt;/p&gt; &lt;p&gt;Any advice for a local coding model and Aider on RTX3080 10GB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fussy-Fur3608"&gt; /u/Fussy-Fur3608 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8sdpzbq280ff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8wi62/do_models_make_fun_of_other_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8wi62/do_models_make_fun_of_other_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T11:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8dln1</id>
    <title>Qwen 3 Thinking is coming very soon</title>
    <updated>2025-07-24T19:19:39+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/"&gt; &lt;img alt="Qwen 3 Thinking is coming very soon" src="https://preview.redd.it/61i8pt44hvef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19b99a58da488472ec93d5842e37998def1cbe76" title="Qwen 3 Thinking is coming very soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/61i8pt44hvef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dln1/qwen_3_thinking_is_coming_very_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T19:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8tmhd</id>
    <title>ByteDance Seed Prover Achieves Silver Medal Score in IMO 2025</title>
    <updated>2025-07-25T08:20:10+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8tmhd/bytedance_seed_prover_achieves_silver_medal_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8tmhd/bytedance_seed_prover_achieves_silver_medal_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T08:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8wg2r</id>
    <title>Open Source Companion Thread</title>
    <updated>2025-07-25T11:17:38+00:00</updated>
    <author>
      <name>/u/aratahikaru5</name>
      <uri>https://old.reddit.com/user/aratahikaru5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm about to start building my personal AI companion and during my research came across this &lt;a href="https://github.com/LongHZ140516/Awesome-GrokAni-VituralMate"&gt;awesome list&lt;/a&gt; of AI companion projects that I wanted to share with the community.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Companion&lt;/th&gt; &lt;th&gt;Lang&lt;/th&gt; &lt;th&gt;License&lt;/th&gt; &lt;th&gt;Stack&lt;/th&gt; &lt;th&gt;Category&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/swordswind/ai_virtual_mate_web"&gt;枫云AI虚拟伙伴Web版&lt;/a&gt; - &lt;a href="https://deepwiki.com/swordswind/ai_virtual_mate_web"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;gpl-3.0&lt;/td&gt; &lt;td&gt;python&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/Moemu/Muice-Chatbot"&gt;Muice-Chatbot&lt;/a&gt; - &lt;a href="https://deepwiki.com/Moemu/Muice-Chatbot"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh, en&lt;/td&gt; &lt;td&gt;mit&lt;/td&gt; &lt;td&gt;python&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/Moemu/MuiceBot"&gt;MuiceBot&lt;/a&gt; - &lt;a href="https://deepwiki.com/Moemu/MuiceBot"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;bsd-3-clause&lt;/td&gt; &lt;td&gt;python&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/lss233/kirara-ai"&gt;kirara-ai&lt;/a&gt; - &lt;a href="https://deepwiki.com/lss233/kirara-ai"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;agpl-3.0&lt;/td&gt; &lt;td&gt;python&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/morettt/my-neuro"&gt;my-neuro&lt;/a&gt; - &lt;a href="https://deepwiki.com/morettt/my-neuro"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh, en&lt;/td&gt; &lt;td&gt;mit&lt;/td&gt; &lt;td&gt;python&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/uezo/aiavatarkit"&gt;AIAvatarKit&lt;/a&gt; - &lt;a href="https://deepwiki.com/uezo/aiavatarkit"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;en&lt;/td&gt; &lt;td&gt;apache-2.0&lt;/td&gt; &lt;td&gt;python&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/lijiaxing1997/xinghe-AI"&gt;xinghe-AI&lt;/a&gt; - &lt;a href="https://deepwiki.com/lijiaxing1997/xinghe-AI"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;python&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/MaiM-with-u/MaiBot"&gt;MaiBot&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;gpl-3.0&lt;/td&gt; &lt;td&gt;python&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/worm128/AI-YinMei"&gt;AI-YinMei&lt;/a&gt; - &lt;a href="https://deepwiki.com/worm128/AI-YinMei"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;bsd-2-clause&lt;/td&gt; &lt;td&gt;python, web&lt;/td&gt; &lt;td&gt;vtuber&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/Open-LLM-VTuber/Open-LLM-VTuber"&gt;Open-LLM-VTuber&lt;/a&gt; - &lt;a href="https://deepwiki.com/Open-LLM-VTuber/Open-LLM-VTuber"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;en&lt;/td&gt; &lt;td&gt;mit&lt;/td&gt; &lt;td&gt;python, web&lt;/td&gt; &lt;td&gt;vtuber, companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/KouriChat/KouriChat"&gt;KouriChat&lt;/a&gt; - &lt;a href="https://deepwiki.com/KouriChat/KouriChat"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;custom&lt;/td&gt; &lt;td&gt;python, web&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/PeterH0323/Streamer-Sales"&gt;Streamer-Sales&lt;/a&gt; - &lt;a href="https://deepwiki.com/PeterH0323/Streamer-Sales"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;agpl-3.0&lt;/td&gt; &lt;td&gt;python, web&lt;/td&gt; &lt;td&gt;vtuber, professional&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/Ikaros-521/AI-Vtuber"&gt;AI-Vtuber&lt;/a&gt; - &lt;a href="https://deepwiki.com/Ikaros-521/AI-Vtuber"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;gpl-3.0&lt;/td&gt; &lt;td&gt;python, web&lt;/td&gt; &lt;td&gt;vtuber&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/SillyTavern/SillyTavern"&gt;SillyTavern&lt;/a&gt; - &lt;a href="https://deepwiki.com/SillyTavern/SillyTavern"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;en&lt;/td&gt; &lt;td&gt;agpl-3.0&lt;/td&gt; &lt;td&gt;web&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/lobehub/lobe-vidol"&gt;lobe-vidol&lt;/a&gt; - &lt;a href="https://deepwiki.com/lobehub/lobe-vidol"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;en&lt;/td&gt; &lt;td&gt;apache-2.0&lt;/td&gt; &lt;td&gt;web&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/Jackywine/Bella"&gt;Bella&lt;/a&gt; - &lt;a href="https://deepwiki.com/Jackywine/Bella"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;mit&lt;/td&gt; &lt;td&gt;web&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/tegnike/aituber-kit"&gt;AITuberKit&lt;/a&gt; - &lt;a href="https://deepwiki.com/tegnike/aituber-kit"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;en, ja&lt;/td&gt; &lt;td&gt;custom&lt;/td&gt; &lt;td&gt;web&lt;/td&gt; &lt;td&gt;vtuber, companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/moeru-ai/airi"&gt;airi&lt;/a&gt; - &lt;a href="https://deepwiki.com/moeru-ai/airi"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;en&lt;/td&gt; &lt;td&gt;mit&lt;/td&gt; &lt;td&gt;tauri&lt;/td&gt; &lt;td&gt;vtuber, companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/semperai/amica"&gt;amica&lt;/a&gt; - &lt;a href="https://deepwiki.com/semperai/amica"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;en&lt;/td&gt; &lt;td&gt;mit&lt;/td&gt; &lt;td&gt;tauri&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/uezo/ChatdollKit"&gt;ChatdollKit&lt;/a&gt; - &lt;a href="https://deepwiki.com/uezo/ChatdollKit"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;en, ja&lt;/td&gt; &lt;td&gt;apache-2.0&lt;/td&gt; &lt;td&gt;unity&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/zhangliwei7758/unity-AI-Chat-Toolkit"&gt;Unity-AI-Chat-Toolkit&lt;/a&gt; - &lt;a href="https://deepwiki.com/zhangliwei7758/unity-AI-Chat-Toolkit"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh&lt;/td&gt; &lt;td&gt;mit&lt;/td&gt; &lt;td&gt;unity&lt;/td&gt; &lt;td&gt;companion&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/Zao-chen/ZcChat"&gt;ZcChat&lt;/a&gt; - &lt;a href="https://deepwiki.com/Zao-chen/ZcChat"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;zh, en&lt;/td&gt; &lt;td&gt;gpl-3.0&lt;/td&gt; &lt;td&gt;c++&lt;/td&gt; &lt;td&gt;galge&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://github.com/fagenorn/handcrafted-persona-engine"&gt;handcrafted-persona-engine&lt;/a&gt; - &lt;a href="https://deepwiki.com/fagenorn/handcrafted-persona-engine"&gt;Wiki&lt;/a&gt;&lt;/td&gt; &lt;td&gt;en&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;dotnet&lt;/td&gt; &lt;td&gt;vtuber, companion&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I've made some edits, such as adding license info (since I might copy the code) and organizing the list into categories for easier navigation.&lt;/li&gt; &lt;li&gt;Not all of these are dedicated companion apps (e.g. SillyTavern), but they can be adapted with some tweaking&lt;/li&gt; &lt;li&gt;Several projects only have Chinese READMEs (marked as zh), but I've included DeepWiki links to help with understanding. There's been significant progress in that community so I think it's worth exploring.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm starting this thread for two reasons: First, I'd love to hear about your favorite AI companion apps or setups that go beyond basic prompting. For me, a true companion needs a name, avatar, personality, backstory, conversational ability, and most importantly, memory. Second, I'm particularly interested in seeing what alternatives to Grok's Ani this community will build in the future.&lt;/p&gt; &lt;p&gt;If I've missed anything, please let me know and I'll update the list.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aratahikaru5"&gt; /u/aratahikaru5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8wg2r/open_source_companion_thread/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T11:17:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8qj9w</id>
    <title>Why I Forked Qwen Code</title>
    <updated>2025-07-25T05:07:40+00:00</updated>
    <author>
      <name>/u/ryanwang4thepeople</name>
      <uri>https://old.reddit.com/user/ryanwang4thepeople</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First of all, I loved the experience using Qwen Code with Qwen-3-Coder, but I can't stomach the cost of Qwen-3-Coder. While yes, you can use any OpenAI-compatible model out of the box, it's not without limitations.&lt;/p&gt; &lt;p&gt;That’s why I forked Qwen CLI Coder (itself derived from Gemini CLI) to create &lt;a href="https://github.com/wren-coder/wren-coder-cli"&gt;&lt;strong&gt;Wren Coder CLI&lt;/strong&gt;&lt;/a&gt;: an open-source, model-agnostic AI agent for coding assistance and terminal workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Fork?&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Big players like Google/Qwen have little incentive to support other models. Wren will be fully model-agnostic by design.&lt;/li&gt; &lt;li&gt;I’m splitting the project into a CLI + SDK (like Claude Code) to enable deeper agent customization.&lt;/li&gt; &lt;li&gt;My priorities as a solo developer probably don't align with respective model companies.&lt;/li&gt; &lt;li&gt;Why not? I just want to experiment and try new things.&lt;/li&gt; &lt;li&gt;I have a lot of time on my hands before I join a new role and want to spend the next month or so heads down building something I will love and use every day.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;What am I shipping?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Over the next few weeks, I plan to focus on the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Improving compatibility with a wide range of models&lt;/li&gt; &lt;li&gt;Adding chunking/compression logic to fix token limit errors with models with smaller context windows *cough* deepseek.&lt;/li&gt; &lt;li&gt;Splitting up the CLI and SDK&lt;/li&gt; &lt;li&gt;Documentation&lt;/li&gt; &lt;li&gt;Multi-model support????&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Maybe this is overly ambitious, but again why not? I'll keep y'all posted! Wish me luck!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/wren-coder/wren-coder-cli"&gt;https://github.com/wren-coder/wren-coder-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ryanwang4thepeople"&gt; /u/ryanwang4thepeople &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8qj9w/why_i_forked_qwen_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T05:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m91mt6</id>
    <title>🚀 Built a Multi-Agent System in 6 Hours That Solves 5/6 IMO 2025 Math Problems - Inspired by Recent Research Breakthroughs</title>
    <updated>2025-07-25T15:06:25+00:00</updated>
    <author>
      <name>/u/Vivid_Might1225</name>
      <uri>https://old.reddit.com/user/Vivid_Might1225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey~&lt;/p&gt; &lt;p&gt;Exciting news in the AI reasoning space! Using AWorld, we just built a Multi-Agent System (MAS) in 6 hours that successfully solved 5 out of 6 IMO 2025 math problems! 🎯&lt;/p&gt; &lt;h1&gt;Research Context:&lt;/h1&gt; &lt;p&gt;This work was inspired by the recent breakthrough paper &amp;quot;Gemini 2.5 Pro Capable of Winning Gold at IMO 2025&amp;quot; (Huang &amp;amp; Yang, 2025). The authors noted that &amp;quot;a multi-agent system where the strengths of different solutions can be combined would lead to stronger mathematical capability.&amp;quot;&lt;/p&gt; &lt;h1&gt;Our Innovation:&lt;/h1&gt; &lt;p&gt;We took this insight and implemented a collective intelligence approach using our AWorld multi-agent framework, proving that properly orchestrated multi-agent systems can indeed surpass single-model performance.&lt;/p&gt; &lt;h1&gt;Key Achievements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;5/6 IMO 2025 problems solved in just 6 hours of development&lt;/li&gt; &lt;li&gt;Collective Intelligence &amp;gt; Single Models: Our results validate the paper's hypothesis about multi-agent superiority&lt;/li&gt; &lt;li&gt;Rapid Prototyping: AWorld framework enabled quick construction of sophisticated reasoning systems&lt;/li&gt; &lt;li&gt;Context Engineering: Demonstrated the critical importance of agent interaction design under current LLM capabilities&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Reproducible Results:&lt;/h1&gt; &lt;p&gt;GitHub Repository: &lt;a href="https://github.com/inclusionAI/AWorld"&gt;https://github.com/inclusionAI/AWorld&lt;/a&gt;&lt;/p&gt; &lt;p&gt;IMO Implementation: examples/imo/ - Complete with setup scripts, environment configuration, and detailed documentation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vivid_Might1225"&gt; /u/Vivid_Might1225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m91mt6/built_a_multiagent_system_in_6_hours_that_solves/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m91mt6/built_a_multiagent_system_in_6_hours_that_solves/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m91mt6/built_a_multiagent_system_in_6_hours_that_solves/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T15:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8dgfu</id>
    <title>Qwen3-235B-A22B-Thinking-2507 is about to be released</title>
    <updated>2025-07-24T19:14:14+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/"&gt; &lt;img alt="Qwen3-235B-A22B-Thinking-2507 is about to be released" src="https://preview.redd.it/6l84nwc3gvef1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab0e139a7c20c4938872504feeddbf3c6b23197f" title="Qwen3-235B-A22B-Thinking-2507 is about to be released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6l84nwc3gvef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8dgfu/qwen3235ba22bthinking2507_is_about_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T19:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vu80</id>
    <title>N + N size GPU != 2N sized GPU, go big if you can</title>
    <updated>2025-07-25T10:43:20+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Buy the largest GPU that you can really afford to. Besides the obvious cost of additional electricity, PCI slots, physical space, cooling etc. Multiple GPUs can be annoying.&lt;/p&gt; &lt;p&gt;For example, I have some 16gb GPUs, 10 of them when trying to run Kimi, each layer is 7gb. If I load 2 layers on each GPU, the most context I can put on them is roughly 4k, since one of the layer is odd and ends up taking up 14.7gb. &lt;/p&gt; &lt;p&gt;So to get more context, 10k, I end up putting 1 layer 7gb on each of them, leaving 9gb free or 90gb of vram free.&lt;/p&gt; &lt;p&gt;If I had 5 32gb GPUs, at that 7gb, I would be able to place 4 layers ~ 28gb and still have about 3-4gb each free, which will allow me to have my 10k context. More context with same sized GPU, and it would be faster too!&lt;/p&gt; &lt;p&gt;Go as big as you can!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vu80/n_n_size_gpu_2n_sized_gpu_go_big_if_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m88jdh</id>
    <title>Ok next big open source model also from China only ! Which is about to release</title>
    <updated>2025-07-24T16:08:57+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/"&gt; &lt;img alt="Ok next big open source model also from China only ! Which is about to release" src="https://preview.redd.it/j6rwug34juef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a04ad517c7ca8eeeb00ee48288d8f17c562ca63c" title="Ok next big open source model also from China only ! Which is about to release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19"&gt;https://x.com/casper_hansen_/status/1948402352320360811?t=sPHOGEKIcaucRVzENlIr1g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j6rwug34juef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m88jdh/ok_next_big_open_source_model_also_from_china/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-24T16:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8l648</id>
    <title>Executive Order: "Preventing Woke AI in the Federal Government"</title>
    <updated>2025-07-25T00:36:06+00:00</updated>
    <author>
      <name>/u/NunyaBuzor</name>
      <uri>https://old.reddit.com/user/NunyaBuzor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/"&gt; &lt;img alt="Executive Order: &amp;quot;Preventing Woke AI in the Federal Government&amp;quot;" src="https://external-preview.redd.it/4FzYal9cqXZ3s9Qt8n9HScEecjdldqOd04HXExzO8i8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb89e898879eb7adef969749433776a6f6a543ad" title="Executive Order: &amp;quot;Preventing Woke AI in the Federal Government&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunyaBuzor"&gt; /u/NunyaBuzor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/07/preventing-woke-ai-in-the-federal-government/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8l648/executive_order_preventing_woke_ai_in_the_federal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T00:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1m93d0r</id>
    <title>New Qwen3 on Fiction.liveBench</title>
    <updated>2025-07-25T16:11:47+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m93d0r/new_qwen3_on_fictionlivebench/"&gt; &lt;img alt="New Qwen3 on Fiction.liveBench" src="https://preview.redd.it/hvi3tvmjo1ff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8c9a4a27cda997bb1fb4bc522e4d9bac9f04231" title="New Qwen3 on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hvi3tvmjo1ff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m93d0r/new_qwen3_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m93d0r/new_qwen3_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T16:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8z2ut</id>
    <title>mini-swe-agent achieves 65% on SWE-bench in just 100 lines of python code</title>
    <updated>2025-07-25T13:24:09+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In 2024, we developed SWE-bench and SWE-agent at Princeton University and helped kickstart the coding agent revolution.&lt;/p&gt; &lt;p&gt;Back then, LMs were optimized to be great at chatting, but not much else. This meant that agent scaffolds had to get very creative (and complicated) to make LMs perform useful work.&lt;/p&gt; &lt;p&gt;But in 2025 LMs are actively optimized for agentic coding, and we ask:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What the simplest coding agent that could still score near SotA on the benchmarks?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Turns out, it just requires 100 lines of code!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;And this system still &lt;strong&gt;resolves 65% of all GitHub issues in the SWE-bench verified benchmark&lt;/strong&gt; with Sonnet 4 (for comparison, when Anthropic launched Sonnet 4, they reported 70% with their own scaffold that was never made public).&lt;/p&gt; &lt;p&gt;Honestly, we're all pretty stunned ourselves—we've now spent more than a year developing SWE-agent, and would not have thought that such a small system could perform nearly as good.&lt;/p&gt; &lt;p&gt;Now, admittedly, this is with Sonnet 4, which has probably the strongest agentic post-training of all LMs. But we're also working on updating the fine-tuning of our SWE-agent-LM-32B model specifically for this setting (we posted about this model here after hitting open-weight SotA on SWE-bench earlier this year).&lt;/p&gt; &lt;p&gt;All open source at &lt;a href="https://github.com/SWE-agent/mini-swe-agent"&gt;https://github.com/SWE-agent/mini-swe-agent&lt;/a&gt;. The hello world example is incredibly short &amp;amp; simple (and literally what gave us the 65% with Sonnet 4). But it is also meant as a serious command line tool + research project, so we provide a Claude-code style UI &amp;amp; some utilities on top of that.&lt;/p&gt; &lt;p&gt;We have some team members from Princeton/Stanford here today, let us know if you have any questions/feedback :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8z2ut/minisweagent_achieves_65_on_swebench_in_just_100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8z2ut/minisweagent_achieves_65_on_swebench_in_just_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8z2ut/minisweagent_achieves_65_on_swebench_in_just_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T13:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8ven3</id>
    <title>Qwen/Qwen3-235B-A22B-Thinking-2507</title>
    <updated>2025-07-25T10:16:41+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/"&gt; &lt;img alt="Qwen/Qwen3-235B-A22B-Thinking-2507" src="https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdec699720d09b0abd832855f564b348eefd2304" title="Qwen/Qwen3-235B-A22B-Thinking-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of Qwen3-235B-A22B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning. We are pleased to introduce &lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving &lt;strong&gt;state-of-the-art results among open-source thinking models&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ven3/qwenqwen3235ba22bthinking2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:16:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vjna</id>
    <title>Qwen/Qwen3-235B-A22B-Thinking-2507</title>
    <updated>2025-07-25T10:25:07+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/"&gt; &lt;img alt="Qwen/Qwen3-235B-A22B-Thinking-2507" src="https://external-preview.redd.it/aFgKkvLRlBq4pkW8wu8xgwzbntIRM6eHR6HNp8MMtiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdec699720d09b0abd832855f564b348eefd2304" title="Qwen/Qwen3-235B-A22B-Thinking-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;its show time folks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vjna/qwenqwen3235ba22bthinking2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:25:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8w9ah</id>
    <title>New Qwen3-235B update is crushing old models in benchmarks</title>
    <updated>2025-07-25T11:07:09+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8w9ah/new_qwen3235b_update_is_crushing_old_models_in/"&gt; &lt;img alt="New Qwen3-235B update is crushing old models in benchmarks" src="https://preview.redd.it/q009687760ff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee6d42068b310b231eceef2e74d8ae35c50e819e" title="New Qwen3-235B update is crushing old models in benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Check out this chart comparing the latest Qwen3-235B-A22B-2507 models (Instruct and Thinking) to the older versions. The improvements are huge across different tests:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• GPQA (Graduate-level reasoning): 81 → 71 • AIME2025 (Math competition problems): 92 → 81 • LiveCodeBench v6 (Code generation and debugging): 74 → 56 • Arena-Hard v2 (General problem-solving): 80 → 62 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Even the new instruct version is way better than the old non-thinking one. Looks like they’ve really boosted reasoning and coding skills here.&lt;/p&gt; &lt;p&gt;What do you think is driving this jump, better training, bigger data, or new techniques?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q009687760ff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8w9ah/new_qwen3235b_update_is_crushing_old_models_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8w9ah/new_qwen3235b_update_is_crushing_old_models_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T11:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m91b98</id>
    <title>Qwen’s TRIPLE release this week + Vid Gen model coming</title>
    <updated>2025-07-25T14:54:14+00:00</updated>
    <author>
      <name>/u/koc_Z3</name>
      <uri>https://old.reddit.com/user/koc_Z3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m91b98/qwens_triple_release_this_week_vid_gen_model/"&gt; &lt;img alt="Qwen’s TRIPLE release this week + Vid Gen model coming" src="https://b.thumbs.redditmedia.com/Hp7akddNSmnkkxGnEtjYFrudFRvRoCTXEnawFbp7MZQ.jpg" title="Qwen’s TRIPLE release this week + Vid Gen model coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen just dropped a triple update. After months out of the spotlight, Qwen is back and bulked up. You can literally see the gains; the training shows. I was genuinely impressed.&lt;/p&gt; &lt;p&gt;I once called Alibaba “the first Chinese LLM team to evolve from engineering to product.” This week, I need to upgrade that take: it’s now setting the release tempo and product standards for open-source AI.&lt;/p&gt; &lt;p&gt;This week’s triple release effectively reclaims the high ground across all three major pillars of open-source models:&lt;/p&gt; &lt;p&gt;1️⃣ Qwen3-235B-A22B-Instruct-2507: Outstanding results across GPQA, AIME25, LiveCodeBench, Arena-Hard, BFCL, and more. It even outperformed Claude 4 (non-thinking variant). The research group Artificial Analysis didn’t mince words: “Qwen3 is the world’s smartest non-thinking base model.”&lt;/p&gt; &lt;p&gt;2️⃣ Qwen3-Coder: This is a full-on ecosystem play for AI programming. It outperformed GPT-4.1 and Claude 4 in multilingual SWE-bench, Mind2Web, Aider-Polyglot, and more—and it took the top spot on Hugging Face’s overall leaderboard. The accompanying CLI tool, Qwen Code, clearly aims to become the “default dev workflow component.”&lt;/p&gt; &lt;p&gt;3️⃣ Qwen3-235B-A22B-Thinking-2507: With 256K context support and top-tier performance on SuperGPQA, LiveCodeBench v6, AIME25, Arena-Hard v2, WritingBench, and MultiIF, this model squares up directly against Gemini 2.5 Pro and o4-mini, pushing open-source inference models to the threshold of closed-source elite.&lt;/p&gt; &lt;p&gt;This isn’t about “can one model compete.” Alibaba just pulled off a coordinated strike: base models, code models, inference models—all firing in sync. Behind it all is a full-stack platform play: cloud infra, reasoning chains, agent toolkits, community release cadence.&lt;/p&gt; &lt;p&gt;And the momentum isn’t stopping. Wan 2.2, Alibaba’s upcoming video generation model, is next. Built on the heels of the highly capable Wan 2.1 (which topped VBench with advanced motion and multilingual text rendering), Wan 2.2 promises even better video quality, controllability, and resource efficiency. It’s expected to raise the bar in open-source T2V (text-to-video) generation—solidifying Alibaba’s footprint not just in LLMs, but in multimodal generative AI.&lt;/p&gt; &lt;p&gt;Open source isn’t just “throwing code over the wall.” It’s delivering production-ready, open products—and Alibaba is doing exactly that.&lt;/p&gt; &lt;p&gt;Let’s not forget: Alibaba has open-sourced 300+ Qwen models and over 140,000 derivatives, making it the largest open-source model family on the planet. And they’ve pledged another ¥380 billion over the next three years into cloud and AI infrastructure. This isn’t a short-term leaderboard sprint. They’re betting big on locking down end-to-end certainty, from model to infrastructure to deployment.&lt;/p&gt; &lt;p&gt;Now look across the Pacific: the top U.S. models are mostly going closed. GPT-4 isn’t open. Gemini’s locked down. Claude’s gated by API. Meanwhile, Alibaba is using the “open-source + engineering + infrastructure” trifecta to set a global usability bar.&lt;/p&gt; &lt;p&gt;This isn’t a “does China have the chops?” moment. Alibaba’s already in the center of the world stage setting the tempo.&lt;/p&gt; &lt;p&gt;Reminds me of that line: “The GOAT doesn’t announce itself. It just keeps dropping.” Right now, it’s Alibaba that’s dropping. And flexing. 💪&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koc_Z3"&gt; /u/koc_Z3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m91b98"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m91b98/qwens_triple_release_this_week_vid_gen_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m91b98/qwens_triple_release_this_week_vid_gen_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T14:54:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8ud84</id>
    <title>A contamination-free coding benchmark shows AI may not be as excellent as claimed</title>
    <updated>2025-07-25T09:09:46+00:00</updated>
    <author>
      <name>/u/Creepy-Document4034</name>
      <uri>https://old.reddit.com/user/Creepy-Document4034</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/"&gt;https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;“If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination-free SWE-Bench, that’s the reality check for me.”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creepy-Document4034"&gt; /u/Creepy-Document4034 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8ud84/a_contaminationfree_coding_benchmark_shows_ai_may/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T09:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m903il</id>
    <title>I created an open-source macOS AI browser that uses MLX and Gemma 3n, feel free to fork it!</title>
    <updated>2025-07-25T14:06:36+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m903il/i_created_an_opensource_macos_ai_browser_that/"&gt; &lt;img alt="I created an open-source macOS AI browser that uses MLX and Gemma 3n, feel free to fork it!" src="https://external-preview.redd.it/NGRzMm4wN3oxMWZmMcBzbBspgkh2rR30WizNU_-HmGEenkhMN_T-yrpm1ere.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5efdce55bad803728b41721d787483c2767bf15" title="I created an open-source macOS AI browser that uses MLX and Gemma 3n, feel free to fork it!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is an AI web browser that uses local AI models. It's still very early, FULL of bugs and missing key features as a browser, but still good to play around with it. &lt;/p&gt; &lt;p&gt;Download it from &lt;a href="https://github.com/nuance-dev/Web"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: AI features only work with M series chips.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fculp27z11ff1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m903il/i_created_an_opensource_macos_ai_browser_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m903il/i_created_an_opensource_macos_ai_browser_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T14:06:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8xmy9</id>
    <title>GLM-4.1V-9B-Thinking - claims to "match or surpass Qwen2.5-72B" on many tasks</title>
    <updated>2025-07-25T12:18:54+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8xmy9/glm41v9bthinking_claims_to_match_or_surpass/"&gt; &lt;img alt="GLM-4.1V-9B-Thinking - claims to &amp;quot;match or surpass Qwen2.5-72B&amp;quot; on many tasks" src="https://external-preview.redd.it/35YSJo7Lmen5bPXSpg8onBoKMeQEGpDpKRxTziQDzj8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=841806f55387309a64d67cd8a7a49351c70e6ab2" title="GLM-4.1V-9B-Thinking - claims to &amp;quot;match or surpass Qwen2.5-72B&amp;quot; on many tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm happy to see this as my experience with these models for image recognition isn't very impressive. They mostly can't even tell when pictures are sideways, for example.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/THUDM/GLM-4.1V-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8xmy9/glm41v9bthinking_claims_to_match_or_surpass/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8xmy9/glm41v9bthinking_claims_to_match_or_surpass/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T12:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vhp3</id>
    <title>Amazing qwen 3 updated thinking model just released !! Open source !</title>
    <updated>2025-07-25T10:21:49+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/"&gt; &lt;img alt="Amazing qwen 3 updated thinking model just released !! Open source !" src="https://preview.redd.it/nx5d8w74yzef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d728468419b7ffc3426c85447250b3cc034f70a" title="Amazing qwen 3 updated thinking model just released !! Open source !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19"&gt;https://x.com/Alibaba_Qwen/status/1948688466386280706?t=7T6_M6vN6HrK4wvLjFNVBg&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nx5d8w74yzef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vhp3/amazing_qwen_3_updated_thinking_model_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8myxl</id>
    <title>Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years.</title>
    <updated>2025-07-25T02:02:13+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/"&gt; &lt;img alt="Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years." src="https://preview.redd.it/nl9jgkkzgxef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5596e2098d9a669775268db5ef71e54bd685cd0d" title="Watching everyone else drop new models while knowing you’re going to release the best open source model of all time in about 20 years." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nl9jgkkzgxef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8myxl/watching_everyone_else_drop_new_models_while/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T02:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8w7ny</id>
    <title>Smaller Qwen Models next week!!</title>
    <updated>2025-07-25T11:04:28+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/"&gt; &lt;img alt="Smaller Qwen Models next week!!" src="https://preview.redd.it/752ts71q50ff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27677972e2a6faf4ae42e2c72e03cfbb90ab79cb" title="Smaller Qwen Models next week!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like we will get smaller instruct and reasoning variants of Qwen3 next week. Hopefully smaller Qwen3 coder variants aswell.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/752ts71q50ff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8w7ny/smaller_qwen_models_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T11:04:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1m8vegq</id>
    <title>Qwen3-235B-A22B-Thinking-2507 released!</title>
    <updated>2025-07-25T10:16:25+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/"&gt; &lt;img alt="Qwen3-235B-A22B-Thinking-2507 released!" src="https://preview.redd.it/bvx1dbl5xzef1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f579818ebd6748b55b90f802c28f4d37095432e" title="Qwen3-235B-A22B-Thinking-2507 released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 We’re excited to introduce Qwen3-235B-A22B-Thinking-2507 — our most advanced reasoning model yet!&lt;/p&gt; &lt;p&gt;Over the past 3 months, we’ve significantly scaled and enhanced the thinking capability of Qwen3, achieving: ✅ Improved performance in logical reasoning, math, science &amp;amp; coding ✅ Better general skills: instruction following, tool use, alignment ✅ 256K native context for deep, long-form understanding&lt;/p&gt; &lt;p&gt;🧠 Built exclusively for thinking mode, with no need to enable it manually. The model now natively supports extended reasoning chains for maximum depth and accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bvx1dbl5xzef1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m8vegq/qwen3235ba22bthinking2507_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T10:16:25+00:00</published>
  </entry>
</feed>
