<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-26T19:34:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lkc5mr</id>
    <title>LM Studio now supports MCP!</title>
    <updated>2025-06-25T17:37:55+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Read the announcement: &lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/mcp"&gt;lmstudio.ai/blog/mcp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkc5mr/lm_studio_now_supports_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll2fyh</id>
    <title>Deepseek V3 0324 vs R1 0528 for coding tasks.</title>
    <updated>2025-06-26T15:03:50+00:00</updated>
    <author>
      <name>/u/ciprianveg</name>
      <uri>https://old.reddit.com/user/ciprianveg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested with java and js coding tasks both locally, both with the largest version i can accommodate on my system, unsloth Q3-XL-UD (almost 300GB) following the recomended settings for coding, temp 0 for V3 and 0.6 for R1 and, to my surprise I find the V3 to make less mistakes and to generate better code for me. I have for both a context size of 74k, Q8 cache. I was expecting that with all the thinking, R1 will create better code than V3. I am usually using large context prompts, 10k-20k cause I paste the relevant code files together with my question. Is this caused by the temperature? R1 needs larger temp for thinking process and this can lead to more errors in the generation? What is your experience with these two?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ciprianveg"&gt; /u/ciprianveg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll2fyh/deepseek_v3_0324_vs_r1_0528_for_coding_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll2fyh/deepseek_v3_0324_vs_r1_0528_for_coding_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll2fyh/deepseek_v3_0324_vs_r1_0528_for_coding_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll442i</id>
    <title>NotebookLM explaining Sparsity in LLMs using Deja Vu &amp; LLM in a Flash</title>
    <updated>2025-06-26T16:09:21+00:00</updated>
    <author>
      <name>/u/Economy-Mud-6626</name>
      <uri>https://old.reddit.com/user/Economy-Mud-6626</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll442i/notebooklm_explaining_sparsity_in_llms_using_deja/"&gt; &lt;img alt="NotebookLM explaining Sparsity in LLMs using Deja Vu &amp;amp; LLM in a Flash" src="https://external-preview.redd.it/qv-trUgr_F5dUKSisR1EF7whOER7-4P323ECjDOJaU0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf2d75e8c7184b265b50253685a654bbfc14023b" title="NotebookLM explaining Sparsity in LLMs using Deja Vu &amp;amp; LLM in a Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We ran an experiment with NotebookLM where we fed it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context from our GitHub repo&lt;/li&gt; &lt;li&gt;Two key papers: Deja Vu and LLM in a Flash&lt;/li&gt; &lt;li&gt;Comments and community insights from LocaLLaMA reddit discussion&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It is surprisingly clear and digestible podcast on sparsity, memory access patterns, and efficient inference in LLMs.&lt;/p&gt; &lt;p&gt;What stood out was how well it turned dense research into something conversational and accessible. Especially the interactive mode was amazing. Worth checking out if you're into retrieval-augmented generation, low-memory LLMs, or just like seeing what LLMs can do with the right context. What topics you'd want us to explore in this format?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Mud-6626"&gt; /u/Economy-Mud-6626 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://open.spotify.com/episode/0540o6A17BhyHkJwFOFd89?si=vjlIj_eZRYqjHDytPux9sQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll442i/notebooklm_explaining_sparsity_in_llms_using_deja/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll442i/notebooklm_explaining_sparsity_in_llms_using_deja/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkh3og</id>
    <title>Introducing: The New BS Benchmark</title>
    <updated>2025-06-25T20:48:12+00:00</updated>
    <author>
      <name>/u/Turdbender3k</name>
      <uri>https://old.reddit.com/user/Turdbender3k</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"&gt; &lt;img alt="Introducing: The New BS Benchmark" src="https://preview.redd.it/4b2ufnhcy49f1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfd8525d5b8c8bc0411893fe54cdd82fd4431a59" title="Introducing: The New BS Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there a bs detector benchmark?^^ what if we can create questions that defy any logic just to bait the llm into a bs answer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turdbender3k"&gt; /u/Turdbender3k &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4b2ufnhcy49f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkh3og/introducing_the_new_bs_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T20:48:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll340q</id>
    <title>I rebuilt Google's Gemini CLI system prompt with better engineering practices</title>
    <updated>2025-06-26T15:30:20+00:00</updated>
    <author>
      <name>/u/PsiACE</name>
      <uri>https://old.reddit.com/user/PsiACE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;TL;DR&lt;/h2&gt; &lt;p&gt;Google's Gemini CLI system prompt is publicly available but it's a monolithic mess. I refactored it into a maintainable, modular architecture that preserves all functionality while making it actually usable for the rest of us.&lt;/p&gt; &lt;h2&gt;Code &amp;amp; Details&lt;/h2&gt; &lt;p&gt;Full implementation available on GitHub: &lt;a href="https://github.com/PsiACE/republic/tree/main/packages/prompt/examples"&gt;republic-prompt examples&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;The Problem&lt;/h2&gt; &lt;p&gt;Google's official Gemini CLI system prompt (&lt;a href="https://github.com/google-gemini/gemini-cli/blob/0915bf7d677504c28b079693a0fe1c853adc456e/packages/core/src/core/prompts.ts"&gt;prompts.ts&lt;/a&gt;) is functionally impressive but architecturally... let's just say it wasn't built with maintenance in mind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No modularity or reusability &lt;/li&gt; &lt;li&gt;Impossible to customize without breaking things&lt;/li&gt; &lt;li&gt;Zero separation of concerns&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It works great for Google's use case, but good luck adapting it for your own projects.&lt;/p&gt; &lt;h2&gt;What I Built&lt;/h2&gt; &lt;p&gt;I completely rebuilt the system using a component-based architecture:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before (Google's approach):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;javascript // One giant hardcoded string with embedded logic const systemPrompt = `You are an interactive CLI agent... ${process.env.SANDBOX ? 'sandbox warning...' : 'no sandbox...'} // more and more lines of this...` &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;After (my approach):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;Modular configuration&lt;/h1&gt; &lt;p&gt;templates/ ├── gemini_cli_system_prompt.md # Main template └── simple_agent.md # Lightweight variant&lt;/p&gt; &lt;p&gt;snippets/ ├── core_mandates.md # Reusable components&lt;br /&gt; ├── command_safety.md └── environment_detection.md&lt;/p&gt; &lt;p&gt;functions/ ├── environment.py # Business logic ├── tools.py └── workflows.py ```&lt;/p&gt; &lt;h2&gt;Example Usage&lt;/h2&gt; &lt;p&gt;```python from republic_prompt import load_workspace, render&lt;/p&gt; &lt;h1&gt;Load the workspace&lt;/h1&gt; &lt;p&gt;workspace = load_workspace(&amp;quot;examples&amp;quot;)&lt;/p&gt; &lt;h1&gt;Generate different variants&lt;/h1&gt; &lt;p&gt;full_prompt = render(workspace.templates[&amp;quot;gemini_cli_system_prompt&amp;quot;], { &amp;quot;use_tools&amp;quot;: True, &amp;quot;max_output_lines&amp;quot;: 8 })&lt;/p&gt; &lt;p&gt;lightweight = render(workspace.templates[&amp;quot;simple_agent&amp;quot;], { &amp;quot;use_tools&amp;quot;: False, &amp;quot;max_output_lines&amp;quot;: 2 }) ```&lt;/p&gt; &lt;h2&gt;Why This Matters&lt;/h2&gt; &lt;p&gt;Google's approach works for them, but the rest of us need something we can actually maintain and customize. This refactor shows that you can have both powerful functionality AND clean architecture.&lt;/p&gt; &lt;p&gt;The original is open source but practically unmaintainable. This version gives you the same power with proper engineering practices.&lt;/p&gt; &lt;p&gt;What do you think? Anyone else frustrated with maintaining these massive system prompts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PsiACE"&gt; /u/PsiACE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll340q/i_rebuilt_googles_gemini_cli_system_prompt_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll340q/i_rebuilt_googles_gemini_cli_system_prompt_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll340q/i_rebuilt_googles_gemini_cli_system_prompt_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:30:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkv8vd</id>
    <title>MUVERA: Making multi-vector retrieval as fast as single-vector search</title>
    <updated>2025-06-26T08:57:50+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkv8vd/muvera_making_multivector_retrieval_as_fast_as/"&gt; &lt;img alt="MUVERA: Making multi-vector retrieval as fast as single-vector search" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="MUVERA: Making multi-vector retrieval as fast as single-vector search" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkv8vd/muvera_making_multivector_retrieval_as_fast_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkv8vd/muvera_making_multivector_retrieval_as_fast_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T08:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkzpdc</id>
    <title>I built an AI Home Assistant with EPC32 and I2S. It works with local models and has my personal context / tools. It’s also helping me become a better Redditor</title>
    <updated>2025-06-26T13:08:22+00:00</updated>
    <author>
      <name>/u/zuluana</name>
      <uri>https://old.reddit.com/user/zuluana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzpdc/i_built_an_ai_home_assistant_with_epc32_and_i2s/"&gt; &lt;img alt="I built an AI Home Assistant with EPC32 and I2S. It works with local models and has my personal context / tools. It’s also helping me become a better Redditor" src="https://external-preview.redd.it/dHdxbjd0Z2R0OTlmMYJTg58zegrAzYwLDecY21tQ6Q7YMhgJ9y6C6hMRxDnx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d49d6c96df81c4c30a597d62ceb6718b3eb822e6" title="I built an AI Home Assistant with EPC32 and I2S. It works with local models and has my personal context / tools. It’s also helping me become a better Redditor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an iPhone, and holding the side button always activates Siri... which I'm not crazy about.&lt;/p&gt; &lt;p&gt;I tried using back-tap to open ChatGPT, but it takes too long, and it's inconsistent.&lt;/p&gt; &lt;p&gt;Wired up a quick circuit to immediately interact with language models of my choice (along with my data / integrations)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zuluana"&gt; /u/zuluana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kkt198rdt99f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzpdc/i_built_an_ai_home_assistant_with_epc32_and_i2s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzpdc/i_built_an_ai_home_assistant_with_epc32_and_i2s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T13:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkr9k7</id>
    <title>AMD can't be THAT bad at LLMs, can it?</title>
    <updated>2025-06-26T04:44:33+00:00</updated>
    <author>
      <name>/u/tojiro67445</name>
      <uri>https://old.reddit.com/user/tojiro67445</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I recently upgraded from a Nvidia 3060 (12GB) to a AMD 9060XT (16GB) and running local models with the new GPU is effectively unusable. I knew Nvidia/CUDA dominate this space, but the difference is so shockingly bad that I feel like I must be doing something wrong. AMD can't possibly be THAT bad at this, right?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt; I actually don't really use LLMs for anything, but they are adjacent to my work on GPU APIs so I like to keep tabs on how things evolve in that space. Call it academic curiosity. In any case, I usually dip in every few months, try a couple of newer local models, and get a feel for what they can and can't do.&lt;/p&gt; &lt;p&gt;I had a pretty good sense for the limits of my previous Nvidia GPU, and would get maybe ~10T/s with quantized 12B models running with koboldcpp. Nothing spectacular but it was fine for my needs.&lt;/p&gt; &lt;p&gt;This time around I decided to switch teams and get an AMD GPU, and I've been genuinely happy with it! Runs the games I throw at it great (because 1440p at 60FPS is perfectly fine IMO). But I was kind of shocked when I spun up koboldcpp with a model I had run earlier and was getting... ~1T/s??? A literal order of magnitude slower than with a GPU nearly 5 years older.&lt;/p&gt; &lt;p&gt;For context, I tried it with kobaldcpp_nocuda on Windows 11, Vulkan backend, gemma-3-12b-it-q4_0 as the model. Seems to load OK:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_tensors: loading model tensors, this can take a while... (mmap = false) load_tensors: relocated tensors: 0 of 627 load_tensors: offloading 48 repeating layers to GPU load_tensors: offloading output layer to GPU load_tensors: offloaded 49/49 layers to GPU load_tensors: Vulkan0 model buffer size = 7694.17 MiB load_tensors: Vulkan_Host model buffer size = 1920.00 MiB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But the output is dreadful.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Processing Prompt [BLAS] (1024 / 1024 tokens) Generating (227 / 300 tokens) (EOS token triggered! ID:106) [20:50:09] CtxLimit:1251/4096, Amt:227/300, Init:0.00s, Process:21.43s (47.79T/s), Generate:171.62s (1.32T/s), Total:193.05s ====== Note: Your generation speed appears rather slow. You can try relaunching KoboldCpp with the high priority toggle (or --highpriority) to see if it helps. ====== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Spoiler alert: &lt;code&gt;--highpriority&lt;/code&gt; does not help.&lt;/p&gt; &lt;p&gt;So my question is am I just doing something wrong, or is AMD just really truly this terrible at the whole AI space? I know that most development in this space is done with CUDA and I'm certain that accounts for some of it, but in my experience devs porting CUDA code over to another GPU environment like Vulkan tend to come back with things like &amp;quot;initial release is 15% slower than the CUDA version because we haven't implemented these 20 vendor-specific extensions yet&amp;quot;, not 10x slower implementations. I also don't think that using a ROCm backend (should it ever get around to supporting the 9000 series on Windows) is magically going to give me a 10x boost. Vulkan is hard, y'all, but it's not THAT hard.&lt;/p&gt; &lt;p&gt;Anyone else have experience with the newer AMD cards that either confirms what I'm seeing or indicates I'm doing something wrong?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Wow! This got more of a response than I was anticipating! Thanks all! At least it's abundantly clear that it's a problem with my setup and not the GPU.&lt;/p&gt; &lt;p&gt;For what it's worth I tried LM Studio this morning and I'm getting the same thing. It reported 1.5T/s. Looking at resource manager when using LM Studio or Kobold I can see that it's using the GPU's compute capabilities at near 100%, so it's not trying to do the inference on the CPU. I did notice in the AMD software that it said only about a gig of VRAM was being used. The windows performance panel shows that 11Gb of &amp;quot;Shared GPU Memory&amp;quot; is being used, but only 1.8 Gb of &amp;quot;Dedicated GPU Memory&amp;quot; was utilized. So my working theory is that somehow the wrong Vulkan memory heap is being used?&lt;/p&gt; &lt;p&gt;In any case, I'll investigate more tonight but thank you again for all the feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tojiro67445"&gt; /u/tojiro67445 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkr9k7/amd_cant_be_that_bad_at_llms_can_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkr9k7/amd_cant_be_that_bad_at_llms_can_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkr9k7/amd_cant_be_that_bad_at_llms_can_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T04:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll0e5d</id>
    <title>Day 4 of 50 Days of Building a Small Language Model from Scratch — Understanding Byte Pair Encoding (BPE) Tokenizer</title>
    <updated>2025-06-26T13:38:53+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll0e5d/day_4_of_50_days_of_building_a_small_language/"&gt; &lt;img alt="Day 4 of 50 Days of Building a Small Language Model from Scratch — Understanding Byte Pair Encoding (BPE) Tokenizer" src="https://external-preview.redd.it/eMGOFT-dCyqrcGU8o4sNWdjVcmCnEWFc2iYXpXWsCCc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9014417b8d198c19f07252830c07d7c077d974f" title="Day 4 of 50 Days of Building a Small Language Model from Scratch — Understanding Byte Pair Encoding (BPE) Tokenizer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yars4a5sy99f1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f41d24b120ac3004968352bd549653db24140944"&gt;https://preview.redd.it/yars4a5sy99f1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f41d24b120ac3004968352bd549653db24140944&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;So far, we’ve explored what a tokenizer is and even built our own from scratch. However, one of the key limitations of building a custom tokenizer is handling unknown or rare words. This is where advanced tokenizers like OpenAI’s tiktoken, which uses Byte Pair Encoding (BPE), really shine.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;We also understood, Language models don’t read or understand in the same way humans do. Before any text can be processed by a model, it needs to be tokenized, that is, broken into smaller chunks called tokens. One of the most efficient and widely adopted techniques to perform this is called Byte Pair Encoding (BPE).&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Let’s dive deep into how it works, why it’s important, and how to use it in practice.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;What Is Byte Pair Encoding?&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Byte Pair Encoding is a data compression algorithm adapted for tokenization. Instead of treating words as whole units, it breaks them down into smaller, more frequent subword units. This allows it to:&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Handle unknown words gracefully&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Strike a balance between character-level and word-level tokenization&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Reduce the overall vocabulary size&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How BPE Works (Step-by-Step)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Let’s understand this with a simplified example.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Step 1: Start with Characters&lt;/h1&gt; &lt;p&gt;&lt;em&gt;We begin by breaking all words in our corpus into characters:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;low&amp;quot;, &amp;quot;lower&amp;quot;, &amp;quot;newest&amp;quot;, &amp;quot;widest&amp;quot; → [&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;, &amp;quot;w&amp;quot;], [&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;, &amp;quot;w&amp;quot;, &amp;quot;e&amp;quot;, &amp;quot;r&amp;quot;], ... &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 2: Count Pair Frequencies&lt;/h1&gt; &lt;p&gt;&lt;em&gt;We count the frequency of adjacent character pairs (bigrams). For example:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;l o&amp;quot;: 2, &amp;quot;o w&amp;quot;: 2, &amp;quot;w e&amp;quot;: 2, &amp;quot;e s&amp;quot;: 2, ... &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Step 3: Merge the Most Frequent Pair&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Merge the most frequent pair into a new token:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Merge &amp;quot;e s&amp;quot; → &amp;quot;es&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Now “newest” becomes:&lt;/em&gt; &lt;code&gt;[&amp;quot;n&amp;quot;, &amp;quot;e&amp;quot;, &amp;quot;w&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;t&amp;quot;]&lt;/code&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Step 4: Repeat Until Vocabulary Limit&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Continue this process until you reach the desired vocabulary size or until no more merges are possible.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Why Is BPE Powerful?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Efficient&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: It reuses frequent subwords to reduce redundancy.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Flexible&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Handles rare and compound words better than word-level tokenizers.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Compact vocabulary&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Essential for performance in large models.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;It solves a key problem: how to tokenize unknown or rare words without bloating the vocabulary.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Where Is BPE Used?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;OpenAI’s GPT (e.g., GPT-2, GPT-3, GPT-4)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Hugging Face’s RoBERTa&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;EleutherAI’s GPT-NeoX&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;em&gt;Most transformer models before newer techniques like Unigram or SentencePiece came in&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Example: Using tiktoken for BPE Tokenization&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Now let’s see how to use the&lt;/em&gt; &lt;a href="https://github.com/openai/tiktoken"&gt;&lt;em&gt;tiktoken&lt;/em&gt;&lt;/a&gt; &lt;em&gt;library by OpenAI, which implements BPE for GPT models.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Installation&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install tiktoken &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;🧑‍💻 Code Example&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;import tiktoken # Load GPT-4 tokenizer (you can also try &amp;quot;gpt2&amp;quot;, &amp;quot;cl100k_base&amp;quot;, etc.) encoding = tiktoken.get_encoding(&amp;quot;cl100k_base&amp;quot;) # Input text text = &amp;quot;IdeaWeaver is building a tokenizer using BPE&amp;quot; # Tokenize token_ids = encoding.encode(text) print(&amp;quot;Token IDs:&amp;quot;, token_ids) # Decode back to text decoded_text = encoding.decode(token_ids) print(&amp;quot;Decoded Text:&amp;quot;, decoded_text) # Optional: Show individual tokens tokens = [encoding.decode([id]) for id in token_ids] print(&amp;quot;Tokens:&amp;quot;, tokens) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Output&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Token IDs: [10123, 91234, ...] Decoded Text: IdeaWeaver is building a tokenizer using BPE Tokens: ['Idea', 'Weaver', ' is', ' building', ' a', ' tokenizer', ' using', ' BPE'] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;You can see that even compound or rare words are split into manageable subword units, which is the strength of BPE.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Byte Pair Encoding may sound simple, but it’s one of the key innovations that made today’s large language models possible. It strikes a balance between efficiency, flexibility, and robustness in handling diverse language input.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Next time you ask a question to GPT, remember, BPE made sure your words were understood!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll0e5d/day_4_of_50_days_of_building_a_small_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll0e5d/day_4_of_50_days_of_building_a_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll0e5d/day_4_of_50_days_of_building_a_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T13:38:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll3qej</id>
    <title>I built an MCP that finally makes your local AI models shine with SQL</title>
    <updated>2025-06-26T15:54:38+00:00</updated>
    <author>
      <name>/u/Durovilla</name>
      <uri>https://old.reddit.com/user/Durovilla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll3qej/i_built_an_mcp_that_finally_makes_your_local_ai/"&gt; &lt;img alt="I built an MCP that finally makes your local AI models shine with SQL" src="https://preview.redd.it/2h8s7lagma9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c677d875fded9f93ef2fcd488b04ac20d1d5c282" title="I built an MCP that finally makes your local AI models shine with SQL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/LocalLLaMA 👋&lt;/p&gt; &lt;p&gt;I'm a huge fan of using local AI models for queries &amp;amp; analytics, but my workflow has been quite painful. I feel like SQL tools never works as intended, and I spend half my day just copy-pasting schemas and table info into the context. I got so fed up with this, I decided to build &lt;a href="https://github.com/kruskal-labs/toolfront"&gt;ToolFront&lt;/a&gt;. It's a free, open-source, and local MCP that finally gives AI a smart, safe way to &lt;strong&gt;understand all your databases and query them&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;So, what does it do?&lt;/h1&gt; &lt;p&gt;ToolFront equips AI models with a set of &lt;strong&gt;read-only database tools&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;discover&lt;/code&gt;: See all your connected databases.&lt;/li&gt; &lt;li&gt;&lt;code&gt;search_tables&lt;/code&gt;: Find tables by name or description.&lt;/li&gt; &lt;li&gt;&lt;code&gt;inspect&lt;/code&gt;: Get the exact schema for any table – no more guessing!&lt;/li&gt; &lt;li&gt;&lt;code&gt;sample&lt;/code&gt;: Grab a few rows to quickly see the data.&lt;/li&gt; &lt;li&gt;&lt;code&gt;query&lt;/code&gt;: Run read-only SQL queries directly.&lt;/li&gt; &lt;li&gt;&lt;code&gt;search_queries&lt;/code&gt; &lt;strong&gt;(The Best Part)&lt;/strong&gt;: Finds the most relevant historical queries written by you or your team to answer new questions. Your AI can actually learn from your team's past SQL!&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Connects to what you're already using&lt;/h1&gt; &lt;p&gt;ToolFront supports the databases you're probably already working with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Snowflake&lt;/strong&gt;, &lt;strong&gt;BigQuery&lt;/strong&gt;, &lt;strong&gt;Databricks&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PostgreSQL&lt;/strong&gt;, &lt;strong&gt;MySQL&lt;/strong&gt;, &lt;strong&gt;SQL Server&lt;/strong&gt;, &lt;strong&gt;SQLite&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DuckDB&lt;/strong&gt; (Yup, analyze local CSV, Parquet, JSON, XLSX files directly!)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why you'll love it&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Privacy-first&lt;/strong&gt;: Your data stays local, and is only shared between your LLMs and databases through a secure MCP server.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agents for your data:&lt;/strong&gt; Build smart agents that understand your databases and know how to navigate them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI-powered DataOps:&lt;/strong&gt; Use ToolFront to explore your databases, iterate on queries, and write schema-aware code.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collaborative learning&lt;/strong&gt;: The more your LLMs use ToolFront, the better they remember your data.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you work with databases and local models, I genuinely think ToolFront can make your life a lot easier.&lt;/p&gt; &lt;p&gt;I'd love your feedback, especially on what database features are most crucial for your daily work.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo&lt;/strong&gt;: &lt;a href="https://github.com/kruskal-labs/toolfront"&gt;https://github.com/kruskal-labs/toolfront&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A ⭐ on GitHub really helps with visibility!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durovilla"&gt; /u/Durovilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2h8s7lagma9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll3qej/i_built_an_mcp_that_finally_makes_your_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll3qej/i_built_an_mcp_that_finally_makes_your_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:54:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkbiva</id>
    <title>Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge.</title>
    <updated>2025-06-25T17:13:56+00:00</updated>
    <author>
      <name>/u/SilverRegion9394</name>
      <uri>https://old.reddit.com/user/SilverRegion9394</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"&gt; &lt;img alt="Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge." src="https://preview.redd.it/11rgwmzvv39f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7039783722436b51c07b3fedff7d641b7b004cd" title="Gemini released an Open Source CLI Tool similar to Claude Code but with a free 1 million token context window, 60 model requests per minute and 1,000 requests per day at no charge." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SilverRegion9394"&gt; /u/SilverRegion9394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/11rgwmzvv39f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkbiva/gemini_released_an_open_source_cli_tool_similar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T17:13:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll49jc</id>
    <title>My Python AI Dev Tool: Avakin - Local LLMs, Project-Specific + Global RAG, &amp; More</title>
    <updated>2025-06-26T16:15:09+00:00</updated>
    <author>
      <name>/u/One_Negotiation_2078</name>
      <uri>https://old.reddit.com/user/One_Negotiation_2078</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll49jc/my_python_ai_dev_tool_avakin_local_llms/"&gt; &lt;img alt="My Python AI Dev Tool: Avakin - Local LLMs, Project-Specific + Global RAG, &amp;amp; More" src="https://preview.redd.it/qiuq20a1pa9f1.gif?width=640&amp;amp;crop=smart&amp;amp;s=dae7fb2838824ee2e900107ef0dde71780954483" title="My Python AI Dev Tool: Avakin - Local LLMs, Project-Specific + Global RAG, &amp;amp; More" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on a project called Avakin, a desktop AI development environment for Python, and wanted to share it with this community. My goal was to create a tool that deeply integrates with the development workflow, leverages local LLMs for privacy and control, and actually understands the context of individual projects.&lt;/p&gt; &lt;p&gt;Avakin runs entirely on your local machine (Windows for packaged release, source runs cross-platform). It's built with Python/PySide6 and orchestrates a team of AI agents (Architect, Coder, etc.) that can be configured to use different LLMs via a local FastAPI backend. This backend interfaces with Ollama for local models (Llama 3, Mistral, CodeLlama, etc.) or can call out to cloud APIs if you provide keys.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/carpsesdema/AvA_Kintsugi"&gt;https://github.com/carpsesdema/AvA_Kintsugi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a breakdown of the core technical features:&lt;/p&gt; &lt;p&gt;Dual-Context Local RAG (Project &amp;amp; Global Knowledge):&lt;/p&gt; &lt;p&gt;Technology:** Utilizes `SentenceTransformers` (`all-MiniLM-L6-v2` by default) for embeddings and `ChromaDB` for persistent local vector storage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project-Specific DBs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each Python project you work on gets its *own isolated `rag_db` directory*. This allows Avakin to build a deep understanding of your current project's specifics (like Game Design Documents, API schemas, or existing proprietary code) without context bleed from other work. The RAG server dynamically switches its active project DB when you switch projects in Avakin.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Global Knowledge Base:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Simultaneously, Avakin supports a separate, persistent global RAG collection (its path configured via the `GLOBAL_RAG_DB_PATH` env var). This is perfect for your large corpus of general Python code examples, programming best practices, or any technical documentation you want the AI to reference across all projects.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Synergistic Context&lt;/strong&gt;: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;When planning, coding, or chatting, AI agents can be fed context retrieved from *both* the active project's RAG and the global RAG. This allows for highly relevant, project-aware suggestions that are also informed by broad, general knowledge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Seamless Chat-to-Code Workflow:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Brainstorm ideas or discuss code with the chat AI (which also benefits from the Dual-Context RAG).&lt;/li&gt; &lt;li&gt; If an AI response in the chat contains a good idea or a snippet you want to build upon, you can instantly send that chat message's content to Avakin's &amp;quot;Build&amp;quot; mode with a right-click. This pre-populates the build prompt, allowing a smooth transition from conversation to code generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Local LLM Orchestration (Ollama Focus):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A dedicated local FastAPI server (`llm_server.py`) acts as a unified gateway to various LLM providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Native Ollama Support:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Directly streams responses from any model hosted by your local Ollama instance (Llama 3, Mistral, CodeLlama, etc.).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Configurable AI Agent Roles:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can assign different models (local or cloud) to distinct roles like 'Architect' (for planning), 'Coder' (for file generation), 'Reviewer' (for debugging), and 'Chat'. This allows for optimizing performance and capability (e.g., a powerful local model for coding, a smaller/faster one for chat).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Full Project Scaffolding &amp;amp; Generation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; From a single prompt, the 'Architect' agent (using its configured LLM and the powerful Dual-Context RAG) designs a multi-file Python application structure.&lt;/li&gt; &lt;li&gt; The 'Coder' agent then generates each file, with access to a dynamically updated symbol index of the project and the full code of already generated files in the current session, promoting better integration.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Surgical Code Modification &amp;amp; Debugging:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Accepts natural language requests to modify existing codebases. The AI is provided with the current code, project structure, and relevant RAG context.&lt;/li&gt; &lt;li&gt; One-Click Debugging: When a script run in the integrated terminal fails, Avakin captures the traceback. The 'Reviewer' agent analyzes this&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm still actively developing Avakin and would love to get your thoughts and feedback, especially from fellow local LLM enthusiasts! What features would you find most useful? Any pain points in local AI development that Avakin could help address?&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Negotiation_2078"&gt; /u/One_Negotiation_2078 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qiuq20a1pa9f1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll49jc/my_python_ai_dev_tool_avakin_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll49jc/my_python_ai_dev_tool_avakin_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:15:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll88pe</id>
    <title>Gemma 3n vs Gemma 3 (4B/12B) Benchmarks</title>
    <updated>2025-06-26T18:49:09+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I compiled all of the available official first-party benchmark results from google's model cards available here &lt;a href="https://ai.google.dev/gemma/docs/core/model_card_3#benchmark_results"&gt;https://ai.google.dev/gemma/docs/core/model_card_3#benchmark_results&lt;/a&gt; into a table to compare how the new 3N models do compared to their older non-n Gemma 3 siblings. Of course not all the same benchmark results were available for both models so I only added the results for tests they had done in common.&lt;/p&gt; &lt;h1&gt;Reasoning and Factuality&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;n-shot&lt;/th&gt; &lt;th align="left"&gt;E2B PT&lt;/th&gt; &lt;th align="left"&gt;E4B PT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 4B&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 12B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1905.07830"&gt;HellaSwag&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;10-shot&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;td align="left"&gt;78.6&lt;/td&gt; &lt;td align="left"&gt;77.2&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1905.10044"&gt;BoolQ&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;76.4&lt;/td&gt; &lt;td align="left"&gt;81.6&lt;/td&gt; &lt;td align="left"&gt;72.3&lt;/td&gt; &lt;td align="left"&gt;78.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1911.11641"&gt;PIQA&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;78.9&lt;/td&gt; &lt;td align="left"&gt;81&lt;/td&gt; &lt;td align="left"&gt;79.6&lt;/td&gt; &lt;td align="left"&gt;81.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1904.09728"&gt;SocialIQA&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;48.8&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;51.9&lt;/td&gt; &lt;td align="left"&gt;53.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1705.03551"&gt;TriviaQA&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;5-shot&lt;/td&gt; &lt;td align="left"&gt;60.8&lt;/td&gt; &lt;td align="left"&gt;70.2&lt;/td&gt; &lt;td align="left"&gt;65.8&lt;/td&gt; &lt;td align="left"&gt;78.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/google-research-datasets/natural-questions"&gt;Natural Questions&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;5-shot&lt;/td&gt; &lt;td align="left"&gt;15.5&lt;/td&gt; &lt;td align="left"&gt;20.9&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;31.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1911.01547"&gt;ARC-c&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;25-shot&lt;/td&gt; &lt;td align="left"&gt;51.7&lt;/td&gt; &lt;td align="left"&gt;61.6&lt;/td&gt; &lt;td align="left"&gt;56.2&lt;/td&gt; &lt;td align="left"&gt;68.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1911.01547"&gt;ARC-e&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;75.8&lt;/td&gt; &lt;td align="left"&gt;81.6&lt;/td&gt; &lt;td align="left"&gt;82.4&lt;/td&gt; &lt;td align="left"&gt;88.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1907.10641"&gt;WinoGrande&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;5-shot&lt;/td&gt; &lt;td align="left"&gt;66.8&lt;/td&gt; &lt;td align="left"&gt;71.7&lt;/td&gt; &lt;td align="left"&gt;64.7&lt;/td&gt; &lt;td align="left"&gt;74.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://paperswithcode.com/dataset/bbh"&gt;BIG-Bench Hard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;few-shot&lt;/td&gt; &lt;td align="left"&gt;44.3&lt;/td&gt; &lt;td align="left"&gt;52.9&lt;/td&gt; &lt;td align="left"&gt;50.9&lt;/td&gt; &lt;td align="left"&gt;72.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1903.00161"&gt;DROP&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Token F1 score&lt;/td&gt; &lt;td align="left"&gt;1-shot&lt;/td&gt; &lt;td align="left"&gt;53.9&lt;/td&gt; &lt;td align="left"&gt;60.8&lt;/td&gt; &lt;td align="left"&gt;60.1&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;GEOMEAN&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;54.46&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;61.08&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;58.57&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;68.99&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Additional/Other Benchmarks&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;n-shot&lt;/th&gt; &lt;th align="left"&gt;E2B IT&lt;/th&gt; &lt;th align="left"&gt;E4B IT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 4B&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 12B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2210.03057"&gt;MGSM&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;53.1&lt;/td&gt; &lt;td align="left"&gt;60.7&lt;/td&gt; &lt;td align="left"&gt;34.7&lt;/td&gt; &lt;td align="left"&gt;64.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.12404v1"&gt;WMT24++ (ChrF)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Character-level F-score&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;42.7&lt;/td&gt; &lt;td align="left"&gt;50.1&lt;/td&gt; &lt;td align="left"&gt;48.4&lt;/td&gt; &lt;td align="left"&gt;53.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.21228"&gt;ECLeKTic&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;ECLeKTic score&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;2.5&lt;/td&gt; &lt;td align="left"&gt;1.9&lt;/td&gt; &lt;td align="left"&gt;4.6&lt;/td&gt; &lt;td align="left"&gt;10.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2311.12022"&gt;GPQA Diamond&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;RelaxedAccuracy/accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;24.8&lt;/td&gt; &lt;td align="left"&gt;23.7&lt;/td&gt; &lt;td align="left"&gt;30.8&lt;/td&gt; &lt;td align="left"&gt;40.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2108.07732"&gt;MBPP&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;pass@1&lt;/td&gt; &lt;td align="left"&gt;3-shot&lt;/td&gt; &lt;td align="left"&gt;56.6&lt;/td&gt; &lt;td align="left"&gt;63.6&lt;/td&gt; &lt;td align="left"&gt;63.2&lt;/td&gt; &lt;td align="left"&gt;73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2107.03374"&gt;HumanEval&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;pass@1&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;66.5&lt;/td&gt; &lt;td align="left"&gt;75&lt;/td&gt; &lt;td align="left"&gt;71.3&lt;/td&gt; &lt;td align="left"&gt;85.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2403.07974"&gt;LiveCodeBench&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;pass@1&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;13.2&lt;/td&gt; &lt;td align="left"&gt;13.2&lt;/td&gt; &lt;td align="left"&gt;12.6&lt;/td&gt; &lt;td align="left"&gt;24.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HiddenMath&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;27.7&lt;/td&gt; &lt;td align="left"&gt;37.7&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite"&gt;Global-MMLU-Lite&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;64.5&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;69.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2009.03300"&gt;MMLU (Pro)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;40.5&lt;/td&gt; &lt;td align="left"&gt;50.6&lt;/td&gt; &lt;td align="left"&gt;43.6&lt;/td&gt; &lt;td align="left"&gt;60.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;GEOMEAN&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;29.27&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;31.81&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32.66&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;46.8&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Overall Geometric-Mean&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt; &lt;/th&gt; &lt;th align="left"&gt; &lt;/th&gt; &lt;th align="left"&gt; &lt;/th&gt; &lt;th align="left"&gt;E2B IT&lt;/th&gt; &lt;th align="left"&gt;E4B IT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 4B&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 12B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;GEOMAN-ALL&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;40.53&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;44.77&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;44.35&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;57.40&lt;/em&gt;&lt;/strong&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Link to google sheets document: &lt;a href="https://docs.google.com/spreadsheets/d/1U3HvtMqbiuO6kVM96d0aE9W40F8b870He0cg6hLPSdA/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1U3HvtMqbiuO6kVM96d0aE9W40F8b870He0cg6hLPSdA/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll88pe/gemma_3n_vs_gemma_3_4b12b_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll88pe/gemma_3n_vs_gemma_3_4b12b_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll88pe/gemma_3n_vs_gemma_3_4b12b_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T18:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll321h</id>
    <title>From "LangGraph is trash" to "pip install langgraph": A Stockholm Syndrome Story</title>
    <updated>2025-06-26T15:28:08+00:00</updated>
    <author>
      <name>/u/FailingUpAllDay</name>
      <uri>https://old.reddit.com/user/FailingUpAllDay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Listen, I get it. We all hate LangGraph. The documentation reads like it was written by someone explaining quantum mechanics to their dog. The examples are either &amp;quot;Hello World&amp;quot; or &amp;quot;Here's how to build AGI, figure out the middle part yourself.&amp;quot;&lt;/p&gt; &lt;p&gt;But I was different. I was going to be the hero LocalLlama needed.&lt;/p&gt; &lt;p&gt;&amp;quot;LangGraph is overcomplicated!&amp;quot; I declared. &amp;quot;State machines for agents? What is this, 1970? I'll build something better in a weekend!&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 1:&lt;/strong&gt; Drew a beautiful architecture diagram. Posted it on Twitter. 47 likes. &amp;quot;This is the way.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 3:&lt;/strong&gt; Okay, turns out managing agent state is... non-trivial. But I'm smart! I'll just use Python dicts!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 7:&lt;/strong&gt; My dict-based state management has evolved into... a graph. With nodes. And edges. Shit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 10:&lt;/strong&gt; Need tool calling. &amp;quot;MCP is the future!&amp;quot; Twitter says. Three days later: it works! (On my desktop. In dev mode. Only one user. When Mercury is in retrograde.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 14:&lt;/strong&gt; Added checkpointing because production agents apparently need to not die when AWS hiccups. My &amp;quot;simple&amp;quot; solution is now 3,000 lines of spaghetti.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 21:&lt;/strong&gt; &amp;quot;Maybe I need human-in-the-loop features,&amp;quot; my PM says. I start drinking during standups.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 30:&lt;/strong&gt; I've essentially recreated LangGraph, but worse. My state transitions look like they were designed by M.C. Escher having a bad trip. The only documentation is my increasingly unhinged commit messages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 45:&lt;/strong&gt; I quietly pip install langgraph. Nobody needs to know.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 55:&lt;/strong&gt; &amp;quot;You need observability,&amp;quot; someone says. I glance at my custom logging system. It's 500 lines of print statements. I sign up for LangSmith. &amp;quot;Just the free tier,&amp;quot; I tell myself. Two hours later I'm on the Teams plan, staring at traces like a detective who just discovered fingerprints exist. &amp;quot;So THAT'S why my agent thinks it's a toaster every third request.&amp;quot; My credit card weeps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 60:&lt;/strong&gt; Boss wants to demo tool calling. Palms sweat. &amp;quot;Define demo?&amp;quot; Someone mutters &lt;code&gt;pip install langchain-arcade&lt;/code&gt;. Ten minutes later, the agent is reading emails. I delete three days of MCP auth code and pride. I hate myself as I utter these words: &amp;quot;LangGraph isn't just a framework—it's an ecosystem of stuff that works.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Today:&lt;/strong&gt; I'm a LangGraph developer. I've memorized which 30% of the documentation actually matches the current version. I know exactly when to use StateGraph vs MessageGraph (hint: just use StateGraph and pray). I've accepted that &amp;quot;conditional_edge&amp;quot; is just how we live now.&lt;/p&gt; &lt;p&gt;The other day, a junior dev complained about LangGraph being &amp;quot;unnecessarily complex.&amp;quot; I laughed. Not a healthy laugh. The laugh of someone who's seen things. &amp;quot;Sure,&amp;quot; I said, &amp;quot;go build your own. I'll see you back here in 6 weeks.&amp;quot;&lt;/p&gt; &lt;p&gt;I've become the very thing I mocked. Yesterday, I actually said out loud: &amp;quot;Once you understand LangGraph's philosophy, it's quite elegant.&amp;quot; My coworkers staged an intervention.&lt;/p&gt; &lt;p&gt;But here's the thing - IT ACTUALLY WORKS. While everyone's writing blog posts about &amp;quot;Why Agent Frameworks Should Be Simple,&amp;quot; I'm shipping production systems with proper state management, checkpointing, and human oversight. My agents don't randomly hallucinate their entire state history anymore!&lt;/p&gt; &lt;p&gt;The final irony? I'm now building a LangGraph tutorial site... using a LangGraph agent to generate the content. It's graphs all the way down.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class MyAgentJourney: def __init__(self): self.confidence = float('inf') self.langgraph_hatred = 100 def build_own_framework(self): self.confidence *= 0.5 self.langgraph_hatred -= 10 self.understanding_of_problem += 50 def eventually(self): return &amp;quot;pip install langgraph&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; - Yes, I've tried CrewAI, AutoGen, and that new framework your favorite AI influencer is shilling. No, they don't handle complex state management. Yes, I'm stuck with LangGraph. No, I'm not happy about it. Yes, I'll defend it viciously if you criticize it because Stockholm Syndrome is real.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; To everyone saying &amp;quot;skill issue&amp;quot; - yes, and?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT 2:&lt;/strong&gt; The LangChain team DMed me asking if I want to help improve the docs. This is either an olive branch or a threat.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT 3:&lt;/strong&gt; RIP my inbox. No, I won't review your &amp;quot;simple&amp;quot; agent framework. We both know where this ends.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT 4:&lt;/strong&gt; This isn't fake. It's satire. :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT 5:&lt;/strong&gt; Yes, I originally posted this to the Langchain subreddit but I figured you'd enjoy it too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FailingUpAllDay"&gt; /u/FailingUpAllDay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll321h/from_langgraph_is_trash_to_pip_install_langgraph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll321h/from_langgraph_is_trash_to_pip_install_langgraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll321h/from_langgraph_is_trash_to_pip_install_langgraph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:28:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll5w6m</id>
    <title>Gemma 3n is now stable on HuggingFace</title>
    <updated>2025-06-26T17:18:14+00:00</updated>
    <author>
      <name>/u/best_codes</name>
      <uri>https://old.reddit.com/user/best_codes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll5w6m/gemma_3n_is_now_stable_on_huggingface/"&gt; &lt;img alt="Gemma 3n is now stable on HuggingFace" src="https://external-preview.redd.it/ELsux0mwxWZPvalnHOQRSKe_mzDmS7uNjsKunK8e1U8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abfda3ccbbaaecf31b079783cc429aaf9abad256" title="Gemma 3n is now stable on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/best_codes"&gt; /u/best_codes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll5w6m/gemma_3n_is_now_stable_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll5w6m/gemma_3n_is_now_stable_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll3j07</id>
    <title>Anubis 70B v1.1 - Just another RP tune... unlike any other L3.3! (allegedly) A breath of fresh prose and lack of positivity (YMMV ofc) + bonus Fallen 70B for mergefuel! (because tuners aren't limited to RP)</title>
    <updated>2025-06-26T15:46:44+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll3j07/anubis_70b_v11_just_another_rp_tune_unlike_any/"&gt; &lt;img alt="Anubis 70B v1.1 - Just another RP tune... unlike any other L3.3! (allegedly) A breath of fresh prose and lack of positivity (YMMV ofc) + bonus Fallen 70B for mergefuel! (because tuners aren't limited to RP)" src="https://external-preview.redd.it/_eIF0Xo1buph34Tuk-bXjGb0GyE839b8Ocdfqz4UUok.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d95c24031eb18602df8ccce61ee45bba3ac35bc3" title="Anubis 70B v1.1 - Just another RP tune... unlike any other L3.3! (allegedly) A breath of fresh prose and lack of positivity (YMMV ofc) + bonus Fallen 70B for mergefuel! (because tuners aren't limited to RP)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you like Fallen R1? Here's the non-R1 version: &lt;a href="https://huggingface.co/TheDrummer/Fallen-Llama-3.3-70B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Llama-3.3-70B-v1&lt;/a&gt; Enjoy the mergefuel!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Anubis-70B-v1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll3j07/anubis_70b_v11_just_another_rp_tune_unlike_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll3j07/anubis_70b_v11_just_another_rp_tune_unlike_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lko09j</id>
    <title>Google's CLI DOES use your prompting data</title>
    <updated>2025-06-26T01:54:24+00:00</updated>
    <author>
      <name>/u/Physical_Ad9040</name>
      <uri>https://old.reddit.com/user/Physical_Ad9040</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"&gt; &lt;img alt="Google's CLI DOES use your prompting data" src="https://preview.redd.it/j1km6ff1h69f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=183f6cf57cbd408bb1e17247c8aba72d8086d1a3" title="Google's CLI DOES use your prompting data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical_Ad9040"&gt; /u/Physical_Ad9040 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j1km6ff1h69f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T01:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll1yjh</id>
    <title>LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA 🚀</title>
    <updated>2025-06-26T14:44:50+00:00</updated>
    <author>
      <name>/u/Additional_Top1210</name>
      <uri>https://old.reddit.com/user/Additional_Top1210</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"&gt; &lt;img alt="LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA 🚀" src="https://external-preview.redd.it/GPs8oonK03Al4q6HtUFhFxh4J-39nPu_HZOBEQOCcn8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faddbc4424a43d6c2043b2d74892e39170e98392" title="LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper Link: &lt;a href="https://huggingface.co/papers/2506.16406"&gt;https://huggingface.co/papers/2506.16406&lt;/a&gt; Project Link: &lt;a href="https://jerryliang24.github.io/DnD/"&gt;https://jerryliang24.github.io/DnD/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Top1210"&gt; /u/Additional_Top1210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ll1yjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T14:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll57uz</id>
    <title>Gemma 3n is on out on Hugging Face!</title>
    <updated>2025-06-26T16:52:30+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just dropped the perfect local model!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4"&gt;https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/gemma3n"&gt;https://huggingface.co/blog/gemma3n&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll57uz/gemma_3n_is_on_out_on_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll57uz/gemma_3n_is_on_out_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll57uz/gemma_3n_is_on_out_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkzynl</id>
    <title>The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)</title>
    <updated>2025-06-26T13:19:50+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"&gt; &lt;img alt="The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)" src="https://external-preview.redd.it/1wJhDztWCANroswcLW3p5i3oMCiTskJ82JKTdTfiCRM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6977975d5861c60901c746f5374dd709bf8cb89" title="The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running GPUs in virtual machines for AI workloads is quickly becoming the golden standard - especially for isolation, orchestration, and multi-tenant setups. So I decided to measure the actual performance penalty of this approach.&lt;/p&gt; &lt;p&gt;I benchmarked some LLMs (via ollama-benchmark) on an AMD RX 9060 XT 16GB - first on bare metal Ubuntu 24.04, then in a VM (Ubuntu 24.04) running under AI Linux (Sbnb Linux) with GPU passthrough via &lt;code&gt;vfio-pci&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Models tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistral:7b&lt;/li&gt; &lt;li&gt;gemma2:9b&lt;/li&gt; &lt;li&gt;phi4:14b&lt;/li&gt; &lt;li&gt;deepseek-r1:14b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;VM performance was just &lt;strong&gt;1–2% slower&lt;/strong&gt; than bare metal. That’s it. Practically a rounding error.&lt;/p&gt; &lt;p&gt;So… yeah. Turns out GPU passthrough isn’t the scary performance killer.&lt;/p&gt; &lt;p&gt;👉 I put together the full setup, AMD ROCm install steps, benchmark commands, results, and even a diagram - all in this README: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-GPU-PASSTHROUGH-BENCHMARK.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-GPU-PASSTHROUGH-BENCHMARK.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or help if you’re setting up something similar!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lkzynl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T13:19:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll68iz</id>
    <title>Gemma 3n Full Launch - Developers Edition</title>
    <updated>2025-06-26T17:31:27+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Today we have the full launch of Gemma 3n, meaning we have support for your favorite tools as well as full support for its capabilities &lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/"&gt;https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recap&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Audio, video, image, and text input; text output&lt;/li&gt; &lt;li&gt;E2B and E4B - while their raw parameter count is 5B and 8B, you can operate them with as little as 2B and 4B effective params&lt;/li&gt; &lt;li&gt;MatFormer: The model architecture allows extracting submodels and doing mix-n-match, allowing you to export additional models in your favorite size between 2B and 4B.&lt;/li&gt; &lt;li&gt;MobileNetV5 and a new audio encoder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And now...for supported tools. We collaborated with many many open source developers to enable its capabilities. So you can now use Gemma in Hugging Face, Kaggle, llama.cpp, Ollama, MLX, LMStudio, transformers.js, Docker model hub, Unsloth, transformers trl and PEFT, VLLM, SGLang, Jetson AI Lab, and many others. Enjoy! We'll also host a Kaggle competition if anyone wants to join &lt;a href="https://www.kaggle.com/competitions/google-gemma-3n-hackathon"&gt;https://www.kaggle.com/competitions/google-gemma-3n-hackathon&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hugging Face &lt;a href="https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4"&gt;https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Unsloth &lt;a href="https://unsloth.ai/blog/gemma-3n"&gt;https://unsloth.ai/blog/gemma-3n&lt;/a&gt;&lt;/li&gt; &lt;li&gt;HF blog &lt;a href="https://huggingface.co/blog/gemma3n"&gt;https://huggingface.co/blog/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;LMStudio &lt;a href="https://lmstudio.ai/models/google/gemma-3n-e4b"&gt;https://lmstudio.ai/models/google/gemma-3n-e4b&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Ollama &lt;a href="https://ollama.com/library/gemma3n"&gt;https://ollama.com/library/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;AI Studio &lt;a href="http://ai.dev"&gt;ai.dev&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Kaggle &lt;a href="https://www.kaggle.com/models/google/gemma-3n"&gt;https://www.kaggle.com/models/google/gemma-3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;MLX &lt;a href="https://huggingface.co/collections/mlx-community/gemma-3n-685d6c8d02d7486c7e77a7dc"&gt;https://huggingface.co/collections/mlx-community/gemma-3n-685d6c8d02d7486c7e77a7dc&lt;/a&gt; &lt;/li&gt; &lt;li&gt;ONNX/transformers.js &lt;a href="https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX"&gt;https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Vertex &lt;a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n"&gt;https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;GGUF &lt;a href="https://huggingface.co/collections/ggml-org/gemma-3n-685d6fc0843071be9e77b6f7"&gt;https://huggingface.co/collections/ggml-org/gemma-3n-685d6fc0843071be9e77b6f7&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkz0hg</id>
    <title>Meta wins AI copyright lawsuit as US judge rules against authors | Meta</title>
    <updated>2025-06-26T12:35:26+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"&gt; &lt;img alt="Meta wins AI copyright lawsuit as US judge rules against authors | Meta" src="https://external-preview.redd.it/P24oFDRu9fwfx1j87kht5i8PPJV3CyEIC0aLVuyN_0U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5020fe75b422d099598cd47f46c61ccb4e8bea63" title="Meta wins AI copyright lawsuit as US judge rules against authors | Meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theguardian.com/technology/2025/jun/26/meta-wins-ai-copyright-lawsuit-as-us-judge-rules-against-authors"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T12:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll38zu</id>
    <title>FLUX.1 Kontext [dev] - an open weights model for proprietary-level image editing performance.</title>
    <updated>2025-06-26T15:35:49+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;weights: &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev"&gt;https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;release news: &lt;a href="https://x.com/bfl_ml/status/1938257909726519640"&gt;https://x.com/bfl_ml/status/1938257909726519640&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll429p</id>
    <title>gemma 3n has been released on huggingface</title>
    <updated>2025-06-26T16:07:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E2B"&gt;https://huggingface.co/google/gemma-3n-E2B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E2B-it"&gt;https://huggingface.co/google/gemma-3n-E2B-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B"&gt;https://huggingface.co/google/gemma-3n-E4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B-it"&gt;https://huggingface.co/google/gemma-3n-E4B-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(You can find benchmark results such as HellaSwag, MMLU, or LiveCodeBench above)&lt;/p&gt; &lt;p&gt;llama.cpp implementation by &lt;a href="https://github.com/ngxson"&gt;&lt;strong&gt;ngxson&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14400"&gt;https://github.com/ggml-org/llama.cpp/pull/14400&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF"&gt;https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF"&gt;https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical announcement:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/"&gt;https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll6jo5</id>
    <title>DeepSeek R2 delayed</title>
    <updated>2025-06-26T17:43:13+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"&gt; &lt;img alt="DeepSeek R2 delayed" src="https://preview.redd.it/718m48of6b9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b5423692617bfdf316daec6232ca857bc69416c" title="DeepSeek R2 delayed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Over the past several months, DeepSeek's engineers have been working to refine R2 until Liang gives the green light for release, according to The Information. However, a fast adoption of R2 could be difficult due to a shortage of Nvidia server chips in China as a result of U.S. export regulations, the report said, citing employees of top Chinese cloud firms that offer DeepSeek's models to enterprise customers.&lt;/p&gt; &lt;p&gt;A potential surge in demand for R2 would overwhelm Chinese cloud providers, who need advanced Nvidia chips to run AI models, the report said.&lt;/p&gt; &lt;p&gt;DeepSeek did not immediately respond to a Reuters request for comment.&lt;/p&gt; &lt;p&gt;DeepSeek has been in touch with some Chinese cloud companies, providing them with technical specifications to guide their plans for hosting and distributing the model from their servers, the report said.&lt;/p&gt; &lt;p&gt;Among its cloud customers currently using R1, the majority are running the model with Nvidia's H20 chips, The Information said.&lt;/p&gt; &lt;p&gt;Fresh export curbs imposed by the Trump administration in April have prevented Nvidia from selling in the Chinese market its H20 chips - the only AI processors it could legally export to the country at the time.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Sources : &lt;a href="https://www.theinformation.com/articles/deepseeks-progress-stalled-u-s-export-controls"&gt;[1]&lt;/a&gt; &lt;a href="https://x.com/kimmonismus/status/1938221881175183740"&gt;[2]&lt;/a&gt; &lt;a href="https://www.reuters.com/world/china/deepseek-r2-launch-stalled-ceo-balks-progress-information-reports-2025-06-26/"&gt;[3]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/718m48of6b9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:43:13+00:00</published>
  </entry>
</feed>
