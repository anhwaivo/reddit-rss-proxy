<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-02T00:27:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jp7o2i</id>
    <title>workflow for recording audio/video, transcript and automatic document generation</title>
    <updated>2025-04-01T21:39:04+00:00</updated>
    <author>
      <name>/u/Dazzling-Gift7189</name>
      <uri>https://old.reddit.com/user/Dazzling-Gift7189</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I need to create a set of video tutorials (and doc/pdf version) on how to use a non-public facing application, and i'm not allowed to send the data to any cloud service.&lt;/p&gt; &lt;p&gt;I was thinking to implement the following workflow:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use OBS(i'm working on mac) to capture screen and audio/voice&lt;/li&gt; &lt;li&gt;Use whisper transcription to create the transcription&lt;/li&gt; &lt;li&gt;Use some local llm to organize the doc and generate output in sphinx format&lt;/li&gt; &lt;li&gt;Once in sphinx format i'll double check and adjust the output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, my questions are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;did someone had a similar use case? How do you deal with it?&lt;/li&gt; &lt;li&gt;what local llm is better to use?&lt;/li&gt; &lt;li&gt;Is there any local app/model i can use that takes i input the audio/file and create the doc with also screenshots? Currently, i have to add them manually when editing the sphinx format, but it would be nice to have them already there.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dazzling-Gift7189"&gt; /u/Dazzling-Gift7189 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp7o2i/workflow_for_recording_audiovideo_transcript_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp7o2i/workflow_for_recording_audiovideo_transcript_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp7o2i/workflow_for_recording_audiovideo_transcript_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T21:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jonibh</id>
    <title>OpenWebUI Adopt OpenAPI and offer an MCP bridge</title>
    <updated>2025-04-01T04:45:39+00:00</updated>
    <author>
      <name>/u/coding_workflow</name>
      <uri>https://old.reddit.com/user/coding_workflow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open Web Ui 0.6 is adoption OpenAPI instead of MCP but offer a bridge.&lt;br /&gt; Release notes: &lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;br /&gt; MCO Bridge: &lt;a href="https://github.com/open-webui/mcpo"&gt;https://github.com/open-webui/mcpo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coding_workflow"&gt; /u/coding_workflow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jonibh/openwebui_adopt_openapi_and_offer_an_mcp_bridge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jonibh/openwebui_adopt_openapi_and_offer_an_mcp_bridge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jonibh/openwebui_adopt_openapi_and_offer_an_mcp_bridge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T04:45:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp96eq</id>
    <title>Powering Multiple GPUs with multiple PSUs</title>
    <updated>2025-04-01T22:43:52+00:00</updated>
    <author>
      <name>/u/eagle6705</name>
      <uri>https://old.reddit.com/user/eagle6705</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I was sent here by the home labbers.&lt;/p&gt; &lt;p&gt;And no this isnt a mining rig, its an application that is in development that is going to develop AI to process protein sequences. End goal is to throw in h100s on an actual server and not some workstation) For now this is what was given to me to work with as a proof of concept. I need to develop a rig to power many gpus for one system. (at least 3)&lt;/p&gt; &lt;p&gt;I was asking a question on how cryptominers power multiple GPUs and they said you guys would be using the same setup. So this is a question on how to power multiple GPUS when the one main unit won't be able to power all of them.&lt;/p&gt; &lt;p&gt;Long story short, i will have 1 4090, and 3 4070 pcie cards in one motherboard. However we obviously don't have the power.&lt;/p&gt; &lt;p&gt;I was looking at the following to use multiple GPUs &lt;a href="https://www.amazon.com/ADD2PSU-Connector-Multiple-Adapter-Synchronous/dp/B09Q11WG4Z/?_encoding=UTF8&amp;amp;pd_rd_w=fQ8L3&amp;amp;content-id=amzn1.sym.255b3518-6e7f-495c-8611-30a58648072e%3Aamzn1.symc.a68f4ca3-28dc-4388-a2cf-24672c480d8f&amp;amp;pf_rd_p=255b3518-6e7f-495c-8611-30a58648072e&amp;amp;pf_rd_r=1YT4D5S3ER7MYTAN393A&amp;amp;pd_rd_wg=fGg7k&amp;amp;pd_rd_r=501f521f-069c-47dc-8b0a-cf212a639286&amp;amp;ref_=pd_hp_d_atf_ci_mcx_mr_ca_hp_atf_d"&gt;https://www.amazon.com/ADD2PSU-Connector-Multiple-Adapter-Synchronous/dp/B09Q11WG4Z/?_encoding=UTF8&amp;amp;pd_rd_w=fQ8L3&amp;amp;content-id=amzn1.sym.255b3518-6e7f-495c-8611-30a58648072e%3Aamzn1.symc.a68f4ca3-28dc-4388-a2cf-24672c480d8f&amp;amp;pf_rd_p=255b3518-6e7f-495c-8611-30a58648072e&amp;amp;pf_rd_r=1YT4D5S3ER7MYTAN393A&amp;amp;pd_rd_wg=fGg7k&amp;amp;pd_rd_r=501f521f-069c-47dc-8b0a-cf212a639286&amp;amp;ref_=pd_hp_d_atf_ci_mcx_mr_ca_hp_atf_d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Basically I want to know how you would be powering them. ANd yes my system can handle it as it had 4 single slot gpus as a proof of concept. we just need to expand now and get more power.&lt;/p&gt; &lt;p&gt;And yes I can buy that thing I linked but I&amp;quot;m just looking into how to run multiple psus or the methods you guys use reliably. obviously i'm using some corsairs but its the matter of getting them to work as one is what I don't really know what to do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eagle6705"&gt; /u/eagle6705 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp96eq/powering_multiple_gpus_with_multiple_psus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp96eq/powering_multiple_gpus_with_multiple_psus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp96eq/powering_multiple_gpus_with_multiple_psus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T22:43:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp9hu6</id>
    <title>Why isn't the whole industry focusing on online-learning?</title>
    <updated>2025-04-01T22:58:03+00:00</updated>
    <author>
      <name>/u/unraveleverything</name>
      <uri>https://old.reddit.com/user/unraveleverything</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs (currently) have no memory. You will always be able to tell LLMs from humans because LLMs are stateless. Right now you basically have a bunch of hacks like system prompts and RAG that tries to make it resemble something its not. &lt;/p&gt; &lt;p&gt;So what about concurrent multi-(Q)LoRA serving? Tell me why there's seemingly no research in this direction? &amp;quot;AGI&amp;quot; to me seems as simple as freezing the base weights, then training 1-pass over the context for memory. Like say your goal is to understand a codebase. Just train a LoRA on 1 pass through that codebase? First you give it the folder/file structure then the codebase. Tell me why this woudn't work. Then 1 node can handle multiple concurrent users and by storing 1 small LoRA for each user.&lt;/p&gt; &lt;p&gt;&lt;a href="https://gitingest.com/microsoft/LoRA"&gt;Ex&lt;/a&gt;: ``` Directory structure: └── microsoft-lora/ ├── README.md ├── LICENSE.md ├── SECURITY.md ├── setup.py ├── examples/ │ ├── NLG/ │ │ ├── README.md ...&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;h1&gt;File: README.md&lt;/h1&gt; &lt;h1&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/h1&gt; &lt;p&gt;This repo contains the source code of the Python package &lt;code&gt;loralib&lt;/code&gt; and several examples of how to integrate it with PyTorch models, such as those in Hugging Face. We only support PyTorch for now. See our paper for a detailed description of LoRA. ...&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;h1&gt;File: LICENSE.md&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;MIT License Copyright (c) Microsoft Corporation. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &amp;quot;Software&amp;quot;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;... ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unraveleverything"&gt; /u/unraveleverything &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9hu6/why_isnt_the_whole_industry_focusing_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9hu6/why_isnt_the_whole_industry_focusing_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9hu6/why_isnt_the_whole_industry_focusing_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T22:58:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp6xsn</id>
    <title>LM Studio gets stuck loading at 97%?</title>
    <updated>2025-04-01T21:08:26+00:00</updated>
    <author>
      <name>/u/intimate_sniffer69</name>
      <uri>https://old.reddit.com/user/intimate_sniffer69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6xsn/lm_studio_gets_stuck_loading_at_97/"&gt; &lt;img alt="LM Studio gets stuck loading at 97%?" src="https://preview.redd.it/2e3d43qggase1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1a989673ccf05c6e11687f6ef2c7fc5c6ad53c1" title="LM Studio gets stuck loading at 97%?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nothing special here, just downloaded LM studio fresh install on Windows 11, and downloaded a model called Stheno v3.2, which installed in a minute flat. But it won't load, and hangs at 97%, just never finishes what could cause this to happen? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intimate_sniffer69"&gt; /u/intimate_sniffer69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2e3d43qggase1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6xsn/lm_studio_gets_stuck_loading_at_97/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6xsn/lm_studio_gets_stuck_loading_at_97/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T21:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jobybk</id>
    <title>OpenAI is open-sourcing a model soon</title>
    <updated>2025-03-31T19:36:01+00:00</updated>
    <author>
      <name>/u/MysteriousPayment536</name>
      <uri>https://old.reddit.com/user/MysteriousPayment536</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is taking feedback for open source model. They will probably release o3-mini based on a poll by Sam Altman in February. &lt;a href="https://x.com/sama/status/1891667332105109653"&gt;https://x.com/sama/status/1891667332105109653&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MysteriousPayment536"&gt; /u/MysteriousPayment536 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/open-model-feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jobybk/openai_is_opensourcing_a_model_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jobybk/openai_is_opensourcing_a_model_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T19:36:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp1sy8</id>
    <title>Smallest model capable of detecting profane/nsfw language?</title>
    <updated>2025-04-01T17:44:27+00:00</updated>
    <author>
      <name>/u/ohcrap___fk</name>
      <uri>https://old.reddit.com/user/ohcrap___fk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I have my first ever steam game about to be released in a week which I couldn't be more excited/nervous about. It is a singleplayer game but I have a global chat that allows people to talk to other people playing. It's a space game, and space is lonely, so I thought that'd be a fun aesthetic.&lt;/p&gt; &lt;p&gt;Anyways, it is in beta-testing phase right now and I had to ban someone for the first time today because of things they were saying over chat. It was a manual process and I'd like to automate the detection/flagging of unsavory messages.&lt;/p&gt; &lt;p&gt;Are &amp;lt;1b parameter models capable of outperforming a simple keyword check? I like the idea of an LLM because it could go beyond matching strings.&lt;/p&gt; &lt;p&gt;Also, if anyone is interested in trying it out, I'm handing out keys like crazy because I'm too nervous to charge $2.99 for the game and then underdeliver. Game info &lt;a href="https://x.com/hvent90"&gt;here&lt;/a&gt;, sorry for the self-promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ohcrap___fk"&gt; /u/ohcrap___fk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1sy8/smallest_model_capable_of_detecting_profanensfw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T17:44:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpavy5</id>
    <title>🧠 Symbolic Memory Loops for Local LLMs – Reflection-Based Continuity Using YAML + Journaling Tools (Now on GitHub)</title>
    <updated>2025-04-02T00:00:54+00:00</updated>
    <author>
      <name>/u/BABI_BOOI_ayyyyyyy</name>
      <uri>https://old.reddit.com/user/BABI_BOOI_ayyyyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I wanted to share a project I’ve been working on for a bit. It’s an experiment in creating &lt;strong&gt;symbolic memory loops&lt;/strong&gt; for local LLMs (e.g. Nous-Hermes-7B GPTQ), built around:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;📝 &lt;strong&gt;Reflections&lt;/strong&gt;: automatically condensed memory entries (&lt;code&gt;reflections.txt&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;🧠 &lt;strong&gt;YAML persona scaffolding&lt;/strong&gt;: updated with symbolic context&lt;/li&gt; &lt;li&gt;🧪 &lt;strong&gt;Stress testing&lt;/strong&gt;: recursive prompt loops to explore continuity fatigue&lt;/li&gt; &lt;li&gt;🩹 &lt;strong&gt;Recovery via breaks&lt;/strong&gt;: guided symbolic decompression&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All tools are local, lightweight, and run fine on 6GB VRAM.&lt;br /&gt; The repo includes real experiment logs, token traces, and even the stress collapse sequence (I called it “The Gauntlet”).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of embedding-based memory, I wanted to test if a model could develop a &lt;em&gt;sense of symbolic continuity&lt;/em&gt; over time using just structured inputs, reflection scaffolds, and self-authored memory hooks.&lt;/p&gt; &lt;p&gt;This project isn’t trying to simulate sentience. It’s not about agents.&lt;br /&gt; It’s about seeing what happens when LLMs are given tools to &lt;strong&gt;reflect&lt;/strong&gt;, &lt;strong&gt;recover&lt;/strong&gt;, and carry symbolic weight between sessions.&lt;/p&gt; &lt;p&gt;🧠 Repo: &lt;a href="https://github.com/babibooi/symbolic-memory-loop"&gt;github.com/babibooi/symbolic-memory-loop&lt;/a&gt;&lt;br /&gt; ☕ Ko-fi: &lt;a href="http://ko-fi.com/babibooi"&gt;ko-fi.com/babibooi&lt;/a&gt; (I’m trying to survive this month lol)&lt;/p&gt; &lt;p&gt;If you’re also experimenting with long-term memory strategies or symbolic persistence, I’d love to swap notes. And if you just want to poke at poetic spaghetti held together by YAML and recursion? That’s there too.&lt;/p&gt; &lt;p&gt;Thanks!&lt;br /&gt; – Booi :3c&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BABI_BOOI_ayyyyyyy"&gt; /u/BABI_BOOI_ayyyyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpavy5/symbolic_memory_loops_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpavy5/symbolic_memory_loops_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpavy5/symbolic_memory_loops_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-02T00:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jojuf4</id>
    <title>Is everyone ready for all of the totally legit AI tools &amp; models being released tomorrow?</title>
    <updated>2025-04-01T01:25:55+00:00</updated>
    <author>
      <name>/u/C_Coffie</name>
      <uri>https://old.reddit.com/user/C_Coffie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I heard Llama 4 is finally coming tomorrow!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/C_Coffie"&gt; /u/C_Coffie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jojuf4/is_everyone_ready_for_all_of_the_totally_legit_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jojuf4/is_everyone_ready_for_all_of_the_totally_legit_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jojuf4/is_everyone_ready_for_all_of_the_totally_legit_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T01:25:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp6sbn</id>
    <title>Dou (道) updated with LM Studio (and Ollama) support</title>
    <updated>2025-04-01T21:02:13+00:00</updated>
    <author>
      <name>/u/shokuninstudio</name>
      <uri>https://old.reddit.com/user/shokuninstudio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6sbn/dou_道_updated_with_lm_studio_and_ollama_support/"&gt; &lt;img alt="Dou (道) updated with LM Studio (and Ollama) support" src="https://preview.redd.it/1i6mjuscfase1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6961bce4f75454ecad32ef613c80eef4630f8da9" title="Dou (道) updated with LM Studio (and Ollama) support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shokuninstudio"&gt; /u/shokuninstudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1i6mjuscfase1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6sbn/dou_道_updated_with_lm_studio_and_ollama_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp6sbn/dou_道_updated_with_lm_studio_and_ollama_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T21:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp9emz</id>
    <title>Easy Whisper UI for Windows</title>
    <updated>2025-04-01T22:54:08+00:00</updated>
    <author>
      <name>/u/mehtabmahir</name>
      <uri>https://old.reddit.com/user/mehtabmahir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an easy to use UI for Whisper on windows. It is completely made with C++ and has Vulkan support for all gpus. I posted it here recently, but I've since made several major improvements. Please let me know your results, the installer should handle absolutely everything for you!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mehtabmahir/easy-whisper-ui"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehtabmahir"&gt; /u/mehtabmahir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9emz/easy_whisper_ui_for_windows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9emz/easy_whisper_ui_for_windows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9emz/easy_whisper_ui_for_windows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T22:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1joxqul</id>
    <title>Tenstorrent's Big Quiet Box of AI</title>
    <updated>2025-04-01T15:01:14+00:00</updated>
    <author>
      <name>/u/muchcharles</name>
      <uri>https://old.reddit.com/user/muchcharles</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joxqul/tenstorrents_big_quiet_box_of_ai/"&gt; &lt;img alt="Tenstorrent's Big Quiet Box of AI" src="https://external-preview.redd.it/Wq00H3F0hVMmkafgNYhCQdrVKy3_PQGmzAF1Qji8D1c.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2780c866e0ac3e9f4c44759d99f39bf0d304170d" title="Tenstorrent's Big Quiet Box of AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muchcharles"&gt; /u/muchcharles &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://m.youtube.com/watch?v=vWw-1bk7k2c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joxqul/tenstorrents_big_quiet_box_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joxqul/tenstorrents_big_quiet_box_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1jogfrz</id>
    <title>Open-source search repo beats GPT-4o Search, Perplexity Sonar Reasoning Pro on FRAMES</title>
    <updated>2025-03-31T22:42:26+00:00</updated>
    <author>
      <name>/u/jiMalinka</name>
      <uri>https://old.reddit.com/user/jiMalinka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jogfrz/opensource_search_repo_beats_gpt4o_search/"&gt; &lt;img alt="Open-source search repo beats GPT-4o Search, Perplexity Sonar Reasoning Pro on FRAMES" src="https://preview.redd.it/q2nifllfs3se1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=734a6613dfcc4ffda59b820ed615cb2ac184b109" title="Open-source search repo beats GPT-4o Search, Perplexity Sonar Reasoning Pro on FRAMES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/sentient-agi/OpenDeepSearch"&gt;https://github.com/sentient-agi/OpenDeepSearch&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Pretty simple to plug-and-play – nice combo of techniques (react / codeact / dynamic few-shot) integrated with search / calculator tools. I guess that’s all you need to beat SOTA billion dollar search companies :) Probably would be super interesting / useful to use with multi-agent workflows too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiMalinka"&gt; /u/jiMalinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q2nifllfs3se1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jogfrz/opensource_search_repo_beats_gpt4o_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jogfrz/opensource_search_repo_beats_gpt4o_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-31T22:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp5g2l</id>
    <title>Is a multimodal focused release from openai the best for us?</title>
    <updated>2025-04-01T20:08:29+00:00</updated>
    <author>
      <name>/u/AryanEmbered</name>
      <uri>https://old.reddit.com/user/AryanEmbered</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5g2l/is_a_multimodal_focused_release_from_openai_the/"&gt; &lt;img alt="Is a multimodal focused release from openai the best for us?" src="https://preview.redd.it/w31a75fy5ase1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf8159e2e72f93f2c1e7edfd8a2bb4a73c81275c" title="Is a multimodal focused release from openai the best for us?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like with the exception of Qwen 2.5 7b(11b) audio, we have seen almost no real progress in multimodality so far in open models.&lt;/p&gt; &lt;p&gt;It seems gippty 4o mini can now do advanced voice mode as well. &lt;/p&gt; &lt;p&gt;They keep saying its a model that can run on your hardware, and 4omini is estimated to be less than a 20B model consider how badly it gets mogged by mistral smol and others. &lt;/p&gt; &lt;p&gt;It would be great if we can get a shittier 4o mini but with all the features intact like audio and image output. (A llamalover can dream)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AryanEmbered"&gt; /u/AryanEmbered &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w31a75fy5ase1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5g2l/is_a_multimodal_focused_release_from_openai_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5g2l/is_a_multimodal_focused_release_from_openai_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T20:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1joyigi</id>
    <title>GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning</title>
    <updated>2025-04-01T15:32:33+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyigi/gemmacoder312b_finetuning_gemma_3_for_code/"&gt; &lt;img alt="GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning" src="https://external-preview.redd.it/gH2ta8Ny0Bg1Qm8qZdfZlafv4Sz_L1pzxh-y3yKJtZ8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2914dd3fff11503f8f5f868b03abfbe2d8a5ee73" title="GemmaCoder3-12b: Fine-Tuning Gemma 3 for Code Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/burtenshaw/google-gemma3-gemma-code"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyigi/gemmacoder312b_finetuning_gemma_3_for_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joyigi/gemmacoder312b_finetuning_gemma_3_for_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jpa1ep</id>
    <title>I got tired of guessing what blackbox AI coding tools were sending as prompt context... so I built a transparent local open-source coding tool</title>
    <updated>2025-04-01T23:22:03+00:00</updated>
    <author>
      <name>/u/wwwillchen</name>
      <uri>https://old.reddit.com/user/wwwillchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"&gt; &lt;img alt="I got tired of guessing what blackbox AI coding tools were sending as prompt context... so I built a transparent local open-source coding tool" src="https://external-preview.redd.it/bjBlY3dlMHYyYnNlMeuto_4yHK9N3Xzvw4yI_cUvoQNTs3J3u5A3WEOq9BHN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a88ce2fe0beae410002f53b85b4dc4272dd04a98" title="I got tired of guessing what blackbox AI coding tools were sending as prompt context... so I built a transparent local open-source coding tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Cursor &amp;amp; GitHub Copilot and found it frustrating that I couldn't see what prompts were actually being sent.&lt;/p&gt; &lt;p&gt;For example, I have no idea why I got wildly different results when I sent the same prompt to Cursor vs ChatGPT with o3-mini, where the Cursor response was much shorter (and also incorrect) compared to ChatGPT's.&lt;/p&gt; &lt;p&gt;So, I've built a new open-source AI coding tool Dyad that runs locally: &lt;a href="https://github.com/dyad-sh/dyad"&gt;https://github.com/dyad-sh/dyad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It just got a new LLM debugging page that shows exactly what’s being sent to the model, so you can finally understand why the LLM is responding the way it does.&lt;/p&gt; &lt;p&gt;More demos of the tool here: &lt;a href="https://dyad.sh/"&gt;https://dyad.sh/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think. Is this useful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wwwillchen"&gt; /u/wwwillchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8xw67g0v2bse1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jpa1ep/i_got_tired_of_guessing_what_blackbox_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T23:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp77z1</id>
    <title>Arch-Function-Chat (1B/3B/7B) - Device friendly, family of fast LLMs for function calling scenarios now trained to chat.</title>
    <updated>2025-04-01T21:20:18+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on feedback from users and the developer community that used Arch-Function (our previous gen) model, I am excited to share our latest work: &lt;a href="https://huggingface.co/katanemo/Arch-Function-Chat-3B"&gt;Arch-Function-Chat&lt;/a&gt; A collection of fast, device friendly LLMs that achieve performance on-par with GPT-4 on function calling, now trained to chat.&lt;/p&gt; &lt;p&gt;These LLMs have three additional training objectives.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Be able to refine and clarify the user request. This means to ask for required function parameters, clarify ambiguous input (e.g., &amp;quot;Transfer $500&amp;quot; without specifying accounts, can be “Transfer from” and “Transfer to”)&lt;/li&gt; &lt;li&gt;Accurately maintain context in two specific scenarios: &lt;ol&gt; &lt;li&gt;Progressive information disclosure such as in multi-turn conversations where information is revealed gradually (i.e., the model asks info of multiple parameters and the user only answers one or two instead of all the info)&lt;/li&gt; &lt;li&gt;Context switch where the model must infer missing parameters from context (e.g., &amp;quot;Check the weather&amp;quot; should prompt for location if not provided) and maintains context between turns (e.g., &amp;quot;What about tomorrow?&amp;quot; after a weather query but still in the middle of clarification)&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Respond to the user based on executed tools results. For common function calling scenarios where the response of the execution is all that's needed to complete the user request, Arch-Function-Chat can interpret and respond to the user via chat. Note, parallel and multiple function calling was already supported so if the model needs to respond based on multiple tools call it still can.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Of course the 3B model will now be the primary LLM used in &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;. Hope you all like the work 🙏. Happy building!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp77z1/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp77z1/archfunctionchat_1b3b7b_device_friendly_family_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp77z1/archfunctionchat_1b3b7b_device_friendly_family_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T21:20:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1josy27</id>
    <title>An idea: an LLM trapped in the past</title>
    <updated>2025-04-01T11:09:36+00:00</updated>
    <author>
      <name>/u/Vehnum</name>
      <uri>https://old.reddit.com/user/Vehnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone ever thought to make an LLM trained on data from before a certain year/time?&lt;/p&gt; &lt;p&gt;For example, an LLM trained on data only from 2010 or prior.&lt;/p&gt; &lt;p&gt;I thought it was an interesting concept but I don’t know if it had been thought of or done before. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vehnum"&gt; /u/Vehnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1josy27/an_idea_an_llm_trapped_in_the_past/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T11:09:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp9tfh</id>
    <title>🪿Qwerky-72B and 32B : Training large attention free models, with only 8 GPU's</title>
    <updated>2025-04-01T23:12:05+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9tfh/qwerky72b_and_32b_training_large_attention_free/"&gt; &lt;img alt="🪿Qwerky-72B and 32B : Training large attention free models, with only 8 GPU's" src="https://preview.redd.it/hzuxqeqn2bse1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=687e079083a01b404da217fc45bd385974523d62" title="🪿Qwerky-72B and 32B : Training large attention free models, with only 8 GPU's" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hzuxqeqn2bse1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9tfh/qwerky72b_and_32b_training_large_attention_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp9tfh/qwerky72b_and_32b_training_large_attention_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T23:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1joyl9t</id>
    <title>New GGUF quants of V3-0324</title>
    <updated>2025-04-01T15:35:49+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyl9t/new_gguf_quants_of_v30324/"&gt; &lt;img alt="New GGUF quants of V3-0324" src="https://external-preview.redd.it/VVDuLhNJdXUv9Ha7btms0J33I6ffqYD7axOIbyejSC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb260f4a149e4b5107b97b86ee6df9cf84939894" title="New GGUF quants of V3-0324" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I cooked up these fresh new quants on &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;ikawrakow/ik_llama.cpp&lt;/a&gt; supporting 32k+ context in under 24GB VRAM with MLA with highest quality tensors for attention/dense layers/shared experts.&lt;/p&gt; &lt;p&gt;Good both for CPU+GPU or CPU only rigs with optimized repacked quant flavours to get the most out of your RAM.&lt;/p&gt; &lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: These quants only work with &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork and won't work with mainline llama.cpp, ollama, lm studio, koboldcpp, etc.&lt;/p&gt; &lt;p&gt;Shout out to &lt;a href="https://www.youtube.com/c/level1techs"&gt;level1techs&lt;/a&gt; for supporting this research on some sweet hardware rigs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/DeepSeek-V3-0324-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joyl9t/new_gguf_quants_of_v30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joyl9t/new_gguf_quants_of_v30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp5y5a</id>
    <title>Different LLM models make different sounds from the GPU when doing inference</title>
    <updated>2025-04-01T20:28:29+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bsky.app/profile/victor.earth/post/3llrphluwb22p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5y5a/different_llm_models_make_different_sounds_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp5y5a/different_llm_models_make_different_sounds_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T20:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1joqnp0</id>
    <title>Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)</title>
    <updated>2025-04-01T08:28:37+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"&gt; &lt;img alt="Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)" src="https://preview.redd.it/lbaxwpako6se1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fe2ebb66027a7c9112a0c9566eaf397ca2d5a18" title="Top reasoning LLMs failed horribly on USA Math Olympiad (maximum 5% score)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to share something that’s blown my mind today. I just came across &lt;a href="https://arxiv.org/abs/2503.21934v1"&gt;this paper &lt;/a&gt;evaluating state-of-the-art LLMs (like O3-MINI, Claude 3.7, etc.) on the 2025 USA Mathematical Olympiad (USAMO). And let me tell you—this is &lt;em&gt;wild&lt;/em&gt; .&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;p&gt;These models were tested on &lt;strong&gt;six proof-based math problems&lt;/strong&gt; from the 2025 USAMO. Each problem was scored out of 7 points, with a max total score of 42. Human experts graded their solutions rigorously.&lt;/p&gt; &lt;p&gt;The highest average score achieved by &lt;strong&gt;any model&lt;/strong&gt; ? &lt;strong&gt;Less than 5%.&lt;/strong&gt; Yes, you read that right: &lt;strong&gt;5%.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Even worse, when these models tried grading their own work (e.g., O3-MINI and Claude 3.7), they consistently &lt;strong&gt;overestimated their scores&lt;/strong&gt; , inflating them by up to &lt;strong&gt;20x&lt;/strong&gt; compared to human graders.&lt;/p&gt; &lt;h1&gt;Why This Matters&lt;/h1&gt; &lt;p&gt;These models have been trained on &lt;strong&gt;all the math data imaginable&lt;/strong&gt; —IMO problems, USAMO archives, textbooks, papers, etc. They’ve seen it all. Yet, they struggle with tasks requiring deep logical reasoning, creativity, and rigorous proofs.&lt;/p&gt; &lt;p&gt;Here are some key issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Logical Failures&lt;/strong&gt; : Models made unjustified leaps in reasoning or labeled critical steps as &amp;quot;trivial.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lack of Creativity&lt;/strong&gt; : Most models stuck to the same flawed strategies repeatedly, failing to explore alternatives.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grading Failures&lt;/strong&gt; : Automated grading by LLMs inflated scores dramatically, showing they can't even evaluate their own work reliably.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given that billions of dollars have been poured into investments on these models with the hope of it can &amp;quot;generalize&amp;quot; and do &amp;quot;crazy lift&amp;quot; in human knowledge, this result is shocking. Given the models here are probably trained on all Olympiad data previous (USAMO, IMO ,... anything)&lt;/p&gt; &lt;p&gt;Link to the paper: &lt;a href="https://arxiv.org/abs/2503.21934v1"&gt;https://arxiv.org/abs/2503.21934v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lbaxwpako6se1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joqnp0/top_reasoning_llms_failed_horribly_on_usa_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T08:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jotzue</id>
    <title>Just upgraded my RTX 3060 with 192GB of VRAM</title>
    <updated>2025-04-01T12:09:43+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"&gt; &lt;img alt="Just upgraded my RTX 3060 with 192GB of VRAM" src="https://a.thumbs.redditmedia.com/0HRndElj4m4MdUTfleIWN0cAk58xxJsG5xvLYGxCDg0.jpg" title="Just upgraded my RTX 3060 with 192GB of VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soldered in some extra memory chips I had lying around. Runs now Deepseek R1 with 1.6 bits at 8 t/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzmtxp5gs7se1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=68bbae0f177ee26b9e9dd5d80ced43ca1ab364b8"&gt;https://preview.redd.it/rzmtxp5gs7se1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=68bbae0f177ee26b9e9dd5d80ced43ca1ab364b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jotzue/just_upgraded_my_rtx_3060_with_192gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T12:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1joy1g9</id>
    <title>You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! 🤗</title>
    <updated>2025-04-01T15:13:10+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"&gt; &lt;img alt="You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! 🤗" src="https://external-preview.redd.it/cjl0NGVwNTJwOHNlMcYNeeStsI4th9K4vfQkpXTEQka5SvAFbcRXwVJ4maQB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d009dc08fd59bef372f2ca0785fa2ef200fe3ea8" title="You can now check if your Laptop/ Rig can run a GGUF directly from Hugging Face! 🤗" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0bo4dp52p8se1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1joy1g9/you_can_now_check_if_your_laptop_rig_can_run_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T15:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jp1555</id>
    <title>DeepMind will delay sharing research to remain competitive</title>
    <updated>2025-04-01T17:17:47+00:00</updated>
    <author>
      <name>/u/mayalihamur</name>
      <uri>https://old.reddit.com/user/mayalihamur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://archive.ph/tkuum"&gt;recent report&lt;/a&gt; in Financial Times claims that Google's DeepMind &amp;quot;has been holding back the release of its world-renowned research&amp;quot; to remain competitive. Accordingly the company will adopt a six-month embargo policy &amp;quot;before strategic papers related to generative AI are released&amp;quot;. &lt;/p&gt; &lt;p&gt;In an interesting statement, a DeepMind researcher said he could &amp;quot;not imagine us putting out the transformer papers for general use now&amp;quot;. Considering the impact of the DeepMind's transformer research on the development of LLMs, just think where we would have been now if they held back the research. The report also claims that some DeepMind staff left the company as their careers would be negatively affected if they are not allowed to publish their research. &lt;/p&gt; &lt;p&gt;I don't have any knowledge about the current impact of DeepMind's open research contributions. But just a couple of months ago we have been talking about the potential contributions the DeepSeek release will make. But as it gets competitive it looks like the big players are slowly becoming &lt;del&gt;Open&lt;/del&gt;ClosedAIs. &lt;/p&gt; &lt;p&gt;Too bad, let's hope that this won't turn into a general trend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mayalihamur"&gt; /u/mayalihamur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-01T17:17:47+00:00</published>
  </entry>
</feed>
