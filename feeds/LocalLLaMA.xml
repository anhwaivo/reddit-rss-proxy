<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-11T17:34:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hyio5w</id>
    <title>Beginner Guide - Creating LLM Datasets with Python</title>
    <updated>2025-01-10T23:50:58+00:00</updated>
    <author>
      <name>/u/0xlisykes</name>
      <uri>https://old.reddit.com/user/0xlisykes</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xlisykes"&gt; /u/0xlisykes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://toolworks.dev/docs/Guides/creating-llm-datasets-python"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyio5w/beginner_guide_creating_llm_datasets_with_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyio5w/beginner_guide_creating_llm_datasets_with_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T23:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy5l18</id>
    <title>Local TTS models that can match ElevenLabs in terms of quality and consistency</title>
    <updated>2025-01-10T14:28:17+00:00</updated>
    <author>
      <name>/u/_megazz</name>
      <uri>https://old.reddit.com/user/_megazz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I should probably start by stating that I'm somewhat new to running AI models locally, but I've tinkered with Ollama + Open WebUI before and was able to get some models running through WSL2 on my RTX 4080 and was pretty impressed with the results.&lt;/p&gt; &lt;p&gt;With that said, I'm now looking for a good local TTS model and I was honestly disappointed with what I could find. Most projects seem to not be updated in months or are simply dead.&lt;/p&gt; &lt;p&gt;From what I've read, the general consensus seems to be that XTTS-v2 is still the best overall model to this day, which is from a startup that has &lt;a href="https://coqui.ai/"&gt;shut down&lt;/a&gt;. I figured I'd try it anyway and I was able to get it running through &lt;a href="https://github.com/daswer123/xtts-webui"&gt;this simple portable version&lt;/a&gt;, but I was honestly disappointed with the results I got, all very inconsistent and not natural sounding, even after tinkering a lot with its different parameters and voices. Not even close to what I can get from ElevenLabs, which could easily pass as real person speaking, but that service is very pricey for me, unfortunately.&lt;/p&gt; &lt;p&gt;There are other popular suggestions like Fish Speech or F5-TTS, but since I need the model to speak Portuguese, that limits my options a lot.&lt;/p&gt; &lt;p&gt;Right now I feel like I'm just wasting my time and that nothing that I can run locally can match EvenLabs currently, but as I said, I'm new to this and maybe I'm missing something obvious. In any case, I'd appreciate any input!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_megazz"&gt; /u/_megazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy5l18/local_tts_models_that_can_match_elevenlabs_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T14:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyvcrg</id>
    <title>Are embeddings invariant to translation?</title>
    <updated>2025-01-11T12:59:33+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. Is the embedding of a sentence close to the embedding of its translation (e.g. EN and DE) in the embedding space? Which embedding models are better in handling multiple languages, and also representing the same semantic in multiple languages?&lt;/p&gt; &lt;p&gt;Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvcrg/are_embeddings_invariant_to_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvcrg/are_embeddings_invariant_to_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvcrg/are_embeddings_invariant_to_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T12:59:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyrke5</id>
    <title>The ASRock Radeon RX 7900 XTX Creator</title>
    <updated>2025-01-11T08:27:26+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People building AI PCs with multiple GPUs on a budget love the RTX 3090 2-slot &amp;quot;Turbo&amp;quot;/&amp;quot;Aero&amp;quot;/&amp;quot;Classic&amp;quot; blower cards that pretty much disappeared from production shortly after the launch of the chip.&lt;/p&gt; &lt;p&gt;That's why i'm surprised these same people (hi!) aren't talking more about the ASRock Radeon RX 7900 XTX Creator card. It's a 2-slot blower card with a single fan. It's 1100€ new so 18% more expensive than the cheapest RX 7900 XTX cards. With a Threadripper mainboard you can easily stick four of these cards (96GB VRAM) into a large PC case without having to deal with PCIe port extenders that can cause instability.&lt;/p&gt; &lt;p&gt;Has someone already done this and want to share? How hard is it to get them cooled? Which case did you use? Which software is best for inferencing with multiple of these AMD GPUs? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyrke5/the_asrock_radeon_rx_7900_xtx_creator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyrke5/the_asrock_radeon_rx_7900_xtx_creator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyrke5/the_asrock_radeon_rx_7900_xtx_creator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T08:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyy91g</id>
    <title>Tweaking settings and choosing a model?</title>
    <updated>2025-01-11T15:28:11+00:00</updated>
    <author>
      <name>/u/AerosolHubris</name>
      <uri>https://old.reddit.com/user/AerosolHubris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hxps2y/transcribing_audio_to_latex/"&gt;posted&lt;/a&gt; about wanting to transcribe my lectures to LaTeX in a way that would be appropriate for distributing notes to students, i.e. making them read more like a textbook and less like a conversation. I got some really good advice in that thread. I decided to move to Markdown and just use Youtube's auto-generated transcript. What has been working for me with ChatGPT is to prompt first with:&lt;/p&gt; &lt;p&gt;&amp;quot;I’m going to paste the transcript from a mathematics lecture. Rewrite it in Markdown. Change the conversational tone and language to written language appropriate for a textbook. Use appropriate mathematical formatting (equations, theorems, proofs, definitions, itemize) throughout the document.&amp;quot;&lt;/p&gt; &lt;p&gt;then paste the Youtube transcript.&lt;/p&gt; &lt;p&gt;It looks very good. But when I try it locally the best I can get is a short summary of the lecture. I've tried a number of models and context lengths - I've definitely tried solving this myself by experimenting - but I'm flying blind. &lt;/p&gt; &lt;p&gt;Can anyone suggest a couple models and some settings worth trying (context length, batch size, max tokens)? I'm on an M1 Ultra Mac Studio, 128GB RAM,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AerosolHubris"&gt; /u/AerosolHubris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyy91g/tweaking_settings_and_choosing_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyy91g/tweaking_settings_and_choosing_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyy91g/tweaking_settings_and_choosing_a_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyvgtw</id>
    <title>training STT model for my local language</title>
    <updated>2025-01-11T13:05:37+00:00</updated>
    <author>
      <name>/u/Alive-Professor5944</name>
      <uri>https://old.reddit.com/user/Alive-Professor5944</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;training STT model for my local language&lt;/p&gt; &lt;p&gt;Guys how can i fine tune STT, to build and ai voice chat that ables to understand my language so i can help my people learn english while they speak to the Ai with their local language please.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alive-Professor5944"&gt; /u/Alive-Professor5944 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvgtw/training_stt_model_for_my_local_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvgtw/training_stt_model_for_my_local_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvgtw/training_stt_model_for_my_local_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T13:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyh9y3</id>
    <title>DeepSeek-V3 imatrix quants by team mradermacher</title>
    <updated>2025-01-10T22:47:05+00:00</updated>
    <author>
      <name>/u/oobabooga4</name>
      <uri>https://old.reddit.com/user/oobabooga4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"&gt; &lt;img alt="DeepSeek-V3 imatrix quants by team mradermacher" src="https://external-preview.redd.it/m-G04wn3IB1jswcKbDUS8jJlKetCzX6HK1WoeuTcULY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=059108de9020787af36b4f6d446ccbfc92d4ba7e" title="DeepSeek-V3 imatrix quants by team mradermacher" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oobabooga4"&gt; /u/oobabooga4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mradermacher/DeepSeek-V3-i1-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyh9y3/deepseekv3_imatrix_quants_by_team_mradermacher/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T22:47:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyu07n</id>
    <title>Localhost LLM Benchmark</title>
    <updated>2025-01-11T11:29:56+00:00</updated>
    <author>
      <name>/u/05032-MendicantBias</name>
      <uri>https://old.reddit.com/user/05032-MendicantBias</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to benchmark my self hosted LLM, I want to run benchmarks like MMLU to evaluate the acceleration and accuracy of various quants against my GPU limitations&lt;/p&gt; &lt;p&gt;I tried the below tool, but it doesn't hit the API at all&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="http://github.com/EleutherAI/lm-evaluation-harness"&gt;github.com/EleutherAI/lm-evaluation-harness&lt;/a&gt;&lt;/p&gt; &lt;p&gt;lm_eval --model local-chat-completions --tasks gsm8k --model_args base_url=http://localhost:8000 --apply_chat_template&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;p&gt;EDIT: As a sanity check I made a quick python program to query my self hosted LLM and it works, so the problem is not the LLM hosting:&lt;/p&gt; &lt;p&gt;CODE:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import requests # Define the URL for the LLM endpoint url = &amp;quot;http://localhost:8000/v1/completions&amp;quot; # Define the payload with your input data payload = { &amp;quot;prompt&amp;quot;: &amp;quot;Once upon a time&amp;quot;, &amp;quot;max_tokens&amp;quot;: 100 } # Set the headers (if needed) headers = { &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;, &amp;quot;Authorization&amp;quot;: &amp;quot;Bearer YOUR_API_KEY&amp;quot; # Replace 'YOUR_API_KEY' with your actual API key if required } # Send the POST request to the LLM endpoint response = requests.post(url, json=payload, headers=headers) # Check if the request was successful if response.status_code == 200: # Print the response from the LLM completion = response.json() print(&amp;quot;Completion:&amp;quot;, completion[&amp;quot;choices&amp;quot;][0][&amp;quot;text&amp;quot;]) else: # Print an error message if the request was not successful print(&amp;quot;Error:&amp;quot;, response.status_code, response.text) import requests # Define the URL for the LLM endpoint url = &amp;quot;http://localhost:8000/v1/completions&amp;quot; # Define the payload with your input data payload = { &amp;quot;prompt&amp;quot;: &amp;quot;Once upon a time&amp;quot;, &amp;quot;max_tokens&amp;quot;: 100 } # Set the headers (if needed) headers = { &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;, &amp;quot;Authorization&amp;quot;: &amp;quot;Bearer YOUR_API_KEY&amp;quot; # Replace 'YOUR_API_KEY' with your actual API key if required } # Send the POST request to the LLM endpoint response = requests.post(url, json=payload, headers=headers) # Check if the request was successful if response.status_code == 200: # Print the response from the LLM completion = response.json() print(&amp;quot;Completion:&amp;quot;, completion[&amp;quot;choices&amp;quot;][0][&amp;quot;text&amp;quot;]) else: # Print an error message if the request was not successful print(&amp;quot;Error:&amp;quot;, response.status_code, response.text) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ANSWER&lt;/p&gt; &lt;pre&gt;&lt;code&gt;2025-01-11 12:53:43 [INFO] [LM STUDIO SERVER] Running completion on text: Once upon a time 2025-01-11 12:53:43 [INFO] [LM STUDIO SERVER] Processing... 2025-01-11 12:53:44 [INFO] Generated prediction: { &amp;quot;id&amp;quot;: &amp;quot;cmpl-kelaozz9r929wvhnt1vse&amp;quot;, &amp;quot;object&amp;quot;: &amp;quot;text_completion&amp;quot;, &amp;quot;created&amp;quot;: 1736596423, &amp;quot;model&amp;quot;: &amp;quot;qwen2.5-7b-instruct&amp;quot;, &amp;quot;choices&amp;quot;: [ { &amp;quot;index&amp;quot;: 0, &amp;quot;text&amp;quot;: &amp;quot;, in the middle ages, there was a great king. He had many friends, but one of them he loved more than all his other friends combined. One day this friend of the king told him that he wanted to be the next king.\n\nThe king was a little upset by this, but he agreed to make the request formal in writing and signed it.\n\nA few days later, the friend of the king sent back the document with some notes written on the side, indicating what parts were acceptable&amp;quot;, &amp;quot;logprobs&amp;quot;: null, &amp;quot;finish_reason&amp;quot;: &amp;quot;length&amp;quot; } ], &amp;quot;usage&amp;quot;: { &amp;quot;prompt_tokens&amp;quot;: 4, &amp;quot;completion_tokens&amp;quot;: 99, &amp;quot;total_tokens&amp;quot;: 103 } } 2025-01-11 12:53:44 [INFO] [LM STUDIO SERVER] Client disconnected. Stopping generation.. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/05032-MendicantBias"&gt; /u/05032-MendicantBias &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu07n/localhost_llm_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu07n/localhost_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu07n/localhost_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T11:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyyrml</id>
    <title>[Mini Rant] Are LLMs trapped in English and the assistant paradigms?</title>
    <updated>2025-01-11T15:52:33+00:00</updated>
    <author>
      <name>/u/Worth-Product-5545</name>
      <uri>https://old.reddit.com/user/Worth-Product-5545</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;br /&gt; It feels like we’re trapped in two mainstream paradigms, and it’s starting to get on my nerves. Let me explain:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLMs (too) focused on English&lt;/strong&gt;&lt;br /&gt; We’re seeing more and more models—Qwen, Mistral, Llama 3.x, etc.—that claim “multilingual” abilities. And if you look closely, everyone approaches the problem differently. However, my empirical scenarios often fail to deliver a good experience with those LLMs, even at a 70B scale.&lt;br /&gt; Yes, I understand English reaches the largest audience, but by focusing everything on English, we’re limiting the nuanced cultural and stylistic richness of other languages (French, Spanish, Italian, etc.).&lt;br /&gt; As a result, we rarely see new “styles” or modes of reasoning outside of English.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The “assistant” obsession&lt;/strong&gt;&lt;br /&gt; Everyone wants to build a conversation assistant. Sure, it’s a popular use case,&lt;br /&gt; but it kind of locks us into a single format: a Q&amp;amp;A flow with a polite, self-censored style.&lt;br /&gt; We forget these are token generators that could be tweaked for creative text manipulation or other forms of generation.&lt;br /&gt; I really wish we’d explore more diverse use cases: scenario generation, data-to-text, or other conversation protocols that aren’t so uniform.&lt;/p&gt; &lt;p&gt;I understand that model publishers invest significant resources into performing benchmarks and enhancing multilingual capabilities. For instance, Aya Expanse by Cohere For AI represents a notable advancement in this area. Despite these efforts, in real-world scenarios, I’ve never been able to achieve the same level of performance in French as in English with open-source models. Conversely, closed-source models maintain a more consistent performance across languages, which is frustrating because I’d prefer using open-source models.&lt;/p&gt; &lt;p&gt;Am I the only one who feels we’re stuck between “big English-only LLMs” and “conversation assistant” paradigms? I think there’s so much potential out there for better multilingual support and more interesting use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth-Product-5545"&gt; /u/Worth-Product-5545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyrml/mini_rant_are_llms_trapped_in_english_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyytg9</id>
    <title>Any local UI for deployed models?</title>
    <updated>2025-01-11T15:54:53+00:00</updated>
    <author>
      <name>/u/xdoso</name>
      <uri>https://old.reddit.com/user/xdoso</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm deploying different models on different machines and normally I find myself searching on what's the URL:port for each deployed model. Also it's shared with more people. I'm looking for some sort of UI that shows all deployed models and the URL. It would be exactly what LiteLLM calls &amp;quot;&lt;a href="https://docs.litellm.ai/docs/proxy/enterprise#public-model-hub"&gt;Public Model Hub&lt;/a&gt;&amp;quot;, but it's an emterprise/paid feature.&lt;/p&gt; &lt;p&gt;Do you know any alternative? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xdoso"&gt; /u/xdoso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyytg9/any_local_ui_for_deployed_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyytg9/any_local_ui_for_deployed_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyytg9/any_local_ui_for_deployed_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:54:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy91m1</id>
    <title>0.5B Distilled QwQ, runnable on IPhone</title>
    <updated>2025-01-10T16:59:44+00:00</updated>
    <author>
      <name>/u/Lord_of_Many_Memes</name>
      <uri>https://old.reddit.com/user/Lord_of_Many_Memes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"&gt; &lt;img alt="0.5B Distilled QwQ, runnable on IPhone" src="https://external-preview.redd.it/hOvT7Zh2EDTGcuqajUYbM7IboIMuAwdCFsY0UWAS0pU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85274e9584cd8dc27f3835483f32b47ea48f28f0" title="0.5B Distilled QwQ, runnable on IPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lord_of_Many_Memes"&gt; /u/Lord_of_Many_Memes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/kz919/Mini-QwQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy91m1/05b_distilled_qwq_runnable_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyruya</id>
    <title>Where is everyone sourcing their hardware?</title>
    <updated>2025-01-11T08:49:26+00:00</updated>
    <author>
      <name>/u/hainesk</name>
      <uri>https://old.reddit.com/user/hainesk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After looking for awhile I finally decided to purchase 3 identical refurbished 3090s through a manufacturer refurb online store and all of them have turned out to be unstable.&lt;br /&gt; One of them locking up the system within a few minutes of being turned on. I thought that by getting a manufacturer refurbished card directly from them that it would be less likely to be an issue. I looked around a lot before purchasing and this seemed like the safest option for a reasonable price ($699 per card).&lt;/p&gt; &lt;p&gt;I am in the process of RMA’ing them, but where does everyone else get their hardware? Has anyone else had issues with bad video cards? Any tips on good places to order from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hainesk"&gt; /u/hainesk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyruya/where_is_everyone_sourcing_their_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyruya/where_is_everyone_sourcing_their_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyruya/where_is_everyone_sourcing_their_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T08:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyapzu</id>
    <title>Phi-4 Finetuning - now with &gt;128K context length + Bug Fix Details</title>
    <updated>2025-01-10T18:09:05+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt; &lt;img alt="Phi-4 Finetuning - now with &amp;gt;128K context length + Bug Fix Details" src="https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fbf9c89972d5c31e3bd2d3354696be4e8d5b9d" title="Phi-4 Finetuning - now with &amp;gt;128K context length + Bug Fix Details" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now fine-tune Phi-4 with &amp;gt;128K context lengths using &lt;a href="https://github.com/unslothai/unsloth/"&gt;Unsloth&lt;/a&gt;! That's 12x longer than Hugging Face + FA2’s 11K on a 48GB GPU.&lt;/p&gt; &lt;p&gt;Phi-4 Finetuning Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also previously announced bug fixes for Phi-4 and so we’ll reveal the details.&lt;/p&gt; &lt;p&gt;But, before we do, some of you were curious if our fixes actually worked? Yes! Our fixed Phi-4 uploads show clear performance gains, with even better scores than Microsoft's original uploads on the &lt;a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=phi-4"&gt;Open LLM Leaderboard&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d8hew26e06ce1.png?width=2366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=173c23feacc625566271470839fe7a5e25eb860e"&gt;https://preview.redd.it/d8hew26e06ce1.png?width=2366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=173c23feacc625566271470839fe7a5e25eb860e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some of you even tested it to show greatly improved results in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example 1: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m665h08/"&gt;Multiple-choice tasks&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qx50pkq706ce1.png?width=1579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316"&gt;https://preview.redd.it/qx50pkq706ce1.png?width=1579&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=437da2cabdbf98ef5a8b8cbdc5592907a20e2316&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Example 2: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hwzmqc/comment/m65wr3e/"&gt;ASCII art generation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ircz0pnc06ce1.png?width=1433&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16c770a0fd58a469af3b98216844447845b98ada"&gt;https://preview.redd.it/ircz0pnc06ce1.png?width=1433&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16c770a0fd58a469af3b98216844447845b98ada&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Bug Fix Details&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Tokenizer Fix: Phi-4 incorrectly uses &amp;lt;|endoftext|&amp;gt; as EOS instead of &amp;lt;|im_end|&amp;gt;.&lt;/li&gt; &lt;li&gt;Finetuning Fix: Use a proper padding token (e.g., &amp;lt;|dummy_87|&amp;gt;).&lt;/li&gt; &lt;li&gt;Chat Template Fix: Avoid adding an assistant prompt unless specified to prevent serving issues.&lt;/li&gt; &lt;li&gt;More in-depth in our blog: &lt;a href="https://unsloth.ai/blog/phi4"&gt;https://unsloth.ai/blog/phi4&lt;/a&gt; or &lt;a href="https://twitter.com/danielhanchen/status/1877781452818968615"&gt;tweet&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Phi-4 Uploads (with our bug fixes)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;GGUFs&lt;/a&gt; including 2, 3, 4, 5, 6, 8, 16-bit&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit"&gt;Unsloth Dynamic 4-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4"&gt;Original 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For all other model uploads, see &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;our docs&lt;/a&gt;&lt;br /&gt; I know this post was a bit long, but I hope it was informative and please ask any questions!! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyapzu/phi4_finetuning_now_with_128k_context_length_bug/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T18:09:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy34ir</id>
    <title>WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js</title>
    <updated>2025-01-10T12:16:13+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt; &lt;img alt="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" src="https://external-preview.redd.it/a3B0bmYzbTJyNWNlMYVrWG7q5Ym6r9MYEdNpGfavLsbyjmwCsGU7oHTw1w8w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06dd6f09c82183918afdcca9863994fcffe8274f" title="WebGPU-accelerated reasoning LLMs running 100% locally in-browser w/ Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vmfpb2m2r5ce1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy34ir/webgpuaccelerated_reasoning_llms_running_100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T12:16:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1hydavt</id>
    <title>New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b</title>
    <updated>2025-01-10T19:56:16+00:00</updated>
    <author>
      <name>/u/iamephemeral</name>
      <uri>https://old.reddit.com/user/iamephemeral</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt; &lt;img alt="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" src="https://external-preview.redd.it/r4CGqgcRPLr1eA9JfvNHSBaN_-4tgT5j575hGH0pgUU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=239946d045e3a552b2d863b9157de34884befd7f" title="New open source SAEs for model steering, including the first ever SAE for Llama 3.3 70b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamephemeral"&gt; /u/iamephemeral &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Goodfire/Llama-3.3-70B-Instruct-SAE-l50"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hydavt/new_open_source_saes_for_model_steering_including/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T19:56:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1hy8733</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models</title>
    <updated>2025-01-10T16:24:05+00:00</updated>
    <author>
      <name>/u/holamifuturo</name>
      <uri>https://old.reddit.com/user/holamifuturo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push Restricting US Allies Such As Poland, Portugal, India or UAE Maker Of Falcon Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/holamifuturo"&gt; /u/holamifuturo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hy8733/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T16:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hz0n8c</id>
    <title>GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025</title>
    <updated>2025-01-11T17:15:58+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"&gt; &lt;img alt="GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025" src="https://external-preview.redd.it/fWekNX9cjJo2NgR6zTyYnqvItoILS5GvTDAQC2foz30.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13f97a5793ce6881c43646a9bce53d9dbbf16b98" title="GMK Announces World’s First Mini-PC Based On AMD Ryzen AI 9 Max+ 395 Processor, Availability Will Be In H1 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/gmk-announces-worlds-first-mini-pc-based-on-amd-ryzen-ai-9-max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hz0n8c/gmk_announces_worlds_first_minipc_based_on_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T17:15:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyukc2</id>
    <title>GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;D</title>
    <updated>2025-01-11T12:09:03+00:00</updated>
    <author>
      <name>/u/Thistleknot</name>
      <uri>https://old.reddit.com/user/Thistleknot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"&gt; &lt;img alt="GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;amp;D" src="https://external-preview.redd.it/2MaaUSNtf5DLbq6ZpF876OWYQdcOtASsj6e_pAKWpKY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec156925c4109c28028bb52b1517ca8eb977cd5a" title="GitHub - tegridydev/dnd-llm-game: MVP of an idea using multiple local LLM models to simulate and play D&amp;amp;D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thistleknot"&gt; /u/Thistleknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tegridydev/dnd-llm-game?tab=readme-ov-file"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyukc2/github_tegridydevdndllmgame_mvp_of_an_idea_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T12:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyf1pf</id>
    <title>Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro?</title>
    <updated>2025-01-10T21:09:12+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt; &lt;img alt="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " src="https://external-preview.redd.it/PSxCcCk18RpMpFh_Tgc1ycbd0zsabOZK7av3YdT9fA4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b75663383244e2aa5f5fcf0207756c5dc28fb51b" title="Text to speech in 82m params is perfect for edge AI. Who's building an audio assistant with Kokoro? " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyf1pf/text_to_speech_in_82m_params_is_perfect_for_edge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-10T21:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyjoau</id>
    <title>This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)</title>
    <updated>2025-01-11T00:38:10+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt; &lt;img alt="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" src="https://b.thumbs.redditmedia.com/niNscGOj9hur8A-QVwFzrElx4sAsFt-GLXQ2A5RCLGw.jpg" title="This is my Powermac G3 sleeper AI workstation. 80gb total ram(32gb vram + 48gb ram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hyjoau"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyjoau/this_is_my_powermac_g3_sleeper_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T00:38:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyomxu</id>
    <title>Biden to Further Limit Nvidia AI Chip Exports in Final Push</title>
    <updated>2025-01-11T05:04:42+00:00</updated>
    <author>
      <name>/u/nate4t</name>
      <uri>https://old.reddit.com/user/nate4t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt; &lt;img alt="Biden to Further Limit Nvidia AI Chip Exports in Final Push" src="https://external-preview.redd.it/JzTX2qRvXmqwwoj4NDPkmIrtAEnnpeeSx1wuD-VSMTA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dc3abfba0983054d99719792b1d834321b47c7b" title="Biden to Further Limit Nvidia AI Chip Exports in Final Push" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nate4t"&gt; /u/nate4t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-01-08/biden-to-further-limit-nvidia-amd-ai-chip-exports-in-final-push?leadSource=reddit_wall"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyomxu/biden_to_further_limit_nvidia_ai_chip_exports_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyvfjq</id>
    <title>What do you think of AI employees?</title>
    <updated>2025-01-11T13:03:35+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am seeing a surge in start-ups and large enterprises building AI employees.&lt;/p&gt; &lt;p&gt;A good number of well-funded start-ups are building AI SDRs, SWEs, marketing agents, and Customer success agents. Even Salesforce is working on AgentForce to create no-code salesforce automation agents.&lt;/p&gt; &lt;p&gt;This trend is growing faster than I thought; dozens of start-ups are probably in YC this year.&lt;/p&gt; &lt;p&gt;I’m not sure if any of them are in production doing the jobs in the real world, and also, these agents may require a dozen integrations to be anywhere close to being functional.&lt;/p&gt; &lt;p&gt;As much as I like LLMs, they still don’t seem capable of handling edge cases in real-world jobs. They may be suitable for building automated pipelines for tightly scoped tasks, but replacing humans seems far-fetched.&lt;/p&gt; &lt;p&gt;Salesforce Chairman Mark Benioff even commented on not hiring human employees anymore; though it could be their sneaky marketing, it shows their intent.&lt;/p&gt; &lt;p&gt;What do you think of this AI employee in general the present and future? I would love to hear your thoughts if you’re building something simillar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyvfjq/what_do_you_think_of_ai_employees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T13:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyyils</id>
    <title>Nvidia 50x0 cards are not better than their 40x0 equivalents</title>
    <updated>2025-01-11T15:40:50+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking closely at the specs, I found 40x0 equivalents for the new 50x0 cards except for 5090. Interestingly, all 50x0 cards are not as energy efficient as the 40x0 cards. Obviously, GDDR7 is the big reason for the significant boost in memory bandwidth for 50x0.&lt;/p&gt; &lt;p&gt;Unless you really need FP4 and DLSS4, there are not that strong a reason to buy the new cards. For the 4070Super/5070 pair, the former can be 15% faster in prompt processing and the latter is 33% faster in inference. If you value prompt processing, it might even make sense to buy the 4070S.&lt;/p&gt; &lt;p&gt;As I mentioned in another thread, this gen is more about memory upgrade than the actual GPU upgrade.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Card&lt;/th&gt; &lt;th align="left"&gt;4070 Super&lt;/th&gt; &lt;th align="left"&gt;5070&lt;/th&gt; &lt;th align="left"&gt;4070Ti Super&lt;/th&gt; &lt;th align="left"&gt;5070Ti&lt;/th&gt; &lt;th align="left"&gt;4080 Super&lt;/th&gt; &lt;th align="left"&gt;5080&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;FP16 TFLOPS&lt;/td&gt; &lt;td align="left"&gt;141.93&lt;/td&gt; &lt;td align="left"&gt;123.37&lt;/td&gt; &lt;td align="left"&gt;176.39&lt;/td&gt; &lt;td align="left"&gt;175.62&lt;/td&gt; &lt;td align="left"&gt;208.9&lt;/td&gt; &lt;td align="left"&gt;225.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TDP&lt;/td&gt; &lt;td align="left"&gt;220&lt;/td&gt; &lt;td align="left"&gt;250&lt;/td&gt; &lt;td align="left"&gt;285&lt;/td&gt; &lt;td align="left"&gt;300&lt;/td&gt; &lt;td align="left"&gt;320&lt;/td&gt; &lt;td align="left"&gt;360&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GFLOPS/W&lt;/td&gt; &lt;td align="left"&gt;656.12&lt;/td&gt; &lt;td align="left"&gt;493.49&lt;/td&gt; &lt;td align="left"&gt;618.93&lt;/td&gt; &lt;td align="left"&gt;585.39&lt;/td&gt; &lt;td align="left"&gt;652.8&lt;/td&gt; &lt;td align="left"&gt;626&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;12GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GB/s&lt;/td&gt; &lt;td align="left"&gt;504&lt;/td&gt; &lt;td align="left"&gt;672&lt;/td&gt; &lt;td align="left"&gt;672&lt;/td&gt; &lt;td align="left"&gt;896&lt;/td&gt; &lt;td align="left"&gt;736&lt;/td&gt; &lt;td align="left"&gt;960&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Price at Launch&lt;/td&gt; &lt;td align="left"&gt;$599&lt;/td&gt; &lt;td align="left"&gt;$549&lt;/td&gt; &lt;td align="left"&gt;$799&lt;/td&gt; &lt;td align="left"&gt;$749&lt;/td&gt; &lt;td align="left"&gt;$999&lt;/td&gt; &lt;td align="left"&gt;$999&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyyils/nvidia_50x0_cards_are_not_better_than_their_40x0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T15:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hyu2dh</id>
    <title>LocalGLaDOS - running on a real LLM-rig</title>
    <updated>2025-01-11T11:34:21+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt; &lt;img alt="LocalGLaDOS - running on a real LLM-rig" src="https://external-preview.redd.it/EfE2n_bbhcmfaS9RbA5FtQq7jGIahU2UIGm8g-a1Uag.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ce4ca891cbd89dfa15f29ba5ffa968064f42e85" title="LocalGLaDOS - running on a real LLM-rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/N-GHKTocDF0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hyu2dh/localglados_running_on_a_real_llmrig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T11:34:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1hys13h</id>
    <title>New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450!</title>
    <updated>2025-01-11T09:02:18+00:00</updated>
    <author>
      <name>/u/appakaradi</name>
      <uri>https://old.reddit.com/user/appakaradi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt; &lt;img alt="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " src="https://external-preview.redd.it/d-6wrohyuoqlKc4TV9mDxgh4ErmzgT4n7gTbj9xeln4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8734d59c4128e9b5f68dcc670051d2d7f3e7fe12" title="New Model from https://novasky-ai.github.io/ Sky-T1-32B-Preview, open-source reasoning model that matches o1-preview on popular reasoning and coding benchmarks — trained under $450! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X: &lt;a href="https://x.com/NovaSkyAI/status/1877793041957933347"&gt;https://x.com/NovaSkyAI/status/1877793041957933347&lt;/a&gt;hf: &lt;a href="https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview"&gt;https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview&lt;/a&gt; blog: &lt;a href="https://novasky-ai.github.io/posts/sky-t1/"&gt;https://novasky-ai.github.io/posts/sky-t1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df"&gt;https://preview.redd.it/64qbzi7pxbce1.png?width=1201&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1a698cd51f4e6e2775d3117ca91f88253478df&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/appakaradi"&gt; /u/appakaradi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hys13h/new_model_from_httpsnovaskyaigithubio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-11T09:02:18+00:00</published>
  </entry>
</feed>
