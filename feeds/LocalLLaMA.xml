<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-10T07:22:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kivbm1</id>
    <title>Are general/shared Rag's a thing</title>
    <updated>2025-05-09T22:35:44+00:00</updated>
    <author>
      <name>/u/nocgeek</name>
      <uri>https://old.reddit.com/user/nocgeek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im in the process of training my first rag based on some documentation it made me wonder why I had not seen specialized rags for example A linux , Docker or Windows Powershell that you could connect to for specific questions in that domain? Do these exist and i have just not seen them or is it a training data issue or something else that i am missing? I have seen this in image generators via Lora's. i would love to read peoples thoughts on this even if it is something i am totally wrong about. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nocgeek"&gt; /u/nocgeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivbm1/are_generalshared_rags_a_thing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivbm1/are_generalshared_rags_a_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kivbm1/are_generalshared_rags_a_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T22:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kimo3j</id>
    <title>Hardware to run 32B models at great speeds</title>
    <updated>2025-05-09T16:24:55+00:00</updated>
    <author>
      <name>/u/Saayaminator</name>
      <uri>https://old.reddit.com/user/Saayaminator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I currently have a PC with a 7800x3d, 32GB of DDR5-6000 and an RTX3090. I am interested in running 32B models with at least 32k context loaded and great speeds. To that end, I thought about getting a second RTX3090 because you can find some acceptable prices for it. Would that be the best option? Any alternatives at a &amp;lt;1000$ budget?&lt;/p&gt; &lt;p&gt;Ideally I would also like to be able to run the larger MoE models at acceptable speeds (decent prompt processing/tft, tg like 15+ t/s). But for that I would probably need a Linux server. Ideally with a good upgrade path. Then I would have a higher budget, like 5k. Can you have decent power efficiency for such a build? I am only interested in inference &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Saayaminator"&gt; /u/Saayaminator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimo3j/hardware_to_run_32b_models_at_great_speeds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimo3j/hardware_to_run_32b_models_at_great_speeds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kimo3j/hardware_to_run_32b_models_at_great_speeds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T16:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kizh16</id>
    <title>lmstudio recommended qwen3 vs unsloth one</title>
    <updated>2025-05-10T02:08:44+00:00</updated>
    <author>
      <name>/u/oxidao</name>
      <uri>https://old.reddit.com/user/oxidao</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;sorry if this question is stupid but i dont know any other place to ask, what is the difference between these two?, and what version and quantification should i be running on my system? (16gb vram + 32gb ram)&lt;/p&gt; &lt;p&gt;thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oxidao"&gt; /u/oxidao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kizh16/lmstudio_recommended_qwen3_vs_unsloth_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kizh16/lmstudio_recommended_qwen3_vs_unsloth_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kizh16/lmstudio_recommended_qwen3_vs_unsloth_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T02:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiz0p6</id>
    <title>Qwen2.5 VL 7B producing only gibberish</title>
    <updated>2025-05-10T01:44:16+00:00</updated>
    <author>
      <name>/u/Dowo2987</name>
      <uri>https://old.reddit.com/user/Dowo2987</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiz0p6/qwen25_vl_7b_producing_only_gibberish/"&gt; &lt;img alt="Qwen2.5 VL 7B producing only gibberish" src="https://external-preview.redd.it/QjD3cl5fFZ_7EkXB-KRKbweNbkRSjx257L_rg2k_emk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70be6c633d8f6abc1a1f61b6913581716b8a6290" title="Qwen2.5 VL 7B producing only gibberish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I was trying to get Qwen2.5 VL to run locally on my machine, which was quite painful. But I ended up being able to execute it and even connect to OpenWebUI with &lt;a href="https://github.com/phildougherty/qwen2.5-VL-inference-openai"&gt;this script&lt;/a&gt; (which would have been a lot less painful if I used that from the beginning). I ran &lt;a href="http://app.py"&gt;app.py&lt;/a&gt; from inside wsl2 on Win11 after installing the requirements, but I had to copy the downloaded model files manually into the folder it wanted them in because else it would run into some weird issue.&lt;/p&gt; &lt;p&gt;It took a looooong while to generate a response to my &amp;quot;Hi!&amp;quot;, and what I got was not at all what I was hoping for:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vhx40g7yxuze1.png?width=1113&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f5f8c221c9d47da8421b1970f2604eef1b5e8fb"&gt;this gibberish continues until cap is hit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I actually ran into the same issue when running it via the example script provided &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;on the huggingface page&lt;/a&gt;, where it would also just produce gibberish with a lot of chinese characters. I then tried the provided script for 3B-Instruct, which resulted in the same kind of gibberish. Interestingly, when I was trying some Qwen2.5-VL versions I found on ollama the other day, I was also running into problems where it would only produce gibberish, but I was thinking that problem wouldn't occur if I got it directly from huggingface instead.&lt;/p&gt; &lt;p&gt;Now, is this in any way a known issue? Like, did I just do some stupid mistake and I just have to set some config properly and it will work? Or is the actual model cooked in some way? Is there any chance for this to be linked to inadequate hardware (running Ryzen 7 9800X3D, 64GB of RAM, RTX 3070)? I would think that would only make it super slow (which it was), but what do I know.&lt;br /&gt; I'd really like to run some vision model locallly, but I wasn't impressed by what I got from gemma3's vision, same for llama3.2-vision. When I tried out Qwen2.5-VL-72B on some hosted service that came a lot closer to my expectations, so I was trying to see what Qwen2.5 I could get to run (and at what speeds) with my system, but the results weren't at all satisfying. What now? Any hopes of fixing the gibberish? Or should I try Qwen2-VL, is that less annoying to run (more established) than Qwen2.5, how does the quality compare? Other vision models you can recommend? I haven't tried any of the Intern ones yet.&lt;/p&gt; &lt;p&gt;edit1: I also tried the 3B-AWQ, which I think fully fit into VRAM, but it also produced only gibber, only this time without chinese characters&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dowo2987"&gt; /u/Dowo2987 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiz0p6/qwen25_vl_7b_producing_only_gibberish/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiz0p6/qwen25_vl_7b_producing_only_gibberish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiz0p6/qwen25_vl_7b_producing_only_gibberish/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T01:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiuexc</id>
    <title>Offloading a 4B LLM to APU, only uses 50% of one CPU core. 21 t/s using Vulkan</title>
    <updated>2025-05-09T21:53:45+00:00</updated>
    <author>
      <name>/u/magnus-m</name>
      <uri>https://old.reddit.com/user/magnus-m</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiuexc/offloading_a_4b_llm_to_apu_only_uses_50_of_one/"&gt; &lt;img alt="Offloading a 4B LLM to APU, only uses 50% of one CPU core. 21 t/s using Vulkan" src="https://external-preview.redd.it/fdN9WJ_uOoSe2kS4aVe8FKGbL-hWzkFf2z6KEoDuAL0.png?width=140&amp;amp;height=45&amp;amp;crop=140:45,smart&amp;amp;auto=webp&amp;amp;s=11aa966ec508af0a4f847488166485a34b1b4962" title="Offloading a 4B LLM to APU, only uses 50% of one CPU core. 21 t/s using Vulkan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you don't use the iGPU of your CPU, you can run a small LLM on it almost without taking a toll of the CPU. &lt;/p&gt; &lt;p&gt;Running llama.cpp server on a AMD Ryzen with a APU only uses 50 % utilization of one CPU when offloading all layers to the iGPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: Gemma 3 4B Q4 fully offloaded to the iGPU.&lt;br /&gt; &lt;strong&gt;System&lt;/strong&gt;: AMD 7 8845HS, DDR5 5600, llama.cpp with Vulkan backend. Ubuntu.&lt;br /&gt; &lt;strong&gt;Performance:&lt;/strong&gt; 21 tokens/sec sustained throughput&lt;br /&gt; &lt;strong&gt;CPU Usage:&lt;/strong&gt; Just ~50% of one core&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x4n0p7n7vtze1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=642e18d7eeefeb932e2e26b39d600d4bfcbfd2e3"&gt;https://preview.redd.it/x4n0p7n7vtze1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=642e18d7eeefeb932e2e26b39d600d4bfcbfd2e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feels like a waste not to utilize the iGPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/magnus-m"&gt; /u/magnus-m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiuexc/offloading_a_4b_llm_to_apu_only_uses_50_of_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiuexc/offloading_a_4b_llm_to_apu_only_uses_50_of_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiuexc/offloading_a_4b_llm_to_apu_only_uses_50_of_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T21:53:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kihrpt</id>
    <title>Best model to have</title>
    <updated>2025-05-09T12:54:17+00:00</updated>
    <author>
      <name>/u/Obvious_Cell_1515</name>
      <uri>https://old.reddit.com/user/Obvious_Cell_1515</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to have a model installed locally for &amp;quot;doomsday prep&amp;quot; (no imminent threat to me just because i can). Which open source model should i keep installed, i am using LM Studio and there are so many models at this moment and i havent kept up with all the new ones releasing so i have no idea. Preferably a uncensored model if there is a latest one which is very good&lt;/p&gt; &lt;p&gt;Sorry, I should give my hardware specifications. Ryzen 5600, Amd RX 580 gpu, 16gigs ram, SSD.&lt;/p&gt; &lt;p&gt;The gemma-3-12b-it-qat model runs good on my system if that helps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Obvious_Cell_1515"&gt; /u/Obvious_Cell_1515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kihrpt/best_model_to_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kihrpt/best_model_to_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kihrpt/best_model_to_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T12:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiz9h2</id>
    <title>LLM with best understanding of medicine?</title>
    <updated>2025-05-10T01:57:38+00:00</updated>
    <author>
      <name>/u/pinkfreude</name>
      <uri>https://old.reddit.com/user/pinkfreude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've had some success with Claude and ChatGPT. Are there any local LLM's that have a decent training background in medical topics?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pinkfreude"&gt; /u/pinkfreude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiz9h2/llm_with_best_understanding_of_medicine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiz9h2/llm_with_best_understanding_of_medicine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiz9h2/llm_with_best_understanding_of_medicine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T01:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki9u9d</id>
    <title>Sam Altman: OpenAI plans to release an open-source model this summer</title>
    <updated>2025-05-09T04:19:19+00:00</updated>
    <author>
      <name>/u/zan-max</name>
      <uri>https://old.reddit.com/user/zan-max</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki9u9d/sam_altman_openai_plans_to_release_an_opensource/"&gt; &lt;img alt="Sam Altman: OpenAI plans to release an open-source model this summer" src="https://external-preview.redd.it/ajdlMmxzcGNsb3plMbWgh0ga0DeDYWGdPekBwNb0wJ3u2lc2Xz7BD3amRjfR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=544abdd728657d68f07df0719bb55b0d05a32eb6" title="Sam Altman: OpenAI plans to release an open-source model this summer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sam Altman stated during today's Senate testimony that OpenAI is planning to release an open-source model this summer.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://www.youtube.com/watch?v=jOqTg1W_F5Q"&gt;https://www.youtube.com/watch?v=jOqTg1W_F5Q&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zan-max"&gt; /u/zan-max &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0cbh8rpcloze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki9u9d/sam_altman_openai_plans_to_release_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki9u9d/sam_altman_openai_plans_to_release_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T04:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kifny6</id>
    <title>I´ve made a Local alternative to "DeepSite" called "LocalSite" - lets you create Web Pages and components like Buttons, etc. with Local LLMs via Ollama and LM Studio</title>
    <updated>2025-05-09T10:59:48+00:00</updated>
    <author>
      <name>/u/Fox-Lopsided</name>
      <uri>https://old.reddit.com/user/Fox-Lopsided</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kifny6/ive_made_a_local_alternative_to_deepsite_called/"&gt; &lt;img alt="I´ve made a Local alternative to &amp;quot;DeepSite&amp;quot; called &amp;quot;LocalSite&amp;quot; - lets you create Web Pages and components like Buttons, etc. with Local LLMs via Ollama and LM Studio" src="https://external-preview.redd.it/cmZiOG5hYWFscXplMUKfOkFzHX-zyyu_0TBeY7g7ib_F1_WyOhrWr9oB-6Wv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=821152f80c82d31fa5be0a732eef101ba76712de" title="I´ve made a Local alternative to &amp;quot;DeepSite&amp;quot; called &amp;quot;LocalSite&amp;quot; - lets you create Web Pages and components like Buttons, etc. with Local LLMs via Ollama and LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may know the HuggingFace Space from &amp;quot;enzostvs&amp;quot; called &amp;quot;DeepSite&amp;quot; which lets you create Web Pages via Text Prompts with DeepSeek V3. I really liked the concept of it, and since Local LLMs have been getting pretty good at coding these days (GLM-4, Qwen3, UIGEN-T2), i decided to create a Local alternative that lets you use Local LLMs via Ollama and LM Studio to do the same as DeepSite locally.&lt;/p&gt; &lt;p&gt;You can also add Cloud LLM Providers via OpenAI Compatible APIs.&lt;/p&gt; &lt;p&gt;Watch the video attached to see it in action, where GLM-4-9B created a pretty nice pricing page for me!&lt;/p&gt; &lt;p&gt;Feel free to check it out and do whatever you want with it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/weise25/LocalSite-ai"&gt;https://github.com/weise25/LocalSite-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know what you guys think.&lt;/p&gt; &lt;p&gt;The development of this was heavily supported with Agentic Coding via Augment Code and also a little help from Gemini 2.5 Pro.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fox-Lopsided"&gt; /u/Fox-Lopsided &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/paflnbaalqze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kifny6/ive_made_a_local_alternative_to_deepsite_called/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kifny6/ive_made_a_local_alternative_to_deepsite_called/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T10:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj11b0</id>
    <title>LLamb a LLM chat client for your terminal</title>
    <updated>2025-05-10T03:37:16+00:00</updated>
    <author>
      <name>/u/s3bastienb</name>
      <uri>https://old.reddit.com/user/s3bastienb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj11b0/llamb_a_llm_chat_client_for_your_terminal/"&gt; &lt;img alt="LLamb a LLM chat client for your terminal" src="https://external-preview.redd.it/pzqNepzep-k1LYXeP2ndbcoOFIfdc5e3fI4vYh43PBo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f678e15fe4ab0fa20136dcf665f8253e9764d4e2" title="LLamb a LLM chat client for your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last night I worked on a LLM client for the terminal. You can connect to LM studio, Ollama, openAI and other providers in your terminal. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can setup as many connections as you like with a model for each&lt;/li&gt; &lt;li&gt;It keeps context via terminal window/ssh session&lt;/li&gt; &lt;li&gt;Can read text files and send it to the llm with your prompt&lt;/li&gt; &lt;li&gt;Can output the llm response to files&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can install it via NPM `npm install -g llamb`&lt;/p&gt; &lt;p&gt;If you check it out please let me know what you think. I had fun working on this with the help of Claude Code, that Max subscription is pretty good!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s3bastienb"&gt; /u/s3bastienb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.3sparks.net/llamb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj11b0/llamb_a_llm_chat_client_for_your_terminal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj11b0/llamb_a_llm_chat_client_for_your_terminal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T03:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki7tg7</id>
    <title>Don't Offload GGUF Layers, Offload Tensors! 200%+ Gen Speed? Yes Please!!!</title>
    <updated>2025-05-09T02:24:58+00:00</updated>
    <author>
      <name>/u/skatardude10</name>
      <uri>https://old.reddit.com/user/skatardude10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Inspired by:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/&lt;/a&gt; but applied to any other model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; I am running a QwQ merge at IQ4_M size that used to run at 3.95 Tokens per second, with 59 of 65 layers offloaded to GPU. By selectively restricting certain FFN tensors to stay on the CPU, I've saved a ton of space on the GPU, now offload all 65 of 65 layers to the GPU and run at 10.61 Tokens per second. Why is this not standard?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This is ONLY relevant if you have some layers on CPU and CANNOT offload ALL layers to GPU due to VRAM constraints. If you already offload all layers to GPU, you're ahead of the game. &lt;em&gt;But&lt;/em&gt; maybe this could allow you to run larger models at acceptable speeds that would otherwise have been too slow for your liking.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Idea:&lt;/strong&gt; With llama.cpp and derivatives like koboldcpp, you offload entire LAYERS typically. Layers are comprised of various attention tensors, feed forward network (FFN) tensors, gates and outputs. Within each transformer layer, from what I gather, attention tensors are GPU heavy and smaller benefiting from parallelization, while FFN tensors are VERY LARGE tensors that use more basic matrix multiplication that can be done on CPU. You can use the --overridetensors flag in koboldcpp or -ot in llama.cpp to selectively keep certain TENSORS on the cpu.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How-To:&lt;/strong&gt; Upfront, here's an example...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10.61 TPS vs 3.95 TPS&lt;/strong&gt; using the same amount of VRAM, &lt;em&gt;just offloading tensors&lt;/em&gt; instead of &lt;em&gt;entire layers:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python ~/koboldcpp/koboldcpp.py --threads 10 --usecublas --contextsize 40960 --flashattention --port 5000 --model ~/Downloads/MODELNAME.gguf --gpulayers 65 --quantkv 1 --overridetensors &amp;quot;\.[13579]\.ffn_up|\.[1-3][13579]\.ffn_up=CPU&amp;quot; ... [18:44:54] CtxLimit:39294/40960, Amt:597/2048, Init:0.24s, Process:68.69s (563.34T/s), Generate:56.27s (10.61T/s), Total:124.96s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Offloading layers baseline:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python ~/koboldcpp/koboldcpp.py --threads 6 --usecublas --contextsize 40960 --flashattention --port 5000 --model ~/Downloads/MODELNAME.gguf --gpulayers 59 --quantkv 1 ... [18:53:07] CtxLimit:39282/40960, Amt:585/2048, Init:0.27s, Process:69.38s (557.79T/s), Generate:147.92s (3.95T/s), Total:217.29s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details on how to? Use regex to match certain FFN layers to target for selectively NOT offloading to GPU as the commands above show.&lt;/p&gt; &lt;p&gt;In my examples above, I targeted FFN up layers because mine were mostly IQ4_XS while my FFN down layers were selectively quantized between IQ4_XS and Q5-Q8, which means those larger tensors vary in size a lot. This is beside the point of this post, but would come into play if you are just going to selectively restrict offloading every/every other/every third FFN_X tensor while assuming they are all the same size with something like Unsloth's Dynamic 2.0 quants that keep certain tensors at higher bits if you were doing math. Realistically though, you're selectively restricting certain tensors from offloading to save GPU space and how you do that doesn't matter all that much as long as you are hitting your VRAM target with your overrides. For example, when I tried to optimize for having every other Q4 FFN tensor stay on CPU versus every third regardless of tensor quant that, included many Q6 and Q8 tensors, to reduce computation load from the higher bit tensors, I only gained 0.4 tokens/second.&lt;/p&gt; &lt;p&gt;So, really how to?? Look at your GGUF's model info. For example, let's use: &lt;a href="https://huggingface.co/MaziyarPanahi/QwQ-32B-GGUF/tree/main?show_file_info=QwQ-32B.Q3_K_M.gguf"&gt;https://huggingface.co/MaziyarPanahi/QwQ-32B-GGUF/tree/main?show_file_info=QwQ-32B.Q3_K_M.gguf&lt;/a&gt; and look at all the layers and all the tensors in each layer.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Tensor&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_down.weight&lt;/td&gt; &lt;td align="left"&gt;[27 648, 5 120]&lt;/td&gt; &lt;td align="left"&gt;Q5_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_gate.weight&lt;/td&gt; &lt;td align="left"&gt;[5 120, 27 648]&lt;/td&gt; &lt;td align="left"&gt;Q3_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_norm.weight&lt;/td&gt; &lt;td align="left"&gt;[5 120]&lt;/td&gt; &lt;td align="left"&gt;F32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_up.weight&lt;/td&gt; &lt;td align="left"&gt;[5 120, 27 648]&lt;/td&gt; &lt;td align="left"&gt;Q3_K&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;In this example, overriding tensors ffn_down at a higher Q5 to CPU would save more space on your GPU that fnn_up or fnn_gate at Q3. My regex from above only targeted ffn_up on layers 1-39, every other layer, to squeeze every last thing I could onto the GPU. I also alternated which ones I kept on CPU thinking maybe easing up on memory bottlenecks but not sure if that helps. &lt;strong&gt;Remember&lt;/strong&gt; to set threads equivalent to -1 of your total CPU &lt;em&gt;CORE count&lt;/em&gt; to optimize CPU inference (12C/24T), --threads 11 is good.&lt;/p&gt; &lt;p&gt;Either way, seeing QwQ run on my card at over double the speed now is INSANE and figured I would share so you guys look into this too. Offloading entire layers uses the same amount of memory as offloading specific tensors, but sucks way more. This way, offload everything to your GPU except the big layers that work well on CPU. Is this common knowledge?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future:&lt;/strong&gt; I would love to see llama.cpp and others be able to automatically, selectively restrict offloading heavy CPU efficient tensors to the CPU rather than whole layers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skatardude10"&gt; /u/skatardude10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T02:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kimq0g</id>
    <title>4B Polish language model based on Qwen3 architecture</title>
    <updated>2025-05-09T16:27:02+00:00</updated>
    <author>
      <name>/u/Significant_Focus134</name>
      <uri>https://old.reddit.com/user/Significant_Focus134</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;I just released the first version of a 4B Polish language model based on the Qwen3 architecture:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/piotr-ai/polanka_4b_v0.1_qwen3_gguf"&gt;https://huggingface.co/piotr-ai/polanka_4b_v0.1_qwen3_gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did continual pretraining of the Qwen3 4B Base model on a single RTX 4090 for around 10 days.&lt;/p&gt; &lt;p&gt;The dataset includes high-quality upsampled Polish content.&lt;/p&gt; &lt;p&gt;To keep the original model’s strengths, I used a mixed dataset: multilingual, math, code, synthetic, and instruction-style data.&lt;/p&gt; &lt;p&gt;The checkpoint was trained on ~1.4B tokens.&lt;/p&gt; &lt;p&gt;It runs really fast on a laptop (thanks to GGUF + llama.cpp).&lt;/p&gt; &lt;p&gt;Let me know what you think or if you run any tests!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Focus134"&gt; /u/Significant_Focus134 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimq0g/4b_polish_language_model_based_on_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kimq0g/4b_polish_language_model_based_on_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kimq0g/4b_polish_language_model_based_on_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T16:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kis38u</id>
    <title>GLM-4-32B-0414 one shot of a Pong game with AI opponent that gets stressed as the game progresses, leading to more mistakes!</title>
    <updated>2025-05-09T20:13:07+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"&gt; &lt;img alt="GLM-4-32B-0414 one shot of a Pong game with AI opponent that gets stressed as the game progresses, leading to more mistakes!" src="https://external-preview.redd.it/qF4fn45-cPqJu4NhTUl8Bwm3SF8Y_jJRcYSsoPgQW40.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5a6eefa6189a89c27706a25a4e0620dbdb8b6ae" title="GLM-4-32B-0414 one shot of a Pong game with AI opponent that gets stressed as the game progresses, leading to more mistakes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code &amp;amp; play at jsfiddle &lt;a href="https://jsfiddle.net/jzsyenqm/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nidzls3bdtze1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=100ec8cc31bb165ed64331a9888721d3915bed93"&gt;https://preview.redd.it/nidzls3bdtze1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=100ec8cc31bb165ed64331a9888721d3915bed93&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kis38u/glm432b0414_one_shot_of_a_pong_game_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T20:13:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kitq9v</id>
    <title>If you had a Blackwell DGX (B200) - what would you run?</title>
    <updated>2025-05-09T21:22:51+00:00</updated>
    <author>
      <name>/u/backnotprop</name>
      <uri>https://old.reddit.com/user/backnotprop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/data-center/dgx-b200/"&gt;x8 180GB cards&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like to know what would you run on a single card?&lt;/p&gt; &lt;p&gt;What would you distribute?&lt;/p&gt; &lt;p&gt;...for any cool, fun, scientific, absurd, etc use case. We are serving models with tabbyapi (support for cuda12.8, others are behind). But we don't just have to serve endpoints.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/backnotprop"&gt; /u/backnotprop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kitq9v/if_you_had_a_blackwell_dgx_b200_what_would_you_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kitq9v/if_you_had_a_blackwell_dgx_b200_what_would_you_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kitq9v/if_you_had_a_blackwell_dgx_b200_what_would_you_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T21:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kigmfo</id>
    <title>Make Qwen3 Think like Gemini 2.5 Pro</title>
    <updated>2025-05-09T11:55:12+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt; &lt;img alt="Make Qwen3 Think like Gemini 2.5 Pro" src="https://external-preview.redd.it/MZqi7CsqO_RyJH6OHbxt3tHe5kTNCKiSqlBbGI5rWyk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08149ac025081d5f8b32a770ddb6097e77e7f25c" title="Make Qwen3 Think like Gemini 2.5 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So when I was reading Apriel-Nemotron-15b-Thinker's README, I saw this:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We ensure the model starts with &lt;code&gt;Here are my reasoning steps:\n&lt;/code&gt; during all our evaluations.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And this reminds me that I can do the same thing to Qwen3 and make it think step by step like Gemini 2.5. So I wrote an open WebUI function that always starts the assistant message with &lt;code&gt;&amp;lt;think&amp;gt;\nMy step by step thinking process went something like this:\n1.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And it actually works—now Qwen3 will think with 1. 2. 3. 4. 5.... just like Gemini 2.5.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;\&lt;/em&gt;This is just a small experiment; it doesn't magically enhance the model's intelligence, but rather encourages it to think in a different format.&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u35xvz8fkqze1.png?width=2266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24bbe37f5dab6affa1cdde41d5ede56487219ef"&gt;https://preview.redd.it/u35xvz8fkqze1.png?width=2266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24bbe37f5dab6affa1cdde41d5ede56487219ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/AaronFeng753/Qwen3-Gemini2.5"&gt;https://github.com/AaronFeng753/Qwen3-Gemini2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T11:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiqqgh</id>
    <title>Local AI Radio Station (uses ACE)</title>
    <updated>2025-05-09T19:15:02+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiqqgh/local_ai_radio_station_uses_ace/"&gt; &lt;img alt="Local AI Radio Station (uses ACE)" src="https://external-preview.redd.it/eHhjdmw5ZzAwdHplMZ8dC80fbuf6S0WKAY4O-4KfqUEFi7xvJoV20v06EMJ_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a6b9b7eb3c33b930707066322e1a03d91e6ddeb" title="Local AI Radio Station (uses ACE)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/PasiKoodaa/ACE-Step-RADIO"&gt;https://github.com/PasiKoodaa/ACE-Step-RADIO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Probably works without gaps on 24GB VRAM. I have only tested it on 12GB. It would be very easy to also add radio hosts (for example DIA).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fratbag00tze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiqqgh/local_ai_radio_station_uses_ace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiqqgh/local_ai_radio_station_uses_ace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T19:15:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj2yjl</id>
    <title>Who else has tried to run Mindcraft locally?</title>
    <updated>2025-05-10T05:34:22+00:00</updated>
    <author>
      <name>/u/Peasant_Sauce</name>
      <uri>https://old.reddit.com/user/Peasant_Sauce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mindcraft is a project that can link to ai api's to power an ingame npc that can do stuff. I initially tried it on L3-8B-Stheno-v3.2-Q6_K and it worked surprisingly well, but has a lot of consistency issues. My main issue right now though is that no other model I've tried is working nearly as well. Deepseek was nonfunctional, and llama3dolphin was incapable of searching for blocks. &lt;/p&gt; &lt;p&gt;If any of yall have tried this and have any recommendations I'd love to hear them&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peasant_Sauce"&gt; /u/Peasant_Sauce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2yjl/who_else_has_tried_to_run_mindcraft_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2yjl/who_else_has_tried_to_run_mindcraft_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2yjl/who_else_has_tried_to_run_mindcraft_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T05:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj1t1o</id>
    <title>Qwen-2.5-VL-7b vs Gemma-3-12b impressions</title>
    <updated>2025-05-10T04:22:01+00:00</updated>
    <author>
      <name>/u/Zc5Gwu</name>
      <uri>https://old.reddit.com/user/Zc5Gwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First impressions of Qwen VL vs Gemma in llama.cpp.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Excellent at recognizing species of plants, animals, etc. Tested with a bunch of dog breeds as well as photos of plants and insects.&lt;/li&gt; &lt;li&gt;More formal tone&lt;/li&gt; &lt;li&gt;Doesn't seem as &amp;quot;general purpose&amp;quot;. When you ask it questions it tends to respond in the same forumlaic way regardless of what you are asking.&lt;/li&gt; &lt;li&gt;More conservative in its responses than Gemma, likely hallucinates less.&lt;/li&gt; &lt;li&gt;Asked a question about a photo of the night sky. Qwen refused to identify any stars or constellations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Gemma&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Good at identifying general objects, themes, etc. but not as good as Qwen at getting into the specifics.&lt;/li&gt; &lt;li&gt;More &amp;quot;friendly&amp;quot; tone, easier to &amp;quot;chat&amp;quot; with&lt;/li&gt; &lt;li&gt;General purpose, will changes it's response style based on the question it's being asked.&lt;/li&gt; &lt;li&gt;Hallucinates up the wazoo. Where Qwen will refuse to answer. Gemma will just make stuff up.&lt;/li&gt; &lt;li&gt;Asking a question about a photo of the night sky. Gemma identified the constellation Casseopia as well as some major stars. I wasn't able to confirm if it was correct, just thought it was cool.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zc5Gwu"&gt; /u/Zc5Gwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj1t1o/qwen25vl7b_vs_gemma312b_impressions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj1t1o/qwen25vl7b_vs_gemma312b_impressions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj1t1o/qwen25vl7b_vs_gemma312b_impressions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T04:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj44n8</id>
    <title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
    <updated>2025-05-10T06:53:16+00:00</updated>
    <author>
      <name>/u/CortaCircuit</name>
      <uri>https://old.reddit.com/user/CortaCircuit</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CortaCircuit"&gt; /u/CortaCircuit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arxiv.org/pdf/2505.03335"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj44n8/absolute_zero_reinforced_selfplay_reasoning_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj44n8/absolute_zero_reinforced_selfplay_reasoning_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T06:53:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kivw6w</id>
    <title>Webollama: A sleek web interface for Ollama, making local LLM management and usage simple. WebOllama provides an intuitive UI to manage Ollama models, chat with AI, and generate completions.</title>
    <updated>2025-05-09T23:02:50+00:00</updated>
    <author>
      <name>/u/phantagom</name>
      <uri>https://old.reddit.com/user/phantagom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivw6w/webollama_a_sleek_web_interface_for_ollama_making/"&gt; &lt;img alt="Webollama: A sleek web interface for Ollama, making local LLM management and usage simple. WebOllama provides an intuitive UI to manage Ollama models, chat with AI, and generate completions." src="https://external-preview.redd.it/y0EgI2XLTqKHcjAaim03gc_zVfisCy4KdfNRmAX06uU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05cff2b86886c237a89577608b4cb69c4870fc6c" title="Webollama: A sleek web interface for Ollama, making local LLM management and usage simple. WebOllama provides an intuitive UI to manage Ollama models, chat with AI, and generate completions." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantagom"&gt; /u/phantagom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/dkruyt/webollama"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kivw6w/webollama_a_sleek_web_interface_for_ollama_making/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kivw6w/webollama_a_sleek_web_interface_for_ollama_making/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T23:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kinwuu</id>
    <title>One transistor modelling one neuron - Nature publication</title>
    <updated>2025-05-09T17:15:59+00:00</updated>
    <author>
      <name>/u/Important-Damage-173</name>
      <uri>https://old.reddit.com/user/Important-Damage-173</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's an exciting Nature paper that finds out the fact that it is possible to model a neuron on a single transistor. For reference: humans have 100 Billion neurons in their brains, the Apple M3 chip has 187 Billion.&lt;/p&gt; &lt;p&gt;Now look, this does not mean that you will be running a superhuman on a pc by end of year (since a synapse also requires a full transistor) but I expect things to radically change in terms of new processors in the next few years. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nature.com/articles/s41586-025-08742-4"&gt;https://www.nature.com/articles/s41586-025-08742-4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Important-Damage-173"&gt; /u/Important-Damage-173 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kinwuu/one_transistor_modelling_one_neuron_nature/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kinwuu/one_transistor_modelling_one_neuron_nature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kinwuu/one_transistor_modelling_one_neuron_nature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T17:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj23yk</id>
    <title>An LLM + a selfhosted self engine looks like black magic</title>
    <updated>2025-05-10T04:40:38+00:00</updated>
    <author>
      <name>/u/marsxyz</name>
      <uri>https://old.reddit.com/user/marsxyz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"&gt; &lt;img alt="An LLM + a selfhosted self engine looks like black magic" src="https://external-preview.redd.it/HSJh1Glwn1cudWqMdR7v0csb93OcXPxZ1DJssuHJXOM.png?width=140&amp;amp;height=55&amp;amp;crop=140:55,smart&amp;amp;auto=webp&amp;amp;s=89b685fdf649d30b4a8df013cf2beef219f7fc0d" title="An LLM + a selfhosted self engine looks like black magic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: I of course meant search engine.&lt;/p&gt; &lt;p&gt;In its last update, open-webui added support for Yacy as a search provider. Yacy is an open source, distributed search engine that does not rely on a central index but rely on distributed peers indexing pages themselves. I already tried Yacy in the past but the problem is that the algorithm that sorts the results is garbage and it is not really usable as a search engine. Of course a small open source software that can run on literally anything (the server it ran on for this experiment is a 12th gen Celeron with 8GB of RAM) cannot compete in term of the intelligence of the algorithm to sort the results with companies like Google or Microsoft. It was practically unusable.&lt;/p&gt; &lt;p&gt;Or It Was ! Coupled with an LLM, the LLM can sort the trash results from Yacy out and keep what is useful ! For the purpose of this exercise I used Deepseek-V3-0324 from OpenRouter but it is trivial to use local models !&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zcq88bwjvvze1.png?width=2492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e7c5c36e0f1770fab88f7baed53cd25e1014d07"&gt;https://preview.redd.it/zcq88bwjvvze1.png?width=2492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8e7c5c36e0f1770fab88f7baed53cd25e1014d07&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That means that we can now have selfhosted AI models that learn from the Web ... without relying on Google or any central entity at all !&lt;/p&gt; &lt;p&gt;Some caveats; 1. Of course this is inferior to using google or even duckduckgo, I just wanted to share that here because I think you'll find it cool. 2. You need a solid CPU to have a lot of concurrent research, my Celeron gets hammered to 100% usage at each query. (open-webui and a bunch of other services are running on this server, that must not help). That's not your average LocalLLama rig costing my yearly salary ahah.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7q2mkkshvvze1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56b75972d9a1e4e98c7cdfe111dad47b7f87cbeb"&gt;https://preview.redd.it/7q2mkkshvvze1.png?width=554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56b75972d9a1e4e98c7cdfe111dad47b7f87cbeb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marsxyz"&gt; /u/marsxyz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj23yk/an_llm_a_selfhosted_self_engine_looks_like_black/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T04:40:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiwbs8</id>
    <title>Where is grok2?</title>
    <updated>2025-05-09T23:24:20+00:00</updated>
    <author>
      <name>/u/gzzhongqi</name>
      <uri>https://old.reddit.com/user/gzzhongqi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember Elon Musk specifically said on live Grok2 will be open-weighted once Grok3 is officially stable and running. Now even Grok3.5 is about to be released, so where is the Grok2 they promoised? Any news on that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gzzhongqi"&gt; /u/gzzhongqi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiwbs8/where_is_grok2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiwbs8/where_is_grok2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiwbs8/where_is_grok2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T23:24:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kipwyo</id>
    <title>Vision support in llama-server just landed!</title>
    <updated>2025-05-09T18:39:48+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kipwyo/vision_support_in_llamaserver_just_landed/"&gt; &lt;img alt="Vision support in llama-server just landed!" src="https://external-preview.redd.it/CP6J3J5fdX2KpZfgtlXLbxjm3T5vBWcf3_9VTbBGdw8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84391c4a85576c89e482f93847f1374edea2bc37" title="Vision support in llama-server just landed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12898"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kipwyo/vision_support_in_llamaserver_just_landed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kipwyo/vision_support_in_llamaserver_just_landed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T18:39:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kj2j6q</id>
    <title>Seed-Coder 8B</title>
    <updated>2025-05-10T05:07:09+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"&gt; &lt;img alt="Seed-Coder 8B" src="https://external-preview.redd.it/qN4W2OErTr-fXyFZh4FVGoCZMT9K6nHi3_DvqJJHr5c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bcd7116e2911f655490d68be32d15c7b0a893b6" title="Seed-Coder 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bytedance has released a new 8B code-specific model that outperforms both Qwen3-8B and Qwen2.5-Coder-7B-Inst. I am curious about the performance of its base model in code FIM tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wbtmpay50wze1.jpg?width=8348&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b7e6bb5d9a152ed6594e5683f582f9d5f9fb81d9"&gt;https://preview.redd.it/wbtmpay50wze1.jpg?width=8348&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b7e6bb5d9a152ed6594e5683f582f9d5f9fb81d9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ByteDance-Seed/Seed-Coder"&gt;github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Instruct"&gt;HF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Base"&gt;Base Model HF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kj2j6q/seedcoder_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-10T05:07:09+00:00</published>
  </entry>
</feed>
