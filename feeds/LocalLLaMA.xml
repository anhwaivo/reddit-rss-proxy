<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-06T07:23:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iit0gq</id>
    <title>Best light weight llm ?</title>
    <updated>2025-02-06T03:20:36+00:00</updated>
    <author>
      <name>/u/Remarkable_Wrap_5484</name>
      <uri>https://old.reddit.com/user/Remarkable_Wrap_5484</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want a llm locally for assisting me with python and R. As I don't have any fancy graphics card, which will be best for me ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Wrap_5484"&gt; /u/Remarkable_Wrap_5484 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iit0gq/best_light_weight_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iit0gq/best_light_weight_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iit0gq/best_light_weight_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T03:20:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1iioooo</id>
    <title>Rant about Language Mixing in &lt;think&gt; ... &lt;/think&gt;</title>
    <updated>2025-02-05T23:48:44+00:00</updated>
    <author>
      <name>/u/hi_im_ryanli</name>
      <uri>https://old.reddit.com/user/hi_im_ryanli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished reading the Deepseek R1 paper - in section 2.3.2, they talked about their mitigation strategies against language mixing, which included CoT cold start and a new reward for language consistency. &lt;/p&gt; &lt;p&gt;I totally understand why they might want to encourage language consistency - better alignment with human preference and such, but I speak 4-ish languages (English (native), Chinese (native), Chinese regional dialet (native), Japanese (conversational)) and my thought processes are usually in mixed languages. &lt;/p&gt; &lt;p&gt;As I learned math at a young age in Chinese, before moving to the States, and studing STEM in English, my &amp;quot;internal reasoning&amp;quot; on lots of STEM-related questions are in a mixture of English and Chinese. I usually found myself reasoning better when this way.&lt;/p&gt; &lt;p&gt;Considering pretraining usually happens in mixed language anyways, it feels a bit odd that the research community decided against language mixing altogether, especially the paper also called out that this comes with a sligh performance degradation. &lt;/p&gt; &lt;p&gt;Thanks for reading my rant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hi_im_ryanli"&gt; /u/hi_im_ryanli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iioooo/rant_about_language_mixing_in_think_think/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iioooo/rant_about_language_mixing_in_think_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iioooo/rant_about_language_mixing_in_think_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T23:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7qfy</id>
    <title>I created a website that tracks AI regulations around the world</title>
    <updated>2025-02-05T11:15:59+00:00</updated>
    <author>
      <name>/u/techie_ray</name>
      <uri>https://old.reddit.com/user/techie_ray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To help you stay on top of what governments are doing on AI, I created an interactive world map that tracks AI regulatory and policy developments around the world. Click on a region (or use the search bar) to view its profile. This website is updated regularly (including new regions to be added).&lt;/p&gt; &lt;p&gt;Free to access. No login required. This is for the community :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techieray.com/GlobalAIRegulationTracker"&gt;https://www.techieray.com/GlobalAIRegulationTracker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techie_ray"&gt; /u/techie_ray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiviql</id>
    <title>I know one of you will get this....</title>
    <updated>2025-02-06T05:40:54+00:00</updated>
    <author>
      <name>/u/_RouteThe_Switch</name>
      <uri>https://old.reddit.com/user/_RouteThe_Switch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/datacenter/s/7Z9ZGKzN1G"&gt;https://www.reddit.com/r/datacenter/s/7Z9ZGKzN1G&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_RouteThe_Switch"&gt; /u/_RouteThe_Switch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiviql/i_know_one_of_you_will_get_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiviql/i_know_one_of_you_will_get_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiviql/i_know_one_of_you_will_get_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T05:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii3qvv</id>
    <title>Google Lifts a Ban on Using Its AI for Weapons and Surveillance</title>
    <updated>2025-02-05T06:18:38+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"&gt; &lt;img alt="Google Lifts a Ban on Using Its AI for Weapons and Surveillance" src="https://external-preview.redd.it/NrA5s-vSHIcN9SaUL0ETUoJ_dGVpcnD0UsdffV5wGi8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afb92e2820ff849c88d693e6467404d7df633be8" title="Google Lifts a Ban on Using Its AI for Weapons and Surveillance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/google-responsible-ai-principles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T06:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iihjq1</id>
    <title>Announcing Sage: Open-source voice chat with LLMs</title>
    <updated>2025-02-05T18:52:04+00:00</updated>
    <author>
      <name>/u/felixatwood</name>
      <uri>https://old.reddit.com/user/felixatwood</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iihjq1/announcing_sage_opensource_voice_chat_with_llms/"&gt; &lt;img alt="Announcing Sage: Open-source voice chat with LLMs" src="https://external-preview.redd.it/Do-ORxNEK_7XDMp8cozVLzedSBauVAy968xPLSTBvJg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=321ce755c82cbf201d2d52979b4c2663df16e1df" title="Announcing Sage: Open-source voice chat with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/felixatwood"&gt; /u/felixatwood &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/farshed/sage"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iihjq1/announcing_sage_opensource_voice_chat_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iihjq1/announcing_sage_opensource_voice_chat_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T18:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiitl5</id>
    <title>Andrej Karpathy: Deep Dive into LLMs Like ChatGPT</title>
    <updated>2025-02-05T19:43:08+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiitl5/andrej_karpathy_deep_dive_into_llms_like_chatgpt/"&gt; &lt;img alt="Andrej Karpathy: Deep Dive into LLMs Like ChatGPT" src="https://external-preview.redd.it/DmIKQR1Qh6xD2A-jd67MgvOzKDXIcFDp0jJD5ODYpIY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b94b4626807e1636998d3593911753a052fdde2" title="Andrej Karpathy: Deep Dive into LLMs Like ChatGPT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=7xTGNNLPyMI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiitl5/andrej_karpathy_deep_dive_into_llms_like_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiitl5/andrej_karpathy_deep_dive_into_llms_like_chatgpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiwgou</id>
    <title>For coders! free&amp;open DeepSeek R1 &gt; $20 o3-mini with rate-limit!</title>
    <updated>2025-02-06T06:43:10+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwgou/for_coders_freeopen_deepseek_r1_20_o3mini_with/"&gt; &lt;img alt="For coders! free&amp;amp;open DeepSeek R1 &amp;gt; $20 o3-mini with rate-limit!" src="https://preview.redd.it/n9sntvkvsghe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a41575abce1f1f8a8cb02b9965f65a613e1a0174" title="For coders! free&amp;amp;open DeepSeek R1 &amp;gt; $20 o3-mini with rate-limit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n9sntvkvsghe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwgou/for_coders_freeopen_deepseek_r1_20_o3mini_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwgou/for_coders_freeopen_deepseek_r1_20_o3mini_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T06:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iirjvz</id>
    <title>How are people using models smaller than 5b parameters?</title>
    <updated>2025-02-06T02:05:49+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I straight up don't understand the real world problems these models are solving. I get them in theory, function calling, guard, and agents once they've been fine tuned. But I'm yet to see people come out and say, &amp;quot;hey we solved this problem with a 1.5b llama model and it works really well.&amp;quot;&lt;/p&gt; &lt;p&gt;Maybe I'm blind or not good enough to use them well some hopefully y'all can enlighten me &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirjvz/how_are_people_using_models_smaller_than_5b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirjvz/how_are_people_using_models_smaller_than_5b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iirjvz/how_are_people_using_models_smaller_than_5b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T02:05:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii9lab</id>
    <title>2B model beats 72B model</title>
    <updated>2025-02-05T13:10:56+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9lab/2b_model_beats_72b_model/"&gt; &lt;img alt="2B model beats 72B model" src="https://preview.redd.it/nxx7b0kblbhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7a412b056534d115469db02236cac1fd22d5d1a" title="2B model beats 72B model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Deep-Agent/R1-V"&gt;https://github.com/Deep-Agent/R1-V&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 2B model outperforms the 72B model. &lt;/p&gt; &lt;p&gt;Only 100 training steps, costing less than $3.&lt;/p&gt; &lt;p&gt;The outperformance is in both effectiveness and out-of-distribution (OOD) robustness for vision language models.&lt;/p&gt; &lt;p&gt;in OOD tests within just 100 training steps. &lt;/p&gt; &lt;p&gt;R1-V is released, and fully open-sourced. &lt;/p&gt; &lt;p&gt;The project shows a 2B-parameter model surpassing a 72B-parameter counterpart in generalization tests. &lt;/p&gt; &lt;p&gt;With only 100 training steps (vs. thousands in conventional methods), 30 minutes on 8 A100 GPUs and $2.62 total cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxx7b0kblbhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9lab/2b_model_beats_72b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9lab/2b_model_beats_72b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iipqvr</id>
    <title>Gemini 2.0 Pro Experimental, 2.0 Flash, 2.0 Flash-Lite are on Google AiStudio</title>
    <updated>2025-02-06T00:38:15+00:00</updated>
    <author>
      <name>/u/robberviet</name>
      <uri>https://old.reddit.com/user/robberviet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iipqvr/gemini_20_pro_experimental_20_flash_20_flashlite/"&gt; &lt;img alt="Gemini 2.0 Pro Experimental, 2.0 Flash, 2.0 Flash-Lite are on Google AiStudio" src="https://preview.redd.it/r35v5vlpzehe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=deb32b1deb2e3e8e937da5b249d2280937e502db" title="Gemini 2.0 Pro Experimental, 2.0 Flash, 2.0 Flash-Lite are on Google AiStudio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robberviet"&gt; /u/robberviet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r35v5vlpzehe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iipqvr/gemini_20_pro_experimental_20_flash_20_flashlite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iipqvr/gemini_20_pro_experimental_20_flash_20_flashlite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T00:38:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iij58e</id>
    <title>S1-32B: The $6 R1 Competitor?</title>
    <updated>2025-02-05T19:56:02+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://timkellogg.me/blog/2025/02/03/s1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iij58e/s132b_the_6_r1_competitor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iij58e/s132b_the_6_r1_competitor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiq831</id>
    <title>ASIC's for LLM infrence</title>
    <updated>2025-02-06T01:00:52+00:00</updated>
    <author>
      <name>/u/jklre</name>
      <uri>https://old.reddit.com/user/jklre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had a meeting today with a really intresting hardware company as part of my work activities. They make ASIC's specifically for LLM infrence. They are souly focused on the Datacenter server market but I brought up making a consumer PCIE card and or a dev board like a RaspberryPi (or even something as small as a google coral TPU). They seemed very intrested in this market but were not sure that it would catch on. What would you guys think about this? An infrence ASIC card that eats up a lot less power (100 -200w) that can host local models and gives near GROQ levels of performance. Any thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jklre"&gt; /u/jklre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiq831/asics_for_llm_infrence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiq831/asics_for_llm_infrence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiq831/asics_for_llm_infrence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T01:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iik4y9</id>
    <title>Train your own reasoning model in 30 minutes with Deepseek R1 and Kiln AI</title>
    <updated>2025-02-05T20:36:22+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"&gt; &lt;img alt="Train your own reasoning model in 30 minutes with Deepseek R1 and Kiln AI" src="https://external-preview.redd.it/fkk_hfuiSuMOZjLy_dEtjSiqJMOwZz9w_oAKY_5Q2Nk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3dadc03291c7ac04f201561f33b9b740f85a835" title="Train your own reasoning model in 30 minutes with Deepseek R1 and Kiln AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just released an update of Kiln on Github which allows you to distill a custom fine-tuned model from Deepseek R1 (or any reasoning model/chain-of-thought). The whole process only takes about 30 minutes, including generating a synthetic training dataset. It doesn't require any coding or command line work.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The attached video shows the process&lt;/li&gt; &lt;li&gt;Our docs have &lt;a href="https://docs.getkiln.ai/docs/guide-train-a-reasoning-model"&gt;a guide for distilling R1&lt;/a&gt; if you want to try it out yourself&lt;/li&gt; &lt;li&gt;Here's the &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Github repo&lt;/a&gt; with all of the source code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also wanted to add a huge thanks to &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt; for the awesome reception to on my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"&gt;last post&lt;/a&gt;. It really inspires me to keep building. I've already made about 30 improvements and built feature requests which came from people who found it via &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Kiln runs locally and we never have access to your dataset. Unsloth is fully supported if you have the GPUs to train locally. You can also use a training service like Fireworks &amp;amp; OpenAI if you prefer (data is sent to them with your keys, we still never have access to it). &lt;/p&gt; &lt;p&gt;If anyone wants to try Kiln, here's the &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;GitHub repository&lt;/a&gt; and &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;docs are here&lt;/a&gt;. Getting started is super easy - it's a one-click install to get setup and running.&lt;/p&gt; &lt;p&gt;I'm curious to get any feedback/ideas. It really helps me improve Kiln. Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1iik4y9/video/1vnufrecrdhe1/player"&gt;Kiln AI demo - distilling Deepseek R1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T20:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiafzm</id>
    <title>We have to fight back now.</title>
    <updated>2025-02-05T13:55:35+00:00</updated>
    <author>
      <name>/u/mr_happy_nice</name>
      <uri>https://old.reddit.com/user/mr_happy_nice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-source innovation is the lifeblood of American progress, and any attempt to lock it down is a threat to our future. Banning open-source AI under harsh penalties will only stifle the creativity, transparency, and collaboration that have fueled our tech breakthroughs for decades. When anyone can build on and improve each other’s work, we all win—especially in the race for a safer, smarter tomorrow.&lt;/p&gt; &lt;p&gt;We need to stand together for a future where ideas flow freely and innovation isn’t held hostage. Embracing open-source means a stronger, more competitive American tech ecosystem that benefits everyone, from citizens to startups to established giants. The open road is the best road—let’s keep it that way.&lt;/p&gt; &lt;p&gt;The only thing that these people understand is money. So, follow the money. Here are some of Hawley’s contributors to get you started. You have a right to have your voice be heard. Let them hear. &lt;/p&gt; &lt;h1&gt;Smead Capital Management&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Addresses &amp;amp; Phone Numbers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Phoenix Office:&lt;/em&gt; 2502 E. Camelback Rd, Suite 210, Phoenix, AZ 85016 Phone: 602.889.3660&lt;/li&gt; &lt;li&gt;&lt;em&gt;Jersey City Office:&lt;/em&gt; 30 Montgomery St, Suite 920, Jersey City, NJ 07302 Phone: 484.535.5121&lt;/li&gt; &lt;li&gt;&lt;em&gt;London Office (UK):&lt;/em&gt; 18th Floor, 100 Bishopsgate, London EC2N 4AG Phone: +44 (0)20.8819.6490&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sales Desk (US):&lt;/strong&gt; 877.701.2883&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@smeadcap.com"&gt;info@smeadcap.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@smeadcap.com"&gt;info@smeadcap.com&lt;/a&gt;) &lt;em&gt;(Additional verified contact: Cole Smead can be reached at&lt;/em&gt; [&lt;em&gt;&lt;a href="mailto:cole@smeadcap.com"&gt;cole@smeadcap.com&lt;/a&gt;&lt;/em&gt;](mailto:&lt;a href="mailto:cole@smeadcap.com"&gt;cole@smeadcap.com&lt;/a&gt;)&lt;em&gt;.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Indeck Energy Services&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;600 N. Buffalo Grove Road, Suite 300, Buffalo Grove, IL 60089&lt;/li&gt; &lt;li&gt;Phone: 847-520-3212&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@indeckenergy.com"&gt;info@indeckenergy.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@indeckenergy.com"&gt;info@indeckenergy.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Peck Enterprises, LLC&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number (as listed via Swagelok Alabama):&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;7290 Cahaba Valley Rd, Birmingham, AL 35242&lt;/li&gt; &lt;li&gt;Phone: 205.988.4812&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@ALFL.Swagelok.com"&gt;info@ALFL.Swagelok.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@ALFL.Swagelok.com"&gt;info@ALFL.Swagelok.com&lt;/a&gt;) &lt;em&gt;(Note: An alternate email – Roderick Douglass at&lt;/em&gt; [&lt;em&gt;&lt;a href="mailto:rodd.douglass@gmail.com"&gt;rodd.douglass@gmail.com&lt;/a&gt;&lt;/em&gt;](mailto:&lt;a href="mailto:rodd.douglass@gmail.com"&gt;rodd.douglass@gmail.com&lt;/a&gt;) &lt;em&gt;– was found on a third‑party directory, but the verified contact on the official Swagelok page is used here.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Northwestern Mutual&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;3601 North Point Parkway, Glendale, WI 53217&lt;/li&gt; &lt;li&gt;Phone: 800-225-5945&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; &lt;em&gt;(None published – inquiries are typically directed through the website’s contact form.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Prime Inc&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;4201 E. Kentucky Ave, Lincoln, NE 68504&lt;/li&gt; &lt;li&gt;Phone: 800-866-2747&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; &lt;em&gt;(No verified email found on the official website; please use the website contact form.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Veterans United Home Loans&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;1701 Wynnton Road, Suite 500, Columbia, MD 21046&lt;/li&gt; &lt;li&gt;Phone: 855-852-4189&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@veteransunited.com"&gt;info@veteransunited.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@veteransunited.com"&gt;info@veteransunited.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Diamond Pet Foods&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;1200 West Kemper Road, Eagan, MN 55122&lt;/li&gt; &lt;li&gt;Phone: 952-787-3400&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@diamondpet.com"&gt;info@diamondpet.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@diamondpet.com"&gt;info@diamondpet.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Leggett &amp;amp; Platt&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Numbers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;One Leggett Parkway, Carson, CA 90746&lt;/li&gt; &lt;li&gt;Customer Care: 800-232-8534; Corporate: (562) 467-2000&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; &lt;em&gt;(No verified email address was confirmed on their official site.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Opko Health&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Numbers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;One Opko Way, Miami, FL 33131&lt;/li&gt; &lt;li&gt;Phone: 800-543-4741 or (305) 300-1234&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@opko.com"&gt;info@opko.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@opko.com"&gt;info@opko.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Edward Jones&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phone Numbers&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Client Relations: (800) 441-2357 (7 a.m. – 5:30 p.m. CT, Monday–Friday)&lt;/li&gt; &lt;li&gt;Headquarters: (314) 515-2000 (7 a.m. – 6 p.m. CT, Monday–Friday)&lt;/li&gt; &lt;li&gt;Toll-Free: (800) 803-3333&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Address&lt;/strong&gt;: 12555 Manchester Road, St. Louis County, Missouri 63131, USA&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Edward Jones does not list a public email for customer service; inquiries are handled via phone or their online access portal.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Diamond Pet Foods&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phone Number&lt;/strong&gt;: (800) 442-0402&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Address&lt;/strong&gt;: PO Box 156, Meta, Missouri 65058, USA&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Diamond Pet Foods does not publicly provide a direct email but offers a contact form on their website for inquiries.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Hunter Engineering Company&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Corporate Headquarters Address&lt;/strong&gt;: 11250 Hunter Drive, Bridgeton, Missouri 63044, USA&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Phone Numbers&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Corporate Office: (314) 731-3020 or (800) 448-6848&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: [&lt;a href="mailto:canadainfo@hunter.com"&gt;canadainfo@hunter.com&lt;/a&gt;](mailto:&lt;a href="mailto:canadainfo@hunter.com"&gt;canadainfo@hunter.com&lt;/a&gt;) (Canada-specific inquiries); [&lt;a href="mailto:info@huntereng.de"&gt;info@huntereng.de&lt;/a&gt;](mailto:&lt;a href="mailto:info@huntereng.de"&gt;info@huntereng.de&lt;/a&gt;) (Germany-specific inquiries)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Hallmark Cards&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phone Numbers&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Toll-Free in the U.S.: (800) 425-5627&lt;/li&gt; &lt;li&gt;Customer Service: (816) 274-3613&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Hallmark does not list a direct customer service email but allows inquiries through a contact form on their website.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For further assistance with these companies, it is recommended to use the provided phone numbers or visit their official websites for additional contact options.&lt;/p&gt; &lt;h1&gt;Fisher Realty (North Carolina)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Belle Hart Schmidt LLC&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GJ Grewe Inc&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Holland Law Firm (Missouri)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found; please refer to a state bar directory for direct contact.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Wilson Logistics&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified email address was found; contact information is available via the company’s “Contact Us” page.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;AGC Partners&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Warren David Properties LLC&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Durham Co&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact email was found. Public details are not available for inclusion.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Ozarks Coca‑Cola Bottling&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in the public sources.)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;*edit *link:&lt;br /&gt; &lt;a href="https://economictimes.indiatimes.com/news/international/us/if-you-download-deepseek-in-the-u-s-you-could-face-20-years-in-prison-and-a-100-million-fine-this-is-what-a-new-bill-introduced-in-the-senate-proposes-to-do/articleshow/117954136.cms?from=mdr"&gt;https://economictimes.indiatimes.com/news/international/us/if-you-download-deepseek-in-the-u-s-you-could-face-20-years-in-prison-and-a-100-million-fine-this-is-what-a-new-bill-introduced-in-the-senate-proposes-to-do/articleshow/117954136.cms?from=mdr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sen Josh Hawley put forth a bill. This bill fines you up to $1,000,000 and up to 20 years in jail for downloading a Chinese LLM. But the way it is worded, it can apply to even a research paper from a Chinese lab. Has support from Elizabeth Warren. &lt;/p&gt; &lt;p&gt;Also, I would just like to thank the people that called me an idiot. I'm not always correct. I *hope* I'm wrong and maybe this will be nothing and he is grand-standing or trying to get political points. Obviously that's a possibility. But the way things are going in the US, it is very much in the realm of possibility that this could get support and pass in a red congress. Peace :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_happy_nice"&gt; /u/mr_happy_nice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiafzm/we_have_to_fight_back_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiafzm/we_have_to_fight_back_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiafzm/we_have_to_fight_back_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikhg3</id>
    <title>I found a way to speed up CPU based LLM inference using a HNSW index on the output embeddings</title>
    <updated>2025-02-05T20:50:51+00:00</updated>
    <author>
      <name>/u/martinloretz</name>
      <uri>https://old.reddit.com/user/martinloretz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To get the next token from an LLM, we compute the probabilities for each individual token in the LLM's vocabulary by multiplying the last hidden state with the output embedding matrix. This matrix is massive, accounting for up to 20% of the total parameters in small multilingual LLMs. &lt;/p&gt; &lt;p&gt;When sampling the next token with top-k sampling, we're only sampling from the 40 most probable tokens out of 128,256 (for Llama 3.2 models). By using an HNSW vector index, we can retrieve these 40 most probable tokens directly through an approximate nearest neighbor search over the output embeddings, avoiding the full matrix multiplication with the output embeddings. &lt;/p&gt; &lt;p&gt;This reduces memory accesses and computation, resulting in up to 28% faster CPU-based inference for Llama 2.1 1B on mid-range laptops.&lt;/p&gt; &lt;h3&gt;For more details, read the full blog post on &lt;a href="https://martinloretz.com/blog/vector-index-cpu/"&gt;martinloretz.com/blog/vector-index-cpu/&lt;/a&gt;&lt;/h3&gt; &lt;h2&gt;Benchmarks&lt;/h2&gt; &lt;p&gt;&lt;code&gt;llama-bench&lt;/code&gt; for Llama 1B F16 (Ubuntu = Intel® Core™ i7-10750H x 12, 2 x 16GiB DDR4 2933 MHz, MacBook = MacBook Pro 16&amp;quot; M4 Pro, vec = vector index, MM = matrix multiplication (reference)):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="right"&gt;threads&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;Vec t/s&lt;/th&gt; &lt;th align="right"&gt;MM t/s&lt;/th&gt; &lt;th align="right"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Ubuntu&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;5.99 ± 0.05&lt;/td&gt; &lt;td align="right"&gt;4.73 ± 0.04&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.27&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ubuntu&lt;/td&gt; &lt;td align="right"&gt;6&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;12.51 ± 0.30&lt;/td&gt; &lt;td align="right"&gt;9.72 ± 0.13&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.29&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;23.56 ± 0.24&lt;/td&gt; &lt;td align="right"&gt;20.11 ± 0.44&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.17&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;12.52 ± 0.31&lt;/td&gt; &lt;td align="right"&gt;11.80 ± 0.18&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.06&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LLama 3.2 1B was selected for these benchmarks because of its relatively large embedding matrix (21% of all parameters). Full model speedups for larger models are lower because less time is spent computing the output embeddings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;To replicate these benchmarks, checkout this code of the &lt;a href="https://github.com/martinloretzzz/llama.cpp"&gt;fork of llama.cpp&lt;/a&gt;.&lt;/strong&gt; Installation instructions are in the Readme.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinloretz"&gt; /u/martinloretz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iikhg3/i_found_a_way_to_speed_up_cpu_based_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iikhg3/i_found_a_way_to_speed_up_cpu_based_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iikhg3/i_found_a_way_to_speed_up_cpu_based_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T20:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1iih21v</id>
    <title>Google's been at work, not Gemma 3 sadly</title>
    <updated>2025-02-05T18:32:11+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iih21v/googles_been_at_work_not_gemma_3_sadly/"&gt; &lt;img alt="Google's been at work, not Gemma 3 sadly" src="https://preview.redd.it/x5uaqeak6dhe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87ac5db8ef998927b98f0559468dd0f99a87fa19" title="Google's been at work, not Gemma 3 sadly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x5uaqeak6dhe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iih21v/googles_been_at_work_not_gemma_3_sadly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iih21v/googles_been_at_work_not_gemma_3_sadly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T18:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii82yg</id>
    <title>DeepSeek just released an official demo for DeepSeek VL2 Small - It's really powerful at OCR, text extraction and chat use-cases (Hugging Face Space)</title>
    <updated>2025-02-05T11:40:12+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small"&gt;https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Vaibhav (VB) Srivastav on X: &lt;a href="https://x.com/reach_vb/status/1887094223469515121"&gt;https://x.com/reach_vb/status/1887094223469515121&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Zizheng Pan on X: Our official huggingface space demo for DeepSeek-VL2 Small is out! A 16B MoE model for various vision-language tasks: &lt;a href="https://x.com/zizhpan/status/1887110842711162900"&gt;https://x.com/zizhpan/status/1887110842711162900&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iisj7j</id>
    <title>Open WebUI drops 3 new releases today. Code Interpreter, Native Tool Calling, Exa Search added</title>
    <updated>2025-02-06T02:55:42+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;0.5.8 had a slew of new adds. 0.5.9 and 0.5.10 seemed to be minor bug fixes for the most part. From their release page:&lt;/p&gt; &lt;p&gt;🖥️ Code Interpreter: Models can now execute code in real time to refine their answers dynamically, running securely within a sandboxed browser environment using Pyodide. Perfect for calculations, data analysis, and AI-assisted coding tasks!&lt;/p&gt; &lt;p&gt;💬 Redesigned Chat Input UI: Enjoy a sleeker and more intuitive message input with improved feature selection, making it easier than ever to toggle tools, enable search, and interact with AI seamlessly.&lt;/p&gt; &lt;p&gt;🛠️ Native Tool Calling Support (Experimental): Supported models can now call tools natively, reducing query latency and improving contextual responses. More enhancements coming soon!&lt;/p&gt; &lt;p&gt;🔗 Exa Search Engine Integration: A new search provider has been added, allowing users to retrieve up-to-date and relevant information without leaving the chat interface.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iisj7j/open_webui_drops_3_new_releases_today_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iisj7j/open_webui_drops_3_new_releases_today_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iisj7j/open_webui_drops_3_new_releases_today_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T02:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiwmsq</id>
    <title>Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost</title>
    <updated>2025-02-06T06:55:03+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"&gt; &lt;img alt="Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost" src="https://b.thumbs.redditmedia.com/U7IbKXWllKMESakzdcsFfg82O-BgJ0wgsGCf2i_dXrc.jpg" title="Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iiwmsq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiwmsq/overtokenized_transformer_new_paper_shows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T06:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiij1d</id>
    <title>DeepSeek R1 ties o1 for first place on the Generalization Benchmark.</title>
    <updated>2025-02-05T19:30:56+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiij1d/deepseek_r1_ties_o1_for_first_place_on_the/"&gt; &lt;img alt="DeepSeek R1 ties o1 for first place on the Generalization Benchmark." src="https://preview.redd.it/7na44xs3gdhe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=77dd2e43eb2352bf9c4ab11068ca9221f8b83934" title="DeepSeek R1 ties o1 for first place on the Generalization Benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7na44xs3gdhe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiij1d/deepseek_r1_ties_o1_for_first_place_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiij1d/deepseek_r1_ties_o1_for_first_place_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiveqd</id>
    <title>So, Google has no state-of-the-art frontier model now?</title>
    <updated>2025-02-06T05:34:09+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiveqd/so_google_has_no_stateoftheart_frontier_model_now/"&gt; &lt;img alt="So, Google has no state-of-the-art frontier model now?" src="https://preview.redd.it/64r0glzkgghe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4b6ad82ec54e92060d2226f5e8ec28c2f2eaf9b" title="So, Google has no state-of-the-art frontier model now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/64r0glzkgghe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiveqd/so_google_has_no_stateoftheart_frontier_model_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiveqd/so_google_has_no_stateoftheart_frontier_model_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T05:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iirej3</id>
    <title>The New Gemini Pro 2.0 Experimental sucks Donkey Balls.</title>
    <updated>2025-02-06T01:58:43+00:00</updated>
    <author>
      <name>/u/Odd-Environment-7193</name>
      <uri>https://old.reddit.com/user/Odd-Environment-7193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wow. Last night, after a long coding bender I heard the great news that Gemini were releasing some new models. I woke up this morning super excited to try them.&lt;/p&gt; &lt;p&gt;My first attempt was a quick OCR with Flesh light 2.0 and I was super impressed with the Speed. This thing is going to make complex OCR an absolute breeze. I cannot wait to incorporate this into my apps. I reckon it's going to cut the processing times in half. (Christmas came early)&lt;/p&gt; &lt;p&gt;Then I moved onto testing the Gemini 2.0 Pro Experimental.&lt;/p&gt; &lt;p&gt;How disappointing... This is such a regression from 1206. I could immediately see the drop in the quality of the tasks I've been working on daily like coding.&lt;/p&gt; &lt;p&gt;It makes shit tons of mistakes. The code that comes out doesn't have valid HTML (Super basic task) and it seems to want to interject and refactor code all the time without permission.&lt;/p&gt; &lt;p&gt;I don't know what the fuck these people are doing. Every single release it's like this. They just can't seem to get it right. 1206 has been a great model, and I've been using it as my daily driver for quite some time. I was actually very impressed with it and had they just released 1206 as Gemini 2.0 pro EXP I would have been stoked. This is an absolute regression.&lt;/p&gt; &lt;p&gt;I have seen this multiple times now with Google products. The previous time the same thing happened with 0827 and then Gemini 002.&lt;/p&gt; &lt;p&gt;For some reason at that time, they chose to force concise answers into everything, basically making it impossible to get full lengthy responses. Even with system prompts, it would just keep shortening code, adding comments into everything and basically forcing this dogshit concise mode behavior into everything.&lt;/p&gt; &lt;p&gt;Now they've managed to do it again. This model is NOT better than 1206. The benchmarks or whatever these people are aiming to beat are just an illusion. If your model cannot do simple tasks like outputting valid code without trying to force refactoring it is just a hot mess.&lt;/p&gt; &lt;p&gt;Why can't they get this right? They seem to regress a lot on updates. I've had discussions with people in the know, and apparently it's difficult to juggle the various needs of all the different types of people. Where some might like lengthy thorough answers for example, others might find that annoying and &amp;quot;too verbose&amp;quot;. So basically we get stuck with these half arsed models that don't seem to excel in anything in particular.&lt;/p&gt; &lt;p&gt;I use these models for coding and for writing, which has always been the case. I might be in the minority of users and just be too entitled about this. But jesus, what a disappointment.&lt;/p&gt; &lt;p&gt;I am not shitting you, when I say I would rather use deepseek than whatever this is. It's ability to give long thorough answers, without changing parts of code unintentionally is extremely valuable to my use cases.&lt;/p&gt; &lt;p&gt;Google is the biggest and most reliable when it comes to serving their models though, and I absolutely love the flash models for building apps. So you could say I am a major lover and hater of them. It's always felt this way. A genuine love-hate relationship. I am secretly rooting for their success but I absolutely loathe some of the things they do and am really surprised they haven't surpassed chatgpt/claude yet.. Like how the fuck?&lt;/p&gt; &lt;p&gt;Maybe it's time to outsource their LLM production to CHHHIIIIINNAAAA. Just like everything else. Hahahaa&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Environment-7193"&gt; /u/Odd-Environment-7193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirej3/the_new_gemini_pro_20_experimental_sucks_donkey/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iirej3/the_new_gemini_pro_20_experimental_sucks_donkey/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iirej3/the_new_gemini_pro_20_experimental_sucks_donkey/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-06T01:58:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiio9u</id>
    <title>Anthropic: ‘Please don’t use AI’</title>
    <updated>2025-02-05T19:36:56+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"&gt; &lt;img alt="Anthropic: ‘Please don’t use AI’" src="https://external-preview.redd.it/XdLbwNiaDfP6hGsSmn44MWaR_4YQK7L36Ar5RuZkt4s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f5b6dec6d423a65124ea27edb0de0e52f12e6ef" title="Anthropic: ‘Please don’t use AI’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;While we encourage people to use AI systems during their role to help them work faster and more effectively, please do not use AI assistants during the application process. We want to understand your personal interest in Anthropic without mediation through an AI system, and we also want to evaluate your non-AI-assisted communication skills. Please indicate ‘Yes’ if you have read and agree.&amp;quot;&lt;/p&gt; &lt;p&gt;There's a certain irony in having one of the biggest AI labs coming against AI applications and acknowledging the enshittification of the whole job application process.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/9b1e6af4-94f2-41c6-bb91-96a74b9b2da1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilrym</id>
    <title>Gemma 3 on the way!</title>
    <updated>2025-02-05T21:43:33+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"&gt; &lt;img alt="Gemma 3 on the way!" src="https://preview.redd.it/q2q4555s4ehe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be6c986a18108bcff251eb781a9cd1a0f4bcbd3" title="Gemma 3 on the way!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/osanseviero/status/1887247587776069957?t=xQ9khq5p-lBM-D2ntK7ZJw&amp;amp;s=19"&gt;https://x.com/osanseviero/status/1887247587776069957?t=xQ9khq5p-lBM-D2ntK7ZJw&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q2q4555s4ehe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T21:43:33+00:00</published>
  </entry>
</feed>
