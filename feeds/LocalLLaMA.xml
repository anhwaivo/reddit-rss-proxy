<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-18T17:05:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k21yub</id>
    <title>Is there a small tool-calling LLM?</title>
    <updated>2025-04-18T10:47:56+00:00</updated>
    <author>
      <name>/u/ashleigh_dashie</name>
      <uri>https://old.reddit.com/user/ashleigh_dashie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So basically i want to do an LLM game engine that resolves missing stuff via an llm. For that i need an LLM which complies with tool calling and actually calls tools whenever there's an opportunity. Is there such an LLM, that's small enough to not boil my room? Ideally a 7B one, it just needs to follow instructions it gets from tool calls.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashleigh_dashie"&gt; /u/ashleigh_dashie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k21yub/is_there_a_small_toolcalling_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k21yub/is_there_a_small_toolcalling_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k21yub/is_there_a_small_toolcalling_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T10:47:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1ahr4</id>
    <title>Wikipedia is giving AI developers its data to fend off bot scrapers - Data science platform Kaggle is hosting a Wikipedia dataset that’s specifically optimized for machine learning applications</title>
    <updated>2025-04-17T11:31:44+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ahr4/wikipedia_is_giving_ai_developers_its_data_to/"&gt; &lt;img alt="Wikipedia is giving AI developers its data to fend off bot scrapers - Data science platform Kaggle is hosting a Wikipedia dataset that’s specifically optimized for machine learning applications" src="https://preview.redd.it/d044iigqrdve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45e7348a9b7348bb6993397c3dbf74ba198c4943" title="Wikipedia is giving AI developers its data to fend off bot scrapers - Data science platform Kaggle is hosting a Wikipedia dataset that’s specifically optimized for machine learning applications" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Verge: &lt;a href="https://www.theverge.com/news/650467/wikipedia-kaggle-partnership-ai-dataset-machine-learning"&gt;https://www.theverge.com/news/650467/wikipedia-kaggle-partnership-ai-dataset-machine-learning&lt;/a&gt;&lt;br /&gt; Wikipedia Kaggle Dataset using Structured Contents Snapshot: &lt;a href="https://enterprise.wikimedia.com/blog/kaggle-dataset/"&gt;https://enterprise.wikimedia.com/blog/kaggle-dataset/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d044iigqrdve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ahr4/wikipedia_is_giving_ai_developers_its_data_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1ahr4/wikipedia_is_giving_ai_developers_its_data_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T11:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1hm53</id>
    <title>BLT model weights just dropped - 1B and 7B Byte-Latent Transformers released!</title>
    <updated>2025-04-17T16:50:53+00:00</updated>
    <author>
      <name>/u/QuackerEnte</name>
      <uri>https://old.reddit.com/user/QuackerEnte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1hm53/blt_model_weights_just_dropped_1b_and_7b/"&gt; &lt;img alt="BLT model weights just dropped - 1B and 7B Byte-Latent Transformers released!" src="https://a.thumbs.redditmedia.com/W3FZqegPamqJWv9vRdKzCg1qWqTclI5Hf1QcM-4Jjj8.jpg" title="BLT model weights just dropped - 1B and 7B Byte-Latent Transformers released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/gargighosh/status/1912908118939541884"&gt;https://x.com/gargighosh/status/1912908118939541884&lt;/a&gt; &lt;a href="https://github.com/facebookresearch/blt/pull/97"&gt;https://github.com/facebookresearch/blt/pull/97&lt;/a&gt; &lt;a href="https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/"&gt;https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;paper: &lt;a href="https://arxiv.org/abs/2412.09871"&gt;https://arxiv.org/abs/2412.09871&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuackerEnte"&gt; /u/QuackerEnte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k1hm53"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1hm53/blt_model_weights_just_dropped_1b_and_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1hm53/blt_model_weights_just_dropped_1b_and_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T16:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k238lw</id>
    <title>Does anyone else feel guilty using big models for tiny tasks?</title>
    <updated>2025-04-18T12:02:06+00:00</updated>
    <author>
      <name>/u/RightCup5772</name>
      <uri>https://old.reddit.com/user/RightCup5772</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if anyone else feels this way, but sometimes when I use a huge model for something super simple, I feel bad, like I'm wasting resources or something.&lt;/p&gt; &lt;p&gt;It feels like these LLMs are way too powerful for little tasks, and I shouldn't be wasting their &amp;quot;time&amp;quot; (even though I know it's not alive lol) or the computational resources.&lt;/p&gt; &lt;p&gt;Because of that, I set up Gemma 3 locally and now I use it for all my tiny tasks.&lt;/p&gt; &lt;p&gt;I can't fully explain why I feel like this — it's not really logical — but it's there.&lt;/p&gt; &lt;p&gt;Does anyone else feel the same way?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RightCup5772"&gt; /u/RightCup5772 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k238lw/does_anyone_else_feel_guilty_using_big_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k238lw/does_anyone_else_feel_guilty_using_big_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k238lw/does_anyone_else_feel_guilty_using_big_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T12:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1k22i44</id>
    <title>OpenAI naming is so confusing they need to include explanations inside Codex CLI system prompt</title>
    <updated>2025-04-18T11:20:26+00:00</updated>
    <author>
      <name>/u/fixtwin</name>
      <uri>https://old.reddit.com/user/fixtwin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22i44/openai_naming_is_so_confusing_they_need_to/"&gt; &lt;img alt="OpenAI naming is so confusing they need to include explanations inside Codex CLI system prompt" src="https://external-preview.redd.it/GzGxq7sjEIOdqa06p0Tb5ISsD5qYfTkIq7FrR23kigg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfa0db6f6bef47eb4fc9a05e50dd4119e49a45df" title="OpenAI naming is so confusing they need to include explanations inside Codex CLI system prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was going through Codex CLI system prompt and found this gem. As a reminder OpenAI released Codex LLM tuned for coding couple of years back. &lt;/p&gt; &lt;p&gt;Here’s the excerpt:&lt;/p&gt; &lt;p&gt;“The Codex CLI is open-sourced. Don't confuse yourself with the old Codex language model built by OpenAI many moons ago (this is understandably top of mind for you!). Within this context, Codex refers to the open-source agentic coding interface.”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fixtwin"&gt; /u/fixtwin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/openai/codex/blob/main/codex-cli/src/utils/agent/agent-loop.ts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22i44/openai_naming_is_so_confusing_they_need_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k22i44/openai_naming_is_so_confusing_they_need_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T11:20:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1rjm1</id>
    <title>How to run Llama 4 fast, even though it's too big to fit in RAM</title>
    <updated>2025-04-18T00:03:01+00:00</updated>
    <author>
      <name>/u/Klutzy-Snow8016</name>
      <uri>https://old.reddit.com/user/Klutzy-Snow8016</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: in your llama.cpp command, add:&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 49 --override-tensor &amp;quot;([0-9]+).ffn_.*_exps.=CPU&amp;quot; --ubatch-size 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Explanation:&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 49&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;offload all 49 layers to GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;--override-tensor &amp;quot;([0-9]+).ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;...except for the MOE weights&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;--ubatch-size 1&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;process the prompt in batches of 1 at a time (instead of the default 512 - otherwise your SSD will be the bottleneck and prompt processing will be slower)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This radically speeds up inference by taking advantage of LLama 4's MOE architecture. LLama 4 Maverick has 400 billion total parameters, but only 17 billion active parameters. Some are needed on every token generation, while others are only occasionally used. So if we put the parameters that are always needed onto GPU, those will be processed quickly, and there will just be a small number that need to be handled by the CPU. This works so well that the weights don't even need to all fit in your CPU's RAM - many of them can memory mapped from NVMe.&lt;/p&gt; &lt;p&gt;My results with Llama 4 Maverick:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unsloth's UD-Q4_K_XL quant is 227GB&lt;/li&gt; &lt;li&gt;Unsloth's Q8_0 quant is 397GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both of those are much bigger than my RAM + VRAM (128GB + 3x24GB). But with these tricks, I get 15 tokens per second with the UD-Q4_K_M and 6 tokens per second with the Q8_0.&lt;/p&gt; &lt;p&gt;Full llama.cpp server commands:&lt;/p&gt; &lt;p&gt;Note: the &lt;code&gt;--override-tensor&lt;/code&gt; command is tweaked because I had some extra VRAM available, so I offloaded most of the MOE layers to CPU, but loaded a few onto each GPU.&lt;/p&gt; &lt;p&gt;UD-Q4_K_XL:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m Llama-4-Maverick-17B-128E-Instruct-UD-Q4_K_XL-00001-of-00005.gguf -ngl 49 -fa -c 16384 --override-tensor &amp;quot;([1][1-9]|[2-9][0-9]).ffn_.*_exps.=CPU,([0-2]).ffn_.*_exps.=CUDA0,([3-6]).ffn_.*_exps.=CUDA1,([7-9]|[1][0]).ffn_.*_exps.=CUDA2&amp;quot; --ubatch-size 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Q8_0:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m Llama-4-Maverick-17B-128E-Instruct-Q8_0-00001-of-00009.gguf -ngl 49 -fa -c 16384 --override-tensor &amp;quot;([6-9]|[1-9][0-9]).ffn_.*_exps.=CPU,([0-1]).ffn_.*_exps.=CUDA0,([2-3]).ffn_.*_exps.=CUDA1,([4-5]).ffn_.*_exps.=CUDA2&amp;quot; --ubatch-size 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Credit goes to the people behind Unsloth for this knowledge. I hadn't seen people talking about this here, so I thought I'd make a post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Klutzy-Snow8016"&gt; /u/Klutzy-Snow8016 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1rjm1/how_to_run_llama_4_fast_even_though_its_too_big/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1rjm1/how_to_run_llama_4_fast_even_though_its_too_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1rjm1/how_to_run_llama_4_fast_even_though_its_too_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T00:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1xgco</id>
    <title>vLLM with transformers backend</title>
    <updated>2025-04-18T05:32:35+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can try out the new integration with which you can run ANY transformers model with vLLM (even if it is not natively supported by vLLM)&lt;/p&gt; &lt;p&gt;Read more about it here: &lt;a href="https://blog.vllm.ai/2025/04/11/transformers-backend.html"&gt;https://blog.vllm.ai/2025/04/11/transformers-backend.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What can one do with this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;1. Read the blog 😌&lt;/li&gt; &lt;li&gt;2. Contribute to transformers - making models vLLM compatible&lt;/li&gt; &lt;li&gt;3. Raise issues if you spot a bug with the integration&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Vision Language Model support is coming very soon! Until any further announcements, we would love for everyone to stick using this integration with text only models 🤗&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xgco/vllm_with_transformers_backend/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xgco/vllm_with_transformers_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xgco/vllm_with_transformers_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T05:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1k249wy</id>
    <title>Google’s Agent2Agent (A2A) Explained</title>
    <updated>2025-04-18T12:56:36+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Just published a new *FREE* blog post on Agent-to-Agent (A2A) – Google’s new framework letting AI systems collaborate like human teammates rather than working in isolation.&lt;/p&gt; &lt;p&gt;In this post, I explain:&lt;/p&gt; &lt;p&gt;- Why specialized AI agents need to talk to each other&lt;/p&gt; &lt;p&gt;- How A2A compares to MCP and why they're complementary&lt;/p&gt; &lt;p&gt;- The essentials of A2A&lt;/p&gt; &lt;p&gt;I've kept it accessible with real-world examples like planning a birthday party. This approach represents a fundamental shift where we'll delegate to teams of AI agents working together rather than juggling specialized tools ourselves.&lt;/p&gt; &lt;p&gt;Link to the full blog post: &lt;/p&gt; &lt;p&gt;&lt;a href="https://open.substack.com/pub/diamantai/p/googles-agent2agent-a2a-explained?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;https://open.substack.com/pub/diamantai/p/googles-agent2agent-a2a-explained?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k249wy/googles_agent2agent_a2a_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k249wy/googles_agent2agent_a2a_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k249wy/googles_agent2agent_a2a_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T12:56:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1nle9</id>
    <title>Inspired by the spinning heptagon test I created the forest fire simulation test (prompt in comments)</title>
    <updated>2025-04-17T21:00:19+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1nle9/inspired_by_the_spinning_heptagon_test_i_created/"&gt; &lt;img alt="Inspired by the spinning heptagon test I created the forest fire simulation test (prompt in comments)" src="https://external-preview.redd.it/Z2JsMWRwbGRrZ3ZlMYv6snaPu9pbXPiAyfaNKBLULaPCIm0pnygnovycxPje.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9eab23a4ac22667d41e99959dfc570a8f5b1ece0" title="Inspired by the spinning heptagon test I created the forest fire simulation test (prompt in comments)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eni76fldkgve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1nle9/inspired_by_the_spinning_heptagon_test_i_created/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1nle9/inspired_by_the_spinning_heptagon_test_i_created/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T21:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k29oe2</id>
    <title>QAT is slowly becoming mainstream now?</title>
    <updated>2025-04-18T16:52:07+00:00</updated>
    <author>
      <name>/u/__amberluz__</name>
      <uri>https://old.reddit.com/user/__amberluz__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just released a QAT optimized Gemma 3 - 27 billion parameter model. The quantization aware training claims to recover close to 97% of the accuracy loss that happens during the quantization. Do you think this is slowly becoming the norm? Will non-quantized safetensors slowly become obsolete?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__amberluz__"&gt; /u/__amberluz__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k29oe2/qat_is_slowly_becoming_mainstream_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28j02</id>
    <title>Llama 4 Maverick MLX performance on M3 Ultra</title>
    <updated>2025-04-18T16:03:30+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LM studio released an MLX update today so we can run Maverick in MLX format.&lt;/p&gt; &lt;p&gt;Q4 version numbers:&lt;/p&gt; &lt;p&gt;Prompt size: 12405&lt;br /&gt; Prompt eval rate: 332 t/s&lt;br /&gt; Token gen rate: 47.42&lt;/p&gt; &lt;p&gt;Right now for me there is a bug where it's not using prompt caching. Promising initial results though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28j02/llama_4_maverick_mlx_performance_on_m3_ultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:03:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28cqz</id>
    <title>I tried fine-tuning Qwen2.5 to generate git commit messages</title>
    <updated>2025-04-18T15:56:26+00:00</updated>
    <author>
      <name>/u/m19990328</name>
      <uri>https://old.reddit.com/user/m19990328</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I recently tried fine-tuning Qwen2.5-Coder-3B-Instruct to generate better commit messages. The main goal is to let it understand the idea behind code changes instead of simply repeating them. Qwen2.5-Coder-3B-Instruct is a sweet model that is capable in coding tasks and lightweight to run. Then, I fine tune it on the dataset &lt;a href="https://huggingface.co/datasets/Maxscha/commitbench"&gt;Maxscha/commitbench&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I think the results are honestly not bad. If the code changes focus on a main goal, the model can guess it pretty well. I released it as a python package and it is available now. You may check the fine tune script to see the training details as well. Hope you find them useful.&lt;/p&gt; &lt;p&gt;You can use it by first installing &lt;code&gt;pip install git-gen-utils&lt;/code&gt; and running &lt;code&gt;git-gen&lt;/code&gt;&lt;/p&gt; &lt;p&gt;🔗Source: &lt;a href="https://github.com/CyrusCKF/git-gen"&gt;https://github.com/CyrusCKF/git-gen&lt;/a&gt;&lt;br /&gt; 🤖Script: &lt;a href="https://github.com/CyrusCKF/git-gen/blob/main/finetune/finetune.ipynb"&gt;https://github.com/CyrusCKF/git-gen/blob/main/finetune/finetune.ipynb&lt;/a&gt;&lt;br /&gt; 🤗Model (on HuggingFace): &lt;a href="https://huggingface.co/CyrusCheungkf/git-commit-3B"&gt;https://huggingface.co/CyrusCheungkf/git-commit-3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/m19990328"&gt; /u/m19990328 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28cqz/i_tried_finetuning_qwen25_to_generate_git_commit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28cqz/i_tried_finetuning_qwen25_to_generate_git_commit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28cqz/i_tried_finetuning_qwen25_to_generate_git_commit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1k22e41</id>
    <title>Good news: 5090s now in stock in my local market. Bad news: cheapest is $3,550</title>
    <updated>2025-04-18T11:13:40+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now I wonder if I should have just bought the 2nd hand 3090s that were on sale for $700.&lt;/p&gt; &lt;p&gt;Can someone tell me what the typical 'street price' for 5090s in the US?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22e41/good_news_5090s_now_in_stock_in_my_local_market/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22e41/good_news_5090s_now_in_stock_in_my_local_market/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k22e41/good_news_5090s_now_in_stock_in_my_local_market/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T11:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1vvy3</id>
    <title>No API keys, no cloud. Just local Al + tools that actually work. Too much to ask?</title>
    <updated>2025-04-18T03:56:37+00:00</updated>
    <author>
      <name>/u/aruntemme</name>
      <uri>https://old.reddit.com/user/aruntemme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been about a month since we first posted Clara here.&lt;/p&gt; &lt;p&gt;Clara is a local-first AI assistant - think of it like ChatGPT, but fully private and running on your own machine using Ollama.&lt;/p&gt; &lt;p&gt;Since the initial release, I've had a small group of users try it out, and I've pushed several updates based on real usage and feedback.&lt;/p&gt; &lt;p&gt;The biggest update is that Clara now comes with n8n built-in.&lt;/p&gt; &lt;p&gt;That means you can now build and run your own tools directly inside the assistant - no setup needed, no external services. Just open Clara and start automating.&lt;/p&gt; &lt;p&gt;With the n8n integration, Clara can now do more than chat. You can use it to:&lt;/p&gt; &lt;p&gt;• Check your emails • Manage your calendar • Call APIs • Run scheduled tasks • Process webhooks • Connect to databases • And anything else you can wire up using n8n's visual flow builder&lt;/p&gt; &lt;p&gt;The assistant can trigger these workflows directly - so you can talk to Clara and ask it to do real tasks, using tools that run entirely on your&lt;/p&gt; &lt;p&gt;device.&lt;/p&gt; &lt;p&gt;Everything happens locally. No data goes out, no accounts, no cloud dependency.&lt;/p&gt; &lt;p&gt;If you're someone who wants full control of your AI and automation setup, this might be something worth trying.&lt;/p&gt; &lt;p&gt;You can check out the project here:&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/badboysm890/ClaraVerse"&gt;https://github.com/badboysm890/ClaraVerse&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to everyone who's been trying it and sending feedback. Still improving things - more updates soon.&lt;/p&gt; &lt;p&gt;Note: I'm aware of great projects like OpenWebUI and LibreChat. Clara takes a slightly different approach - focusing on reducing dependencies, offering a native desktop app, and making the overall experience more user-friendly so that more people can easily get started with local AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aruntemme"&gt; /u/aruntemme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1vvy3/no_api_keys_no_cloud_just_local_al_tools_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1vvy3/no_api_keys_no_cloud_just_local_al_tools_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1vvy3/no_api_keys_no_cloud_just_local_al_tools_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T03:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1fi5w</id>
    <title>New society is taking shape</title>
    <updated>2025-04-17T15:24:23+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fi5w/new_society_is_taking_shape/"&gt; &lt;img alt="New society is taking shape" src="https://preview.redd.it/05n7cxquxeve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3503ecb35d404084ae9522bb771aecd69177ee0c" title="New society is taking shape" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/05n7cxquxeve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fi5w/new_society_is_taking_shape/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1fi5w/new_society_is_taking_shape/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T15:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k22lyx</id>
    <title>FULL LEAKED Replit Agent System Prompts and Tools</title>
    <updated>2025-04-18T11:26:40+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest system prompt: 18/04/2025)&lt;/p&gt; &lt;p&gt;I managed to get full official Replit Agent system prompts, including its tools (JSON). Over 400 lines.&lt;/p&gt; &lt;p&gt;You can check it out at: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k22lyx/full_leaked_replit_agent_system_prompts_and_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T11:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1v9rq</id>
    <title>CSM 1B is real-time now and has fine-tuning</title>
    <updated>2025-04-18T03:21:15+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/davidbrowne17/csm-streaming"&gt;https://github.com/davidbrowne17/csm-streaming&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not sure if many of you have been following this model, but the open-source community has managed to reach real-time with streaming and figured out fine-tuning. This is my repo with fine-tuning and a real-time local chat demo, my version of fine-tuning is lora but there is also full fine tuning out there as well. Give it a try and let me know how it compares to other TTS models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1v9rq/csm_1b_is_realtime_now_and_has_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1v9rq/csm_1b_is_realtime_now_and_has_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1v9rq/csm_1b_is_realtime_now_and_has_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T03:21:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1qpr6</id>
    <title>microsoft/MAI-DS-R1, DeepSeek R1 Post-Trained by Microsoft</title>
    <updated>2025-04-17T23:22:11+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1qpr6/microsoftmaidsr1_deepseek_r1_posttrained_by/"&gt; &lt;img alt="microsoft/MAI-DS-R1, DeepSeek R1 Post-Trained by Microsoft" src="https://external-preview.redd.it/oacNTVfe15Ozahhiv8YMZ-Teu__pPBVygtAgzE9FP3c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=140a805e8a294974bbef97d4e1035ef969130c5a" title="microsoft/MAI-DS-R1, DeepSeek R1 Post-Trained by Microsoft" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/MAI-DS-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1qpr6/microsoftmaidsr1_deepseek_r1_posttrained_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1qpr6/microsoftmaidsr1_deepseek_r1_posttrained_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-17T23:22:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k1xvvr</id>
    <title>Where is the promised open Grok 2?</title>
    <updated>2025-04-18T06:01:19+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As far as I know, Grok 2 was supposed to be open-sourced some time after Grok 3's release. But I'm afraid that by the time they decide to open-source Grok 2, it will already be completely obsolete. This is because even now, it significantly lags behind in performance compared to the likes of DeepSeek V3, and we also have Qwen 3 and Llama 4 Reasoning on the horizon (not to mention a potential open model from OpenAI). I believe that when they eventually decide to release it to the community, it will be of no use to anyone anymore, much like what happened with Grok 1. What are your thoughts on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k1xvvr/where_is_the_promised_open_grok_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T06:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28ulo</id>
    <title>Time to step up the /local reasoning game</title>
    <updated>2025-04-18T16:17:11+00:00</updated>
    <author>
      <name>/u/vornamemitd</name>
      <uri>https://old.reddit.com/user/vornamemitd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt; &lt;img alt="Time to step up the /local reasoning game" src="https://preview.redd.it/wtibm8c3cmve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f80a0bad3e3f79619d29663e49d519eaa7898d" title="Time to step up the /local reasoning game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Latest OAI models tucked away behind intrusive &amp;quot;ID verification&amp;quot;....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vornamemitd"&gt; /u/vornamemitd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wtibm8c3cmve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28ulo/time_to_step_up_the_local_reasoning_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T16:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1k27fz2</id>
    <title>I created an interactive tool to visualize *every* attention weight matrix within GPT-2!</title>
    <updated>2025-04-18T15:18:17+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt; &lt;img alt="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" src="https://external-preview.redd.it/YW45M2FibXYwbXZlMWaepLM_4Oin4KjR_zAxiUwp5NOaLzCHkxa3urw0ZqL6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60742f26e3a407482898d8e82f2a5d6e8f6ee5f" title="I created an interactive tool to visualize *every* attention weight matrix within GPT-2!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dgo9qamv0mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k27fz2/i_created_an_interactive_tool_to_visualize_every/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250fu</id>
    <title>Gemma 3 QAT launch with MLX, llama.cpp, Ollama, LM Studio, and Hugging Face</title>
    <updated>2025-04-18T13:31:34+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Some weeks ago we released GGUFs corresponding to the QAT checkpoints of Gemma 3. Thanks to QAT, the model is able to preserve similar quality as &lt;code&gt;bfloat16&lt;/code&gt; while significantly reducing the memory requirements to load the model. That is, QAT is an additional fine-tuning that makes the model more rigorous to quantization.&lt;/p&gt; &lt;p&gt;As we only released the GGUFs, we got feedback that it would be great to have the unquantized QAT-based checkpoints to allow people to quantize for their own tools. So...we did it! Today we're releasing the unquantized QAT-based checkpoints. The models preserve quality better than naive quantization. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We also collaborated with Prince (from MLX), llama.cpp, Ollama, LM Studio, and Hugging Face to make sure you can use the models in all your favorite tools!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog post : &lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/"&gt;https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Unquantized checkpoints: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Ollama: &lt;a href="https://ollama.com/library/gemma3"&gt;https://ollama.com/library/gemma3&lt;/a&gt; (try ollama run gemma3:12b-it-qat)&lt;/li&gt; &lt;li&gt;LM Studio: &lt;a href="https://lmstudio.ai/model/gemma-3-12b-it-qat"&gt;https://lmstudio.ai/model/gemma-3-12b-it-qat&lt;/a&gt; &lt;/li&gt; &lt;li&gt;MLX: &lt;a href="https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae"&gt;https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae&lt;/a&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: &lt;a href="https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b"&gt;https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Enjoy! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250fu/gemma_3_qat_launch_with_mlx_llamacpp_ollama_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k250r6</id>
    <title>New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality.</title>
    <updated>2025-04-18T13:32:01+00:00</updated>
    <author>
      <name>/u/Sea_Sympathy_495</name>
      <uri>https://old.reddit.com/user/Sea_Sympathy_495</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt; &lt;img alt="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." src="https://external-preview.redd.it/5lq32BTIzHqmPYcHvNrCp8JMhag9gsSSkR3cQgoYZBU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed6a861b423ef5ef481e863b5c6947b3cef14c0c" title="New QAT-optimized int4 Gemma 3 models by Google, slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Sympathy_495"&gt; /u/Sea_Sympathy_495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/?linkId=14034718"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k250r6/new_qatoptimized_int4_gemma_3_models_by_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k28f3f</id>
    <title>Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark</title>
    <updated>2025-04-18T15:59:16+00:00</updated>
    <author>
      <name>/u/ZhalexDev</name>
      <uri>https://old.reddit.com/user/ZhalexDev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt; &lt;img alt="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" src="https://external-preview.redd.it/d3J6N2xwMm84bXZlMeIZf5sR-oXFPwhpDTHMtN-Je-w0GMxJeu96UcIYpm6F.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74e3a1f897d051cfccf4d8820a610d3c5dbe54b1" title="Playing DOOM II and 19 other DOS/GB games with LLMs as a new benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AK (@akhaliq)&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce a research preview of VideoGameBench, a benchmark which challenges vision-language models to complete, in real-time, a suite of 20 different popular video games from both hand-held consoles and PC &lt;/p&gt; &lt;p&gt;GPT-4o, Claude Sonnet 3.7, Gemini 2.5 Pro, and Gemini 2.0 Flash playing Doom II (default difficulty) on VideoGameBench-Lite with the same input prompt! Models achieve varying levels of success but none are able to pass even the first level.&amp;quot;&lt;/p&gt; &lt;p&gt;project page: &lt;a href="https://vgbench.com"&gt;https://vgbench.com&lt;/a&gt; &lt;/p&gt; &lt;p&gt;try on other games: &lt;a href="https://github.com/alexzhang13/VideoGameBench"&gt;https://github.com/alexzhang13/VideoGameBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZhalexDev"&gt; /u/ZhalexDev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1i2op2o8mve1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k28f3f/playing_doom_ii_and_19_other_dosgb_games_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T15:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1k25876</id>
    <title>Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama</title>
    <updated>2025-04-18T13:41:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt; &lt;img alt="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" src="https://preview.redd.it/23ut7jd3klve1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f940165ab5ba660103d9f5f61872b1dc70698cbb" title="Google QAT - optimized int4 Gemma 3 slash VRAM needs (54GB -&amp;gt; 14.1GB) while maintaining quality - llama.cpp, lmstudio, MLX, ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/23ut7jd3klve1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k25876/google_qat_optimized_int4_gemma_3_slash_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-18T13:41:47+00:00</published>
  </entry>
</feed>
