<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-07T22:37:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lu75js</id>
    <title>Please gut-check these W7900 vs 7900 XTX server builds</title>
    <updated>2025-07-07T21:50:18+00:00</updated>
    <author>
      <name>/u/rgroadie2707</name>
      <uri>https://old.reddit.com/user/rgroadie2707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;(US$ first, ₹ in brackets, used ChatGPT to capture the requirements)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Constraints&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Item&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Limit&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Total budget&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;US $ 6 000&lt;/strong&gt; (₹ 5 16 000) — can stretch to &lt;strong&gt;₹ 6 00 000&lt;/strong&gt; if the spec really earns it&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Workload&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Mostly LLM &lt;strong&gt;inference&lt;/strong&gt; on 7 B → 70 B models, occasional fine-tune (FP16/BF16 + Q-formats)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Have already&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;30 TB SATA array • Cooler Master 1 000 W PSU (fine for ≤2 GPUs)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Currency used: &lt;strong&gt;US $ at ₹ 86 = $ 1&lt;/strong&gt; (July ’25 Indian street pricing).&lt;/p&gt; &lt;h1&gt;Four build options (all totals include GPU + CPU + board + 128 GB RAM + PSU upgrade if needed + case)**&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;ID&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;GPUs&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;CPU • Board&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;PSU need&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Total cost US $ (₹)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Why it tempts me / worries me&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;A – “One-and-done”&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1 × W7900 48 GB&lt;/strong&gt; – $ 2 950 (₹ 2 54 k)&lt;/td&gt; &lt;td align="left"&gt;Ryzen 9 7950X • mid-tier X670E&lt;/td&gt; &lt;td align="left"&gt;1 000 W (reuse)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$ 4 700&lt;/strong&gt; (₹ 4 04 k)&lt;/td&gt; &lt;td align="left"&gt;✅ Single card, ECC, 48 GB holds 70 B FP16❌ Lower raw FP16 vs XTX, ROCm quirks?&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;B – “Dual XTX”&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2 × 7900 XTX 24 GB&lt;/strong&gt; – $ 2 160 (₹ 1 86 k)&lt;/td&gt; &lt;td align="left"&gt;Ryzen 9 7950X • X670E&lt;/td&gt; &lt;td align="left"&gt;1 000 W (reuse)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$ 3 900&lt;/strong&gt; (₹ 3 36 k)&lt;/td&gt; &lt;td align="left"&gt;✅ Best $/TFLOP, huge community tips ✅ Big headroom for better RAM / NVMe❌ Split VRAM – no single-GPU 70 B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;C – “Mixed bag”&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1 × W7900 + 1 × 7900 XTX&lt;/strong&gt; – $ 4 035 (₹ 3 47 k)&lt;/td&gt; &lt;td align="left"&gt;Ryzen 9 7950X • X670E&lt;/td&gt; &lt;td align="left"&gt;1 200 W upgrade (+$ 140 / ₹ 12 k)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$ 5 920&lt;/strong&gt; (₹ 5 09 k)&lt;/td&gt; &lt;td align="left"&gt;✅ 72 GB total, still one 48 GB card❌ Any ROCm pain mixing Pro + consumer drivers?&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;D – “Three XTX”&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3 × 7900 XTX 24 GB&lt;/strong&gt; – $ 3 240 (₹ 2 79 k)&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Used&lt;/em&gt; Threadripper 3970X • TRX40&lt;/td&gt; &lt;td align="left"&gt;1 600 W Platinum (+$ 523 / ₹ 45 k)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$ 6 520&lt;/strong&gt; (₹ 5 61 k) ← still under 6 L&lt;/td&gt; &lt;td align="left"&gt;✅ 72 GB &amp;amp; tensor-parallel across 3 cards ✅ Lots of PCIe lanes❌ Old platform, higher power/heat&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;* Assumptions per build&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt; — 128 GB (4 × 32 GB) DDR5-5600 for AM5 builds ($ 523 / ₹ 45 k) or DDR4-3200 ECC for TRX40 ($ 372 / ₹ 32 k)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Case + fans&lt;/strong&gt; — roomy SSI-EEB or XL-ATX chassis (~$ 233 / ₹ 20 k)&lt;/li&gt; &lt;li&gt;Storage reused; NVMe OS drive not factored (≲$ 100).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I need from you&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Is one 48 GB W7900 (Option A) really simpler&lt;/strong&gt; for &amp;gt;40 B models, or do clever tensor-parallel tricks make dual/triple XTX just as usable?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ROCm stability&lt;/strong&gt; – any real-life driver differences between W-class and XTX? Horror stories mixing them (Option C)?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-XTX on PCIe 4 ×8&lt;/strong&gt; (Options B &amp;amp; D): do you actually hit bottlenecks in inference?&lt;/li&gt; &lt;li&gt;If you’ve built &lt;strong&gt;TRX40/3970X rigs recently&lt;/strong&gt; (Option D), are they still worth it in 2025 vs a fresh AM5 or TRX50?&lt;/li&gt; &lt;li&gt;Any smarter combo that sneaks 48 GB+ of contiguous VRAM into the box while still living under &lt;strong&gt;₹ 6 00 000 / US $ 7 k&lt;/strong&gt;?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Benchmarks, thermals, power-draw screenshots, “don’t do it!” tales—everything helps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Help me burn (or save) this budget wisely—thanks!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rgroadie2707"&gt; /u/rgroadie2707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu75js/please_gutcheck_these_w7900_vs_7900_xtx_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu75js/please_gutcheck_these_w7900_vs_7900_xtx_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu75js/please_gutcheck_these_w7900_vs_7900_xtx_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T21:50:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu7hd6</id>
    <title>Has anyone set up a generalized work/research assistant?</title>
    <updated>2025-07-07T22:03:53+00:00</updated>
    <author>
      <name>/u/JanusTheDoorman</name>
      <uri>https://old.reddit.com/user/JanusTheDoorman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is kind of just a &amp;quot;wishlist&amp;quot;, but I'm looking for locally hosted alternatives to a variety of work tools like SciSpace/Notebook LM, Zapier, etc.&lt;/p&gt; &lt;p&gt;Recently I've been building out a knowledge management system using Obsidian/Dataview to track and link research topics, events/conferences/journals/specific papers, other researchers in the field, etc. and I'm looking to tie it together with my email, calendar, to-do list, etc.&lt;/p&gt; &lt;p&gt;There are lots of domain-specific subtools, etc. but ideally I'm shooting for an integrated system that would, e.g. ingest an RSS feed of research papers (overnight if necessary), identify which papers are relevant to research topics in my notes, follow the links in the markdown files to identify related people and events, look through my emails/calendar to see if there are recent contacts from those people or upcoming events where I'd be in contact with them, and add a tentative &amp;quot;Ask XYZ person about result ABC from this paper&amp;quot; to my to-do list.&lt;/p&gt; &lt;p&gt;In other words, combine continual ingestion of new content, and manage updates to the knowledge base itself (inclusive of notes, emails, calendar, to-do's as part of the knowledge base). It seems that Zapier might be able to handle that with the right set of automations, but both the pricing and privacy concerns are pushing me towards a locally hosted solution.&lt;/p&gt; &lt;p&gt;Thus far I've really only played with basic tools like LM Studio and Anything LLM, with a bit of work using guidance directly with llama.cpp. It seems like LangGraph would be the baseline tool for stringing together a more complex set of agents like this, but I'm not sure if that's the best place to start or if there are more developed tools that would make this easier, or if others have already developed a framework for this kind of automation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JanusTheDoorman"&gt; /u/JanusTheDoorman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7hd6/has_anyone_set_up_a_generalized_workresearch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7hd6/has_anyone_set_up_a_generalized_workresearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7hd6/has_anyone_set_up_a_generalized_workresearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T22:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu1z10</id>
    <title>Learning triton &amp; cuda: How far can colab + nsight-compute take me?</title>
    <updated>2025-07-07T18:29:55+00:00</updated>
    <author>
      <name>/u/Zealousideal_Elk109</name>
      <uri>https://old.reddit.com/user/Zealousideal_Elk109</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks!&lt;/p&gt; &lt;p&gt;I've recently been learning Triton and CUDA, writing my own kernels and optimizing them using a lot of great tricks I’ve picked up from blog-posts and docs. However, I currently don’t have access to any local GPUs.&lt;/p&gt; &lt;p&gt;Right now, I’m using Google Colab with T4 GPUs to run my kernels. I collect telemetry and kernel stats using nsight-compute, then download the reports and inspect them locally using the GUI.&lt;/p&gt; &lt;p&gt;It’s been workable thus far, but I’m wondering: how far can I realistically go with this workflow? I’m also a bit concerned about optimizing against the T4, since it’s now three generations behind the latest architecture and I’m not sure how transferable performance insights will be.&lt;/p&gt; &lt;p&gt;Also, I’d love to hear how you are writing and profiling your kernels, especially if you're doing inference-time optimizations. Any tips or suggestions would be much appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Elk109"&gt; /u/Zealousideal_Elk109 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T18:29:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltnpsl</id>
    <title>How good is Qwen3-14B for local use? Any benchmarks vs other models?</title>
    <updated>2025-07-07T07:14:32+00:00</updated>
    <author>
      <name>/u/abubakkar_s</name>
      <uri>https://old.reddit.com/user/abubakkar_s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I'm looking into running a larger language model locally and came across Qwen3-14B (or Qwen3\_14B depending on naming). I know it's been getting some hype lately, but I wanted to hear from people who’ve actually used it.&lt;/p&gt; &lt;p&gt;* How does it perform compared to other 13B/14B class models like Gemma, Mistral, LLaMA 2/3, Yi, etc.?&lt;/p&gt; &lt;p&gt;* Any real-world performance/benchmark comparisons in terms of speed, context handling, or reasoning?&lt;/p&gt; &lt;p&gt;* How’s the quantization support (GGUF/ExLlama/AutoGPTQ)? Is it efficient enough to run on a single GPU (e.g. 24GB VRAM of Macmini m4, token/secs)?&lt;/p&gt; &lt;p&gt;* How does it do with coding, long-context tasks, or general instruction following?&lt;/p&gt; &lt;p&gt;Would like to hear your experience, whether it’s through serious benchmarking or just specific use. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abubakkar_s"&gt; /u/abubakkar_s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T07:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu818k</id>
    <title>Locally run TTS Models</title>
    <updated>2025-07-07T22:26:57+00:00</updated>
    <author>
      <name>/u/Tankerspam</name>
      <uri>https://old.reddit.com/user/Tankerspam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I'm not familiar with coding in general and have been banging my head against chatGPT and online tutorials trying to make things such as Tortoise-TTS work, but it's so out of date that ChatGPT can't help me install it because of the amount of deprecation and I just don't know what I'm doing.&lt;/p&gt; &lt;p&gt;Does anyone have a simple, easy to use, preferably GUI TTS that is simple to install?&lt;/p&gt; &lt;p&gt;I thought bark_win might work, but nope, the 1 click installer doesn't download all the packages and after attempting to install them it still won't run. I'm not skilled enough in this area to figure this out. I'm trying to TTS Univeristy readings so I can listen to them.&lt;/p&gt; &lt;p&gt;Won't lie it's been incredibly frustrating, I spent literally 8 hours yesterday trying to make tortoise-tts work. (Well actually it would run, but has a word limit of each run, and won't save the hash for the AI model it generates between runs, so to TTS a reading would take a solid day of me sitting there babying it.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tankerspam"&gt; /u/Tankerspam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu818k/locally_run_tts_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu818k/locally_run_tts_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu818k/locally_run_tts_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T22:26:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltbrlf</id>
    <title>🎧 Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)</title>
    <updated>2025-07-06T20:53:01+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/"&gt; &lt;img alt="🎧 Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)" src="https://preview.redd.it/bwd1gqkrfbbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57eba9e11159f3d51759d5ca917254faf9332203" title="🎧 Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We have been exploring various open-source Text-to-Speech (TTS) models, and decided to create a Hugging Face demo space that makes it easy to compare their quality side-by-side.&lt;/p&gt; &lt;p&gt;The demo features &lt;strong&gt;12 popular TTS models&lt;/strong&gt;, all tested using a consistent prompt, so you can quickly hear and compare their synthesized speech and choose the best one for your audio projects.&lt;/p&gt; &lt;p&gt;Would love to get feedback or suggestions!&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://huggingface.co/spaces/Inferless/Open-Source-TTS-Gallary"&gt;Check out the demo space and detailed comparison here!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://www.inferless.com/learn/comparing-different-text-to-speech---tts--models-part-2"&gt;Check out the blog: Choosing the Right Text-to-Speech Model: Part 2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Share your use-case and we will update this space as required! &lt;/p&gt; &lt;p&gt;Which TTS model sounds most natural to you?&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwd1gqkrfbbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T20:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltstdt</id>
    <title>[PAPER] Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs</title>
    <updated>2025-07-07T12:25:56+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The thought progress bar looks cool.&lt;/p&gt; &lt;p&gt;Unfortunately, this needs to train something to modify hidden state.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://royeisen.github.io/OverclockingLLMReasoning-paper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltstdt/paper_overclocking_llm_reasoning_monitoring_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltstdt/paper_overclocking_llm_reasoning_monitoring_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T12:25:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltt72w</id>
    <title>Free context tool that runs local</title>
    <updated>2025-07-07T12:44:12+00:00</updated>
    <author>
      <name>/u/wuu73</name>
      <uri>https://old.reddit.com/user/wuu73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I believe my tool is unique even though there are like 40 different similar tools for giving LLMs context of lots of code files. Different for:&lt;/p&gt; &lt;p&gt;Saving the state of which files you include for next time you use it in that same directory,&lt;/p&gt; &lt;p&gt;The User Interface (works anywhere python and Qt can run) can just type ‘aicp + enter’. Option to install right click menu on any OS for finder, file explorer, nautilus. &lt;/p&gt; &lt;p&gt;Prompt on top and/or bottom (both can enhance response from LLM)&lt;/p&gt; &lt;p&gt;Preset buttons, can add your own bits of text you find yourself asking often, like “write solution in single code tag to paste into Cline or Cursor”. &lt;/p&gt; &lt;p&gt;I posted here cuz it runs local and does not need GitHub like some of the similar tools. I get some great feedback and there is a thing in the help menu to complain or send your thoughts about it anonymously. Easy install with pipx.&lt;/p&gt; &lt;p&gt;&lt;a href="https://wuu73.org/aicp"&gt;https://wuu73.org/aicp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hate those tech bro phrases so I really hate to even say this but “context engineering” does seem appropriate lol that is what the tool does basically &lt;/p&gt; &lt;p&gt;Shaves off seconds every time you have to IDE &amp;lt;——&amp;gt; tabs of web chat interfaces&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuu73"&gt; /u/wuu73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltt72w/free_context_tool_that_runs_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltt72w/free_context_tool_that_runs_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltt72w/free_context_tool_that_runs_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T12:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltamap</id>
    <title>Cheapest way to stack VRAM in 2025?</title>
    <updated>2025-07-06T20:04:49+00:00</updated>
    <author>
      <name>/u/gnad</name>
      <uri>https://old.reddit.com/user/gnad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to get a total of at least 140 GB RAM/VRAM combined to run Qwen 235B Q4. Current i have 96 GB RAM so next step is to get some cheap VRAM. After some research i found the following options at around 1000$ each: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;4x RTX 3060 (48 GB)&lt;/li&gt; &lt;li&gt;4x P100 (64 GB)&lt;/li&gt; &lt;li&gt;3x P40 (72 GB)&lt;/li&gt; &lt;li&gt;3x RX 9060 (48 GB)&lt;/li&gt; &lt;li&gt;4x MI50 32GB (128GB)&lt;/li&gt; &lt;li&gt;3x RTX 4060 ti/5060 ti (48 GB)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Edit: add more suggestion from comments. &lt;/p&gt; &lt;p&gt;Which GPU do you recommend or is there anything else better? I know 3090 is king here but cost per GB is around double the above GPU. Any suggestion is appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnad"&gt; /u/gnad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T20:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltgayn</id>
    <title>Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA</title>
    <updated>2025-07-07T00:18:28+00:00</updated>
    <author>
      <name>/u/woct0rdho</name>
      <uri>https://old.reddit.com/user/woct0rdho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/"&gt; &lt;img alt="Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA" src="https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=094a72eeef2838a876e8835f533b8146c2ead2a9" title="Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a for loop to access the experts, resulting in &amp;lt; 20% GPU usage. It's been two months and there are still very few LoRAs of Qwen3-30B-A3B in the public. (If you search 'qwen3 30b a3b lora' on HuggingFace, that's... interesting)&lt;/p&gt; &lt;p&gt;This should be made easier. I've made a fused version of Qwen3 MoE Layer that's much faster, while being compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. On a single GPU with 24GB VRAM, it reaches 100% GPU usage and 5x speedup of training compared to the unfused model.&lt;/p&gt; &lt;p&gt;There is still room for further optimization, but you can try it now and train your own LoRA.&lt;/p&gt; &lt;p&gt;Also, please help if you know how to upstream this to Transformers or Unsloth. (Transformers itself never includes Triton or CUDA kernels in the package, but they have a HuggingFace Kernels project to do so.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woct0rdho"&gt; /u/woct0rdho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/woct0rdho/transformers-qwen3-moe-fused"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T00:18:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltxzad</id>
    <title>Hardware recommendations? Mac Mini, NVIDIA Orin, Ryzen AI... ?</title>
    <updated>2025-07-07T16:00:41+00:00</updated>
    <author>
      <name>/u/lizard121n6</name>
      <uri>https://old.reddit.com/user/lizard121n6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there! I recently started being interested in getting an &amp;quot;affordable&amp;quot; Mini PC type machine that can run LLMs without being too power hungry. &lt;/p&gt; &lt;p&gt;The first challenge is to try and understand what is required for this. What I have gathered so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAM is important (double the model size in billions and leave room for some overhead, e.g. 7B*2 = 14 =&amp;gt; 16GB should work)&lt;/li&gt; &lt;li&gt;Memory Bandwidth is another very important factor, which is why graphics cards with enough VRAM work better than CPUs with much more RAM&lt;/li&gt; &lt;li&gt;There are options with shared/unified RAM, especially the Apple Silicon ones&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That being said, I just don't know how to find out what to get. So many options, so little information. No LLM benchmarks.&lt;/p&gt; &lt;p&gt;The Apple Silicon Chips are doing a good job with their high RAM configurations and unified RAM and good bandwidth. So what about Ryzen AI, e.g. AMD Ryzen AI 9 HX370. It has a CPU, GPU, NPU; where would the LLM run, can it run on the NPU? Ho do I know how the performance compares with e.g. a Mac Mini M2 Pro? And then there are dedicated AI options like the NVIDIA Orin NX, which come with &amp;quot;only&amp;quot; 16GB of RAM max. I also tried running LLama 3.1 7B on my 2060 Super and the result was satisfactory.. So some Mini-PC with a decent graphics card might also work?&lt;/p&gt; &lt;p&gt;I just don't know where to start, what to buy, how do I find out? &lt;/p&gt; &lt;p&gt;What I really want is the best option for 500-800€. A setup with a full sized (external) graphics card is not an option. I would love for it to be upgradeable. I started with just wanting to tinker with a RasPI-AI Hat and then everything grew from there. I don't have huge demands, running a 7B model on an (upgradeable) Mini-PC would make me happy. &lt;/p&gt; &lt;p&gt;Some examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GMtec Evo X1 (AMD Ryzen AI 9 HX370 with unified memory (?))&lt;/li&gt; &lt;li&gt;Mac Mini M2 Pro&lt;/li&gt; &lt;li&gt;Mac Mini M4&lt;/li&gt; &lt;li&gt; MINISFORUM AI X1 370&lt;/li&gt; &lt;li&gt;NVIDIA Orin NX 8/16GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am very thankful for any advice!&lt;/p&gt; &lt;p&gt;Edit: &lt;a href="https://ivoras.substack.com/p/4-month-minipc-review-minisforum"&gt;Minisforum&lt;/a&gt; doesnt seem to be suited for my case. Probably the same for the GMtec&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lizard121n6"&gt; /u/lizard121n6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxzad/hardware_recommendations_mac_mini_nvidia_orin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxzad/hardware_recommendations_mac_mini_nvidia_orin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxzad/hardware_recommendations_mac_mini_nvidia_orin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T16:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltvkqq</id>
    <title>(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama.</title>
    <updated>2025-07-07T14:26:54+00:00</updated>
    <author>
      <name>/u/DanielKramer_</name>
      <uri>https://old.reddit.com/user/DanielKramer_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltvkqq/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt; &lt;img alt="(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama." src="https://external-preview.redd.it/9sqw7guDNkr_lh4DzQzAQ3_oGbJPe0qHLVbjofkhPuc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d5b553bbacb3aa769ebe7746d6025ee8190093ba" title="(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a small project I built for my own purposes: Kramer UI for Ollama.&lt;/p&gt; &lt;p&gt;I love Ollama for its simplicity and its model management, but setting up a UI for it has always been a pain point. I used to use OpenWebUI and it was great, but I'd rather not have to set up docker. And using Ollama through the CLI makes me feel like a simpleton because I can't even edit my messages.&lt;/p&gt; &lt;p&gt;I wanted a UI as simple as Ollama to accompany it. So I built it. Kramer UI is a single, portable executable file for Windows. There's no installer. You just run the .exe and you're ready to start chatting.&lt;/p&gt; &lt;p&gt;My goal was to make interacting with your local models as frictionless as possible.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses 45mb of ram&lt;/li&gt; &lt;li&gt;Edit your messages&lt;/li&gt; &lt;li&gt;Models' thoughts are hidden behind dropdown&lt;/li&gt; &lt;li&gt;Model selector&lt;/li&gt; &lt;li&gt;Currently no support for conversation history&lt;/li&gt; &lt;li&gt;You can probably compile it for Linux and Mac too&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can download the executable directly from the GitHub releases page [here.] (&lt;a href="https://github.com/dvkramer/kramer-ui/releases/"&gt;https://github.com/dvkramer/kramer-ui/releases/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wn2nw8zjrgbf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca6f0470fa54f4ae06a9a19a88cf9c3fbbe8632e"&gt;https://preview.redd.it/wn2nw8zjrgbf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca6f0470fa54f4ae06a9a19a88cf9c3fbbe8632e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All feedback, suggestions, and ideas are welcome! Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielKramer_"&gt; /u/DanielKramer_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltvkqq/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltvkqq/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltvkqq/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T14:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu7506</id>
    <title>What are the best options currently for a real time voice chat?</title>
    <updated>2025-07-07T21:49:42+00:00</updated>
    <author>
      <name>/u/vulcan4d</name>
      <uri>https://old.reddit.com/user/vulcan4d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m building a safe, easy-to-use voice chat powered by an LLM for my kids and something that enhances their learning at home while keeping it fun. So far, I haven’t found a solution that’s both reliable and user-friendly. I’m running a local Ollama server with Open WebUI and tried using the chat feature alongside Kokoro TTS, but it repeatedly freezes after just a few prompts. Next, I tested KoljaB RealtimeVoiceChat, which showed promise but is still in early development. Most of the other projects I’ve seen are mere proofs of concept with no ongoing updates. Has anyone come across a stable, fully functioning tool that actually works? I think with system prompts and my local ollama server I can have enough control to keep this safe but I'm sure there are other ways too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vulcan4d"&gt; /u/vulcan4d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7506/what_are_the_best_options_currently_for_a_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7506/what_are_the_best_options_currently_for_a_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7506/what_are_the_best_options_currently_for_a_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T21:49:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu7hm7</id>
    <title>Let the LLM Write the Prompts: An Intro to Building with DSPy</title>
    <updated>2025-07-07T22:04:13+00:00</updated>
    <author>
      <name>/u/contextbot</name>
      <uri>https://old.reddit.com/user/contextbot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7hm7/let_the_llm_write_the_prompts_an_intro_to/"&gt; &lt;img alt="Let the LLM Write the Prompts: An Intro to Building with DSPy" src="https://external-preview.redd.it/BeGr9QgwLXtSX0q13TLbHKwgMGLwOIT30Cd4YNUl1DU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13424b44313afca4ef1218d890f9af80a4388468" title="Let the LLM Write the Prompts: An Intro to Building with DSPy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/contextbot"&gt; /u/contextbot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=I9ZtkgYZnOw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7hm7/let_the_llm_write_the_prompts_an_intro_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7hm7/let_the_llm_write_the_prompts_an_intro_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T22:04:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltfgoy</id>
    <title>I drew a silly comic about Llama model</title>
    <updated>2025-07-06T23:37:41+00:00</updated>
    <author>
      <name>/u/Organic-Mechanic-435</name>
      <uri>https://old.reddit.com/user/Organic-Mechanic-435</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/"&gt; &lt;img alt="I drew a silly comic about Llama model" src="https://a.thumbs.redditmedia.com/ntblmHJuZXo-K_j-B7phe4Ko7b3I1mCMnzblLD25_K8.jpg" title="I drew a silly comic about Llama model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a roleplayer using SillyTavern. Llama models are often used as 'base' for fine tunes in Huggingface. Seeing what people can do with local models also fascinate me. &lt;sup&gt;^&lt;/sup&gt; Hello!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Organic-Mechanic-435"&gt; /u/Organic-Mechanic-435 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ltfgoy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T23:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltxiy4</id>
    <title>LangChain/Crew/AutoGen made it easy to build agents, but operating them is a joke</title>
    <updated>2025-07-07T15:43:04+00:00</updated>
    <author>
      <name>/u/ImmuneCoder</name>
      <uri>https://old.reddit.com/user/ImmuneCoder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an internal support agent using LangChain + OpenAI + some simple tool calls.&lt;/p&gt; &lt;p&gt;Getting to a working prototype took 3 days with Cursor and just messing around. Great.&lt;/p&gt; &lt;p&gt;But actually trying to operate that agent across multiple teams was absolute chaos.&lt;/p&gt; &lt;p&gt;– No structured logs of intermediate reasoning&lt;/p&gt; &lt;p&gt;– No persistent memory or traceability&lt;/p&gt; &lt;p&gt;– No access control (anyone could run/modify it)&lt;/p&gt; &lt;p&gt;– No ability to validate outputs at scale&lt;/p&gt; &lt;p&gt;It’s like deploying a microservice with no logs, no auth, and no monitoring. The frameworks are designed for demos, not real workflows. And everyone I know is duct-taping together JSON dumps + Slack logs to stay afloat.&lt;/p&gt; &lt;p&gt;So, what does agent infra actually look like after the first prototype for you guys?&lt;/p&gt; &lt;p&gt;Would love to hear real setups. Especially if you’ve gone past the LangChain happy path.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImmuneCoder"&gt; /u/ImmuneCoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxiy4/langchaincrewautogen_made_it_easy_to_build_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxiy4/langchaincrewautogen_made_it_easy_to_build_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxiy4/langchaincrewautogen_made_it_easy_to_build_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T15:43:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt4y1z</id>
    <title>Self-hosted AI coding that just works</title>
    <updated>2025-07-06T16:09:28+00:00</updated>
    <author>
      <name>/u/send_me_a_ticket</name>
      <uri>https://old.reddit.com/user/send_me_a_ticket</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;TLDR&lt;/em&gt;&lt;/strong&gt;: VSCode + RooCode + LM Studio + Devstral + snowflake-arctic-embed2 + docs-mcp-server. A fast, cost-free, self-hosted AI coding assistant setup supports lesser-used languages and minimizes hallucinations on less powerful hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Long Post:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hello everyone, sharing my findings on trying to find a self-hosted agentic AI coding assistant that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Responds reasonably well on a variety of hardware.&lt;/li&gt; &lt;li&gt;Doesn’t hallucinate outdated syntax.&lt;/li&gt; &lt;li&gt;Costs $0 (except electricity).&lt;/li&gt; &lt;li&gt;Understands less common languages, e.g., KQL, Flutter, etc.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;After experimenting with several setups, here’s the combo I found that actually works.&lt;br /&gt; Please forgive any mistakes and feel free to let me know of any improvements you are aware of.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;br /&gt; Tested on a Ryzen 5700 + RTX 3080 (10GB VRAM), 48GB RAM.&lt;br /&gt; Should work on both low, and high-end setups, your mileage may vary.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Stack&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;VSCode +(with) RooCode +(connected to) LM Studio +(running both) Devstral +(and) snowflake-arctic-embed2 +(supported by) docs-mcp-server&lt;/code&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 1:&lt;/strong&gt; Setup Process for users saying this is too complicated&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install &lt;code&gt;VSCode&lt;/code&gt; then get &lt;code&gt;RooCode&lt;/code&gt; Extension&lt;/li&gt; &lt;li&gt;Install &lt;code&gt;LMStudio&lt;/code&gt; and pull &lt;code&gt;snowflake-arctic-embed2&lt;/code&gt; embeddings model, as well as &lt;code&gt;Devstral&lt;/code&gt; large language model which suits your computer. Start LM Studio server and load both models from &amp;quot;Power User&amp;quot; tab.&lt;/li&gt; &lt;li&gt;Install &lt;code&gt;Docker&lt;/code&gt; or &lt;code&gt;NodeJS&lt;/code&gt;, depending on which config you prefer &lt;em&gt;(recommend Docker)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Include &lt;code&gt;docs-mcp-server&lt;/code&gt; in your RooCode MCP configuration &lt;em&gt;(see json below)&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Edit 2&lt;/strong&gt;: I had been &lt;a href="https://docs.useanything.com/setup/embedder-configuration/local/lmstudio"&gt;misinformed&lt;/a&gt; that running embeddings and LLM together via LM Studio is not possible, it certainly is! I have updated this guide to remove Ollama altogether and only use LM Studio.&lt;/p&gt; &lt;p&gt;LM Studio made it slightly confusing because you cannot load embeddings model from &amp;quot;Chat&amp;quot; tab, you must load it from &amp;quot;Developer&amp;quot; tab.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VSCode + RooCode&lt;/strong&gt;&lt;br /&gt; RooCode is a VS Code extension that enables agentic coding and has MCP support.&lt;/p&gt; &lt;p&gt;VS Code: &lt;a href="https://code.visualstudio.com/download"&gt;https://code.visualstudio.com/download&lt;/a&gt;&lt;br /&gt; Alternative - VSCodium: &lt;a href="https://github.com/VSCodium/vscodium/releases"&gt;https://github.com/VSCodium/vscodium/releases&lt;/a&gt; - No telemetry&lt;/p&gt; &lt;p&gt;RooCode: &lt;a href="https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline"&gt;https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alternative to this setup is Zed Editor: &lt;a href="https://zed.dev/download"&gt;https://zed.dev/download&lt;/a&gt;&lt;/p&gt; &lt;p&gt;( Zed is nice, but you cannot yet pass problems as context. Released only for MacOS and Linux, coming soon for windows. Unofficial windows nightly here: &lt;a href="https://github.com/send-me-a-ticket/zedforwindows"&gt;github.com/send-me-a-ticket/zedforwindows&lt;/a&gt; )&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://lmstudio.ai/download"&gt;https://lmstudio.ai/download&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nice UI with real-time logs&lt;/li&gt; &lt;li&gt;GPU offloading is too simple. Changing AI model parameters is a breeze. You can achieve same effect in ollama by creating custom models with changed num_gpu and num_ctx parameters&lt;/li&gt; &lt;li&gt;Good (better?) OpenAI-compatible API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Devstral (Unsloth finetune)&lt;/strong&gt;&lt;br /&gt; Solid coding model with good tool usage.&lt;/p&gt; &lt;p&gt;I use &lt;code&gt;devstral-small-2505@iq2_m&lt;/code&gt;, which fully fits within 10GB VRAM. token context 32768.&lt;br /&gt; Other variants &amp;amp; parameters may work depending on your hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;snowflake-arctic-embed2&lt;/strong&gt;&lt;br /&gt; Tiny embeddings model used with docs-mcp-server. Feel free to substitute for any better ones.&lt;br /&gt; I use &lt;code&gt;text-embedding-snowflake-arctic-embed-l-v2.0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;br /&gt; Recommend Docker use instead of NPX, for security and ease of use.&lt;/p&gt; &lt;p&gt;Portainer is my recommended extension for ease of use:&lt;br /&gt; &lt;a href="https://hub.docker.com/extensions/portainer/portainer-docker-extension"&gt;https://hub.docker.com/extensions/portainer/portainer-docker-extension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/arabold/docs-mcp-server"&gt;https://github.com/arabold/docs-mcp-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what makes it all click. MCP server scrapes documentation (with versioning) so the AI can look up the &lt;em&gt;correct&lt;/em&gt; syntax for &lt;em&gt;your&lt;/em&gt; version of language implementation, and avoid hallucinations.&lt;/p&gt; &lt;p&gt;You &lt;em&gt;should&lt;/em&gt; also be able to run &lt;code&gt;localhost:6281&lt;/code&gt; to open web UI for the &lt;code&gt;docs-mcp-server&lt;/code&gt;, however web UI doesn't seem to be working for me, which I can ignore because AI is managing that anyway.&lt;/p&gt; &lt;p&gt;You can implement this MCP server as following -&lt;/p&gt; &lt;p&gt;&lt;em&gt;Docker version (needs Docker Installed)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;docker&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;run&amp;quot;, &amp;quot;-i&amp;quot;, &amp;quot;--rm&amp;quot;, &amp;quot;-p&amp;quot;, &amp;quot;6280:6280&amp;quot;, &amp;quot;-p&amp;quot;, &amp;quot;6281:6281&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;OPENAI_API_KEY&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;, &amp;quot;-v&amp;quot;, &amp;quot;docs-mcp-data:/data&amp;quot;, &amp;quot;ghcr.io/arabold/docs-mcp-server:latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:1234/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-snowflake-arctic-embed-l-v2.0&amp;quot; } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;NPX version (needs NodeJS installed)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;@arabold/docs-mcp-server@latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:1234/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-snowflake-arctic-embed-l-v2.0&amp;quot; } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Adding documentation for your language&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ask AI to use the &lt;code&gt;scrape_docs&lt;/code&gt; tool with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;url&lt;/strong&gt; (link to the documentation),&lt;/li&gt; &lt;li&gt;&lt;strong&gt;library&lt;/strong&gt; (name of the documentation/programming language),&lt;/li&gt; &lt;li&gt;&lt;strong&gt;version&lt;/strong&gt; (version of the documentation)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;you can also provide (optional):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;maxPages&lt;/strong&gt; (maximum number of pages to scrape, default is 1000).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;maxDepth&lt;/strong&gt; (maximum navigation depth, default is 3).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;scope&lt;/strong&gt; (crawling boundary, which can be 'subpages', 'hostname', or 'domain', default is 'subpages').&lt;/li&gt; &lt;li&gt;&lt;strong&gt;followRedirects&lt;/strong&gt; (whether to follow HTTP 3xx redirects, default is true).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can ask AI to use &lt;strong&gt;search_docs&lt;/strong&gt; tool any time you want to make sure the syntax or code implementation is correct. It should also check docs automatically if it is smart enough.&lt;/p&gt; &lt;p&gt;This stack isn’t limited to coding, Devstral handles logical, non-coding tasks well too.&lt;br /&gt; The MCP setup helps reduce hallucinations by grounding the AI in real documentation, making this a flexible and reliable solution for a variety of tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thanks for reading... If you have used and/or improved on this, I’d love to hear about it..!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/send_me_a_ticket"&gt; /u/send_me_a_ticket &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T16:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltze9d</id>
    <title>Do you use prompt caching to save chat history in your LLM apps?</title>
    <updated>2025-07-07T16:53:44+00:00</updated>
    <author>
      <name>/u/Physical_Ad9040</name>
      <uri>https://old.reddit.com/user/Physical_Ad9040</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to hear from others building LLM-based chat apps: Do you implement &lt;strong&gt;prompt caching&lt;/strong&gt; to store chat history or previous responses? Or do you send the chat history with each user's prompt?&lt;/p&gt; &lt;p&gt;Caching is more expensive to write, but the costs are then net positive if the conversation becomes long, no?&lt;/p&gt; &lt;p&gt;Would appreciate your insights — thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical_Ad9040"&gt; /u/Physical_Ad9040 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T16:53:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu4t37</id>
    <title>Octominer + P102-100 build... worth it?</title>
    <updated>2025-07-07T20:18:26+00:00</updated>
    <author>
      <name>/u/UsualResult</name>
      <uri>https://old.reddit.com/user/UsualResult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just for luls I was looking at some of the &amp;quot;Octominer&amp;quot; boards available. I thought it would be a fun build to get like 8x P104-100 / P102-100 and load one up.&lt;/p&gt; &lt;p&gt;However, they mostly have something wimpy for CPU... like a dual core Celeron or similar. Will that kill any possible chance of fun on a build like that because certain things need to get handled by the CPU?&lt;/p&gt; &lt;p&gt;I was curious because there are a lot of Octominers floating around for $200 - $300 and it seems like it's an easy way to host a lot of cards.&lt;/p&gt; &lt;p&gt;I have a box with dual P104-100 and it's been fun to play around with but it has a new(ish) i5 to work with. I can run 7b-13b models with &amp;quot;acceptable&amp;quot; speed but it would be neat to be able to bring that up to 30b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UsualResult"&gt; /u/UsualResult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu4t37/octominer_p102100_build_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu4t37/octominer_p102100_build_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu4t37/octominer_p102100_build_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T20:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lthtbn</id>
    <title>8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top.</title>
    <updated>2025-07-07T01:35:48+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt; &lt;img alt="8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top." src="https://b.thumbs.redditmedia.com/YH48KR3uSeLFmipFEDX0ai8FZ4TwQmmccEKSaaUx3Fk.jpg" title="8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was working on a &lt;a href="https://www.designarena.ai/"&gt;research&lt;/a&gt; project (note that the votes and data is completely free and open, so not profiting off this, but just showing research as context) where users write a prompt, and then vote on content generated (e.g. websites, games, 3D visualizations) from 4 randomly generated models each. Note that when &lt;a href="https://www.designarena.ai/vote"&gt;voting&lt;/a&gt;, model names are hidden, so people don't immediately know which models generated what. &lt;/p&gt; &lt;p&gt;From the data collected so far, Llama 4 Maverick is 19th and Llama 4 Scout is 23rd. On the other extreme, Claude and Deepseek are taking up most of the spots in the top 10 while Mistral and Grok have been surprising dark horses. &lt;/p&gt; &lt;p&gt;Anything surprise you here? What models have you noticed been the best for UI/UX and frontend development? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lthtbn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T01:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu7lsi</id>
    <title>UI/UX Benchmark Update and Response: More Models, Updating Ranking, Open Data Soon</title>
    <updated>2025-07-07T22:09:06+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/"&gt; &lt;img alt="UI/UX Benchmark Update and Response: More Models, Updating Ranking, Open Data Soon" src="https://b.thumbs.redditmedia.com/VkhU8Mt9acaQeJtSLndzIlRsVXJlfJ84thb8pJB8_6o.jpg" title="UI/UX Benchmark Update and Response: More Models, Updating Ranking, Open Data Soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, a few times on here I've been sharing progress on a &lt;a href="https://www.designarena.ai/"&gt;UI/UX benchmark&lt;/a&gt; that I have been working on with a small team. In particular, I made &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt;a post yesterday&lt;/a&gt; that gave us a ton of useful feedback so thank you to everyone that put in a comment and voted on our platform! I just wanted to address some concerns, provide some updates on what we are working on, and create an open discussion on how the benchmark can be improved. This post will be a bit long since I want to be as detailed as possible, but here we go:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; We released the benchmark just a few weeks ago (3 weeks ago I think?) and mostly it started out as an internal tool among my team since we were interested in the current UI/UX capabilities of LLMs and HCI and wanted to see which models are best at designing and implementing interfaces. We really just pushed the benchmark out initially as a fun side project to see what would happen, but really didn't forsee that we would get over 10K people on our site at some point! Our motivation here is that something like UI/UX data for AI seems that it will be heavily reliant on public opinion, rather than a deterministic benchmark or private evaluation. &lt;/p&gt; &lt;p&gt;As I said, we received a lot of very helpful feedback, and as we're still in very early early stages with developing the benchmark, we're really trying to do our best to make our benchmark as transparent and useful as possible. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;More Models and Voting Inconsistency:&lt;/strong&gt; Many people have noted that many premier models are missing such as GLM-4, Qwen, Gemini 2.5-Flash, etc. We are working on adding those and hope to add those models in the next couple of days and will update you all when those are added. I realize I have been saying that more models will be added for more than a few days now haha, but honestly we are a small team with not an infinite amount of money lol, so we're just waiting to get some more credits. I hope that makes sense and thank you for your patience! &lt;/p&gt; &lt;p&gt;Another comment we got is that the number of votes received for the different models are vastly different even though voting should be recruiting models at random. There are few reasons for this: (1) we added some models earlier (notably Claude when we were first developing the benchmark) and other models later (Mistral, Llama, etc.), (2) we did deactivate some models that became deprecated or because we ran out of credits (such as Llama which we're deploying on Vertex but we will add back) and (3) for slower models like DeepSeek, we do notice churn from voters in the sense that people won't wait for those models to finish generating all the time. &lt;/p&gt; &lt;p&gt;For (1) and (2) we will address by providing exact details on when we added each model and adding back models (assuming they are not deprecated) such as Llama. For (3), we have put some thought into this over the last few weeks but honestly not sure how exactly we should tackle this issue since this is a bit of a limitation of having a public crowdsource benchmark. We did get some suggestions to perhaps have some priority for models with fewer votes, but there is a correlation between having fewer votes and slower generation times, so we don't think there is an immediate fix there but we likely incorporate some kind of priority system. That said, we would appreciate any suggestions on (3)! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Voting Data:&lt;/strong&gt; To be clear, this is standard preference dataset that we collect when users do binary comparisons on our &lt;a href="https://www.designarena.ai/vote"&gt;voting page&lt;/a&gt;. We'll be releasing a preference dataset that can be accessed through Hugging Face and/or a REST API that will be updated periodically and that people can use to replicate the leaderboard. Note that the &lt;a href="https://www.designarena.ai/leaderboard"&gt;leaderboard page is currently being updated every hour&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;System Prompts and Model Configs:&lt;/strong&gt; We will also release these along with the preference dataset and make our current settings much more clear. You'll get full access to these configs, but for the we're asking each model (with the same sys prompt across the board) to create an interface using HTML/CSS/JS with some restrictions (to ensure sure the code is sandboxed as possible + allowing it to use specific libraries like ThreeJs for 3D viz, Tailwind, etc.). For model configs, we are setting temperature to 0.8. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tournaments:&lt;/strong&gt; This was more of an aesthetic choice on our part to make the voting process more interesting for the user and get more comparisons for the same prompt across models. We'll also provide exact details on how these are being constructed, but the idea is that we're recruiting X number of models that are each being voted on in a group. We have had too kind of tournament structures. In the first, we would serve two models, have a user vote, and then continually have the winner go against the next served model. We decided to change this structure because we weren't able to compare losers in the bracket. For the current tournament system, we have two models A and B go against each other and then two other models C and D go against each other in round 1. Then the winners from the first round and losers from the last round go against each other. After that the loser in the winners' bracket will go against the winner in the losers' bracket to decide 2nd and 3rd place. We don't think this structure is necessarily perfect, but just more of an aesthetic choice so people could see different models at the same time in a grouping. We acknowledge that with the preference data, you could certainly structure the tournament data differently and our tournament structure shouldn't be considered as the absolute &amp;quot;correct&amp;quot; one. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stack Ranking/Leaderboard:&lt;/strong&gt; This is where we acknowledge that there's certainly room for improvement here on how we can construct the leaderboard based on the preference data. Some of the concerns raised we did think about briefly in the past, but will certainly take more time to consider what's the best kind of ranking. Right now though, we have a ranking by win rate, and then an &amp;quot;Elo&amp;quot; score (which we're using an approximate formula based on win rate for which you can find at the bottom of the &lt;a href="https://www.designarena.ai/leaderboard"&gt;leaderboard&lt;/a&gt;). A concern raised that is relevant to what was said above is that the number of votes a model has does have an effect on the placement in the leaderboard. We will probably add some way to weight win rate / elo score by number votes, and any suggestions on what would be the best stack ranking here would be appreciated! That said, I do think it might be good to not take the leaderboard as this definitive ranking, since one could construct their own different kind of leaderboards / rankings based on how they choose to structure the preference data, but more so treat it as a general &amp;quot;tier list&amp;quot; for the models. &lt;/p&gt; &lt;p&gt;Let us know what you think and if you have any questions in the comments! &lt;/p&gt; &lt;p&gt;Please also join our &lt;a href="https://discord.gg/5AagpZd5"&gt;Discord&lt;/a&gt; for the best way to message us directly. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lu7lsi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T22:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lts4wd</id>
    <title>Inside Google Gemma 3n: my PyTorch Profiler insights</title>
    <updated>2025-07-07T11:51:50+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"&gt; &lt;img alt="Inside Google Gemma 3n: my PyTorch Profiler insights" src="https://external-preview.redd.it/iyG6eCUPhSylmQBjhmsazmQyUh0CUb3n-N54OyLJmm0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25644e360804c2abbce6ff1e1b66b61cd330cee0" title="Inside Google Gemma 3n: my PyTorch Profiler insights" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;If you’ve ever wondered what really happens inside modern vision-language models, here’s a hands-on look. I profiled the Google Gemma 3n model on an NVIDIA GPU using PyTorch Profiler, asking it to describe a &lt;a href="https://cdn-lfs.hf.co/datasets/huggingface/documentation-images/8b21ba78250f852ca5990063866b1ace6432521d0251bde7f8de783b22c99a6d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27bee.jpg%3B+filename%3D%22bee.jpg%22%3B&amp;amp;response-content-type=image%2Fjpeg&amp;amp;Expires=1751892238&amp;amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTg5MjIzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy9odWdnaW5nZmFjZS9kb2N1bWVudGF0aW9uLWltYWdlcy84YjIxYmE3ODI1MGY4NTJjYTU5OTAwNjM4NjZiMWFjZTY0MzI1MjFkMDI1MWJkZTdmOGRlNzgzYjIyYzk5YTZkP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&amp;amp;Signature=FWMAYJoqhsk9AHs1%7EyIoOHBmh53A16J6Xyj-vhFVXTW%7EFkL2tRptgpALUSWppQKXjCnJZsnMXtDFcZAvDm-PFgQaK3UycJD%7ElNShdj5yopPA2F5U2gT4wEvXc-AibMF5mUrzeNKxfY56CjsiFWCfKczLZKzV-kfrXZu7t60d4o5ZdY6jmkdeMHMkYmLROTFE-tmPiKqmN7jVcMIdW43xmaEvova9oA4akIqKphaQUUvvVTToqPjILfn2LLhqwH5BgnbAE5OZ9DtreQirvzS75Xhkgi8GN7LEyrX2nt7LSYtS2vv1SfeSmWca8MY0eO7KEqF71jyA5DquPofRkEEesQ__&amp;amp;Key-Pair-Id=K3RPWS32NSSJCE"&gt;bee image&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I visualized the profiling results using &lt;a href="https://ui.perfetto.dev/"&gt;https://ui.perfetto.dev/&lt;/a&gt;, as shown in the animated GIF below:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/frlijkwkwfbf1.gif"&gt;https://i.redd.it/frlijkwkwfbf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Along the way, I captured and analyzed the key inference phases, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Image feature extraction&lt;/strong&gt; with MobileNetV5 (74 msec) - the trace shows the &lt;code&gt;get_image_features&lt;/code&gt; function of Gemma3n (&lt;a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma3n/modular_gemma3n.py#L2253"&gt;source&lt;/a&gt;), which then calls &lt;code&gt;forward_features&lt;/code&gt; in MobileNetV5 (&lt;a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/mobilenetv5.py#L535"&gt;source&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/afzke1tdxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=899a055b776818546205514b3d9e29fe7dee38cd"&gt;https://preview.redd.it/afzke1tdxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=899a055b776818546205514b3d9e29fe7dee38cd&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Text decoding&lt;/strong&gt; through a stack of Gemma3nTextDecoderLayer layers (142 msec) - a series of &lt;code&gt;Gemma3nTextDecoderLayer&lt;/code&gt; (&lt;a href="https://github.com/huggingface/transformers/blob/ca7e1a3756c022bf31429c452b2f313f043f32de/src/transformers/models/gemma3n/modular_gemma3n.py#L1829"&gt;source&lt;/a&gt;) calls. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6hlcdthfxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=833ae582e5eb759a1eba9adbca1841deeba07195"&gt;https://preview.redd.it/6hlcdthfxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=833ae582e5eb759a1eba9adbca1841deeba07195&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Token generation&lt;/strong&gt; with per-token execution broken down to kernel launches and synchronizations (244 msec total for 10 tokens, ~24 msec per token) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xzoilykgxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16f504610e8821d686d63aa83e255a4feb8dfd60"&gt;https://preview.redd.it/xzoilykgxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16f504610e8821d686d63aa83e255a4feb8dfd60&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’ve shared the full code, profiling scripts, and raw trace data, so you can dive in, reproduce the results, and explore the model’s internals for yourself.&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://github.com/sbnb-io/gemma3n-profiling/"&gt;https://github.com/sbnb-io/gemma3n-profiling/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’re looking to better understand how these models run under the hood, this is a solid place to start. Happy to hear your thoughts or suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T11:51:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltubvs</id>
    <title>Jamba 1.7 - a ai21labs Collection</title>
    <updated>2025-07-07T13:35:12+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltubvs/jamba_17_a_ai21labs_collection/"&gt; &lt;img alt="Jamba 1.7 - a ai21labs Collection" src="https://external-preview.redd.it/T-WGV8JGl5ddvynFCnHkV0GApDuiD0OUmPGVN858nB8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35b8dfb877220fc5dfb06c711e11e9b9d474f083" title="Jamba 1.7 - a ai21labs Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ai21labs/jamba-17-68653e9be386dc69b1f30828"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltubvs/jamba_17_a_ai21labs_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltubvs/jamba_17_a_ai21labs_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T13:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu5g8c</id>
    <title>Thanks to you, I built an open-source website that can watch your screen and trigger actions. It runs 100% locally and was inspired by all of you!</title>
    <updated>2025-07-07T20:43:03+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: I'm a solo dev who wanted a simple, private way to have local LLMs watch my screen and do simple logging/notifying. I'm launching the open-source tool for it, Observer AI, this Friday. It's built for this community, and I'd love your feedback.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Some of you might remember my earlier posts showing off a local agent framework I was tinkering with. Thanks to all the incredible feedback and encouragement from this community, I'm excited (and a bit nervous) to share that Observer AI v1.0 is launching this &lt;strong&gt;Friday&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;This isn't just an announcement; it's a huge &lt;strong&gt;thank you&lt;/strong&gt; note.&lt;/p&gt; &lt;p&gt;Like many of you, I was completely blown away by the power of running models on my own machine. But I hit a wall: I wanted a super simple, minimal, but powerful way to connect these models to my own computer—to let them see my screen, react to events, and log things.&lt;/p&gt; &lt;p&gt;That's why I started building &lt;strong&gt;Observer AI 👁️&lt;/strong&gt;: a privacy-first, open-source platform for building your own micro-agents that run entirely locally!&lt;/p&gt; &lt;h1&gt;What Can You Actually Do With It?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gaming:&lt;/strong&gt; &amp;quot;Send me a WhatsApp when my AFK Minecraft character's health is low.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Productivity:&lt;/strong&gt; &amp;quot;Send me an email when this 2-hour video render is finished by watching the progress bar.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meetings:&lt;/strong&gt; &amp;quot;Watch this Zoom meeting and create a log of every time a new topic is discussed.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt; &amp;quot;Start a screen recording the moment a person appears on my security camera feed.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try it out in your browser with zero setup, and make it &lt;strong&gt;100% local with a single command:&lt;/strong&gt; docker compose up --build.&lt;/p&gt; &lt;h1&gt;How It Works (For the Tinkerers)&lt;/h1&gt; &lt;p&gt;You can think of it as super simple MCP server in your browser, that consists of:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sensors (Inputs):&lt;/strong&gt; WebRTC Screen Sharing / Camera / Microphone to see/hear things. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model (The Brain):&lt;/strong&gt; Any Ollama model, running locally. You give it a system prompt and the sensor data. (adding support for llama.cpp soon!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools (Actions):&lt;/strong&gt; What the agent can do with the model's response. notify(), sendEmail(), startClip(), and you can even run your own code.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;My Commitment &amp;amp; A Sustainable Future&lt;/h1&gt; &lt;p&gt;The core Observer AI platform is, and will always be, &lt;strong&gt;free and open-source.&lt;/strong&gt; That's non-negotiable. The code is all on GitHub for you to use, fork, and inspect.&lt;/p&gt; &lt;p&gt;To keep this project alive and kicking long-term (I'm a solo dev, so server costs and coffee are my main fuel!), I'm also introducing an optional &lt;strong&gt;Observer Pro&lt;/strong&gt; subscription. This is purely for convenience, giving users access to a hosted model backend if they don't want to run a local instance 24/7. It’s my attempt at making the project sustainable without compromising the open-source core.&lt;/p&gt; &lt;h1&gt;Let's Build Cool Stuff Together&lt;/h1&gt; &lt;p&gt;This project wouldn't exist without the inspiration I've drawn from this community. You are the people I'm building this for.&lt;/p&gt; &lt;p&gt;I'd be incredibly grateful if you'd take a look. Star the repo if you think it's cool, try building an agent, and please, let me know what you think. Your feedback is what will guide v1.1 and beyond.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (All the code is here!):&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FRoy3838%2FObserver"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fapp.observer-ai.com%2F"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fdiscord.gg%2FwnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Twitter/X:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fx.com%2FAppObserverAI"&gt;https://x.com/AppObserverAI&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out here all day to answer any and all questions. Thank you again for everything!&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu5g8c/thanks_to_you_i_built_an_opensource_website_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T20:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltxsqh</id>
    <title>Qwen3-8B-BitNet</title>
    <updated>2025-07-07T15:53:44+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a decent Qwen3 BitNet model I trained with ~1B tokens using SYNTHETIC-1 data. BitNet Hunyuan A13B is training this week.&lt;br /&gt; &lt;a href="https://huggingface.co/codys12/Qwen3-8B-BitNet"&gt;model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1GT0GEyjzOQUiOI0tphvhiFDwUw-F6v7l?usp=sharing"&gt;notebook&lt;/a&gt; to try out the model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxsqh/qwen38bbitnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxsqh/qwen38bbitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxsqh/qwen38bbitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T15:53:44+00:00</published>
  </entry>
</feed>
