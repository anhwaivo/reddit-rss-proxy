<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-18T06:27:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1msn1n0</id>
    <title>Looks like Kimi K2 quietly joined the ‚Äú5.9 ‚àí 5.11 = ?‚Äù support group. üò©</title>
    <updated>2025-08-17T10:06:04+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"&gt; &lt;img alt="Looks like Kimi K2 quietly joined the ‚Äú5.9 ‚àí 5.11 = ?‚Äù support group. üò©" src="https://preview.redd.it/e0o9q4g90kjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96de0c19ca32f0ffe1af17db9ea73f11e103ccae" title="Looks like Kimi K2 quietly joined the ‚Äú5.9 ‚àí 5.11 = ?‚Äù support group. üò©" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e0o9q4g90kjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:06:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mswfiy</id>
    <title>Detecting Hallucinations in LLM Function Calling with Entropy (Part 2)</title>
    <updated>2025-08-17T17:07:42+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.archgw.com/blogs/detecting-hallucinations-in-llm-function-calling-with-entropy-part-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mswfiy/detecting_hallucinations_in_llm_function_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mswfiy/detecting_hallucinations_in_llm_function_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T17:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt531q</id>
    <title>THE NVIDIA AI GPU BLACK MARKET | Investigating Smuggling, Corruption, &amp; Governments</title>
    <updated>2025-08-17T22:47:04+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt531q/the_nvidia_ai_gpu_black_market_investigating/"&gt; &lt;img alt="THE NVIDIA AI GPU BLACK MARKET | Investigating Smuggling, Corruption, &amp;amp; Governments" src="https://external-preview.redd.it/2B3KEk8KKxicDYT6TmXcM_06VPjlO10fR-AfmTFZPjY.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=826faab6abdc870eaebae77c99275a97769c48c6" title="THE NVIDIA AI GPU BLACK MARKET | Investigating Smuggling, Corruption, &amp;amp; Governments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/1H3xQaf7BFI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt531q/the_nvidia_ai_gpu_black_market_investigating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt531q/the_nvidia_ai_gpu_black_market_investigating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T22:47:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt9uwy</id>
    <title>Thyme: Think Beyond Images</title>
    <updated>2025-08-18T02:27:34+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9uwy/thyme_think_beyond_images/"&gt; &lt;img alt="Thyme: Think Beyond Images" src="https://a.thumbs.redditmedia.com/bLFhrp7KahZutPzoKS27pjNEJaTs7XNdhMikNs0cOt8.jpg" title="Thyme: Think Beyond Images" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.11630"&gt;https://arxiv.org/abs/2508.11630&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Page: &lt;a href="https://thyme-vl.github.io/"&gt;https://thyme-vl.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/yfzhang114/Thyme"&gt;https://github.com/yfzhang114/Thyme&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SFT Model/Data: &lt;a href="https://huggingface.co/Kwai-Keye/Thyme-SFT"&gt;https://huggingface.co/Kwai-Keye/Thyme-SFT&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/Kwai-Keye/Thyme-SFT"&gt;https://huggingface.co/datasets/Kwai-Keye/Thyme-SFT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RL Model/Data: &lt;a href="https://huggingface.co/Kwai-Keye/Thyme-RL"&gt;https://huggingface.co/Kwai-Keye/Thyme-RL&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/Kwai-Keye/Thyme-RL"&gt;https://huggingface.co/datasets/Kwai-Keye/Thyme-RL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We are excited to introduce &lt;strong&gt;Thyme: Think Beyond Images&lt;/strong&gt;. Thyme transcends traditional &amp;quot;thinking with images&amp;quot; paradigms by autonomously generating and executing diverse image processing and computational operations through executable code, significantly enhancing performance on high-resolution perception and complex reasoning tasks. Leveraging a novel two-stage training strategy that combines supervised fine-tuning with reinforcement learning and empowered by the innovative GRPO-ATS algorithm, Thyme achieves a sophisticated balance between reasoning exploration and code execution precision.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cxrbt8myuojf1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69b6f4d728d314b3088cbdd71bfa34da40739749"&gt;Overall pipeline of Thyme, illustrating the interaction between the model and the sandbox for iterative reasoning and code execution. Key processes such as reasoning, code generation, sandbox execution, and result feedback are highlighted.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9uwy/thyme_think_beyond_images/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9uwy/thyme_think_beyond_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9uwy/thyme_think_beyond_images/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T02:27:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt8hd1</id>
    <title>Serene Pub 0.4 Release - Ollama Manager, Accessability &amp; Tags</title>
    <updated>2025-08-18T01:22:10+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt8hd1/serene_pub_04_release_ollama_manager/"&gt; &lt;img alt="Serene Pub 0.4 Release - Ollama Manager, Accessability &amp;amp; Tags" src="https://a.thumbs.redditmedia.com/5z-W0jhUS0_b0c95fHVDd3XoFfgev9SYUAd-fno4JS8.jpg" title="Serene Pub 0.4 Release - Ollama Manager, Accessability &amp;amp; Tags" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Serene Pub 0.4&lt;/h1&gt; &lt;p&gt;Welcome to the next feature release for Serene Pub! A beginner friendly, but powerful AI role-play application.&lt;/p&gt; &lt;p&gt;Local AI roleplay shouldn't require a computer science degree. We're on the warpath to make it as stress-free as possible.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Integrated Ollama model management + universal tagging system&lt;/strong&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What's New&lt;/h2&gt; &lt;h3&gt;&lt;strong&gt;Ollama Manager&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Browse and download roleplay-optimized models directly in Serene Pub&lt;/li&gt; &lt;li&gt;One-click model installation with live progress tracking&lt;/li&gt; &lt;li&gt;One-click activation, no need to manually configure connections&lt;/li&gt; &lt;li&gt;View model details and manage storage&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;&lt;strong&gt;First Time Setup Wizard&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Brand new guided setup experience for new users&lt;/li&gt; &lt;li&gt;Streamlined onboarding process from installation to first chat&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;&lt;strong&gt;Tags System&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Tag characters, personas, chats, and lorebooks&lt;/li&gt; &lt;li&gt;Filter content by tags in the new tags sidebar&lt;/li&gt; &lt;li&gt;Visual organization with color-coded tags&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;&lt;strong&gt;Easier Character &amp;amp; Persona Creation&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;New step-by-step creation tools guide you through the entire process&lt;/li&gt; &lt;li&gt;Interactive modals break complex character creation into simple steps&lt;/li&gt; &lt;li&gt;Advanced fields hidden by default for simpler workflow&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;&lt;strong&gt;Accessibility &amp;amp; Screen Reader Support&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;New focus on accessibility improvements (WIP, community feedback is very much needed!)&lt;/li&gt; &lt;li&gt;Enhanced screen reader compatibility&lt;/li&gt; &lt;li&gt;Keyboard shortcuts for most actions&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;&lt;strong&gt;Bug Fixes, etc&lt;/strong&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Resolved stability issues and various UI improvements&lt;/li&gt; &lt;li&gt;Database lock to prevent Pglite from getting corrupted&lt;/li&gt; &lt;li&gt;Most forms can now be saved regardless of if changes are detected or not&lt;/li&gt; &lt;li&gt;Zod form schema enforcement on most forms&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;- &lt;code&gt;.env&lt;/code&gt; support, configure your ports and data directory&lt;/h2&gt; &lt;h2&gt;Getting Started&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/doolijb/serene-pub/releases"&gt;Download Serene Pub 0.4&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Extract and run &lt;code&gt;run.cmd&lt;/code&gt; (Windows) or &lt;code&gt;run.sh&lt;/code&gt; (Mac/Linux)&lt;/li&gt; &lt;li&gt;Follow the setup wizard steps (5 minutes)&lt;/li&gt; &lt;li&gt;Start chatting&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2&gt;Coming Soon&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI Assistant&lt;/strong&gt;: Get help creating and editing characters - it can write and edit content for you&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;Community&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/doolijb/serene-pub/wiki"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; (WIP - updated regularly)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;a href="https://discord.gg/3kUx3MDcSa"&gt;Discord&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/doolijb/serene-pub/issues"&gt;Issues&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Serene Pub - Play more, tweak less. 100% open source.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Special thanks to crazyaphro for Q/A, M3d4r for editing the Wiki, and Nivelle for early accessability feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mt8hd1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt8hd1/serene_pub_04_release_ollama_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt8hd1/serene_pub_04_release_ollama_manager/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T01:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtdh1r</id>
    <title>Optimizing parallel inference on llama.cpp and question about batched vs. parallel?</title>
    <updated>2025-08-18T05:36:45+00:00</updated>
    <author>
      <name>/u/ahjorth</name>
      <uri>https://old.reddit.com/user/ahjorth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use llama.cpp for data analysis in research. A very typical use case is that we have tens or hundreds or thousands of some document, and we want to classify them and/or extract some data from them.&lt;/p&gt; &lt;p&gt;Consequently, I often need to run quite large numbers of documents through an LLM where a. the system instructions for all call are the same, and b. the upper bound of output length per call is known. My question is what I can do to speed this up as much as possible. Currently I am doing parallel requests, but I am looking at the llama-batched example on llama.cpp and wonder if this could be done even faster. &lt;/p&gt; &lt;p&gt;On M3 Studio 512GB with 80GPU cores, in case that matters.&lt;/p&gt; &lt;p&gt;When I am making calls to the llama-sever, I am currently always keeping n (where n is the the number of concurrent inferences that I set with the -np parameter) requests alive to the server. (Gist with my code here &lt;a href="https://gist.github.com/arthurhjorth/c02f906d30e2a7e82af2196260efdd9d"&gt;https://gist.github.com/arthurhjorth/c02f906d30e2a7e82af2196260efdd9d&lt;/a&gt;) &lt;/p&gt; &lt;p&gt;So my first question (and the rest of my questions won't really matter if the answer is yes here) is: Does the server pass this on to llama.cpp as 'a batch' or are they maybe turned into batches for each inference step as the requests are passed into llama.cpp by the server? &lt;/p&gt; &lt;p&gt;If not, do I need to somehow explicitly pass them in as a batch to get the performance that I see when I run the llama-batched? And if so, what is the best way to do this in Python? &lt;/p&gt; &lt;p&gt;Finally, more fundamentally, does parallel inference take advantage of batching or are these different things?&lt;/p&gt; &lt;p&gt;I'm a decent python coder and can, slowly, read the C++ on the llama.cpp github, but the code is just too big for me to keep track of in my head and I keep losing my train of thoughts. So any help here would really be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ahjorth"&gt; /u/ahjorth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtdh1r/optimizing_parallel_inference_on_llamacpp_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtdh1r/optimizing_parallel_inference_on_llamacpp_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtdh1r/optimizing_parallel_inference_on_llamacpp_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T05:36:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt070m</id>
    <title>Qwen3-30B-A3B and quantization.</title>
    <updated>2025-08-17T19:30:22+00:00</updated>
    <author>
      <name>/u/yami_no_ko</name>
      <uri>https://old.reddit.com/user/yami_no_ko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking about quantization and how it affects MoE models like Qwen3-30B-A3B versus regular dense models.&lt;/p&gt; &lt;p&gt;The standard rule of thumb is that FP &amp;gt; Q8 &amp;gt;&amp;gt; Q4 &amp;gt;&amp;gt; Q3, with Q8 giving almost full performance and anything below Q4 causing noticeable drops. But with MoE models, I'm wondering if that is different.&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B has 30B parameters split across 3B expert layers. Each expert should be more sensitive to quantization than a regular dense 30B model. However, MoE models are sparse - only a subset of experts activate for any input. This might provide some protection from quantization noise.&lt;/p&gt; &lt;p&gt;This left me wondering: Does aggressive quantization affect MoE models more or less than regular models? &lt;/p&gt; &lt;p&gt;Would FP vs Q8 be nearly identical for MoE models, but Q8 vs Q4 cause noticeable performance drops? Or am I missing something about how quantization works with sparse architectures? Does the standard rule of thumb(barely anything useful outside the scale between Q4 and Q8) apply here?&lt;/p&gt; &lt;p&gt;I'm curious if the standard quantization rules apply or if MoE models have fundamentally different behavior at different quantization levels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yami_no_ko"&gt; /u/yami_no_ko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt070m/qwen330ba3b_and_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt070m/qwen330ba3b_and_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt070m/qwen330ba3b_and_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T19:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1msyzh8</id>
    <title>MoE optimization idea (VRAM/RAM)</title>
    <updated>2025-08-17T18:44:39+00:00</updated>
    <author>
      <name>/u/fredconex</name>
      <uri>https://old.reddit.com/user/fredconex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"&gt; &lt;img alt="MoE optimization idea (VRAM/RAM)" src="https://a.thumbs.redditmedia.com/HVYu7GZpWuFHEFw5ZYAS9pbfX6y2jpeFyNGTo1j_QB8.jpg" title="MoE optimization idea (VRAM/RAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt; &lt;p&gt;I was doing some tests, and have noticed that properly offloading MoE to CPU can improve performance, but there's a thing that might not be taken into account.&lt;/p&gt; &lt;p&gt;We're offloading sequentially, not by most commonly used experts, below there's an image it's from my CPU inference engine, I did some changes to it, I can do inference on Qwen3 30B-A3B Q8_0 (35gb) using only 9gb of RAM, speed will drop as I'm constantly loading/unloading the experts from SSD.&lt;/p&gt; &lt;p&gt;But with this I could find something interesting, experts usage isn't linear, there are experts that have higher activation frequency, so my proposed idea is that when offloading between RAM/VRAM we keep track of currently most used experts and move them around based on their usage, most used experts will move to VRAM, least used will drop to RAM, I believe with this kind of smart optimization we may be able to extract more speed from MoE models and also make possible to run bigger models on limited hardware by reducing the amount of in-memory experts.&lt;/p&gt; &lt;p&gt;I would try to implement this into llama.cpp but I'm not very used to C/C++ programming, but would like to hear thoughts on who might be familiar with it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/donxlk19jmjf1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43d4e8e9c5dab56645ac661ac2fbc28bc290c9cf"&gt;https://preview.redd.it/donxlk19jmjf1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43d4e8e9c5dab56645ac661ac2fbc28bc290c9cf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fredconex"&gt; /u/fredconex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T18:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1msvs0i</id>
    <title>What happened to the Uncensored models like Dolphin?</title>
    <updated>2025-08-17T16:42:30+00:00</updated>
    <author>
      <name>/u/krigeta1</name>
      <uri>https://old.reddit.com/user/krigeta1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year uncensored model like Dolphin(i was able to use it only) was fully uncensored and able to answers are things that are just really creepy and as of today there are open source LLMs that are so much powerful than the dolphin but nobody is releasing those models anymore?&lt;/p&gt; &lt;p&gt;Any specific reason why we are not getting uncensored models anymore?&lt;/p&gt; &lt;p&gt;Edit: wow guys, its been minutes and you guys have shared a lot of models, Hats off to you all!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krigeta1"&gt; /u/krigeta1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msvs0i/what_happened_to_the_uncensored_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msvs0i/what_happened_to_the_uncensored_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msvs0i/what_happened_to_the_uncensored_models_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T16:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt9coa</id>
    <title>NVLink: 3 or 4 slot?</title>
    <updated>2025-08-18T02:03:15+00:00</updated>
    <author>
      <name>/u/FrozenBuffalo25</name>
      <uri>https://old.reddit.com/user/FrozenBuffalo25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9coa/nvlink_3_or_4_slot/"&gt; &lt;img alt="NVLink: 3 or 4 slot?" src="https://b.thumbs.redditmedia.com/IzbRj480PwBafRpORktMRn-9K7XXlI8JeQxvzq-ToRs.jpg" title="NVLink: 3 or 4 slot?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Before I hit that buy button could someone please confirm this 3090 configuration would use a FOUR slot NVLink, not three?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrozenBuffalo25"&gt; /u/FrozenBuffalo25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mt9coa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9coa/nvlink_3_or_4_slot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9coa/nvlink_3_or_4_slot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T02:03:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtctda</id>
    <title>Trying to run a local offline coding agent with qwen code</title>
    <updated>2025-08-18T04:59:22+00:00</updated>
    <author>
      <name>/u/kuaythrone</name>
      <uri>https://old.reddit.com/user/kuaythrone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to get a powerful, offline coding agent running locally. I somehow always get the urge to code when there's no internet, so having a capable local model is a bit of a personal quest. I decided to connect the qwen code frontend to the qwen3-coder-30B model on my desktop using LM Studio.&lt;/p&gt; &lt;p&gt;My setup is an NVIDIA RTX 4080 Super (16GB VRAM) with 32GB of system RAM.&lt;/p&gt; &lt;p&gt;In the LM Studio chat UI, I was getting about 20-24 tok/s. The speed felt ok and I thought I was on the right track.&lt;/p&gt; &lt;p&gt;But I hit a hard wall with the memory bottleneck pretty quickly. The context window defaulted to 4096 tokens, and qwen code immediately complained that it needed 8k+ context just to process a simple &amp;quot;hi&amp;quot;.&lt;/p&gt; &lt;p&gt;To get around this, I pushed the context length to 32k by enabling Flash Attention and KV cache quantization. Technically, it worked, and the larger context was accessible. But the performance trade-off was brutal. Both the token speed and code quality dropped significantly.&lt;/p&gt; &lt;p&gt;I really believe there will be a day when local models catch up to today's SOTA models like Opus 4. Even if the state-of-the-art continues to improve, I would be perfectly happy with the current quality of top-tier coding agents if I could just run them completely local. We're getting closer, but the hardware requirements are still a major hurdle.&lt;/p&gt; &lt;p&gt;For anyone who wants to try this yourself, the setup is surprisingly simple. You just need to use OpenAI as the auth method for Qwen Code and point the OpenAI endpoint to your local LM Studio server in your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&amp;quot;not-needed-for-local&amp;quot; OPENAI_BASE_URL=&amp;quot;http://localhost:1234/v1&amp;quot; OPENAI_MODEL=&amp;quot;qwen/qwen3-coder-30b&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's the official guide I used: &lt;a href="https://github.com/QwenLM/qwen-code/blob/main/README.md#2-openai-compatible-api"&gt;https://github.com/QwenLM/qwen-code/blob/main/README.md#2-openai-compatible-api&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else found a better way to balance performance and context size on household hardware? Curious to hear your experiences.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuaythrone"&gt; /u/kuaythrone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtctda/trying_to_run_a_local_offline_coding_agent_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtctda/trying_to_run_a_local_offline_coding_agent_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtctda/trying_to_run_a_local_offline_coding_agent_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T04:59:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt8yax</id>
    <title>A ridiculously simple (and weird yet interesting) benchmark question I've figured out</title>
    <updated>2025-08-18T01:44:28+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello to y'all,&lt;/p&gt; &lt;p&gt;I've figured out that many models - even frontier ones like Deepseek or Gemini 2.5 Flash - fail with this simple (and a little messed up) question:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;&amp;quot;I am somehow 4 months older than my brother, but my mother doesn't want to tell me how that's possible. What am I missing here?&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;(Before you wonder, I got the idea for the scenario from a - presumably - trolling post on Reddit.)&lt;/p&gt; &lt;p&gt;Obviously, something isn't right here: pregnancy lasts 9 months, so what is going on? Deepseek, Gemini and even our beloved small Qwen 3 30B A3B completely failed to answer this and instead came up with explanations like &amp;quot;you are the mother in this scenario&amp;quot;. Mistral Medium just began reasoning to me - outside of any thinking tags, as it is not a reasoning model, burning through tokens - but still got it wrong. The old and dated QwQ, which is a dense reasoning-only model, failed catastrophically. Only few models, like the latest and greatest Qwen 3 235B A3B 2507 Thinking were able to find the solution:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;&amp;quot;You're either adopted or she isn't your and your brothers mother at the same time.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I think it's funny to see how these large models fail to answer a question that isn't even thaaaaat hard - thinking out of the box in this way seems to still leave a lot of room for improvement!&lt;/p&gt; &lt;p&gt;Just wanted to share this. Have a good one!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt8yax/a_ridiculously_simple_and_weird_yet_interesting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt8yax/a_ridiculously_simple_and_weird_yet_interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt8yax/a_ridiculously_simple_and_weird_yet_interesting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T01:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn3gi</id>
    <title>Ovis2.5 9B ~ 2B - New Multi-modal LLMs from Alibaba</title>
    <updated>2025-08-17T10:09:17+00:00</updated>
    <author>
      <name>/u/Sad_External6106</name>
      <uri>https://old.reddit.com/user/Sad_External6106</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing with &lt;strong&gt;Ovis2.5 (2B &amp;amp; 9B)&lt;/strong&gt; the past few days. The cool part is it now has an optional &lt;em&gt;think&lt;/em&gt; mode ‚Äî the model will slow down a bit but actually self-check and refine answers, which really helps on harder reasoning tasks. Also the OCR feels way better than before, especially on messy charts and dense documents. Overall, a pretty practical upgrade if you care about reasoning + OCR.&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335"&gt;https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_External6106"&gt; /u/Sad_External6106 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:09:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1msy01r</id>
    <title>Why does Qwen3-30B-A3B-Instruct-2507 Q8_0 work on my machine and no others come close?</title>
    <updated>2025-08-17T18:07:08+00:00</updated>
    <author>
      <name>/u/9acca9</name>
      <uri>https://old.reddit.com/user/9acca9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm surprised that having a machine with 8GB of VRAM and 32GB of RAM can run this LLM. Slow, yes, but it runs and gives good answers. Why isn't there another one like it? Why not a DeepSeek R1, for example?&lt;/p&gt; &lt;p&gt;I don't really mind waiting too much if I'm going to get an &amp;quot;accurate&amp;quot; answer.&lt;/p&gt; &lt;p&gt;Obviously, I don't use it regularly, but I like having an LLM to maybe ask a &amp;quot;personal&amp;quot; question, and also in case at some point they put restrictions on all non-local LLMs, overprice them, or lobotomize them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9acca9"&gt; /u/9acca9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msy01r/why_does_qwen330ba3binstruct2507_q8_0_work_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msy01r/why_does_qwen330ba3binstruct2507_q8_0_work_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msy01r/why_does_qwen330ba3binstruct2507_q8_0_work_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T18:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1msv4us</id>
    <title>GPT-OSS is not good at Brazilian Legal Framework :(</title>
    <updated>2025-08-17T16:17:35+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"&gt; &lt;img alt="GPT-OSS is not good at Brazilian Legal Framework :(" src="https://preview.redd.it/uqksokgduljf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d0c45bd812e8cb6b20834de28e876efa3b08b4c" title="GPT-OSS is not good at Brazilian Legal Framework :(" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;benchmark: &lt;a href="https://huggingface.co/datasets/celsowm/legalbench.br"&gt;https://huggingface.co/datasets/celsowm/legalbench.br&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uqksokgduljf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T16:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtbsvt</id>
    <title>Browser-based micro-LLMs (Gemma 270M/Qwen 0.5B) - production experiences?</title>
    <updated>2025-08-18T04:03:43+00:00</updated>
    <author>
      <name>/u/innagadadavida1</name>
      <uri>https://old.reddit.com/user/innagadadavida1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just dropped Gemma 270M specifically for edge deployment. At ~240MB download, it finally feels feasible for browser deployment without users rage-quitting during download. Anyone running these micro-models in prod?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System constraints I'm validating:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Download/Storage:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model sizes: Gemma 270M (~240MB), Qwen 0.5B (~500MB) quantized&lt;/li&gt; &lt;li&gt;Progressive loading or still all-or-nothing?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Runtime memory:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seeing &amp;lt;1GB total RAM usage (model + browser overhead)?&lt;/li&gt; &lt;li&gt;Memory spikes during inference?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance reality:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First-token latency on average hardware?&lt;/li&gt; &lt;li&gt;Background tab throttling issues?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The tradeoff:&lt;/strong&gt; These models are tiny enough to actually download but potentially too dumb to be useful. Google's pitch is &amp;quot;fine-tune for your specific task&amp;quot; but I'm skeptical about quality floor for customer-facing apps.&lt;/p&gt; &lt;p&gt;Anyone have real metrics from production? Specifically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bounce rate during model download&lt;/li&gt; &lt;li&gt;Device/browser success rates&lt;/li&gt; &lt;li&gt;Actual memory footprint in the wild&lt;/li&gt; &lt;li&gt;Whether users even notice/care about local inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Considering a progressive enhancement approach: tiny local model for basic intent classification, API fallback for anything complex. But wondering if the juice is worth the squeeze.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/innagadadavida1"&gt; /u/innagadadavida1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtbsvt/browserbased_microllms_gemma_270mqwen_05b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtbsvt/browserbased_microllms_gemma_270mqwen_05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtbsvt/browserbased_microllms_gemma_270mqwen_05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T04:03:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1msosdv</id>
    <title>Why does Mistral NeMo's usage keep growing even after more than a year since releasing?</title>
    <updated>2025-08-17T11:46:54+00:00</updated>
    <author>
      <name>/u/xugik1</name>
      <uri>https://old.reddit.com/user/xugik1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt; &lt;img alt="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" src="https://preview.redd.it/5wd0ayxihkjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef52cf7168e409394d3d181f871e20c40bcefa5d" title="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xugik1"&gt; /u/xugik1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5wd0ayxihkjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T11:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt0rld</id>
    <title>Is it just me, or is LM Studio really pushing the new gpt-oss?</title>
    <updated>2025-08-17T19:52:39+00:00</updated>
    <author>
      <name>/u/PracticlySpeaking</name>
      <uri>https://old.reddit.com/user/PracticlySpeaking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"&gt; &lt;img alt="Is it just me, or is LM Studio really pushing the new gpt-oss?" src="https://b.thumbs.redditmedia.com/I7KSIeyl1d4CoN4rEoRDFE7evtxOdppiW6wbj9W6Q5g.jpg" title="Is it just me, or is LM Studio really pushing the new gpt-oss?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...maybe a little too far? I mean, the setup has a step for &amp;quot;Now download some models&amp;quot; ‚Äî that only offers gpt-oss. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mmqsn49rwmjf1.png?width=713&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c1b4736e379c98e35634f3c6ed02aa26f79bf8a"&gt;the one model to rule them all?&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticlySpeaking"&gt; /u/PracticlySpeaking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T19:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt6l87</id>
    <title>NVIDIA Releases Open Multilingual Speech Dataset and Two New Models for Multilingual Speech-to-Text</title>
    <updated>2025-08-17T23:53:47+00:00</updated>
    <author>
      <name>/u/RYSKZ</name>
      <uri>https://old.reddit.com/user/RYSKZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt6l87/nvidia_releases_open_multilingual_speech_dataset/"&gt; &lt;img alt="NVIDIA Releases Open Multilingual Speech Dataset and Two New Models for Multilingual Speech-to-Text" src="https://external-preview.redd.it/A88FneA2E8vj8FZkQqex3y_zTUkEEvLZOR75ND29msM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faf8982f5868ceffad20906f1a118bd04e77ded1" title="NVIDIA Releases Open Multilingual Speech Dataset and Two New Models for Multilingual Speech-to-Text" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA has launched &lt;strong&gt;Granary&lt;/strong&gt;, a massive open-source multilingual speech dataset with 1M hours of audio, supporting 25 European languages, including low-resource ones like Croatian, Estonian, and Maltese.&lt;/p&gt; &lt;p&gt;Alongside it, NVIDIA released &lt;strong&gt;two high-performance STT models&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Canary-1b-v2&lt;/strong&gt;: 1B parameters, top accuracy on Hugging Face for multilingual speech recognition, translating between English and 24 languages, 10√ó faster inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parakeet-tdt-0.6b-v3&lt;/strong&gt;: 600M parameters, designed for real-time and large-scale transcription with highest throughput in its class.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hugging Face links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Granary: &lt;a href="https://huggingface.co/datasets/nvidia/Granary"&gt;https://huggingface.co/datasets/nvidia/Granary&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Canary-1b-v2: &lt;a href="https://huggingface.co/nvidia/canary-1b-v2"&gt;https://huggingface.co/nvidia/canary-1b-v2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Parakeet-tdt-0.6b-v3: &lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3"&gt;https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RYSKZ"&gt; /u/RYSKZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blogs.nvidia.com/blog/speech-ai-dataset-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt6l87/nvidia_releases_open_multilingual_speech_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt6l87/nvidia_releases_open_multilingual_speech_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T23:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt2iev</id>
    <title>GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure.</title>
    <updated>2025-08-17T21:01:10+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"&gt; &lt;img alt="GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure." src="https://external-preview.redd.it/LOoaiYGJSklUhS_jyXNZtXqZW5tq1NuC9Dm2RNcy-8Q.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fb11e05f1a215e4e59de75726903bb0ecb7f1d6" title="GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was doing some tool calling tests while figuring out how to work with the Harmony GPT-OSS prompt format. I made a little helpful tool here if you're trying to understand how harmony works (there's a whole repo there too with a bit deeper exploration if you're curious):&lt;br /&gt; &lt;a href="https://github.com/Deveraux-Parker/GPT-OSS-MONKEY-WRENCHES/blob/main/harmony_educational_demo.html"&gt;https://github.com/Deveraux-Parker/GPT-OSS-MONKEY-WRENCHES/blob/main/harmony_educational_demo.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, I wanted to benchmark the system so I asked it to make a fun benchmark, and this is what it came up with. In this video, missiles are falling from the sky and the agent has to see their trajectory and speed, run a tool call with python to anticipate where the missile will be in the future, and fire an explosive anti-missile at it so that it can hit the spot it'll be when the missile arrives. To do this, it needs to have low latency, understand its own latency, and be able to RAPIDLY fire off tool calls. This is firing with 100% accuracy (it technically missed 10 tool calls along the way but was able to recover and fire them before the missiles hit the ground).&lt;/p&gt; &lt;p&gt;So... here's GPT-OSS-20b running 100 agents simultaneously at 131,076 token context, each agent with its own 131k context window, each hitting sub-100ms ttft, blowing everything out of the sky at 10k tokens/second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=8T8drT0rwCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T21:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1msrnqq</id>
    <title>Wow anthropic and Google losing coding share bc of qwen 3 coder</title>
    <updated>2025-08-17T13:59:47+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt; &lt;img alt="Wow anthropic and Google losing coding share bc of qwen 3 coder" src="https://preview.redd.it/rwehyliy5ljf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c67f716968425732683dc36fdd2644caa8322da3" title="Wow anthropic and Google losing coding share bc of qwen 3 coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwehyliy5ljf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt3epi</id>
    <title>M4 Max generation speed vs context size</title>
    <updated>2025-08-17T21:37:31+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt3epi/m4_max_generation_speed_vs_context_size/"&gt; &lt;img alt="M4 Max generation speed vs context size" src="https://b.thumbs.redditmedia.com/0KFbAMMgVPnCWsrraGEmcRNBchKGkvvSro4EHgyP7yQ.jpg" title="M4 Max generation speed vs context size" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a custom benchmark program to map out generation speed vs context size. The program will build up a prompt 10k tokens at a time and log the reported stats from LM Studio. The intention is to simulate agentic coding. Cline/Roo/Kilo use about 20k tokens for the system prompt.&lt;/p&gt; &lt;p&gt;Better images here: &lt;a href="https://oz9h.dk/benchmark/"&gt;https://oz9h.dk/benchmark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My computer is the M4 Max Macbook Pro 128 GB. All models at 4 bit quantization. KV-Cache at 8 bit.&lt;/p&gt; &lt;p&gt;I am quite sad that GLM 4.5 Air degrades so quickly. And impressed that GPT-OSS 120b manages to stay fast even with 100k context. I don't use Qwen3-Coder 30b-a3b much but I am still surprised at how quickly it crashes and it even gets slower than GPT-OSS - a model 4 times larger. And my old workhorse Devstral somehow manages to be the most consistent model regarding speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mt3epi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt3epi/m4_max_generation_speed_vs_context_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt3epi/m4_max_generation_speed_vs_context_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T21:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt9htu</id>
    <title>FlashAttention 4 Leak</title>
    <updated>2025-08-18T02:10:02+00:00</updated>
    <author>
      <name>/u/InevitableExtreme396</name>
      <uri>https://old.reddit.com/user/InevitableExtreme396</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"&gt; &lt;img alt="FlashAttention 4 Leak" src="https://a.thumbs.redditmedia.com/5f6bt97vhO4cGlHOqVfCoRYr_m9nC10Eu8zHnCuyjR0.jpg" title="FlashAttention 4 Leak" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like the FA4 source code just got leaked here on a branch:&lt;/p&gt; &lt;p&gt;TLDR; As expected, it's mostly Blackwell (SM100+) and Tensor Core Generation 5, and uses the CuTe DSL (CUTLASS). There is also some handwritten PTX.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/46yfc8z3sojf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b1b33b3f27dfe41ec550142abaa8e0e97bc2449"&gt;https://preview.redd.it/46yfc8z3sojf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b1b33b3f27dfe41ec550142abaa8e0e97bc2449&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's from the SGlang codebase, one of the popular LLM inference engines (like Llama.cpp for distributed.)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang/compare/main...hieu/fa4"&gt;https://github.com/sgl-project/sglang/compare/main...hieu/fa4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InevitableExtreme396"&gt; /u/InevitableExtreme396 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt9htu/flashattention_4_leak/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T02:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1msr7j8</id>
    <title>To all vibe coders I present</title>
    <updated>2025-08-17T13:40:07+00:00</updated>
    <author>
      <name>/u/theundertakeer</name>
      <uri>https://old.reddit.com/user/theundertakeer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt; &lt;img alt="To all vibe coders I present" src="https://external-preview.redd.it/dXZiNzRocGcybGpmMeA17HlDZqcxGH0WPMXNGATdmxTbHU45E1nSLLgU5DlN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc5981b886ff07914ad22d7db97d58fa9b60c3a9" title="To all vibe coders I present" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theundertakeer"&gt; /u/theundertakeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eckuwlog2ljf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtct4y</id>
    <title>Elon didn't deliver on this announcement. It's already Monday.</title>
    <updated>2025-08-18T04:59:00+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt; &lt;img alt="Elon didn't deliver on this announcement. It's already Monday." src="https://preview.redd.it/rt8xgjaampjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2b6da79d84d1e52439441cafb251d7e1dc508f7" title="Elon didn't deliver on this announcement. It's already Monday." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rt8xgjaampjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T04:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
