<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-05T15:48:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j462ja</id>
    <title>What's the best model for coding/programming and has access to internet?</title>
    <updated>2025-03-05T15:47:34+00:00</updated>
    <author>
      <name>/u/Zealousideal_Try3409</name>
      <uri>https://old.reddit.com/user/Zealousideal_Try3409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, I am very new to this and would like to learn more about this and open to opinions and thoughts, so basically my final year of school is going to finish in about a month and my exams too, after that I will receive a laptop, the specs are legion pro 7i (i9-14900HX/4080m) I was wondering if you could tell me which model would be the best to use and how much and how can I utilise it, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Try3409"&gt; /u/Zealousideal_Try3409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j462ja/whats_the_best_model_for_codingprogramming_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j462ja/whats_the_best_model_for_codingprogramming_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j462ja/whats_the_best_model_for_codingprogramming_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T15:47:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3k8o8</id>
    <title>There is a space on HF where you can convert models to MLX without downloading them</title>
    <updated>2025-03-04T20:10:48+00:00</updated>
    <author>
      <name>/u/BaysQuorv</name>
      <uri>https://old.reddit.com/user/BaysQuorv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been downloading models and converting them to MLX with the convert script from the MLX library. This takes lots of time and I'm limited on how big models I can convert by my 16gb ram.&lt;/p&gt; &lt;p&gt;Well I just learned that there is a space on HF where you can convert models, you just enter the model name and the quant and it uploads it straight to your HF profile! No need to download or do anything at all. This also means I'm not limited by my ram, I can convert any model now and so should you :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/mlx-community/mlx-my-repo"&gt;https://huggingface.co/spaces/mlx-community/mlx-my-repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BaysQuorv"&gt; /u/BaysQuorv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T20:10:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3xn8k</id>
    <title>Making vision language models point to objects in image, introducing new modality to a language model</title>
    <updated>2025-03-05T07:21:28+00:00</updated>
    <author>
      <name>/u/SmallTimeCSGuy</name>
      <uri>https://old.reddit.com/user/SmallTimeCSGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying something similar as MoonDream, and Molmo. i.e make the language model capable of producing normalized coordinates of objects asked about. &amp;quot;Point: Dog&amp;quot; e.g.&lt;/p&gt; &lt;p&gt;I am trying to make smolvlm do this as a fun project to get better understanding. I am trying on a subset(1mil) of pixmo-points dataset.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;tried plain SFT, both full and PEFT, obviously that did not work, as the model does not have notion of points being output.&lt;/li&gt; &lt;li&gt;tried GRPO, that too, did not work, as the model evidently did not have latent capabilities as such for this to emerge.&lt;/li&gt; &lt;li&gt;taking some inspiration from moondream, I introduced a new modality for points altogether. i.e. points are encoded, same embedding dimension as accepted by the autoregressive part of the model, then after autoregressive, have another decoder decode the points. Keeping the other parts frozen. I tried SFT with cross entropy, though am a bit skeptical of it being used for a pointing task, where MSE loss seems more suitable. But this too, failed though showing a nice loss characteristics during training. The model just produces random points.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Has anyone tried something similar? Any suggestions on what else I can try? Any pointer on how to make some progress would be good, as clearly this is feasible. What am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmallTimeCSGuy"&gt; /u/SmallTimeCSGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3xn8k/making_vision_language_models_point_to_objects_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3xn8k/making_vision_language_models_point_to_objects_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3xn8k/making_vision_language_models_point_to_objects_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T07:21:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3fkax</id>
    <title>LLM Quantization Comparison</title>
    <updated>2025-03-04T17:02:00+00:00</updated>
    <author>
      <name>/u/dat1-co</name>
      <uri>https://old.reddit.com/user/dat1-co</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"&gt; &lt;img alt="LLM Quantization Comparison" src="https://external-preview.redd.it/CGbzq4JDmMgH-DT2hVt-MPGAGrs7Io3E0dHabckY9J8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbd8d1effbe5db86d79260e6b8463c84c31b3a11" title="LLM Quantization Comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dat1-co"&gt; /u/dat1-co &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://dat1.co/blog/llm-quantization-comparison"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3fkax/llm_quantization_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:02:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3bldn</id>
    <title>C4AI Aya Vision</title>
    <updated>2025-03-04T14:09:38+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"&gt; &lt;img alt="C4AI Aya Vision" src="https://external-preview.redd.it/2FtgBIdrUTjZVqld2wWXrJgxCyutz4lA4knvupaJc-g.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15cffd187923d62691d6d92d4dd9ba2db2a4f098" title="C4AI Aya Vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/CohereForAI/c4ai-aya-vision-67c4ccd395ca064308ee1484"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3bldn/c4ai_aya_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T14:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3lbck</id>
    <title>SCANN: A Self-Organizing Coherent Attention Neural Network</title>
    <updated>2025-03-04T20:56:00+00:00</updated>
    <author>
      <name>/u/vesudeva</name>
      <uri>https://old.reddit.com/user/vesudeva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"&gt; &lt;img alt="SCANN: A Self-Organizing Coherent Attention Neural Network" src="https://external-preview.redd.it/JLOcLL_amzNZJjQB5eq2ns36cBxnwXJySZseItLLLD8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58af66c266b498548139bfda71fa96ad039e54f4" title="SCANN: A Self-Organizing Coherent Attention Neural Network" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few weeks ago, in my latest deep dive into random thought experiments, I went the furthest I've gone in terms of research/depth/experiments and was able to come out of the other side with an interesting information based field equation which I've now been able to apply to a new ML and neural network for learning. It's in the early stages, but the results so far are incredible with the clear path to scaling into a full LLM architecture.&lt;/p&gt; &lt;p&gt;So then what am I talking about?&lt;/p&gt; &lt;p&gt;Instead of conventional gradient-based optimization used in machine learning models such as logistic regression, random forests, and deep learning, or the attention-based token weighting in LLMs, SCANN employs a fundamentally different approach.&lt;/p&gt; &lt;p&gt;What Makes SCANN Different?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Self-Organization Instead of Training&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Unlike traditional models that explicitly train weights via backpropagation, SCANN allows features to evolve dynamically over time.&lt;/p&gt; &lt;p&gt;The transformation follows a mathematically governed Partial Differential Equation (PDE):&lt;/p&gt; &lt;p&gt;SCANN Equation =&lt;/p&gt; &lt;p&gt;D[ψ[t, x], t] == -γ ψ[t, x] - ∇ ⋅ (D[ψ[t, x]] ∇ ψ[t, x]) +&lt;/p&gt; &lt;p&gt;λnl Sum[ψ[t, xi], {xi, Neighbors}] + β Tanh[ψ[t, x]^2]&lt;/p&gt; &lt;p&gt;Where:&lt;/p&gt; &lt;p&gt;- Diffusion spreads feature information naturally:&lt;/p&gt; &lt;p&gt;D(ψ) = D0 (1 + α ψ^2)&lt;/p&gt; &lt;p&gt;- Nonlocal interactions allow features to learn from global structures.&lt;/p&gt; &lt;p&gt;- Resonance amplifies meaningful patterns.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SCANN Generalizes Without Dataset-Specific Tuning&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;SCANN has been evaluated across multiple datasets (Digits, Wine Classification, Breast Cancer, etc.) and has consistently performed well without dataset-specific retraining.&lt;/p&gt; &lt;p&gt;Increasing the number of time steps improves representation learning, allowing SCANN to refine feature structures dynamically over time.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;SCANN vs. LLMs and Traditional Machine Learning&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Traditional Machine Learning models (e.g., SVMs, Neural Networks) require explicit parameter training to fit a loss function.&lt;/p&gt; &lt;p&gt;LLMs use layered token attention to interpret complex relationships in text.&lt;/p&gt; &lt;p&gt;SCANN, however, does not rely on pre-set parameters or static learning mechanisms. Instead, it evolves feature representations dynamically, resembling a physical system seeking equilibrium.&lt;/p&gt; &lt;p&gt;Why This is Exciting&lt;/p&gt; &lt;p&gt;SCANN represents a new perspective on representation learning—one that does not depend on large datasets or brute-force optimization. It offers a self-organizing mechanism for feature discovery, potentially revealing patterns in ways that traditional ML approaches cannot.&lt;/p&gt; &lt;p&gt;Further refinements and formalization are ongoing, but these early results highlight SCANN’s potential for a fundamentally different kind of machine learning.&lt;/p&gt; &lt;p&gt;I'll be open sourcing all the code and releasing a paper once I get some more tests done and hopefully a small LLM built from it as well.&lt;/p&gt; &lt;p&gt;In the meantime, if you're interested in the core information-based field equation I built and then integrated into this ML model you can check out all the details and experiments here: &lt;a href="https://github.com/severian42/Informational-Relative-Evolution"&gt;https://github.com/severian42/Informational-Relative-Evolution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And a more longform paper here: &lt;a href="https://huggingface.co/blog/Severian/informational-relative-evolution"&gt;https://huggingface.co/blog/Severian/informational-relative-evolution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Test Results:&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jbde7qr5lqme1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e24bd4507f3ce4964da85bed5ade35582cc8ef88"&gt;https://preview.redd.it/jbde7qr5lqme1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e24bd4507f3ce4964da85bed5ade35582cc8ef88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2rvuqo2okqme1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d821722df31e9d41f363d1884893ab6d90a22143"&gt;https://preview.redd.it/2rvuqo2okqme1.png?width=737&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d821722df31e9d41f363d1884893ab6d90a22143&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vesudeva"&gt; /u/vesudeva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3lbck/scann_a_selforganizing_coherent_attention_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T20:56:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3m8v5</id>
    <title>Cohere Blog: Aya Vision — Expanding the Worlds AI Can See</title>
    <updated>2025-03-04T21:34:39+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3m8v5/cohere_blog_aya_vision_expanding_the_worlds_ai/"&gt; &lt;img alt="Cohere Blog: Aya Vision — Expanding the Worlds AI Can See" src="https://external-preview.redd.it/N2GDJUmAUpd9T37b2tIt2DUV5G6cVnF0cK_wUy2iAqI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1371c07fff0fcedfb4dc146d3550992b34c14bbd" title="Cohere Blog: Aya Vision — Expanding the Worlds AI Can See" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cohere.com/blog/aya-vision"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3m8v5/cohere_blog_aya_vision_expanding_the_worlds_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3m8v5/cohere_blog_aya_vision_expanding_the_worlds_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T21:34:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3yc4l</id>
    <title>Apple Notes + Ollama</title>
    <updated>2025-03-05T08:14:06+00:00</updated>
    <author>
      <name>/u/arne226</name>
      <uri>https://old.reddit.com/user/arne226</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a desktop app to converse with Apple Notes using Ollama. Could be interesting to add other Notes sources like Obsidian for example. &lt;/p&gt; &lt;p&gt;Theres a version that only supports local processing &lt;a href="https://github.com/arnestrickmann/Notechat"&gt;https://github.com/arnestrickmann/Notechat&lt;/a&gt; but also one that offers processing with Gemini. &lt;/p&gt; &lt;p&gt;Would be happy about your feedback.&lt;br /&gt; Arne &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arne226"&gt; /u/arne226 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3yc4l/apple_notes_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3yc4l/apple_notes_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3yc4l/apple_notes_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T08:14:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1j41q9y</id>
    <title>Whats a good/best model for realistic chatting ?</title>
    <updated>2025-03-05T12:16:15+00:00</updated>
    <author>
      <name>/u/toopanpan</name>
      <uri>https://old.reddit.com/user/toopanpan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm wondering if theres a model out there thats finetuned to be as human-like as possible when generating text? think of old Char ai and how people used to be convinced they were talking to a real person. I'd like to finetune my own model one day but I'd like to try out if theres one out there thats on par with how convincing it could get like char ai.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toopanpan"&gt; /u/toopanpan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j41q9y/whats_a_goodbest_model_for_realistic_chatting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j41q9y/whats_a_goodbest_model_for_realistic_chatting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j41q9y/whats_a_goodbest_model_for_realistic_chatting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T12:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43tcd</id>
    <title>Think Toggles are Dumb - Sketch toward automatically thoughtful chatbots</title>
    <updated>2025-03-05T14:05:17+00:00</updated>
    <author>
      <name>/u/NiloCKM</name>
      <uri>https://old.reddit.com/user/NiloCKM</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43tcd/think_toggles_are_dumb_sketch_toward/"&gt; &lt;img alt="Think Toggles are Dumb - Sketch toward automatically thoughtful chatbots" src="https://external-preview.redd.it/xysnssK0wWdIRckvWVwaBSbIhMo96eApOHbJ846j7qQ.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cd1045517eda93c2aaafc19130bea85c7466318" title="Think Toggles are Dumb - Sketch toward automatically thoughtful chatbots" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NiloCKM"&gt; /u/NiloCKM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.paritybits.me/think-toggles-are-dumb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43tcd/think_toggles_are_dumb_sketch_toward/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43tcd/think_toggles_are_dumb_sketch_toward/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:05:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j455pp</id>
    <title>Offering text, images, video, songs, streaming voice chat, speech recognition, all on one AI server?</title>
    <updated>2025-03-05T15:07:31+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How can i setup an AI server (with one or more GPUs), so it can offer various self-hosted AI (LLM) based services? In particular, not just the code and chat completion that Ollama or vLLM can offer, but also:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Speech recognition (with whisper)&lt;/li&gt; &lt;li&gt;image generation with something like Flux.1 or Stable Diffusion&lt;/li&gt; &lt;li&gt;Song generation with &lt;a href="https://aslp-lab.github.io/DiffRhythm.github.io/"&gt;DiffRhythm&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Video generation with &lt;a href="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B"&gt;Wan 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Audio chat with Sesame or Llama 4 (hopefully using a nice mobile app with Carplay support)&lt;/li&gt; &lt;li&gt;Text to speech to read texts with one of the available models, perhaps also voice cloning&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Ideally there is a software that does everything (one can dream!), but realistically some of these would be competing for the GPU and there needs to be a good way to allocate it (or parts of it) to a particular user or purpose for a given time slot. With multiple GPUs i guess more than one thing can happen at once on the server on different GPUs.&lt;/p&gt; &lt;p&gt;Are there good solutions already?&lt;/p&gt; &lt;p&gt;Thanks for pointers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j455pp/offering_text_images_video_songs_streaming_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j455pp/offering_text_images_video_songs_streaming_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j455pp/offering_text_images_video_songs_streaming_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T15:07:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1j45zg7</id>
    <title>AMD Medusa Halo, coming after Strix Halo in H1/2026 with (optional) 384bit memory bus</title>
    <updated>2025-03-05T15:43:40+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the latest rumors: &lt;a href="https://www.notebookcheck.net/Powerful-Zen-6-Medusa-Halo-iGPU-could-challenge-desktop-GPUs-with-20-more-CUs-and-50-wider-bus.971869.0.html"&gt;https://www.notebookcheck.net/Powerful-Zen-6-Medusa-Halo-iGPU-could-challenge-desktop-GPUs-with-20-more-CUs-and-50-wider-bus.971869.0.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We can expect a 50% memory speed increase, that should provide a 50% boost in tokens per second. I hope they also offer beyond 128GB RAM, perhaps 192GB or 256GB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j45zg7/amd_medusa_halo_coming_after_strix_halo_in_h12026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j45zg7/amd_medusa_halo_coming_after_strix_halo_in_h12026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j45zg7/amd_medusa_halo_coming_after_strix_halo_in_h12026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T15:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3hjxb</id>
    <title>Perplexity R1 1776 climbed to first place after being re-tested in lineage-bench logical reasoning benchmark</title>
    <updated>2025-03-04T18:21:37+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"&gt; &lt;img alt="Perplexity R1 1776 climbed to first place after being re-tested in lineage-bench logical reasoning benchmark" src="https://preview.redd.it/6jg8ae9drpme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85bce8c13b47a201032eadf727ee61bfb00869ed" title="Perplexity R1 1776 climbed to first place after being re-tested in lineage-bench logical reasoning benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6jg8ae9drpme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3hjxb/perplexity_r1_1776_climbed_to_first_place_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T18:21:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3w4zs</id>
    <title>Why I say ollama is great for PoC but bad for production. (yet another rabbit hole) They finally support setting the default context size but I don't think beginners would notice this.</title>
    <updated>2025-03-05T05:40:32+00:00</updated>
    <author>
      <name>/u/henryclw</name>
      <uri>https://old.reddit.com/user/henryclw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How time flies. The latest ollama release finally support the setting of default context length. I'm sure that 2k default context length have killed tons of beginners. Why is my model so dummy when I give it 10 pages PDF? Because you haven't set the context length, surprise!&lt;/p&gt; &lt;p&gt;Okay, now they support setting the default context length. You may wonder, how?&lt;br /&gt; Well it's written in a release note. But other than that, there's zero documentation about it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.5.13"&gt;https://github.com/ollama/ollama/releases/tag/v0.5.13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You could see there's an environment variable called &lt;code&gt;OLLAMA_CONTEXT_LENGTH&lt;/code&gt; . But if you search through their poorly organized documents, you would find nothing. If you went on searching the GitHub code, you would find ... &lt;a href="https://github.com/search?q=repo%3Aollama%2Follama+OLLAMA_CONTEXT_LENGTH+&amp;amp;type=code"&gt;https://github.com/search?q=repo%3Aollama%2Follama+OLLAMA_CONTEXT_LENGTH+&amp;amp;type=code&lt;/a&gt; There is no markdown document that mentions this newly introduced environment variable. &lt;/p&gt; &lt;p&gt;Actually if you are smart enough, you could sense that this is not ready for production when you find out there's serval different markdown file you need to go through to set this thing up correctly. &lt;a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md"&gt;https://github.com/ollama/ollama/blob/main/docs/modelfile.md&lt;/a&gt; &lt;a href="https://github.com/ollama/ollama/blob/main/docs/api.md"&gt;https://github.com/ollama/ollama/blob/main/docs/api.md&lt;/a&gt; &lt;a href="https://github.com/ollama/ollama/blob/main/docs/faq.md"&gt;https://github.com/ollama/ollama/blob/main/docs/faq.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. ollama was not my first framework that brings me to LLM, however I did use it for quite a while before I finally switch to llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryclw"&gt; /u/henryclw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3w4zs/why_i_say_ollama_is_great_for_poc_but_bad_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3w4zs/why_i_say_ollama_is_great_for_poc_but_bad_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3w4zs/why_i_say_ollama_is_great_for_poc_but_bad_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T05:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43hv0</id>
    <title>Open source voice2voice</title>
    <updated>2025-03-05T13:50:24+00:00</updated>
    <author>
      <name>/u/Qnt-</name>
      <uri>https://old.reddit.com/user/Qnt-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any opensource voice2voice solution which works? I remember hearing about Qwen Audio,,,,did anyone try this stuff , yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qnt-"&gt; /u/Qnt- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43hv0/open_source_voice2voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43hv0/open_source_voice2voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43hv0/open_source_voice2voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T13:50:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3r8ls</id>
    <title>Deepseek V2.5 Becomes No.1 on Copilot Arena</title>
    <updated>2025-03-05T01:17:48+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"&gt; &lt;img alt="Deepseek V2.5 Becomes No.1 on Copilot Arena" src="https://external-preview.redd.it/qz0mj3UiKjc9f5igcmafTaZjnzBMMP1WUULpUFnNC8Y.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=904655aa1b01986e22fef932164110cccbc659a5" title="Deepseek V2.5 Becomes No.1 on Copilot Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the latest Copilot Arena rankings, &lt;strong&gt;Deepseek V2.5 (FIM)&lt;/strong&gt; has reached the &lt;strong&gt;top position&lt;/strong&gt; with an &lt;strong&gt;Arena Score of 1028&lt;/strong&gt;, outperforming strong competitors like &lt;strong&gt;Claude 3.5 Sonnet&lt;/strong&gt; and &lt;strong&gt;Codetral&lt;/strong&gt; to become the highest-ranked AI coding assistant! 🚀&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0q3v0xiurme1.png?width=1740&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=333d8e307f646075cbe1f4cbd5da55fe0b31bafe"&gt;Rank&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The leaderboard differs from existing evaluations. In particular, smaller models over perform in static benchmarks compared to real development workflows.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z8hb4h4ourme1.png?width=1174&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63100690e5403f3abe8bc8224be9c9af67a0b8db"&gt;https://preview.redd.it/z8hb4h4ourme1.png?width=1174&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63100690e5403f3abe8bc8224be9c9af67a0b8db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Compared to previous benchmarks, Copilot Arena observes more programming languages (PL), natural languages (NL), longer context lengths, multiple task types, and various code structures.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2276cjwuurme1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21b61eb97054808650e4dd23bb287a01af99c4a0"&gt;Data Distribution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/iamwaynechi/status/1896996806481109377"&gt;X&lt;/a&gt; &lt;a href="https://lmarena.ai/?leaderboard"&gt;Leaderboard&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2502.09328"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3r8ls/deepseek_v25_becomes_no1_on_copilot_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T01:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j429et</id>
    <title>Why is Qwen 2.5 32b Coder the best local text analysis LLM?</title>
    <updated>2025-03-05T12:46:48+00:00</updated>
    <author>
      <name>/u/custodiam99</name>
      <uri>https://old.reddit.com/user/custodiam99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried a lot of models, even Llama 3.3 70b q_4 with 32k context, but Qwen 2.5 32b q_8 Coder is the best &lt;em&gt;text analysis model&lt;/em&gt; I can find. It really mirrors the source text and can explore very nuanced details. It makes very good word lists (obviously not concordance) from the input file. Can there be a connection between the &lt;strong&gt;coder function&lt;/strong&gt; and the &lt;em&gt;text analysis functionality&lt;/em&gt;? Let's talk about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/custodiam99"&gt; /u/custodiam99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j429et/why_is_qwen_25_32b_coder_the_best_local_text/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j429et/why_is_qwen_25_32b_coder_the_best_local_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j429et/why_is_qwen_25_32b_coder_the_best_local_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T12:46:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3gahy</id>
    <title>NVIDIA’s GeForce RTX 4090 With 96GB VRAM Reportedly Exists; The GPU May Enter Mass Production Soon, Targeting AI Workloads.</title>
    <updated>2025-03-04T17:31:10+00:00</updated>
    <author>
      <name>/u/metallicamax</name>
      <uri>https://old.reddit.com/user/metallicamax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/"&gt;https://wccftech.com/nvidia-rtx-4090-with-96gb-vram-reportedly-exists/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highly highly interested. If this will be true.&lt;/p&gt; &lt;p&gt;Price around 6k. &lt;/p&gt; &lt;p&gt;Source; &amp;quot;The user did confirm that the one with a 96 GB VRAM won't guarantee stability and that its cost, due to a higher VRAM, will be twice the amount you would pay on the 48 GB edition. As per the user, this is one of the reasons why the factories are considering making only the 48 GB edition but may prepare the 96 GB in about 3-4 months.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/metallicamax"&gt; /u/metallicamax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3gahy/nvidias_geforce_rtx_4090_with_96gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T17:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3vbfh</id>
    <title>Ollama v0.5.13 has been released</title>
    <updated>2025-03-05T04:52:19+00:00</updated>
    <author>
      <name>/u/Inevitable-Rub8969</name>
      <uri>https://old.reddit.com/user/Inevitable-Rub8969</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"&gt; &lt;img alt="Ollama v0.5.13 has been released" src="https://b.thumbs.redditmedia.com/ZjpiHa9AL7vA9lJSktHNr9Xm0TdCOx4eLZRQfB67ZNA.jpg" title="Ollama v0.5.13 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ijerrfvrxsme1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf51d6eb86ad43c56cc56e4a441141a640722c8f"&gt;https://preview.redd.it/ijerrfvrxsme1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bf51d6eb86ad43c56cc56e4a441141a640722c8f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable-Rub8969"&gt; /u/Inevitable-Rub8969 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3vbfh/ollama_v0513_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T04:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j44vep</id>
    <title>Mac Studio just got 512GB of memory!</title>
    <updated>2025-03-05T14:55:01+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/"&gt;https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For $10,499 (in US), you get 512GB of memory and 4TB storage @ 819 GB/s memory bandwidth. This could be enough to run Llama 3.1 405B @ 8 tps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j44vep/mac_studio_just_got_512gb_of_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j44vep/mac_studio_just_got_512gb_of_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j44vep/mac_studio_just_got_512gb_of_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43ziq</id>
    <title>The new king? M3 Ultra, 80 Core GPU, 512GB Memory</title>
    <updated>2025-03-05T14:13:32+00:00</updated>
    <author>
      <name>/u/Hanthunius</name>
      <uri>https://old.reddit.com/user/Hanthunius</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"&gt; &lt;img alt="The new king? M3 Ultra, 80 Core GPU, 512GB Memory" src="https://preview.redd.it/jkhal4p0qvme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8cb3ce2fdbe1423c5cf740e8f17c9c8df2f9e7b2" title="The new king? M3 Ultra, 80 Core GPU, 512GB Memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all. With 512GB of memory a world of possibilities opens up. What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hanthunius"&gt; /u/Hanthunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jkhal4p0qvme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43ziq/the_new_king_m3_ultra_80_core_gpu_512gb_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1j42py0</id>
    <title>OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp; 20+ Rich Interactions</title>
    <updated>2025-03-05T13:11:17+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j42py0/oasis_opensourced_social_media_simulator_that/"&gt; &lt;img alt="OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp;amp; 20+ Rich Interactions" src="https://preview.redd.it/jw78717wevme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=992ec58ae3641fac0387c04211452b938e040836" title="OASIS: Open-Sourced Social Media Simulator that uses up to 1 million agents &amp;amp; 20+ Rich Interactions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw78717wevme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j42py0/oasis_opensourced_social_media_simulator_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j42py0/oasis_opensourced_social_media_simulator_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T13:11:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1j417qh</id>
    <title>llama.cpp is all you need</title>
    <updated>2025-03-05T11:45:16+00:00</updated>
    <author>
      <name>/u/s-i-e-v-e</name>
      <uri>https://old.reddit.com/user/s-i-e-v-e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only started paying somewhat serious attention to locally-hosted LLMs earlier this year.&lt;/p&gt; &lt;p&gt;Went with ollama first. Used it for a while. Found out by accident that it is using llama.cpp. Decided to make life difficult by trying to compile the llama.cpp ROCm backend from source on Linux for a somewhat unsupported AMD card. Did not work. Gave up and went back to ollama.&lt;/p&gt; &lt;p&gt;Built a simple story writing helper cli tool for myself based on file includes to simplify lore management. Added ollama API support to it.&lt;/p&gt; &lt;p&gt;ollama randomly started to use CPU for inference while &lt;code&gt;ollama ps&lt;/code&gt; claimed that the GPU was being used. Decided to look for alternatives.&lt;/p&gt; &lt;p&gt;Found koboldcpp. Tried the same ROCm compilation thing. Did not work. Decided to run the regular version. To my surprise, it worked. Found that it was using vulkan. Did this for a couple of weeks.&lt;/p&gt; &lt;p&gt;Decided to try llama.cpp again, but the vulkan version. And it worked!!!&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; gives you a clean and extremely competent web-ui. Also provides an API endpoint (including an OpenAI compatible one). llama.cpp comes with a million other tools and is extremely tunable. You do not have to wait for other dependent applications to expose this functionality.&lt;/p&gt; &lt;p&gt;llama.cpp is all you need.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s-i-e-v-e"&gt; /u/s-i-e-v-e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j417qh/llamacpp_is_all_you_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T11:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j43us5</id>
    <title>Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory</title>
    <updated>2025-03-05T14:07:13+00:00</updated>
    <author>
      <name>/u/iCruiser7</name>
      <uri>https://old.reddit.com/user/iCruiser7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"&gt; &lt;img alt="Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory" src="https://external-preview.redd.it/IUc-sq0jBjlLBbxyREexc_Ijkq_kHcRXYNu-Mr7u5LI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86cb29351c6e2bde66e4d208acbdf5c007acd170" title="Apple releases new Mac Studio with M4 Max and M3 Ultra, and up to 512GB unified memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iCruiser7"&gt; /u/iCruiser7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.apple.com/newsroom/2025/03/apple-unveils-new-mac-studio-the-most-powerful-mac-ever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j43us5/apple_releases_new_mac_studio_with_m4_max_and_m3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T14:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1j3zxwn</id>
    <title>Are we ready!</title>
    <updated>2025-03-05T10:16:41+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"&gt; &lt;img alt="Are we ready!" src="https://preview.redd.it/m0ktikjrjume1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9dc8037f70763ba02e1ed164ff1654c69921dfd" title="Are we ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m0ktikjrjume1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j3zxwn/are_we_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-05T10:16:41+00:00</published>
  </entry>
</feed>
