<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-13T14:48:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ioewvw</id>
    <title>WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows</title>
    <updated>2025-02-13T08:53:08+00:00</updated>
    <author>
      <name>/u/Elegant_Fish_3822</name>
      <uri>https://old.reddit.com/user/Elegant_Fish_3822</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt; &lt;img alt="WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows" src="https://external-preview.redd.it/PMbSHk0WW6PoDIccKf_6k0rFhzH7cvXADJSNQbeOQeM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5df5a740fb78975f904e1d12f013d08df810dc2" title="WebRover 2.0 - AI Copilot for Browser Automation and Research Workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wondered if AI could autonomously navigate the web to perform complex research tasks—tasks that might take you hours or even days—without stumbling over context limitations like existing large language models?&lt;/p&gt; &lt;p&gt;Introducing WebRover 2.0, an open-source web automation agent that efficiently orchestrates complex research tasks using Langchains's agentic framework, LangGraph, and retrieval-augmented generation (RAG) pipelines. Simply provide the agent with a topic, and watch as it takes control of your browser to conduct human-like research.&lt;/p&gt; &lt;p&gt;I welcome your feedback, suggestions, and contributions to enhance WebRover further. Let's collaborate to push the boundaries of autonomous AI agents! 🚀&lt;/p&gt; &lt;p&gt;Explore the the project on Github : &lt;a href="https://github.com/hrithikkoduri/WebRover"&gt;https://github.com/hrithikkoduri/WebRover&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Curious to see it in action? 🎥 In the demo video below, I prompted the deep research agent to write a detailed report on AI systems in healthcare. It autonomously browses the web, opens links, reads through webpages, self-reflects, and infers to build a comprehensive report with references. Additionally, it also opens Google Docs and types down the entire report for you to use later.]&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ioewvw/video/jzfc8ncjevie1/player"&gt;https://reddit.com/link/1ioewvw/video/jzfc8ncjevie1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elegant_Fish_3822"&gt; /u/Elegant_Fish_3822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioewvw/webrover_20_ai_copilot_for_browser_automation_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T08:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioh75z</id>
    <title>DeepseekR1 with voice message?</title>
    <updated>2025-02-13T11:40:34+00:00</updated>
    <author>
      <name>/u/Greenhacker</name>
      <uri>https://old.reddit.com/user/Greenhacker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love sending voice messages to LLMs, and read their replies. Claude is impressive on this because it's almost instant transcription. ChatGPT is great too.&lt;/p&gt; &lt;p&gt;I've been using DeepseekR1 on OpenRouter, but OpenRouter doesn't offer voice messages. Do you know a service with R1 that offers voice messages?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Greenhacker"&gt; /u/Greenhacker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioh75z/deepseekr1_with_voice_message/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioh75z/deepseekr1_with_voice_message/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioh75z/deepseekr1_with_voice_message/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T11:40:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohaj5</id>
    <title>Correct my prompts for text summaries (Llama, but suggestions are welcome)</title>
    <updated>2025-02-13T11:46:44+00:00</updated>
    <author>
      <name>/u/brian-the-porpoise</name>
      <uri>https://old.reddit.com/user/brian-the-porpoise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to build a little handy assistant that summarizes my digital notes for me. &lt;/p&gt; &lt;p&gt;I am using llama.cpp with llama-3.2-3b-instruct-q8_0. &lt;/p&gt; &lt;p&gt;No matter what I try as prompts, the result is not adhering to the requirements I set.&lt;br /&gt; E.g. I want it to narrate from the first person perspective, but it never does it. I specifically asked it to refer to me as &amp;quot;you&amp;quot;, doesnt do it either. &lt;/p&gt; &lt;p&gt;I have found that after going back and forth with it, it will eventually do as I require, but I would need a one-shot prompt so I can automate the whole thing. &lt;/p&gt; &lt;p&gt;The core issue is that it does not narrate from the desired POV, and continues to refer to the subject as &amp;quot;the speaker&amp;quot;, and that it keeps summarizing in bullet points. &lt;/p&gt; &lt;p&gt;Here are things I have tried: &lt;/p&gt; &lt;p&gt;&lt;code&gt;the following is a diary entry written by me. summarize it in 200 words. No bullet points. First person narration:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Assume you wrote the following text, but now need to make it more concise, 200 words or less.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Trying to assign a role, as suggested by other posts:&lt;/p&gt; &lt;p&gt;&lt;code&gt;You are an expert psychologist who needs to summarize your own diary entry in 200 words. This is the entry:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Turn the following text into a diary entry of max 200 words using &amp;quot;I&amp;quot; language/point of view:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Turn the following text into a diary entry narrated from YOUR perspective. Max 200 words:&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brian-the-porpoise"&gt; /u/brian-the-porpoise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohaj5/correct_my_prompts_for_text_summaries_llama_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohaj5/correct_my_prompts_for_text_summaries_llama_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohaj5/correct_my_prompts_for_text_summaries_llama_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T11:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioho0e</id>
    <title>stripping unecessary comments or anotations from results</title>
    <updated>2025-02-13T12:10:36+00:00</updated>
    <author>
      <name>/u/ben74940x</name>
      <uri>https://old.reddit.com/user/ben74940x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, given theses prompts for llava:7b I'm stuck with having various responses with comments, introductions, anotations, and repetition of the prompt itself, and I wanna get rid of theses in order to exploit raw responses ...&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;- Give the 20 main keywords without introduction in a raw response, separated by commas with no special characters, without any comment or anotations&lt;/p&gt; &lt;p&gt;- Describe me this image without comments or anotations for someone who cant see&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;=&amp;gt; where I get : &lt;/p&gt; &lt;p&gt;&lt;code&gt;- here's a list of keywords&lt;/code&gt;&lt;br /&gt; &lt;code&gt;- sure, here's the description ...&lt;/code&gt; &lt;/p&gt; &lt;p&gt;I'm also facing the same problem with llama3.2:3b on translation tasks:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;-Traduis moi ce texte en francais sans commentaire ni anotations: the quick fox jumps over the white rabbit&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;=&amp;gt; Where i get most of the time&lt;/p&gt; &lt;pre&gt;&lt;code&gt;- Ceci est une liste de mots - un texte à traduire. Je vais donc simplement traduire les mots : - Voici la traduction en français : - Remarque : Je suis désolé, mais je n'ai pas trouvé de traduction parfaite &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;My question is hence simple, how can I get rid of these ?&lt;/p&gt; &lt;p&gt;Thanks in advance for any clue, notice, comment, enlightenment ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ben74940x"&gt; /u/ben74940x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioho0e/stripping_unecessary_comments_or_anotations_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioho0e/stripping_unecessary_comments_or_anotations_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioho0e/stripping_unecessary_comments_or_anotations_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T12:10:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1io8qe0</id>
    <title>AceInstruct 1.5B / 7B / 72B by Nvidia</title>
    <updated>2025-02-13T02:24:12+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt; &lt;img alt="AceInstruct 1.5B / 7B / 72B by Nvidia" src="https://external-preview.redd.it/AW9WUUjiULOHbAfYY66Sx6D3OmGPFlGm47TagKzBqgo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94eb08024b4ddeaf5f136dca632fc922d506f5fb" title="AceInstruct 1.5B / 7B / 72B by Nvidia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-1.5B"&gt;https://huggingface.co/nvidia/AceInstruct-1.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-7B"&gt;https://huggingface.co/nvidia/AceInstruct-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/AceInstruct-72B"&gt;https://huggingface.co/nvidia/AceInstruct-72B&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We introduce AceInstruct, a family of advanced SFT models for coding, mathematics, and general-purpose tasks. The AceInstruct family, which includes AceInstruct-1.5B, 7B, and 72B, is &lt;strong&gt;Improved using Qwen&lt;/strong&gt;. These models are fine-tuned on Qwen2.5-Base using &lt;a href="https://huggingface.co/datasets/nvidia/AceMath-Instruct-Training-Data"&gt;general SFT datasets&lt;/a&gt;. These same datasets are also used in the training of &lt;a href="https://huggingface.co/nvidia/AceMath-72B-Instruct"&gt;AceMath-Instruct&lt;/a&gt;. Different from AceMath-Instruct which is specialized for math questions, AceInstruct is versatile and can be applied to a wide range of domains. Benchmark evaluations across coding, mathematics, and general knowledge tasks demonstrate that AceInstruct delivers performance comparable to Qwen2.5-Instruct.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5v30ob7mgtie1.png?width=708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c419909e48136207192ee44705b79c037068d73"&gt;https://preview.redd.it/5v30ob7mgtie1.png?width=708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c419909e48136207192ee44705b79c037068d73&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bruh, from 1.5b to 7b and then straight up to 72b, it's the same disappointing release strategy as Meta Llama. I guess I'll keep using Qwen 2.5 32b until Qwen 3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io8qe0/aceinstruct_15b_7b_72b_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T02:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioj04s</id>
    <title>Implementing a Forward Pass of DeepSeek-R1-Distill-Qwen-1.5B in Pure Python?</title>
    <updated>2025-02-13T13:27:17+00:00</updated>
    <author>
      <name>/u/Dxbson</name>
      <uri>https://old.reddit.com/user/Dxbson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm a developer with plenty of coding experience, but completely new to deep learning and machine learning. To better understand how LLMs work, I thought what better way than to implement a forward pass from scratch in pure Python—no dependencies, just standard Python libraries. I know this won’t be efficient, but speed isn’t my goal. I just want to grasp the inner workings of transformers through hands-on implementation.&lt;/p&gt; &lt;p&gt;I got this idea after discovering &lt;a href="https://github.com/karpathy/llama2.c/tree/master"&gt;Andrej Karpathy's llama2.c&lt;/a&gt;, which is an inference implementation for Llama-2 in pure, which I'm sure you guys are familiar with around here. In that repo, he manually implements the matrix multiplications, softmax, tokenization, sampling, decoding etc. I’d love to do the same but in Python (C might come later), and I figured a good model to try this on would be &lt;strong&gt;DeepSeek-R1-Distill-Qwen-1.5B&lt;/strong&gt; since it’s a distilled model and more manageable than larger LLMs.&lt;/p&gt; &lt;p&gt;I found &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/tree/main"&gt;this Hugging Face repo&lt;/a&gt;, which contains config files, tokenizer info and safetensors, but I’m struggling to determine the actual architecture of the model. Should I be referencing &lt;strong&gt;DeepSeek-V3&lt;/strong&gt; or &lt;strong&gt;Qwen-1.5B&lt;/strong&gt;? Where can I find the exact model architecture, including layer details, attention mechanisms?&lt;/p&gt; &lt;p&gt;I know they have some demo code in the &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/tree/main/inference"&gt;DeepSeek-V3 repo&lt;/a&gt;, but I'm unsure if I can utilize this same architecture here on this distilled model. They do say &amp;quot;&lt;em&gt;DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1. We slightly change their configs and tokenizers. Please use our setting to run these models.&lt;/em&gt;, but I'm not really sure what to make of this, and which setting they are referring to. &lt;/p&gt; &lt;p&gt;If anyone has pointers on where to start (e.g, resources on interpreting these config files, implementing a transformer forward pass from scratch, or whether I should be using Qwen’s or DeepSeek’s model architecture), I’d really appreciate it :)&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dxbson"&gt; /u/Dxbson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj04s/implementing_a_forward_pass_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj04s/implementing_a_forward_pass_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj04s/implementing_a_forward_pass_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:27:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2vq5</id>
    <title>Promptable object tracking robots with Moondream VLM &amp; OpenCV Optical Flow (open source)</title>
    <updated>2025-02-12T21:53:10+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"&gt; &lt;img alt="Promptable object tracking robots with Moondream VLM &amp;amp; OpenCV Optical Flow (open source)" src="https://external-preview.redd.it/N2xjdjR4MG80c2llMTEDy-zmwY-2zxEHn6L-Fnq1X838PMp4mnmxIFCi0bu_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f82c0e1ecceb7465617120ea97715cdb5a48e9" title="Promptable object tracking robots with Moondream VLM &amp;amp; OpenCV Optical Flow (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/z5buym0o4sie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2vq5/promptable_object_tracking_robots_with_moondream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohbij</id>
    <title>I built a knowledge management system that enables you to connect knowledge to any RAG</title>
    <updated>2025-02-13T11:48:33+00:00</updated>
    <author>
      <name>/u/Outside-Project-1451</name>
      <uri>https://old.reddit.com/user/Outside-Project-1451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm excited to introduce Simba – an open-source solution I developed to simplify managing and leveraging knowledge in Retrieval-Augmented Generation (RAG) systems.&lt;/p&gt; &lt;p&gt;In simple terms, Simba enables you to structure and connect a knowledge base (Word, PDF, PowerPoint documents, etc.) to any chatbot.&lt;/p&gt; &lt;p&gt;🔍 Why Simba?&lt;/p&gt; &lt;p&gt;While working on AI projects, I frequently encountered challenges such as:&lt;/p&gt; &lt;p&gt;📂 Handling long, complex documents (including tables, images, multiple sections…)&lt;/p&gt; &lt;p&gt;🔎 Indexing and structuring information for effective retrieval&lt;/p&gt; &lt;p&gt;🛠️ Controlling the sources that a chatbot uses&lt;/p&gt; &lt;p&gt;Simba addresses these issues with:&lt;/p&gt; &lt;p&gt;✅ Advanced parsing that automatically structures documents using state-of-the-art algorithms&lt;/p&gt; &lt;p&gt;✅ An intuitive interface to visualize, modify, and organize data chunks&lt;/p&gt; &lt;p&gt;✅ Precise knowledge control to include or exclude sources as needed&lt;/p&gt; &lt;p&gt;✅ A flexible architecture allowing you to choose your LLMs, vector databases, chunking strategies, and parsers&lt;/p&gt; &lt;p&gt;📌 When to Use Simba?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For long and complex documents (tables, images, multiple sections…)&lt;/li&gt; &lt;li&gt;When you need granular control over which sources are included during conversations&lt;/li&gt; &lt;li&gt;When managing data access is critical (permissions and roles – a feature coming soon)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🎯 Who Is Simba For?&lt;/p&gt; &lt;p&gt;Simba is crafted for developers aiming to integrate a structured knowledge base into their RAG systems.&lt;/p&gt; &lt;p&gt;🛠️ Although the project is still evolving and doesn’t yet cover every planned feature, it’s on track to become a powerful tool for the community.&lt;/p&gt; &lt;p&gt;💡 Feedback Is a Gift!&lt;/p&gt; &lt;p&gt;The magic of open source lies in collaboration. If you encounter bugs, unclear areas, or simply have suggestions, please share your feedback. You can propose improvements, bug fixes, or new features directly on GitHub.&lt;/p&gt; &lt;p&gt;Check out the repository here: &lt;a href="https://github.com/GitHamza0206/simba"&gt;https://github.com/GitHamza0206/simba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;⭐ Simba is nearing 100 stars on GitHub, and the goal is to reach 1000 stars within the next 2 months! If you appreciate the project, please give it a star ⭐ – your support means a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Project-1451"&gt; /u/Outside-Project-1451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohbij/i_built_a_knowledge_management_system_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T11:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoui5</id>
    <title>AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory</title>
    <updated>2025-02-12T11:36:29+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt; &lt;img alt="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" src="https://external-preview.redd.it/qxSKCWeduksNqEDRWvwQaww7R41JuTdE_uY1z8NDX_M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b97591e394a959b1d54b453c3148692e6cab6ca" title="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-reportedly-working-on-gaming-radeon-rx-9000-gpu-with-32gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:36:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofdch</id>
    <title>VRAM Requirements for Training a 70B Model with GRPO &amp; ZeRO-3?</title>
    <updated>2025-02-13T09:28:43+00:00</updated>
    <author>
      <name>/u/thanhdouwu</name>
      <uri>https://old.reddit.com/user/thanhdouwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to estimate VRAM usage when training a 70B parameter model with GRPO. If I use ZeRO-3, set the context length to 8k or 16k, and use a rule-based reward model, how much VRAM would I need?&lt;/p&gt; &lt;p&gt;Additionally, if you've trained with LoRA or different model sizes, I'd love to hear about your experience—VRAM consumption, setup details, and any optimizations you found helpful. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thanhdouwu"&gt; /u/thanhdouwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iofdch/vram_requirements_for_training_a_70b_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4x5c</id>
    <title>OpenThinker-32B &amp; 7B</title>
    <updated>2025-02-12T23:21:11+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-32B"&gt;https://huggingface.co/open-thoughts/OpenThinker-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-7B"&gt;https://huggingface.co/open-thoughts/OpenThinker-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io4x5c/openthinker32b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:21:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioi4lm</id>
    <title>Benchmark Paper: Vision-Language Models vs Traditional OCR in Videos</title>
    <updated>2025-02-13T12:38:18+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new benchmark paper just dropped evaluating how well &lt;strong&gt;Vision-Language Models (VLMs)&lt;/strong&gt; perform compared to &lt;strong&gt;traditional OCR&lt;/strong&gt; tools in dynamic video environments. &lt;/p&gt; &lt;p&gt;The study, led by the team at &lt;strong&gt;VideoDB&lt;/strong&gt;, introduces a &lt;strong&gt;curated dataset of 1,477 manually annotated frames&lt;/strong&gt; spanning diverse domains—code editors, news broadcasts, YouTube videos, and advertisements.&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;Read the paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2502.06445"&gt;https://arxiv.org/abs/2502.06445&lt;/a&gt;&lt;br /&gt; 🔗 &lt;strong&gt;Explore the dataset &amp;amp; repo&lt;/strong&gt;: &lt;a href="https://github.com/video-db/ocr-benchmark"&gt;https://github.com/video-db/ocr-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Three state-of-the-art VLMs – Claude-3, Gemini-1.5, and GPT-4o – were benchmarked against traditional OCR tools like EasyOCR and RapidOCR, using metrics such as Word Error Rate (WER), Character Error Rate (CER), and Accuracy.&lt;/p&gt; &lt;h1&gt;🔍 Key Findings:&lt;/h1&gt; &lt;p&gt;✅ &lt;strong&gt;VLMs outperformed&lt;/strong&gt; traditional OCR in many cases, demonstrating robustness across varied video contexts.&lt;br /&gt; ⚠️ &lt;strong&gt;Challenges persist&lt;/strong&gt; – hallucinated text, security policy triggers, and difficulty with occluded/stylized text.&lt;br /&gt; 📂 &lt;strong&gt;Public Dataset Available&lt;/strong&gt; – The full dataset and benchmarking framework are open for research &amp;amp; collaboration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioi4lm/benchmark_paper_visionlanguage_models_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioi4lm/benchmark_paper_visionlanguage_models_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioi4lm/benchmark_paper_visionlanguage_models_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T12:38:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1io655d</id>
    <title>The endgame of Tool-Use, toolmaking</title>
    <updated>2025-02-13T00:17:33+00:00</updated>
    <author>
      <name>/u/fractalcrust</name>
      <uri>https://old.reddit.com/user/fractalcrust</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stop making bespoke tools for every usecase. What we need is a tool-making tool, enabling LLMs to create their own tools to solve their tasks. Nothing could possibly go wrong and I'm 100% comfortable leaving my LLM unsupervised &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fractalcrust"&gt; /u/fractalcrust &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io655d/the_endgame_of_tooluse_toolmaking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T00:17:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1io9lfc</id>
    <title>DeepSeek Distilled Qwen 1.5B on NPU for Windows on Snapdragon</title>
    <updated>2025-02-13T03:09:14+00:00</updated>
    <author>
      <name>/u/SkyFeistyLlama8</name>
      <uri>https://old.reddit.com/user/SkyFeistyLlama8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released a Qwen 1.5B DeepSeek Distilled local model that targets the Hexagon NPU on Snapdragon X Plus/Elite laptops. Finally, we have an LLM that officially runs on the NPU for prompt eval (inference runs on CPU). &lt;/p&gt; &lt;p&gt;To run it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;run VS Code under Windows on ARM&lt;/li&gt; &lt;li&gt;download the AI Toolkit extension&lt;/li&gt; &lt;li&gt;Ctrl-Shift-P to load the command palette, type &amp;quot;Load Model Catalog&amp;quot;&lt;/li&gt; &lt;li&gt;scroll down to the DeepSeek (NPU Optimized) card, click +Add. The extension then downloads a bunch of ONNX files.&lt;/li&gt; &lt;li&gt;to run inference, Ctrl-Shift-P to load the command palette, then type &amp;quot;Focus on my models view&amp;quot; to load, then have fun in the chat playground&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Task Manager shows NPU usage at 50% and CPU at 25% during inference so it's working as intended. Larger Qwen and Llama models are coming so we finally have multiple performant inference stacks on Snapdragon.&lt;/p&gt; &lt;p&gt;The actual executable is in the &amp;quot;ai-studio&amp;quot; directory under VS Code's extensions directory. There's an ONNX runtime .exe along with a bunch of QnnHtp DLLs. It might be interesting to code up a PowerShell workflow for this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyFeistyLlama8"&gt; /u/SkyFeistyLlama8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io9lfc/deepseek_distilled_qwen_15b_on_npu_for_windows_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T03:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioj9ly</id>
    <title>Hugging Face just open sourced the free agents course!</title>
    <updated>2025-02-13T13:40:53+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj9ly/hugging_face_just_open_sourced_the_free_agents/"&gt; &lt;img alt="Hugging Face just open sourced the free agents course!" src="https://external-preview.redd.it/dZG5o7Z-X0P3aHLdfTQ585OGGBpbWn7hxgShkrQ_wfw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c62c131ea4a762f597958b6a4288a1baf7a8d965" title="Hugging Face just open sourced the free agents course!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/agents-course"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj9ly/hugging_face_just_open_sourced_the_free_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioj9ly/hugging_face_just_open_sourced_the_free_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1io811j</id>
    <title>Who builds PCs that can handle 70B local LLMs?</title>
    <updated>2025-02-13T01:48:52+00:00</updated>
    <author>
      <name>/u/Moist-Mongoose4467</name>
      <uri>https://old.reddit.com/user/Moist-Mongoose4467</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are only a few videos on YouTube that show folks buying old server hardware and cobbling together affordable PCs with a bunch of cores, RAM, and CPU RAM. Is there a company or person that does that for a living (or side hustle)? I don't have $10,000 to $50,000 for a home server with multiple high-end GPUs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist-Mongoose4467"&gt; /u/Moist-Mongoose4467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io811j/who_builds_pcs_that_can_handle_70b_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T01:48:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioclzg</id>
    <title>InternVideo2.5 released！Has anyone tried it out? How well does it perform?</title>
    <updated>2025-02-13T06:04:38+00:00</updated>
    <author>
      <name>/u/vansinhu</name>
      <uri>https://old.reddit.com/user/vansinhu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt; &lt;img alt="InternVideo2.5 released！Has anyone tried it out? How well does it perform?" src="https://external-preview.redd.it/Y7gp2ezADJTiI3oUU3P5TMgIEsAjig-29MVwWQpiG_c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de07f828e6653fd6667798fac5614b252c311381" title="InternVideo2.5 released！Has anyone tried it out? How well does it perform?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/47l93seekuie1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5b0206424f31cc8412405cdc02e780ce5763b9b"&gt;https://preview.redd.it/47l93seekuie1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5b0206424f31cc8412405cdc02e780ce5763b9b&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles videos 6x longer than predecessors&lt;/li&gt; &lt;li&gt;Pinpoints objects/actions with surgical precision&lt;/li&gt; &lt;li&gt;Trained on 300K+ hours of diverse video data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Outperforms SOTA on multiple benchmarks &amp;amp; unlocks possibilities for Autonomous Driving, VR, and more!Code: &lt;a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5"&gt;https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.12386"&gt;https://arxiv.org/abs/2501.12386&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B"&gt;https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xieqwfmhkuie1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c9e39c7d538478ba3387fa5077d06c0017df073"&gt;https://preview.redd.it/xieqwfmhkuie1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c9e39c7d538478ba3387fa5077d06c0017df073&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vansinhu"&gt; /u/vansinhu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioclzg/internvideo25_releasedhas_anyone_tried_it_out_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T06:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iofe4w</id>
    <title>[update] aiaio: simple, lightweight ui with more features now</title>
    <updated>2025-02-13T09:30:25+00:00</updated>
    <author>
      <name>/u/abhi1thakur</name>
      <uri>https://old.reddit.com/user/abhi1thakur</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt; &lt;img alt="[update] aiaio: simple, lightweight ui with more features now" src="https://external-preview.redd.it/eWJ1dWZtaTlsdmllMTTMNvywGLfHKtiMdeeDDuKKJ-xtwCq_lpvrE6nUhuq6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f5f1683ed07dbea3c137ac3ecb29bfaf68079ce" title="[update] aiaio: simple, lightweight ui with more features now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abhi1thakur"&gt; /u/abhi1thakur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/1bduxmi9lvie1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iofe4w/update_aiaio_simple_lightweight_ui_with_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iof0r2</id>
    <title>When it comes to fine-tuning LLMs, the training dataset isn’t just a factor—it’s the kingmaker.</title>
    <updated>2025-02-13T09:01:31+00:00</updated>
    <author>
      <name>/u/Excellent_Delay_3701</name>
      <uri>https://old.reddit.com/user/Excellent_Delay_3701</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let’s take a look at the current SOTA models—Llama 3.x, DeepSeek, Mistral, and others. (Don't forget I am talking about fine-tune for specific tasks, not pre-train)&lt;/p&gt; &lt;p&gt;The real kingmaker for top performance? A meticulously cleaned, balanced, and well-structured dataset. Even if a “perfect” dataset doesn’t exist, getting as close as possible makes all the difference.&lt;/p&gt; &lt;p&gt;Sure, training variables and hyperparameters impact an LLM’s performance. But in the end, isn’t the dataset everything?&lt;/p&gt; &lt;p&gt;If you’re fine-tuning an LLM or SLM for a specific task and not seeing the results you want after a few iterations, the first place you should look is the dataset. &lt;/p&gt; &lt;p&gt;How many of you changes model architectures, apply something new?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Delay_3701"&gt; /u/Excellent_Delay_3701 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iof0r2/when_it_comes_to_finetuning_llms_the_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T09:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1io4s4s</id>
    <title>This paper might be a breakthrough Google doesn't know they have</title>
    <updated>2025-02-12T23:14:52+00:00</updated>
    <author>
      <name>/u/Ok-Possibility-5586</name>
      <uri>https://old.reddit.com/user/Ok-Possibility-5586</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.03824"&gt;2105.03824&lt;/a&gt;&lt;/p&gt; &lt;p&gt;FNet: Mixing Tokens with Fourier Transforms&lt;/p&gt; &lt;p&gt;^^^ this paper is from 2022 before LLMs blew up in the public imagination.&lt;/p&gt; &lt;p&gt;If someone is able to replicate this, maybe by training a smaller model and cutting out the layers and splicing into a bigger model (or something else, I'm winging it here) then maybe we get some big speedups. According to the paper (from Google) it's looking at a 90% speedup and memory reduction.&lt;/p&gt; &lt;p&gt;&lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; have you seen this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Possibility-5586"&gt; /u/Ok-Possibility-5586 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io4s4s/this_paper_might_be_a_breakthrough_google_doesnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1io3hn2</id>
    <title>NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models.</title>
    <updated>2025-02-12T22:19:00+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt; &lt;img alt="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." src="https://preview.redd.it/95ysyjzs8sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=846630231480ed6a71d97aeaed4938ab9b5cc355" title="NoLiMa: Long-Context Evaluation Beyond Literal Matching - Finally a good benchmark that shows just how bad LLM performance is at long context. Massive drop at just 32k context for all models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95ysyjzs8sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io3hn2/nolima_longcontext_evaluation_beyond_literal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T22:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1io5o9a</id>
    <title>How do LLMs actually do this?</title>
    <updated>2025-02-12T23:56:05+00:00</updated>
    <author>
      <name>/u/No-Conference-8133</name>
      <uri>https://old.reddit.com/user/No-Conference-8133</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt; &lt;img alt="How do LLMs actually do this?" src="https://preview.redd.it/m6rfcv5tqsie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e3e2e816211a62c38e8c3c60368cca7c8d38d4" title="How do LLMs actually do this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The LLM can’t actually see or look close. It can’t zoom in the picture and count the fingers carefully or slower.&lt;/p&gt; &lt;p&gt;My guess is that when I say &amp;quot;look very close&amp;quot; it just adds a finger and assumes a different answer. Because LLMs are all about matching patterns. When I tell someone to look very close, the answer usually changes.&lt;/p&gt; &lt;p&gt;Is this accurate or am I totally off?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Conference-8133"&gt; /u/No-Conference-8133 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m6rfcv5tqsie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io5o9a/how_do_llms_actually_do_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T23:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ioikl0</id>
    <title>Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445</title>
    <updated>2025-02-13T13:03:22+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"&gt; &lt;img alt="Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445" src="https://preview.redd.it/8u7jixwzmwie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f7ab3742f3fd3c2245bf8eadfbaad2fecacd6ac" title="Gemini beats everyone is OCR benchmarking tasks in videos. Full Paper : https://arxiv.org/abs/2502.06445" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8u7jixwzmwie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ioikl0/gemini_beats_everyone_is_ocr_benchmarking_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T13:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iohk4o</id>
    <title>Let's build DeepSeek from Scratch | Taught by MIT PhD graduate</title>
    <updated>2025-02-13T12:03:45+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt; &lt;img alt="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" src="https://external-preview.redd.it/pAa68GpmjnpZeahm_YMGQkYTs9KtW9HemhGbAYHU02s.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555355166a247eb92939344c89b96ed48dd7655a" title="Let's build DeepSeek from Scratch | Taught by MIT PhD graduate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/vjwhw6ticwie1.gif"&gt;https://i.redd.it/vjwhw6ticwie1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join us for the 6pm Youtube premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ever since DeepSeek was launched, everyone is focused on: &lt;/p&gt; &lt;p&gt;- Flashy headlines&lt;/p&gt; &lt;p&gt;- Company wars&lt;/p&gt; &lt;p&gt;- Building LLM applications powered by DeepSeek&lt;/p&gt; &lt;p&gt;I very strongly think that students, researchers, engineers and working professionals should focus on the foundations. &lt;/p&gt; &lt;p&gt;The real question we should ask ourselves is: &lt;/p&gt; &lt;p&gt;“Can I build the DeepSeek architecture and model myself, from scratch?”&lt;/p&gt; &lt;p&gt;If you ask this question, you will discover that to make DeepSeek work, there are a number of key ingredients which play a role:&lt;/p&gt; &lt;p&gt;(1) Mixture of Experts (MoE)&lt;/p&gt; &lt;p&gt;(2) Multi-head Latent Attention (MLA)&lt;/p&gt; &lt;p&gt;(3) Rotary Positional Encodings (RoPE)&lt;/p&gt; &lt;p&gt;(4) Multi-token prediction (MTP)&lt;/p&gt; &lt;p&gt;(5) Supervised Fine-Tuning (SFT)&lt;/p&gt; &lt;p&gt;(6) Group Relative Policy Optimisation (GRPO)&lt;/p&gt; &lt;p&gt;My aim with the “Build DeepSeek from Scratch” playlist is: &lt;/p&gt; &lt;p&gt;- To teach you the mathematical foundations behind all the 6 ingredients above.&lt;/p&gt; &lt;p&gt;- To code all 6 ingredients above, from scratch.&lt;/p&gt; &lt;p&gt;- To assemble these ingredients and to run a “mini Deep-Seek” on your own.&lt;/p&gt; &lt;p&gt;After this, you will among the top 0.1%. of ML/LLM engineers who can build DeepSeek ingredients on their own.&lt;/p&gt; &lt;p&gt;This playlist won’t be a 1 hour or 2 hour video. This will be a mega playlist of 35-40 videos with a duration of 40+ hours. &lt;/p&gt; &lt;p&gt;It will be in-depth. No fluff. Solid content. &lt;/p&gt; &lt;p&gt;Join us for the 6pm premier here: &lt;a href="https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ"&gt;https://youtu.be/QWNxQIq0hMo?si=YVHJtgMRjlVj2SZJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S: Attached is a small GIF showing the notes we have made. This is just 5-10% of the total amount of notes and material we have prepared for this series!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iohk4o/lets_build_deepseek_from_scratch_taught_by_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-13T12:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1io2ija</id>
    <title>Is Mistral's Le Chat truly the FASTEST?</title>
    <updated>2025-02-12T21:37:41+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt; &lt;img alt="Is Mistral's Le Chat truly the FASTEST?" src="https://preview.redd.it/zk2uyy142sie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abb4eab5a990f54584b5bb28366386e39bb58419" title="Is Mistral's Le Chat truly the FASTEST?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zk2uyy142sie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1io2ija/is_mistrals_le_chat_truly_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T21:37:41+00:00</published>
  </entry>
</feed>
