<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-21T09:42:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m57iep</id>
    <title>Pseudo RAID and Kimi-K2</title>
    <updated>2025-07-21T02:45:13+00:00</updated>
    <author>
      <name>/u/Defiant_Diet9085</name>
      <uri>https://old.reddit.com/user/Defiant_Diet9085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have Threadripper 2970WX uses a PCI-Express Gen 3&lt;/p&gt; &lt;p&gt;256GB DDR4 + 5090&lt;/p&gt; &lt;p&gt;I ran Kimi-K2-Instruct-UD-Q2_K_XL (354.9GB) and got 2t/sec&lt;/p&gt; &lt;p&gt;I have 4 SSD drives. I made symbolic links. I put 2 files on each drive and got 2.3t/sec&lt;/p&gt; &lt;p&gt;cheers! =)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Defiant_Diet9085"&gt; /u/Defiant_Diet9085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m57iep/pseudo_raid_and_kimik2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m57iep/pseudo_raid_and_kimik2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m57iep/pseudo_raid_and_kimik2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T02:45:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4s9nn</id>
    <title>Chess Llama - Training a tiny Llama model to play chess</title>
    <updated>2025-07-20T15:51:13+00:00</updated>
    <author>
      <name>/u/LazyGuy-_-</name>
      <uri>https://old.reddit.com/user/LazyGuy-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/"&gt; &lt;img alt="Chess Llama - Training a tiny Llama model to play chess" src="https://external-preview.redd.it/EkWIWtbbH4xlzhYZBKN8etSvG-CzFTB0R2srsCELe50.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=295d14462b9edc402d72b6436f8270d15502e48a" title="Chess Llama - Training a tiny Llama model to play chess" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LazyGuy-_-"&gt; /u/LazyGuy-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lazy-guy.github.io/blog/chessllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9nn/chess_llama_training_a_tiny_llama_model_to_play/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:51:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4o37k</id>
    <title>MediPhi-Instruct</title>
    <updated>2025-07-20T12:47:51+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"&gt; &lt;img alt="MediPhi-Instruct" src="https://external-preview.redd.it/rMf-9D6Y4sgIJc1XvCzarHFTmb103pT_gKU9N1hAZmg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9841ead70ffdc17a5775d37d5326e57acfc45ef" title="MediPhi-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/MediPhi-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4o37k/mediphiinstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T12:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m58ohn</id>
    <title>Model to retrieve information from Knowledge.</title>
    <updated>2025-07-21T03:45:53+00:00</updated>
    <author>
      <name>/u/themungbeans</name>
      <uri>https://old.reddit.com/user/themungbeans</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently using Ollama with OpenWebUI on a dedicated PC. This has a Intel Xeon E5v2, 32gb Ram and 2x Titan V 12GB (have a third on its way). Limited budget and this is roughly what I have to play with right now.&lt;/p&gt; &lt;p&gt;I was wanting to add about 20-30 pdf documents to a knowledge base. I would then have an LLM to find and provide resources from that information.&lt;/p&gt; &lt;p&gt;I have been experimenting with a few different models but am seeking advice as I have not found an ideal solution.&lt;/p&gt; &lt;p&gt;My main goal was to be able to use an LLM, was initially thing a &lt;/p&gt; &lt;p&gt;Vision models (Gemma &amp;amp; Qwen2.5VL) worked well at retrieving information but not very intelligent at following instructions. Possibly because they were quite small (7b &amp;amp; 12b). The larger vision models (27b &amp;amp; 32b) were fitting into VRAM with 2GB-6GB free. Small images etc were handled fast and accurate. Larger images (full desktop screenshots) started ignoring GPU space and I noticed near 100% load on all 20 CPU threads.&lt;/p&gt; &lt;p&gt;I thought maybe a more traditional text only model with only text based PDF's as knowledge might be worth a shot. I then used faster non reasoning model (Phi4 14B &amp;amp; Qwen 2.5 Coder 14B). These were great and accurate but were not able to understand the images in the documents.&lt;/p&gt; &lt;p&gt;Am I going about this wrong?&lt;/p&gt; &lt;p&gt;I thought uploading the documents to &amp;quot;Knowledge&amp;quot; was RAG. This is configured as default and no changes. It seems too quick so I dont think it is.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themungbeans"&gt; /u/themungbeans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m58ohn/model_to_retrieve_information_from_knowledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m58ohn/model_to_retrieve_information_from_knowledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m58ohn/model_to_retrieve_information_from_knowledge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T03:45:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5586n</id>
    <title>Best novel writing workflow?</title>
    <updated>2025-07-21T00:54:02+00:00</updated>
    <author>
      <name>/u/AccidentalFolklore</name>
      <uri>https://old.reddit.com/user/AccidentalFolklore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m writing a novel that’s near-future literary fiction / soft dystopia / psychological tragedy with erotic elements. I’m subscribed to ChatGPT and Claude, but built a PC to move to local AI without limits and guardrails for the NSFW stuff. &lt;/p&gt; &lt;p&gt;What’s the best workflow for me? I downloaded Oobabooga and a MythosMax model, but not really sure how to add in context and instructions. There are pre populated templates and I don’t understand if I’m supposed to work within those or overwrite them. Also not sure if these were the best choices so appreciate any recommendations. &lt;/p&gt; &lt;p&gt;Want something that’s really good for my genre, especially dark/gritty/nsfw with lyrical prose and stream of consciousness style. &lt;/p&gt; &lt;p&gt;My hardware: - CPU: Ryzen 7950x - GPU: 3090 - RAM: 96GB 6400mhz &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccidentalFolklore"&gt; /u/AccidentalFolklore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T00:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5d2cv</id>
    <title>GGUF on Android Studio</title>
    <updated>2025-07-21T08:07:00+00:00</updated>
    <author>
      <name>/u/neural-learner</name>
      <uri>https://old.reddit.com/user/neural-learner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there way to run the GGUF files on Android Studio? Maybe with llama.cpp? I have been trying to build a wrapper around llama.cpp with Kotlin+Java but there must be a better solution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neural-learner"&gt; /u/neural-learner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5d2cv/gguf_on_android_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5d2cv/gguf_on_android_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5d2cv/gguf_on_android_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T08:07:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5dq1e</id>
    <title>How does LLMs get more creative?</title>
    <updated>2025-07-21T08:49:29+00:00</updated>
    <author>
      <name>/u/ba2sYd</name>
      <uri>https://old.reddit.com/user/ba2sYd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, Kimi K2 is out, and it's currently topping benchmarks in creative writing. I was wondering,how exactly do LLMs become more creative? From what I know, Kimi K2 uses DeepSeek's architecture but with more experts. So is improving creative writing mostly about scaling the model (more parameters, more experts) and not really about architecture, or is it more about the kind, size and quality of training data? Also, do companies even prioritize creativity? It feels like most of them is focusing on improving math, coding, and benchmark scores in these days, not on storytelling, nuance, or imagination. and I was wondering if there is any a proper benchmark for evaluating creativity? As I know models are ranked using human votes or scored by any other LLM, but how can we meaningfully compare creative performance without testing them directly? Lastly, are there any emerging architectures, like Liquid Foundation or Mamba, that seem especially promising for improving creativity in language models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ba2sYd"&gt; /u/ba2sYd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5dq1e/how_does_llms_get_more_creative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5dq1e/how_does_llms_get_more_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5dq1e/how_does_llms_get_more_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T08:49:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4u7j6</id>
    <title>What's the most crackhead garbage local LLM setup you can think of?</title>
    <updated>2025-07-20T17:08:13+00:00</updated>
    <author>
      <name>/u/caraccidentGAMING</name>
      <uri>https://old.reddit.com/user/caraccidentGAMING</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright so basically - I want to run qwen3 235b MoE. I dont wanna pay 235b MoE money tho. So far I've been eyeing grabbing an old dell xeon workstation, slapping in lots of RAM &amp;amp; two mi50 cards &amp;amp; calling it a day. Would that work? probably i guess, hell you'd even get good performance out of that running 32b models which do the job for most cases. but i want real crackhead technology. completely out of the box shit. the funnier in its sheer absurdity/cheaper/faster the better. let's hear what you guys can think of &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caraccidentGAMING"&gt; /u/caraccidentGAMING &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4u7j6/whats_the_most_crackhead_garbage_local_llm_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T17:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m58qf3</id>
    <title>First time using QLoRa results in gibberish</title>
    <updated>2025-07-21T03:48:51+00:00</updated>
    <author>
      <name>/u/Emotional-Sundae4075</name>
      <uri>https://old.reddit.com/user/Emotional-Sundae4075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to fine tune a LlaVa model, I have a training set of 7800 high quality conversations, each with an image. &lt;/p&gt; &lt;p&gt;I am using qlora to fine tune the model, and regardless of the batch size, the lr, and the rank, so far all of my trials were resulted in gibberish on evaluation. &lt;/p&gt; &lt;p&gt;I did some reading, and in order to avoid catastrophic forgetting, it says that we should limit our tuning of the lora model to three epochs max. In addition, I understand that the data size I have is allegedly enough. Together there is something that I am not sure about. The qlora model has about 10m weights (even without bias terms). It looks like much too many to be able to fit on my miniature data. &lt;/p&gt; &lt;p&gt;Any tips would be greatly appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Emotional-Sundae4075"&gt; /u/Emotional-Sundae4075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T03:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5ckr0</id>
    <title>ONNX or GGUF</title>
    <updated>2025-07-21T07:35:34+00:00</updated>
    <author>
      <name>/u/Xitizdumb</name>
      <uri>https://old.reddit.com/user/Xitizdumb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;am having a hard time with which one is good and why ???!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xitizdumb"&gt; /u/Xitizdumb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ckr0/onnx_or_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ckr0/onnx_or_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5ckr0/onnx_or_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T07:35:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4mfs8</id>
    <title>Next big thing after LLMs - World Model [explained on the example of V-JEPA2]</title>
    <updated>2025-07-20T11:17:11+00:00</updated>
    <author>
      <name>/u/VR-Person</name>
      <uri>https://old.reddit.com/user/VR-Person</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt; &lt;img alt="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" src="https://external-preview.redd.it/bzd6M2RyaWJqMGVmMazIDtYX-m4G2qSnSaRS5wvuc50lS7cqMTyw9S71POit.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d257e66a7ebc3ed1e883ca7dc0ba9dadc223c155" title="Next big thing after LLMs - World Model [explained on the example of V-JEPA2]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;&lt;sup&gt;#I'm starting a new series of explaining intriguing new AI papers&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;LLMs learn from text and lack an inherent understanding of the physical world. Their &amp;quot;knowledge&amp;quot; is &lt;strong&gt;mostly&lt;/strong&gt; limited to what's been described in the text they were trained on. This means they mostly struggle with concepts that are not easily described in words, like how objects move, interact, and deform over time. This is a form of &amp;quot;common sense&amp;quot; that is impossible to acquire from text alone.&lt;/p&gt; &lt;p&gt;During training, the goal of LLM is to predict the following word in a sentence, given the preceding words. By learning to generate the appropriate next word, grammar knowledge and semantics emerge in the model, as those abilities are necessary for understanding which word will follow in a sentence. &lt;/p&gt; &lt;p&gt;Why not to apply this self-supervised approach for teaching AI how life works via videos? &lt;/p&gt; &lt;p&gt;Take all the videos on the internet, randomly mask video-frames, and challenge the generating model to learn to accurately recover(reconstruct) the masked parts of the video-frames, so during training, the need of learning to predict what is happening in the masked parts of the videos, will develop the intuitive understanding of physics and in general how the world works. &lt;/p&gt; &lt;p&gt;But, for example, if in a video, a cup turns over, and we challenge the model to recover the masked part, the model should predict the precise location of each falling droplet, as the generative objective expects pixel-level precision. And because we are challenging the model to do the impossible, the learning process will just collapse.&lt;/p&gt; &lt;p&gt;Let's see how Meta approaches this issue &lt;a href="https://arxiv.org/pdf/2506.09985"&gt;https://arxiv.org/pdf/2506.09985&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Their new architecture, called V-JEPA 2, consists of an encoder and a predictor.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;encoder&lt;/strong&gt; takes in raw video-frames and outputs embeddings that capture useful semantic information about the state of the observed world.&lt;/p&gt; &lt;p&gt;In other words, it learns to extract the predictable aspects of a scene, for example, the approximate trajectory of the falling water, and does not get bogged down into the unpredictable, tiny details of every single pixel. So that the predictor learns to predict the high-level process that happens in the masked region of the video. &lt;em&gt;(see until 0:07 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;This helps the model to underpin a high-level understanding of how life works, which opens the possibility to finally train truly generally intelligent robots that don’t do impressive actions just for show in specific cases. So, in the post-training stage, they train on videos that show a robotic arm’s interaction.&lt;/p&gt; &lt;p&gt;This time, they encode part of a video and also give information about robot’s intended action in the last video-frame and train the model to predict what will happen at high-level in the following video-frames. &lt;em&gt;(see 0:08 to 0:16 in the video)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So, by predicting what will happen next, given the intended action, it learns to predict the consequences of actions.&lt;/p&gt; &lt;p&gt;After training, the robot, powered by this model, in the latent space can imagine the consequence of various chain-of-action scenarios to find a sequence of actions whose predicted outcome matches the desired outcome.&lt;/p&gt; &lt;p&gt;And for tasks requiring planning across multiple time scales, it needs to learn how to break down a high-level task into smaller steps, such as making food or loading a dishwasher. For that, the Meta team wants to train a hierarchical JEPA model that is capable of learning, reasoning, and planning across multiple temporal and spatial scales.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VR-Person"&gt; /u/VR-Person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h0ivgtibj0ef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4mfs8/next_big_thing_after_llms_world_model_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T11:17:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m568j8</id>
    <title>[2507.09850] The Challenge of Teaching Reasoning to LLMs Without RL or Distillation</title>
    <updated>2025-07-21T01:43:09+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;gt; Reasoning-capable language models achieve state-of-the-art performance in diverse complex tasks by generating long, explicit Chain-of-Thought (CoT) traces. While recent works show that base models can acquire such reasoning traces via reinforcement learning or distillation from stronger models like DeepSeek-R1, previous works demonstrate that even short CoT prompting without fine-tuning is able to improve reasoning. We ask whether long CoT can be induced in a base model using only prompting or minimal tuning. Using just 20 long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of high-quality examples can unlock strong reasoning capabilities. We further explore using CoT data from non-reasoning models and human annotators, enhanced with prompt engineering, multi-pass editing, and structural guidance. However, neither matches the performance of reasoning model traces, suggesting that certain latent qualities of expert CoT are difficult to replicate. We analyze key properties of reasoning data, such as problem difficulty, diversity, and answer length, that influence reasoning distillation. While challenges remain, we are optimistic that carefully curated human-written CoT, even in small quantities, can activate reasoning behaviors in base models. We release our human-authored dataset across refinement stages and invite further investigation into what makes small-scale reasoning supervision so effective.&lt;/p&gt; &lt;p&gt;tl;dr Human reasoning is different from LLM reasoning, and human reasoning can't be distilled into LLMs such that they significantly perform better on benchmarks compared to their foundational models. There seem to be certain structural patterns that lead to the emergence of reasoning abilities in LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.09850"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m568j8/250709850_the_challenge_of_teaching_reasoning_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m568j8/250709850_the_challenge_of_teaching_reasoning_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T01:43:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1m59xzv</id>
    <title>What makes a model ethical?</title>
    <updated>2025-07-21T04:54:59+00:00</updated>
    <author>
      <name>/u/KnownDairyAcolyte</name>
      <uri>https://old.reddit.com/user/KnownDairyAcolyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People have started throwing the terms ethical and ethics around with respect and I'm not sure how to read those terms. Is a more ethical model one which was trained using &amp;quot;less&amp;quot; electricity with something made on a raspberry pi approaching &amp;quot;peak&amp;quot; ethicalness? Are the inputs to a model more important? Less? How do both matter? Something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KnownDairyAcolyte"&gt; /u/KnownDairyAcolyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m59xzv/what_makes_a_model_ethical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m59xzv/what_makes_a_model_ethical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m59xzv/what_makes_a_model_ethical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T04:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4yo0g</id>
    <title>DiffRhythm 1.2 music generation model produces "Avicii vs Nicky Romero - I Could Be the One" nearly verbatim</title>
    <updated>2025-07-20T20:06:46+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/"&gt; &lt;img alt="DiffRhythm 1.2 music generation model produces &amp;quot;Avicii vs Nicky Romero - I Could Be the One&amp;quot; nearly verbatim" src="https://external-preview.redd.it/YTYwaWo1bmQ1M2VmMb_CBqNLMKq7B8TFWMuXRmq1kpK-35NHOdsHUCPSQtde.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc68db074fff49faf68ce5480e7b3265555e05bf" title="DiffRhythm 1.2 music generation model produces &amp;quot;Avicii vs Nicky Romero - I Could Be the One&amp;quot; nearly verbatim" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And this is how you get sued, lol. I noticed this while playing around with DiffRhythm; I had unrelated lyrics and an unrelated audio prompt set for the generation, and it still injected Avicii into the output, which was really funny.&lt;/p&gt; &lt;p&gt;Skip to 1:00 in the video to skip the generation process&lt;/p&gt; &lt;p&gt;Seed: 50518556518147&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ulng63nd53ef1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4yo0g/diffrhythm_12_music_generation_model_produces/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T20:06:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m55rrt</id>
    <title>How fast is gemma 3 27b on an H100? how many tokens per second can I expect?</title>
    <updated>2025-07-21T01:20:35+00:00</updated>
    <author>
      <name>/u/ThatIsNotIllegal</name>
      <uri>https://old.reddit.com/user/ThatIsNotIllegal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen people say 60/s and i've seen 22000/sec, I don't even know who to believe anymore.&lt;/p&gt; &lt;p&gt;Also how much does optimizing boost the tokens output speed? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThatIsNotIllegal"&gt; /u/ThatIsNotIllegal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T01:20:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4of82</id>
    <title>What's the smartest tiny LLM you've actually used?</title>
    <updated>2025-07-20T13:04:37+00:00</updated>
    <author>
      <name>/u/Luston03</name>
      <uri>https://old.reddit.com/user/Luston03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for something small but still usable. What's your go-to?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luston03"&gt; /u/Luston03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4of82/whats_the_smartest_tiny_llm_youve_actually_used/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T13:04:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4r4j1</id>
    <title>Open source is humanity’s last hope!</title>
    <updated>2025-07-20T15:04:05+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m just making this post as I want opinions on the idea that if open source doesn’t consistently stay within a reasonable margin of the smartest AI systems out there we will move into a world where government almost certainly as their unbeatable, informants and enforcers via AI and I personally see it as a almost guarantee of a dystopian future with a power gap between a individual empowered by the system and one not being insurmountable with strategy no longer being a factor via agi. I really just see it as if the government wants something. It happens. A lot of people view that as our reality today, but AGI has the potential to create a government that has a 0% chance of being overthrown or replaced if it became unjust. For this reason, I believe open source being the leader in intelligent AI rather than closed individuals or companies is the only way to not move into a reality where individuals reach power that can quite literally be compared to God’s from fiction. The risk of tyranny from centralized power is greater than the risk of chaos from distributed power so open source is the way forward or at least the best we have. What’s you take? It is not a magical solution that will solve all problems. However, it is the single most important counterweight we have. It fosters transparency, allows for independent safety research, prevents a single corporate or state actor from setting all the rules, and provides the tools for resistance and balance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4r4j1/open_source_is_humanitys_last_hope/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:04:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1m56z4m</id>
    <title>why are there quite different quant strategies of bartowski and unsloth on MoE?</title>
    <updated>2025-07-21T02:18:43+00:00</updated>
    <author>
      <name>/u/Remarkable-Pea645</name>
      <uri>https://old.reddit.com/user/Remarkable-Pea645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/"&gt; &lt;img alt="why are there quite different quant strategies of bartowski and unsloth on MoE?" src="https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dcb3e29b76660bae85af28320f25e9b8191473f" title="why are there quite different quant strategies of bartowski and unsloth on MoE?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF"&gt;https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF"&gt;https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;they are quant of a same model. at a same quant, e.g. both Q3_K_M, there are non-negligible count of blocks, which bartowski quantized as Q8_0, while unsloth Q3_K or Q4_K.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v3rjrmrbz4ef1.png?width=520&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=838059d64089a5e17092169c48e88ab90b8d92a9"&gt;this is a part. count 67 in total&lt;/a&gt;&lt;/p&gt; &lt;p&gt;btw, the unsloth Q3_K_XL is smaller than Q3_K_M. I am really curious on the flavor of unloth naming.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Pea645"&gt; /u/Remarkable-Pea645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T02:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5827d</id>
    <title>Which LLMs, tools, or research have been overlooked or deserve more attention?</title>
    <updated>2025-07-21T03:13:18+00:00</updated>
    <author>
      <name>/u/MDT-49</name>
      <uri>https://old.reddit.com/user/MDT-49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I feel like there have been a lot of new releases in the past few weeks after a relatively quiet period following the Qwen3 release.&lt;/p&gt; &lt;p&gt;Of course, there was the new Deepseek model, and now Kimi. But what is the consensus on the other, somewhat smaller LLMs that came out? Models like Jamba-Mini-1.7, Hunyuan-A13B-Instruct or ERNIE-4.5-21B-A3B?&lt;/p&gt; &lt;p&gt;What's everyone's go-to model these days?&lt;/p&gt; &lt;p&gt;And what are some other LLMs, tools, or research papers that you think flew under the radar because of the many big releases recently? For example, things like the recently released &lt;a href="https://huggingface.co/allenai/FlexOlmo-7x7B-1T"&gt;FlexOlmo&lt;/a&gt; LLM/paradigm?&lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MDT-49"&gt; /u/MDT-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T03:13:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4y3cj</id>
    <title>Fine-tuned her the perfect local model. Still got API’d 💔</title>
    <updated>2025-07-20T19:43:10+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/"&gt; &lt;img alt="Fine-tuned her the perfect local model. Still got API’d 💔" src="https://preview.redd.it/xitr9w9f13ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f099b23bb6d2f68855a9689333e79824231cdf0" title="Fine-tuned her the perfect local model. Still got API’d 💔" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xitr9w9f13ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4y3cj/finetuned_her_the_perfect_local_model_still_got/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T19:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m5d66p</id>
    <title>ik_llama.cpp 404: temporary repo up to commit d44c2d3</title>
    <updated>2025-07-21T08:13:37+00:00</updated>
    <author>
      <name>/u/PieBru</name>
      <uri>https://old.reddit.com/user/PieBru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those interested, here is a temporary copy pulled just before the official repo went 404.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PieBru/ik_llama.cpp_temp_copy"&gt;https://github.com/PieBru/ik_llama.cpp_temp_copy&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PieBru"&gt; /u/PieBru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T08:13:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4vw29</id>
    <title>Ikllamacpp repository gone, or it is only me?</title>
    <updated>2025-07-20T18:14:47+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was seeing if there was a new commit today but when refreshed the page got a 404.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/commits/main/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4vw29/ikllamacpp_repository_gone_or_it_is_only_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T18:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1m58695</id>
    <title>Which local 100B+ heavy weight models are your favorite and why?</title>
    <updated>2025-07-21T03:19:13+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Mistral_large-Instruct&lt;/li&gt; &lt;li&gt;Qwen3-235B&lt;/li&gt; &lt;li&gt;Command-A&lt;/li&gt; &lt;li&gt;Deepseek-V3&lt;/li&gt; &lt;li&gt;Deepseek-R1&lt;/li&gt; &lt;li&gt;Deepseek-R1-0528&lt;/li&gt; &lt;li&gt;Deepseek-TNG-R1T2-Chimera&lt;/li&gt; &lt;li&gt;Kimi-K2&lt;/li&gt; &lt;li&gt;Ernie-4.5-300b&lt;/li&gt; &lt;li&gt;llama3.1-405B&lt;/li&gt; &lt;li&gt;llama3.1-Nemotron-Ultra-253b?&lt;/li&gt; &lt;li&gt;Others?&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-21T03:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1m52h10</id>
    <title>I posted 3 weeks ago about training my own model. Progress report.</title>
    <updated>2025-07-20T22:46:55+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"&gt; &lt;img alt="I posted 3 weeks ago about training my own model. Progress report." src="https://b.thumbs.redditmedia.com/tGNWILy7NUwjWBRL__Qs6HOJImwQ5Z22Np_wBFcIDdM.jpg" title="I posted 3 weeks ago about training my own model. Progress report." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I posted that I wanted to train an LLM for under $1000 here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had to crunch a lot to fit in 24gb of ram. The final project is a 960M model trained on 19.2B tokens ( chinchilla optimal). Cost projection is about $500 for this run. It has flash attention 2, a 3:1 GQA, a 3k context window. and sink tokens. Training is 70% project gutenberg and 30% US congressional reports ( the Govremorts dataset). The corpus is english only, which I'm hoping will give it an edge.&lt;/p&gt; &lt;p&gt;I have had two false starts where I had to restart training. The first because I set up my streaming datasets wrong, and the model kep training on the same thing due to restarts. The second because the LR was too high and my loss curve was all fucked up.&lt;/p&gt; &lt;p&gt;Now at about 2% on the 3rd run, the loss looks textbook, and I am letting it run till the tokens are done. Projections show a final loss around 2.6-2.3 which is great.&lt;/p&gt; &lt;p&gt;Happy to answer any questions! Pic is the beautiful loss curve.&lt;/p&gt; &lt;p&gt;Edit: It's called Libremodel I, codename Gigi, and I made a website with more info here: &lt;a href="https://libremodel.xyz"&gt;https://libremodel.xyz&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1"&gt;https://preview.redd.it/lf78xbsfy3ef1.png?width=711&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc75b919255aa91b8cbf0b65b1420cb43fe26a1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m52h10/i_posted_3_weeks_ago_about_training_my_own_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T22:46:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m4s9mt</id>
    <title>I'm sorry Zuck please don't leave us we were just having fun</title>
    <updated>2025-07-20T15:51:11+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"&gt; &lt;img alt="I'm sorry Zuck please don't leave us we were just having fun" src="https://preview.redd.it/p9mxxen7w1ef1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ce4b5b89949e56107fd26431dd9d275053d6cf2" title="I'm sorry Zuck please don't leave us we were just having fun" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p9mxxen7w1ef1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m4s9mt/im_sorry_zuck_please_dont_leave_us_we_were_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-20T15:51:11+00:00</published>
  </entry>
</feed>
