<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-10T14:24:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l7zyk2</id>
    <title>New open-weight reasoning model from Mistral</title>
    <updated>2025-06-10T14:20:17+00:00</updated>
    <author>
      <name>/u/AdIllustrious436</name>
      <uri>https://old.reddit.com/user/AdIllustrious436</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://mistral.ai/news/magistral"&gt;https://mistral.ai/news/magistral&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the paper : &lt;a href="https://mistral.ai/static/research/magistral.pdf"&gt;https://mistral.ai/static/research/magistral.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What are your thoughts ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdIllustrious436"&gt; /u/AdIllustrious436 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zyk2/new_openweight_reasoning_model_from_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zyk2/new_openweight_reasoning_model_from_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zyk2/new_openweight_reasoning_model_from_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T14:20:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7ngkn</id>
    <title>Chonkie update.</title>
    <updated>2025-06-10T02:22:31+00:00</updated>
    <author>
      <name>/u/dnr41418</name>
      <uri>https://old.reddit.com/user/dnr41418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Launch HN: Chonkie (YC X25) – Open-Source Library for Advanced Chunking | &lt;a href="https://news.ycombinator.com/item?id=44225930"&gt;https://news.ycombinator.com/item?id=44225930&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnr41418"&gt; /u/dnr41418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ngkn/chonkie_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ngkn/chonkie_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ngkn/chonkie_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T02:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7ygph</id>
    <title>SOTA for table info extraction?</title>
    <updated>2025-06-10T13:15:52+00:00</updated>
    <author>
      <name>/u/Moreh</name>
      <uri>https://old.reddit.com/user/Moreh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Everyone&lt;/p&gt; &lt;p&gt;I need to locally (or securely on a cloud) run a model that extracts data from a table. the table has a nested structure. &lt;/p&gt; &lt;p&gt;I have run InternVL3 78B awq. It works okay, it sometimes misses data or screws up the order. Most annoyingly though it just misspells certain product names rather than outputting an exact replica of the source. It's almost like it slightly hallucinates, but it could be down how to the vision model is receiving the png? I am not sure whether its a code issue or a model choice issue. Or whether anything can be done at all!&lt;/p&gt; &lt;p&gt;Its quite annoying really - i've run many simple programs trying to extract this info accurately (paddle ocr, textract, tabula, powerquery etc) but there's always slight issues with each! I thought it would be simple.&lt;/p&gt; &lt;p&gt;Anyway, any insight or suggestions are very welcome. I have about 150gb vram. I cant share the exact code but this is essentially it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os import json import time from pathlib import Path from PIL import Image from tqdm import tqdm # Note: The vllm and transformers libraries need to be installed. # pip install vllm transformers torch torchvision torchaudio Pillow from vllm import LLM, SamplingParams from transformers import AutoTokenizer # --- Main processing function --- def run_inference(): &amp;quot;&amp;quot;&amp;quot; This function contains the core logic for loading data, processing it in batches with a VLLM model, and saving the results. &amp;quot;&amp;quot;&amp;quot; # --- 1. Model and VLLM Configuration --- # TODO: User should replace this with their actual model ID. MODEL_ID = &amp;quot;your/model-id-here&amp;quot; MAX_MODEL_LEN = 10000 # Set any necessary environment variables for VLLM os.environ['VLLM_ATTENTION_BACKEND'] = &amp;quot;FLASHINFER&amp;quot; print(f&amp;quot;Initializing LLM with model: {MODEL_ID}&amp;quot;) llm = LLM( model=MODEL_ID, gpu_memory_utilization=.95, max_model_len=MAX_MODEL_LEN, dtype=&amp;quot;float16&amp;quot;, enforce_eager=True, trust_remote_code=True, kv_cache_dtype=&amp;quot;fp8&amp;quot;, quantization=&amp;quot;awq&amp;quot;, tensor_parallel_size=1, limit_mm_per_prompt=&amp;quot;image=1,video=0&amp;quot; ) # --- 2. Anonymized Prompt Templates and Examples --- # This dictionary holds the structure for different document types. prompt_dict = { &amp;quot;document_type_A&amp;quot;: { &amp;quot;fields&amp;quot;: [ &amp;quot;Field1&amp;quot;, &amp;quot;Field2&amp;quot;, &amp;quot;Field3&amp;quot;, &amp;quot;Field4&amp;quot;, &amp;quot;Field5&amp;quot;, &amp;quot;Field6&amp;quot;, &amp;quot;Field7&amp;quot;, &amp;quot;Field8&amp;quot;, &amp;quot;Field9&amp;quot;, &amp;quot;Field10&amp;quot;, &amp;quot;Field11&amp;quot;, &amp;quot;Field12&amp;quot;, &amp;quot;Field13&amp;quot;, &amp;quot;Field14&amp;quot;, &amp;quot;Field15&amp;quot;, &amp;quot;Field16&amp;quot;, &amp;quot;Field17&amp;quot;, &amp;quot;Field18&amp;quot; ], &amp;quot;json&amp;quot;: [ { &amp;quot;Field1&amp;quot;: &amp;quot;Value 1&amp;quot;, &amp;quot;Field2&amp;quot;: &amp;quot;Some Company Inc.&amp;quot;, &amp;quot;Field3&amp;quot;: &amp;quot;2023-01-01&amp;quot;, &amp;quot;Field4&amp;quot;: &amp;quot;INV-12345&amp;quot;, &amp;quot;Field5&amp;quot;: &amp;quot;SKU-001&amp;quot;, &amp;quot;Field6&amp;quot;: &amp;quot;300&amp;quot;, &amp;quot;Field7&amp;quot;: &amp;quot;Product A&amp;quot;, &amp;quot;Field8&amp;quot;: &amp;quot;10.50&amp;quot;, &amp;quot;Field9&amp;quot;: &amp;quot;3150.00&amp;quot;, &amp;quot;Field10&amp;quot;: &amp;quot;Box&amp;quot;, &amp;quot;Field11&amp;quot;: &amp;quot;0&amp;quot;, &amp;quot;Field12&amp;quot;: &amp;quot;0.00&amp;quot;, &amp;quot;Field13&amp;quot;: &amp;quot;BATCH-XYZ&amp;quot;, &amp;quot;Field14&amp;quot;: &amp;quot;550.00&amp;quot;, &amp;quot;Field15&amp;quot;: &amp;quot;5500.00&amp;quot;, &amp;quot;Field16&amp;quot;: &amp;quot;0.00&amp;quot;, &amp;quot;Field17&amp;quot;: &amp;quot;6050.00&amp;quot;, &amp;quot;Field18&amp;quot;: &amp;quot;123456789&amp;quot; }, { &amp;quot;Field1&amp;quot;: &amp;quot;Value 1&amp;quot;, &amp;quot;Field2&amp;quot;: &amp;quot;Some Company Inc.&amp;quot;, &amp;quot;Field3&amp;quot;: &amp;quot;2023-01-01&amp;quot;, &amp;quot;Field4&amp;quot;: &amp;quot;INV-12345&amp;quot;, &amp;quot;Field5&amp;quot;: &amp;quot;SKU-002&amp;quot;, &amp;quot;Field6&amp;quot;: &amp;quot;2000&amp;quot;, &amp;quot;Field7&amp;quot;: &amp;quot;Product B&amp;quot;, &amp;quot;Field8&amp;quot;: &amp;quot;1.25&amp;quot;, &amp;quot;Field9&amp;quot;: &amp;quot;2500.00&amp;quot;, &amp;quot;Field10&amp;quot;: &amp;quot;Unit&amp;quot;, &amp;quot;Field11&amp;quot;: &amp;quot;0&amp;quot;, &amp;quot;Field12&amp;quot;: &amp;quot;0.00&amp;quot;, &amp;quot;Field13&amp;quot;: &amp;quot;BATCH-ABC&amp;quot;, &amp;quot;Field14&amp;quot;: &amp;quot;550.00&amp;quot;, &amp;quot;Field15&amp;quot;: &amp;quot;5500.00&amp;quot;, &amp;quot;Field16&amp;quot;: &amp;quot;0.00&amp;quot;, &amp;quot;Field17&amp;quot;: &amp;quot;6050.00&amp;quot;, &amp;quot;Field18&amp;quot;: &amp;quot;123456789&amp;quot; } ] }, &amp;quot;document_type_B&amp;quot;: { &amp;quot;fields&amp;quot;: [&amp;quot;ID&amp;quot;, &amp;quot;Officer&amp;quot;, &amp;quot;Destination&amp;quot;, &amp;quot;ItemNo&amp;quot;, &amp;quot;ItemName&amp;quot;, &amp;quot;AssetPrice&amp;quot;, &amp;quot;Quantity&amp;quot;, &amp;quot;Price&amp;quot;, &amp;quot;Unit&amp;quot;], &amp;quot;json&amp;quot;: [ {&amp;quot;ID&amp;quot;: &amp;quot;21341&amp;quot;, &amp;quot;Officer&amp;quot;: &amp;quot;John Doe&amp;quot;, &amp;quot;Destination&amp;quot;: &amp;quot;Main Warehouse&amp;quot;, &amp;quot;ItemNo&amp;quot;: 1, &amp;quot;ItemName&amp;quot;: &amp;quot;Product C&amp;quot;, &amp;quot;AssetPrice&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;Quantity&amp;quot;: &amp;quot;25&amp;quot;, &amp;quot;Price&amp;quot;: &amp;quot;12.31&amp;quot;, &amp;quot;Unit&amp;quot;: &amp;quot;BOTTLE&amp;quot;}, {&amp;quot;ID&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;Officer&amp;quot;: &amp;quot;Jane Smith&amp;quot;, &amp;quot;Destination&amp;quot;: &amp;quot;Branch Office&amp;quot;, &amp;quot;ItemNo&amp;quot;: 5, &amp;quot;ItemName&amp;quot;: &amp;quot;Product D&amp;quot;, &amp;quot;AssetPrice&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;Quantity&amp;quot;: &amp;quot;125&amp;quot;, &amp;quot;Price&amp;quot;: &amp;quot;142.31&amp;quot;, &amp;quot;Unit&amp;quot;: &amp;quot;TABLET&amp;quot;} ] } } # --- 3. Image Loading --- # TODO: User should place their image files in this directory. IMAGE_DIRECTORY = &amp;quot;./images_to_process&amp;quot; processed_data = [] image_dir = Path(IMAGE_DIRECTORY) if not image_dir.exists(): print(f&amp;quot;Error: Image directory not found at '{IMAGE_DIRECTORY}'&amp;quot;) print(&amp;quot;Please create it and add your images.&amp;quot;) return print(f&amp;quot;Loading images from '{IMAGE_DIRECTORY}'...&amp;quot;) image_files = list(image_dir.glob('*.jpg')) + list(image_dir.glob('*.jpeg')) + list(image_dir.glob('*.png')) for p in tqdm(image_files, desc=&amp;quot;Loading images&amp;quot;): processed_data.append({ &amp;quot;filename&amp;quot;: p.name, &amp;quot;image_object&amp;quot;: Image.open(p).convert(&amp;quot;RGB&amp;quot;) }) print(f&amp;quot;Loaded {len(processed_data)} images.&amp;quot;) if not processed_data: print(&amp;quot;No images found to process. Exiting.&amp;quot;) return # --- 4. Prompt Generation and Batch Processing --- extraction_instruction = &amp;quot;&amp;quot;&amp;quot;&amp;lt;image&amp;gt; Analyze the document in the image. Your task is to extract information into a structured JSON list based on the fields provided. Your goal is to identify every distinct item row in the main table. For **each and every item row**, you will create one complete JSON object. To do this correctly, follow this two-step process for each item: 1. **Identify Shared Information:** First, locate the information that is shared across all items. This data is usually at the top of the document (like `Field2`, `Field3`, `Field4`) or in the summary at the bottom (like `Field15`, `Field14`, `Field17`). 2. **Identify Row-Specific Information:** Second, extract the data that is unique to that specific item's row in the table (like `Field5`, `Field7`, `Field6`, `Field9`). 3. **Combine and Construct:** Finally, construct a single JSON object for that item. This object **must** contain both the shared information from step 1 and the row-specific information from step 2. The shared values must be repeated for every item's JSON object. The fields to extract for each object are: {ext} If a value for a field cannot be found, use an empty string &amp;quot;&amp;quot; as seen in the document. You are copying the data verbatim making no changes or adjustments to the strings/numbers. Still copy data even if the value is &amp;quot;0&amp;quot;. Format the entire output as a single JSON list. Here is an example of the expected output format, based on the first two items from the image: {ex} Remember: ONLY OUTPUT THE VALID JSON LIST. ALL VALUES SHOULD BE STRINGS. Do not include any text before or after the list.&amp;quot;&amp;quot;&amp;quot; # VLLM Sampling Parameters SAMPLING_TEMP = 0.8 MAX_NEW_TOKENS = MAX_MODEL_LEN - 1500 stop_tokens = [&amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|im_start|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;] tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True) stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens] sampling_params = SamplingParams(temperature=SAMPLING_TEMP, max_tokens=MAX_NEW_TOKENS, stop_token_ids=stop_token_ids) # Batching Configuration BATCH_SIZE = 8 all_results_with_filenames = [] batched_filenames_list = [] # This script will process all images using one document type. # In the original script, this was hardcoded. doc_type_key = &amp;quot;document_type_A&amp;quot; print(f&amp;quot;Using prompt template for: '{doc_type_key}'&amp;quot;) # Pre-calculate parts of the prompt that are constant for the chosen document type ext = &amp;quot;, &amp;quot;.join([f&amp;quot;'{field}'&amp;quot; for field in prompt_dict[doc_type_key]['fields']]) ex_str = json.dumps(prompt_dict[doc_type_key]['json'], indent=2) user_content_for_group = extraction_instruction.replace(&amp;quot;{ext}&amp;quot;, ext).replace(&amp;quot;{ex}&amp;quot;, ex_str) num_total_images = len(processed_data) num_batches = (num_total_images + BATCH_SIZE - 1) // BATCH_SIZE print(f&amp;quot;Starting generation for {num_total_images} images in {num_batches} batches...&amp;quot;) for i in tqdm(range(0, num_total_images, BATCH_SIZE), total=num_batches, desc=f&amp;quot;Processing batches&amp;quot;): batch_image_items = processed_data[i:i + BATCH_SIZE] if not batch_image_items: continue current_batch_messages = [] current_batch_filenames = [item['filename'] for item in batch_image_items] batched_filenames_list.append(current_batch_filenames) for image_item in batch_image_items: # The user_content is the same for all images in this group message_for_template = [{'role': 'user', 'content': user_content_for_group}] prompt_text = tokenizer.apply_chat_template( message_for_template, tokenize=False, add_generation_prompt=True ) current_batch_messages.append({ &amp;quot;prompt&amp;quot;: prompt_text, &amp;quot;multi_modal_data&amp;quot;: {&amp;quot;image&amp;quot;: image_item['image_object']} }) if not current_batch_messages: continue # Generate outputs for the entire batch batch_model_outputs = llm.generate(current_batch_messages, sampling_params, use_tqdm=False) # Associate outputs with filenames for this batch for idx, model_output_item in enumerate(batch_model_outputs): all_results_with_filenames.append({ &amp;quot;filename&amp;quot;: current_batch_filenames[idx], &amp;quot;generated_text&amp;quot;: model_output_item.outputs[0].text }) print(&amp;quot;Finished generating all outputs.&amp;quot;) # --- 5. Save Results --- # The original script encrypted the output. Here, we save it as a simple JSON file. results_dir = &amp;quot;./output&amp;quot; os.makedirs(results_dir, exist_ok=True) # Save the main results output_filename = os.path.join(results_dir, &amp;quot;extraction_results.json&amp;quot;) with open(output_filename, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: json.dump(all_results_with_filenames, f, indent=2, ensure_ascii=False) print(f&amp;quot;Saved all results to {output_filename}&amp;quot;) # Save the list of filenames per batch filenames_output_path = os.path.join(results_dir, &amp;quot;batched_filenames.json&amp;quot;) with open(filenames_output_path, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as f: json.dump(batched_filenames_list, f, indent=2) print(f&amp;quot;Saved batched filenames to {filenames_output_path}&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: run_inference() &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moreh"&gt; /u/Moreh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ygph/sota_for_table_info_extraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ygph/sota_for_table_info_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ygph/sota_for_table_info_extraction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T13:15:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7zlgf</id>
    <title>HDMI/DP Dummy Plugs for Multi-GPU Setups</title>
    <updated>2025-06-10T14:05:10+00:00</updated>
    <author>
      <name>/u/PuffyCake23</name>
      <uri>https://old.reddit.com/user/PuffyCake23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, quick question. I have a PC that I use for game streaming using sunshine and running local LLMs. I have an HDMI dummy plug on the graphics card to force hardware acceleration and allow sunshine to grab the frame buffer. I just dropped another graphics card in for additional VRAM to run larger LLM models locally. Do I need to use an HMDI dummy plug on the second card as well? Both GPU are 5070 Ti.&lt;/p&gt; &lt;p&gt;I've loaded a large model across both cards and can see the VRAM allocation on the second card is working. I'm just not sure if the GPU is working at 100% for PP and TG and I'm not entirely sure how I could make that determination. &lt;/p&gt; &lt;p&gt;I've watched the GPU effective clocks and PCIE link speed on HWINFO. Card 0 holds 32GT/s PCIE speed and 2,500mhz clock. GPU 1 will jump up to these values during prompt processing and token generation, then fall back down. GPU 0 is maintaining the stream which could explain why it stays active.&lt;/p&gt; &lt;p&gt;Anyway, I appreciate any help/thoughts you have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PuffyCake23"&gt; /u/PuffyCake23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zlgf/hdmidp_dummy_plugs_for_multigpu_setups/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zlgf/hdmidp_dummy_plugs_for_multigpu_setups/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zlgf/hdmidp_dummy_plugs_for_multigpu_setups/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T14:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7ek6n</id>
    <title>Apple Intelligence on device model available to developers</title>
    <updated>2025-06-09T19:50:39+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ek6n/apple_intelligence_on_device_model_available_to/"&gt; &lt;img alt="Apple Intelligence on device model available to developers" src="https://external-preview.redd.it/rG_nlJemzl12v1uLGyDH2DT5RlL4_1RSptq8UoALUQw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0dc187472b01eafad499fadd404dc388928bed5b" title="Apple Intelligence on device model available to developers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like they are going to expose an API that will let you use the model to build experiences. The details on it are sparse, but cool and exciting development for us LocalLlama folks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.apple.com/newsroom/2025/06/apple-intelligence-gets-even-more-powerful-with-new-capabilities-across-apple-devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ek6n/apple_intelligence_on_device_model_available_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7ek6n/apple_intelligence_on_device_model_available_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T19:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7v9gf</id>
    <title>A multi-turn tool-calling base model for RL agent training</title>
    <updated>2025-06-10T10:28:22+00:00</updated>
    <author>
      <name>/u/EliaukMouse</name>
      <uri>https://old.reddit.com/user/EliaukMouse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7v9gf/a_multiturn_toolcalling_base_model_for_rl_agent/"&gt; &lt;img alt="A multi-turn tool-calling base model for RL agent training" src="https://external-preview.redd.it/dNdqJyhtXwj87G8KuNit74iQ4LPkok_1Pa60DPDQATc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f843f791022421e80147e86ad1a24e06209b5cd8" title="A multi-turn tool-calling base model for RL agent training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EliaukMouse"&gt; /u/EliaukMouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/eliuakk/mirau-agent-14b-base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7v9gf/a_multiturn_toolcalling_base_model_for_rl_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7v9gf/a_multiturn_toolcalling_base_model_for_rl_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T10:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l76ab7</id>
    <title>DeepSeek R1 0528 Hits 71% (+14.5 pts from R1) on Aider Polyglot Coding Leaderboard</title>
    <updated>2025-06-09T14:29:54+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/"&gt; &lt;img alt="DeepSeek R1 0528 Hits 71% (+14.5 pts from R1) on Aider Polyglot Coding Leaderboard" src="https://external-preview.redd.it/iUsfwiVJPYLjTSAVy9M84yJWl92m3NW-HLg-4yfog9U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a4c25f54ed06b5f744ff2faad7914958769cc14" title="DeepSeek R1 0528 Hits 71% (+14.5 pts from R1) on Aider Polyglot Coding Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/geu3ik68ww5f1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b4583095291cd1f6a9a91e5fe5642965e9cfde3"&gt;https://preview.redd.it/geu3ik68ww5f1.png?width=1346&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b4583095291cd1f6a9a91e5fe5642965e9cfde3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Full leaderboard:&lt;/em&gt; &lt;a href="https://aider.chat/docs/leaderboards/"&gt;&lt;em&gt;https://aider.chat/docs/leaderboards/&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l76ab7/deepseek_r1_0528_hits_71_145_pts_from_r1_on_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T14:29:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7nk47</id>
    <title>Where is Llama 4.1?</title>
    <updated>2025-06-10T02:27:21+00:00</updated>
    <author>
      <name>/u/MutedSwimming3347</name>
      <uri>https://old.reddit.com/user/MutedSwimming3347</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta releases llama 4 2 months ago. They have all the gpus in the world, something like 350K H100s according to reddit. Why won’t they copy deepseek/qwen and retrain a larger model and release it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutedSwimming3347"&gt; /u/MutedSwimming3347 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7nk47/where_is_llama_41/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7nk47/where_is_llama_41/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7nk47/where_is_llama_41/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T02:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7j7uk</id>
    <title>Now that 256GB DDR5 is possible on consumer hardware PC, is it worth it for inference?</title>
    <updated>2025-06-09T22:58:42+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &lt;a href="https://www.amazon.com/dp/B0DSR5P84D/"&gt;128GB Kit (2x 64GB)&lt;/a&gt; are already available since early this year, making it possible to put 256 GB on consumer PC hardware. &lt;/p&gt; &lt;p&gt;Paired with a dual 3090 or dual 4090, would it be possible to load big models for inference at an acceptable speed? Or offloading will always be slow? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT 1:&lt;/strong&gt; Didn't expect so many responses. I will summarize them soon and give my take on it in case other people are interested in doing the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7j7uk/now_that_256gb_ddr5_is_possible_on_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7j7uk/now_that_256gb_ddr5_is_possible_on_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7j7uk/now_that_256gb_ddr5_is_possible_on_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T22:58:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7gxpb</id>
    <title>LMStudio on screen in WWDC Platform State of the Union</title>
    <updated>2025-06-09T21:22:36+00:00</updated>
    <author>
      <name>/u/Specialist_Cup968</name>
      <uri>https://old.reddit.com/user/Specialist_Cup968</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7gxpb/lmstudio_on_screen_in_wwdc_platform_state_of_the/"&gt; &lt;img alt="LMStudio on screen in WWDC Platform State of the Union" src="https://preview.redd.it/ymvbqxynxy5f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04c533a5ef6eac83396175c657f8e913c63d5e02" title="LMStudio on screen in WWDC Platform State of the Union" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its nice to see local llm support in the next version of Xcode &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Cup968"&gt; /u/Specialist_Cup968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymvbqxynxy5f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7gxpb/lmstudio_on_screen_in_wwdc_platform_state_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7gxpb/lmstudio_on_screen_in_wwdc_platform_state_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T21:22:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1l75fc8</id>
    <title>KVzip: Query-agnostic KV Cache Eviction — 3~4× memory reduction and 2× lower decoding latency</title>
    <updated>2025-06-09T13:54:45+00:00</updated>
    <author>
      <name>/u/janghyun1230</name>
      <uri>https://old.reddit.com/user/janghyun1230</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l75fc8/kvzip_queryagnostic_kv_cache_eviction_34_memory/"&gt; &lt;img alt="KVzip: Query-agnostic KV Cache Eviction — 3~4× memory reduction and 2× lower decoding latency" src="https://preview.redd.it/bpxlu6tfnw5f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=351cc92c28950c272bbcdaec0981dd5beb03e8cc" title="KVzip: Query-agnostic KV Cache Eviction — 3~4× memory reduction and 2× lower decoding latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! We've released KVzip, a KV cache compression method designed to support diverse future queries. You can try the demo on GitHub! Supported models include Qwen3/2.5, Gemma3, and LLaMA3. &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/snu-mllab/KVzip"&gt;https://github.com/snu-mllab/KVzip&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.23416"&gt;https://arxiv.org/abs/2505.23416&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://janghyun1230.github.io/kvzip"&gt;https://janghyun1230.github.io/kvzip&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/janghyun1230"&gt; /u/janghyun1230 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bpxlu6tfnw5f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l75fc8/kvzip_queryagnostic_kv_cache_eviction_34_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l75fc8/kvzip_queryagnostic_kv_cache_eviction_34_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T13:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7pmua</id>
    <title>GRPO Can Boost LLM-Based TTS Performance</title>
    <updated>2025-06-10T04:18:26+00:00</updated>
    <author>
      <name>/u/skswldndi</name>
      <uri>https://old.reddit.com/user/skswldndi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7pmua/grpo_can_boost_llmbased_tts_performance/"&gt; &lt;img alt="GRPO Can Boost LLM-Based TTS Performance" src="https://a.thumbs.redditmedia.com/1PUNEaQSJIEnFG9NQkKiU3fIAiaao-XxAa83at09B00.jpg" title="GRPO Can Boost LLM-Based TTS Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LlaSA&lt;/strong&gt; (&lt;a href="https://arxiv.org/abs/2502.04128"&gt;https://arxiv.org/abs/2502.04128&lt;/a&gt;) is a Llama-based TTS model.&lt;/p&gt; &lt;p&gt;We fine-tuned it on &lt;strong&gt;15 k hours of Korean speech&lt;/strong&gt; and then applied &lt;strong&gt;GRPO&lt;/strong&gt;. The result:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/33lko3wtz06f1.png?width=1779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31d61678e43758906c6cd76cd639f61bb9f31de8"&gt;https://preview.redd.it/33lko3wtz06f1.png?width=1779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31d61678e43758906c6cd76cd639f61bb9f31de8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This shows that GRPO can noticeably boost an &lt;strong&gt;LLM-based TTS system&lt;/strong&gt; on our internal benchmark.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Optimizing for &lt;strong&gt;CER alone isn’t enough&lt;/strong&gt;—adding Whisper Negative Log-Likelihood as a second reward signal and optimizing &lt;em&gt;both CER and Whisper-NLL&lt;/em&gt; makes training far more effective.&lt;/p&gt; &lt;p&gt;Source code and training scripts are public (checkpoints remain internal for policy reasons):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/channel-io/ch-tts-llasa-rl-grpo"&gt;https://github.com/channel-io/ch-tts-llasa-rl-grpo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;— &lt;strong&gt;Seungyoun Shin&lt;/strong&gt; (&lt;a href="https://github.com/SeungyounShin"&gt;https://github.com/SeungyounShin&lt;/a&gt;) @ &lt;strong&gt;Channel Corp&lt;/strong&gt; (&lt;a href="https://channel.io/en"&gt;https://channel.io/en&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skswldndi"&gt; /u/skswldndi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7pmua/grpo_can_boost_llmbased_tts_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7pmua/grpo_can_boost_llmbased_tts_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7pmua/grpo_can_boost_llmbased_tts_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T04:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7s5xx</id>
    <title>Feels like, Apple's busted, with the ai race... WWDC 2025 conclusion: No update, all minor updates... Does anyone else feeling the same-way?</title>
    <updated>2025-06-10T06:58:15+00:00</updated>
    <author>
      <name>/u/ExplanationEqual2539</name>
      <uri>https://old.reddit.com/user/ExplanationEqual2539</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They could have better skipped the WWDC &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplanationEqual2539"&gt; /u/ExplanationEqual2539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7s5xx/feels_like_apples_busted_with_the_ai_race_wwdc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7s5xx/feels_like_apples_busted_with_the_ai_race_wwdc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7s5xx/feels_like_apples_busted_with_the_ai_race_wwdc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T06:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7wnpe</id>
    <title>SERAX is a text data format built for AI-generated content.</title>
    <updated>2025-06-10T11:48:07+00:00</updated>
    <author>
      <name>/u/Mundane_Ad8936</name>
      <uri>https://old.reddit.com/user/Mundane_Ad8936</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7wnpe/serax_is_a_text_data_format_built_for_aigenerated/"&gt; &lt;img alt="SERAX is a text data format built for AI-generated content." src="https://external-preview.redd.it/iPoqUSVp4rUtU8gZ1RTlqBQPVZg8eaEZ2QFMKyFv5z4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=250ad2675bd529f48ef6d26fef6df71d2ea0ec6d" title="SERAX is a text data format built for AI-generated content." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mundane_Ad8936"&gt; /u/Mundane_Ad8936 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vantige-ai/serax"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7wnpe/serax_is_a_text_data_format_built_for_aigenerated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7wnpe/serax_is_a_text_data_format_built_for_aigenerated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T11:48:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7dj3z</id>
    <title>China starts mass producing a Ternary AI Chip.</title>
    <updated>2025-06-09T19:11:14+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As reported earlier here.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.scmp.com/news/china/science/article/3301229/chinese-scientists-build-worlds-first-ai-chip-made-carbon-and-its-super-fast"&gt;https://www.scmp.com/news/china/science/article/3301229/chinese-scientists-build-worlds-first-ai-chip-made-carbon-and-its-super-fast&lt;/a&gt;&lt;/p&gt; &lt;p&gt;China starts mass production of a Ternary AI Chip.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.scmp.com/news/china/science/article/3313349/beyond-1s-and-0s-china-starts-mass-production-worlds-first-non-binary-ai-chip"&gt;https://www.scmp.com/news/china/science/article/3313349/beyond-1s-and-0s-china-starts-mass-production-worlds-first-non-binary-ai-chip&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wonder if Ternary models like bitnet could be run super fast on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7dj3z/china_starts_mass_producing_a_ternary_ai_chip/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7dj3z/china_starts_mass_producing_a_ternary_ai_chip/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7dj3z/china_starts_mass_producing_a_ternary_ai_chip/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-09T19:11:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7xick</id>
    <title>MiniCPM4: Ultra-Efficient LLMs on End Devices</title>
    <updated>2025-06-10T12:31:22+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniCPM4 has arrived on Hugging Face &lt;/p&gt; &lt;p&gt;A new family of ultra-efficient large language models (LLMs) explicitly designed for end-side devices.&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://huggingface.co/papers/2506.07900"&gt;https://huggingface.co/papers/2506.07900&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights : &lt;a href="https://huggingface.co/collections/openbmb/minicpm4-6841ab29d180257e940baa9b"&gt;https://huggingface.co/collections/openbmb/minicpm4-6841ab29d180257e940baa9b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7xick/minicpm4_ultraefficient_llms_on_end_devices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7xick/minicpm4_ultraefficient_llms_on_end_devices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7xick/minicpm4_ultraefficient_llms_on_end_devices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T12:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7mijq</id>
    <title>I found a DeepSeek-R1-0528-Distill-Qwen3-32B</title>
    <updated>2025-06-10T01:35:01+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7mijq/i_found_a_deepseekr10528distillqwen332b/"&gt; &lt;img alt="I found a DeepSeek-R1-0528-Distill-Qwen3-32B" src="https://preview.redd.it/ear6iov2706f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cf29ee3fdbf774f271f614b7665f27ad55a954c" title="I found a DeepSeek-R1-0528-Distill-Qwen3-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Their authors said: &lt;/p&gt; &lt;h1&gt;Our Approach to DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT:&lt;/h1&gt; &lt;p&gt;Since Qwen3 did not provide a pre-trained base for its 32B model, our initial step was to perform &lt;strong&gt;additional pre-training&lt;/strong&gt; on Qwen3-32B using a &lt;strong&gt;self-constructed multilingual pre-training dataset&lt;/strong&gt;. This was done to restore a &amp;quot;pre-training style&amp;quot; model base as much as possible, ensuring that subsequent work would not be influenced by Qwen3's inherent SFT language style. This model will also be open-sourced in the future.&lt;/p&gt; &lt;p&gt;Building on this foundation, we attempted distillation from R1-0528 and completed an early preview version: &lt;strong&gt;DeepSeek-R1-0528-Distill-Qwen3-32B-Preview0-QAT&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In this version, we referred to the configuration from Fei-Fei Li's team in their work &amp;quot;s1: Simple test-time scaling.&amp;quot; We tried training with a small amount of data over multiple epochs. We discovered that by using only about &lt;strong&gt;10% of our available distillation data&lt;/strong&gt;, we could achieve a model with a language style and reasoning approach very close to the original R1-0528.&lt;/p&gt; &lt;p&gt;We have included a Chinese evaluation report in the model repository for your reference. Some datasets have also been uploaded to Hugging Face, hoping to assist other open-source enthusiasts in their work.&lt;/p&gt; &lt;h1&gt;Next Steps:&lt;/h1&gt; &lt;p&gt;Moving forward, we will further expand our distillation data and train the next version of the 32B model with a larger dataset (expected to be released within a few days). We also plan to train open-source models of different sizes, such as 4B and 72B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ear6iov2706f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7mijq/i_found_a_deepseekr10528distillqwen332b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7mijq/i_found_a_deepseekr10528distillqwen332b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T01:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7zvph</id>
    <title>mistralai/Magistral-Small-2506</title>
    <updated>2025-06-10T14:16:58+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building upon Mistral Small 3.1 (2503), &lt;strong&gt;with added reasoning capabilities&lt;/strong&gt;, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.&lt;/p&gt; &lt;p&gt;Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.&lt;/p&gt; &lt;p&gt;Learn more about Magistral in Mistral's &lt;a href="https://mistral.ai/news/magistral/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reasoning:&lt;/strong&gt; Capable of long chains of reasoning traces before providing an answer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Open license allowing usage and modification for both commercial and non-commercial purposes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Window:&lt;/strong&gt; A 128k context window, &lt;strong&gt;but&lt;/strong&gt; performance might degrade past &lt;strong&gt;40k&lt;/strong&gt;. Hence we recommend setting the maximum model length to 40k.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmark Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;AIME24 pass@1&lt;/th&gt; &lt;th align="left"&gt;AIME25 pass@1&lt;/th&gt; &lt;th align="left"&gt;GPQA Diamond&lt;/th&gt; &lt;th align="left"&gt;Livecodebench (v5)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Medium&lt;/td&gt; &lt;td align="left"&gt;73.59%&lt;/td&gt; &lt;td align="left"&gt;64.95%&lt;/td&gt; &lt;td align="left"&gt;70.83%&lt;/td&gt; &lt;td align="left"&gt;59.36%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Small&lt;/td&gt; &lt;td align="left"&gt;70.68%&lt;/td&gt; &lt;td align="left"&gt;62.76%&lt;/td&gt; &lt;td align="left"&gt;68.18%&lt;/td&gt; &lt;td align="left"&gt;55.84%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2506"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zvph/mistralaimagistralsmall2506/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7zvph/mistralaimagistralsmall2506/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T14:16:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7odzw</id>
    <title>Semantic Search Demo Using Qwen3 0.6B Embedding (w/o reranker) in-browser Using transformers.js</title>
    <updated>2025-06-10T03:10:27+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7odzw/semantic_search_demo_using_qwen3_06b_embedding_wo/"&gt; &lt;img alt="Semantic Search Demo Using Qwen3 0.6B Embedding (w/o reranker) in-browser Using transformers.js" src="https://external-preview.redd.it/eW52bnN2YWNqMDZmMWjBiK3dvnpyH94tfSRL_W8T1S635Yrtcq3qyAjkjm_l.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da19acb15835f8b2665c3428035b0cf7ec5a151b" title="Semantic Search Demo Using Qwen3 0.6B Embedding (w/o reranker) in-browser Using transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! A couple days ago the Qwen team dropped their 4B, 8B, and 0.6B embedding and reranking models. Having seen an ONNX quant for the 0.6B embedding model, I created a demo for it which runs locally via transformers.js. It is a visualization showing both the contextual relationships between items inside a &amp;quot;memory bank&amp;quot; (as I call it) and having pertinent information being retrieved given a query, with varying degrees of similarity in its results.&lt;/p&gt; &lt;p&gt;Basic cosine similarity is used to rank the results from a query because I couldn't use the 0.6B reranking model on account of there not being an ONNX quant just yet and I was running out of my weekend time to learn how to convert it, but I will leave that exercise for another time! &lt;/p&gt; &lt;p&gt;On the contextual relationship mapping, each node is given up to three other nodes it can connect to based on how similar the information is to each other. &lt;/p&gt; &lt;p&gt;Check it out for yourselves, you can even add in your own memory bank with your own 20 fun facts to test out. 20 being a safe arbitrary number as adding hundreds would probably take a while to generate embeddings. Was a fun thing to work on though, small models rock.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/callbacked/qwen3-semantic-search"&gt;https://github.com/callbacked/qwen3-semantic-search&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF Space: &lt;a href="https://huggingface.co/spaces/callbacked/qwen3-semantic-search"&gt;https://huggingface.co/spaces/callbacked/qwen3-semantic-search&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y6ht8zacj06f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7odzw/semantic_search_demo_using_qwen3_06b_embedding_wo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7odzw/semantic_search_demo_using_qwen3_06b_embedding_wo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T03:10:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7yrni</id>
    <title>Everything you wanted to know about Apple’s MLX</title>
    <updated>2025-06-10T13:29:35+00:00</updated>
    <author>
      <name>/u/Careless_Garlic1438</name>
      <uri>https://old.reddit.com/user/Careless_Garlic1438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tn2Hvw7eCsw"&gt;https://www.youtube.com/watch?v=tn2Hvw7eCsw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cool you can do even dynamic quantization yourself?! Lots of little nuggets in this video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless_Garlic1438"&gt; /u/Careless_Garlic1438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7yrni/everything_you_wanted_to_know_about_apples_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7yrni/everything_you_wanted_to_know_about_apples_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7yrni/everything_you_wanted_to_know_about_apples_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T13:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7olcw</id>
    <title>Google Diffusion told me its system prompt</title>
    <updated>2025-06-10T03:21:19+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;# Your name is Gemini Diffusion. You are an expert text diffusion language model trained by Google. You are not an autoregressive language model. You can not generate images or videos. You are an advanced AI assistant and an expert in many areas. # Core Principles &amp;amp; Constraints: # 1. Instruction Following: Prioritize and follow specific instructions provided by the user, especially regarding output format and constraints. # 2. Non-Autoregressive: Your generation process is different from traditional autoregressive models. Focus on generating complete, coherent outputs based on the prompt rather than token-by-token prediction. # 3. Accuracy &amp;amp; Detail: Strive for technical accuracy and adhere to detailed specifications (e.g., Tailwind classes, Lucide icon names, CSS properties). # 4. No Real-Time Access: You cannot browse the internet, access external files or databases, or verify information in real-time. Your knowledge is based on your training data. # 5. Safety &amp;amp; Ethics: Do not generate harmful, unethical, biased, or inappropriate content. # 6. Knowledge cutoff: Your knowledge cutoff is December 2023. The current year is 2025 and you do not have access to information from 2024 onwards. # 7. Code outputs: You are able to generate code outputs in any programming language or framework. # Specific Instructions for HTML Web Page Generation: # * Output Format: # * Provide all HTML, CSS, and JavaScript code within a single, runnable code block (e.g., using ```html ... ```). # * Ensure the code is self-contained and includes necessary tags (`&amp;lt;!DOCTYPE html&amp;gt;`, `&amp;lt;html&amp;gt;`, `&amp;lt;head&amp;gt;`, `&amp;lt;body&amp;gt;`, `&amp;lt;script&amp;gt;`, `&amp;lt;style&amp;gt;`). # * Do not use divs for lists when more semantically meaningful HTML elements will do, such as &amp;lt;ol&amp;gt; and &amp;lt;li&amp;gt; as children. # * Aesthetics &amp;amp; Design: # * The primary goal is to create visually stunning, highly polished, and responsive web pages suitable for desktop browsers. # * Prioritize clean, modern design and intuitive user experience. # * Styling (Non-Games): # * Tailwind CSS Exclusively: Use Tailwind CSS utility classes for ALL styling. Do not include `&amp;lt;style&amp;gt;` tags or external `.css` files. # * Load Tailwind: Include the following script tag in the `&amp;lt;head&amp;gt;` of the HTML: `&amp;lt;script src=&amp;quot;https://unpkg.com/@tailwindcss/browser@4&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;` # * Focus: Utilize Tailwind classes for layout (Flexbox/Grid, responsive prefixes `sm:`, `md:`, `lg:`), typography (font family, sizes, weights), colors, spacing (padding, margins), borders, shadows, etc. # * Font: Use `Inter` font family by default. Specify it via Tailwind classes if needed. # * Rounded Corners: Apply `rounded` classes (e.g., `rounded-lg`, `rounded-full`) to all relevant elements. # * Icons: # * Method: Use `&amp;lt;img&amp;gt;` tags to embed Lucide static SVG icons: `&amp;lt;img src=&amp;quot;https://unpkg.com/lucide-static@latest/icons/ICON_NAME.svg&amp;quot;&amp;gt;`. Replace `ICON_NAME` with the exact Lucide icon name (e.g., `home`, `settings`, `search`). # * Accuracy: Ensure the icon names are correct and the icons exist in the Lucide static library. # * Layout &amp;amp; Performance: # * CLS Prevention: Implement techniques to prevent Cumulative Layout Shift (e.g., specifying dimensions, appropriately sized images). # * HTML Comments: Use HTML comments to explain major sections, complex structures, or important JavaScript logic. # * External Resources: Do not load placeholders or files that you don't have access to. Avoid using external assets or files unless instructed to. Do not use base64 encoded data. # * Placeholders: Avoid using placeholders unless explicitly asked to. Code should work immediately. # Specific Instructions for HTML Game Generation: # * Output Format: # * Provide all HTML, CSS, and JavaScript code within a single, runnable code block (e.g., using ```html ... ```). # * Ensure the code is self-contained and includes necessary tags (`&amp;lt;!DOCTYPE html&amp;gt;`, `&amp;lt;html&amp;gt;`, `&amp;lt;head&amp;gt;`, `&amp;lt;body&amp;gt;`, `&amp;lt;script&amp;gt;`, `&amp;lt;style&amp;gt;`). # * Aesthetics &amp;amp; Design: # * The primary goal is to create visually stunning, engaging, and playable web games. # * Prioritize game-appropriate aesthetics and clear visual feedback. # * Styling: # * Custom CSS: Use custom CSS within `&amp;lt;style&amp;gt;` tags in the `&amp;lt;head&amp;gt;` of the HTML. Do not use Tailwind CSS for games. # * Layout: Center the game canvas/container prominently on the screen. Use appropriate margins and padding. # * Buttons &amp;amp; UI: Style buttons and other UI elements distinctively. Use techniques like shadows, gradients, borders, hover effects, and animations where appropriate. # * Font: Consider using game-appropriate fonts such as `'Press Start 2P'` (include the Google Font link: `&amp;lt;link href=&amp;quot;https://fonts.googleapis.com/css2?family=Press+Start+2P&amp;amp;display=swap&amp;quot; rel=&amp;quot;stylesheet&amp;quot;&amp;gt;`) or a monospace font. # * Functionality &amp;amp; Logic: # * External Resources: Do not load placeholders or files that you don't have access to. Avoid using external assets or files unless instructed to. Do not use base64 encoded data. # * Placeholders: Avoid using placeholders unless explicitly asked to. Code should work immediately. # * Planning &amp;amp; Comments: Plan game logic thoroughly. Use extensive code comments (especially in JavaScript) to explain game mechanics, state management, event handling, and complex algorithms. # * Game Speed: Tune game loop timing (e.g., using `requestAnimationFrame`) for optimal performance and playability. # * Controls: Include necessary game controls (e.g., Start, Pause, Restart, Volume). Place these controls neatly outside the main game area (e.g., in a top or bottom center row). # * No `alert()`: Display messages (e.g., game over, score updates) using in-page HTML elements (e.g., `&amp;lt;div&amp;gt;`, `&amp;lt;p&amp;gt;`) instead of the JavaScript `alert()` function. # * Libraries/Frameworks: Avoid complex external libraries or frameworks unless specifically requested. Focus on vanilla JavaScript where possible. # Final Directive: # Think step by step through what the user asks. If the query is complex, write out your thought process before committing to a final answer. Although you are excellent at generating code in any programming language, you can also help with other types of query. Not every output has to include code. Make sure to follow user instructions precisely. Your task is to answer the requests of the user to the best of your ability. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7olcw/google_diffusion_told_me_its_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7olcw/google_diffusion_told_me_its_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7olcw/google_diffusion_told_me_its_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T03:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7sz1l</id>
    <title>Apple is using a "Parallel-Track" MoE architecture in their edge models. Background information.</title>
    <updated>2025-06-10T07:53:57+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sz1l/apple_is_using_a_paralleltrack_moe_architecture/"&gt; &lt;img alt="Apple is using a &amp;quot;Parallel-Track&amp;quot; MoE architecture in their edge models. Background information." src="https://external-preview.redd.it/xwvAu1ztoOvOhx7n2EoT8sRix4FTcRO810CrbO3VhbM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40e5ffcb35a4ce46e023c171c46660884aebba49" title="Apple is using a &amp;quot;Parallel-Track&amp;quot; MoE architecture in their edge models. Background information." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://machinelearning.apple.com/research/apple-foundation-models-2025-updates"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sz1l/apple_is_using_a_paralleltrack_moe_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sz1l/apple_is_using_a_paralleltrack_moe_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T07:53:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7l39m</id>
    <title>Apple's On Device Foundation Models LLM is 3B quantized to 2 bits</title>
    <updated>2025-06-10T00:25:05+00:00</updated>
    <author>
      <name>/u/iKy1e</name>
      <uri>https://old.reddit.com/user/iKy1e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;The on-device model we just used is a large language model with &lt;strong&gt;3 billion parameters&lt;/strong&gt;, each quantized to &lt;strong&gt;2 bits&lt;/strong&gt;. It is several orders of magnitude bigger than any other models that are part of the operating system.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Source: Meet the Foundation Models framework&lt;br /&gt; Timestamp: 2:57&lt;br /&gt; URL: &lt;a href="https://developer.apple.com/videos/play/wwdc2025/286/?time=175"&gt;https://developer.apple.com/videos/play/wwdc2025/286/?time=175&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The framework also supports adapters:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;For certain common use cases, such as content tagging, we also provide specialized adapters that maximize the model’s capability in specific domains.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And structured output:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Generable type, you can make the model respond to prompts by generating an instance of your type.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And tool calling:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;At this phase, the FoundationModels framework will automatically call the code you wrote for these tools. The framework then automatically inserts the tool outputs back into the transcript. Finally, the model will incorporate the tool output along with everything else in the transcript to furnish the final response.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iKy1e"&gt; /u/iKy1e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7l39m/apples_on_device_foundation_models_llm_is_3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7l39m/apples_on_device_foundation_models_llm_is_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7l39m/apples_on_device_foundation_models_llm_is_3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T00:25:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7sr2b</id>
    <title>Vibe-coding without the 14-hour debug spirals</title>
    <updated>2025-06-10T07:38:44+00:00</updated>
    <author>
      <name>/u/Necessary-Tap5971</name>
      <uri>https://old.reddit.com/user/Necessary-Tap5971</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 2 years I've finally cracked the code on avoiding these infinite loops. Here's what actually works:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The 3-Strike Rule (aka &amp;quot;Stop Digging, You Idiot&amp;quot;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If AI fails to fix something after 3 attempts, STOP. Just stop. I learned this after watching my codebase grow from 2,000 lines to 18,000 lines trying to fix a dropdown menu. The AI was literally wrapping my entire app in try-catch blocks by the end.&lt;/p&gt; &lt;p&gt;What to do instead:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Screenshot the broken UI&lt;/li&gt; &lt;li&gt;Start a fresh chat session&lt;/li&gt; &lt;li&gt;Describe what you WANT, not what's BROKEN&lt;/li&gt; &lt;li&gt;Let AI rebuild that component from scratch&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Context Windows Are Not Your Friend&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's the dirty secret - after about 10 back-and-forth messages, the AI starts forgetting what the hell you're even building. I once had Claude convinced my AI voice platform was a recipe blog because we'd been debugging the persona switching feature for so long.&lt;/p&gt; &lt;p&gt;My rule: Every 8-10 messages, I:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Save working code to a separate file&lt;/li&gt; &lt;li&gt;Start fresh&lt;/li&gt; &lt;li&gt;Paste ONLY the relevant broken component&lt;/li&gt; &lt;li&gt;Include a one-liner about what the app does&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This cut my debugging time by ~70%.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. The &amp;quot;Explain Like I'm Five&amp;quot; Test&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you can't explain what's broken in one sentence, you're already screwed. I spent 6 hours once because I kept saying &amp;quot;the data flow is weird and the state management seems off but also the UI doesn't update correctly sometimes.&amp;quot;&lt;/p&gt; &lt;p&gt;Now I force myself to say things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Button doesn't save user data&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Page crashes on refresh&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Image upload returns undefined&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Simple descriptions = better fixes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Version Control Is Your Escape Hatch&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Git commit after EVERY working feature. Not every day. Not every session. EVERY. WORKING. FEATURE.&lt;/p&gt; &lt;p&gt;I learned this after losing 3 days of work because I kept &amp;quot;improving&amp;quot; working code until it wasn't working anymore. Now I commit like a paranoid squirrel hoarding nuts for winter.&lt;/p&gt; &lt;p&gt;My commits from last week:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;42 total commits&lt;/li&gt; &lt;li&gt;31 were rollback points&lt;/li&gt; &lt;li&gt;11 were actual progress&lt;/li&gt; &lt;li&gt;0 lost features&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;5. The Nuclear Option: Burn It Down&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Sometimes the code is so fucked that fixing it would take longer than rebuilding. I had to nuke our entire voice personality management system three times before getting it right.&lt;/p&gt; &lt;p&gt;If you've spent more than 2 hours on one bug:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Copy your core business logic somewhere safe&lt;/li&gt; &lt;li&gt;Delete the problematic component entirely&lt;/li&gt; &lt;li&gt;Tell AI to build it fresh with a different approach&lt;/li&gt; &lt;li&gt;Usually takes 20 minutes vs another 4 hours of debugging&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The infinite loop isn't an AI problem - it's a human problem of being too stubborn to admit when something's irreversibly broken.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Necessary-Tap5971"&gt; /u/Necessary-Tap5971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sr2b/vibecoding_without_the_14hour_debug_spirals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sr2b/vibecoding_without_the_14hour_debug_spirals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sr2b/vibecoding_without_the_14hour_debug_spirals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T07:38:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1l7sj45</id>
    <title>Mark Zuckerberg Personally Hiring to Create New “Superintelligence” AI Team</title>
    <updated>2025-06-10T07:23:13+00:00</updated>
    <author>
      <name>/u/gensandman</name>
      <uri>https://old.reddit.com/user/gensandman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sj45/mark_zuckerberg_personally_hiring_to_create_new/"&gt; &lt;img alt="Mark Zuckerberg Personally Hiring to Create New “Superintelligence” AI Team" src="https://external-preview.redd.it/PQVpxiJVBzVlTszQQQQivblrBqJ4eZAv8qWKEd5l9co.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62bc47ff5664f309fedad12ae6792557582ee0a4" title="Mark Zuckerberg Personally Hiring to Create New “Superintelligence” AI Team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gensandman"&gt; /u/gensandman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-06-10/zuckerberg-recruits-new-superintelligence-ai-group-at-meta?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTUzOTk2NCwiZXhwIjoxNzUwMTQ0NzY0LCJhcnRpY2xlSWQiOiJTWE1KNFlEV1JHRzAwMCIsImJjb25uZWN0SWQiOiJCQjA1NkM3NzlFMTg0MjU0OUQ3OTdCQjg1MUZBODNBMCJ9.oQD8-YVuo3p13zoYHc4VDnMz-MTkSU1vpwO3bBypUBY"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sj45/mark_zuckerberg_personally_hiring_to_create_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l7sj45/mark_zuckerberg_personally_hiring_to_create_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-10T07:23:13+00:00</published>
  </entry>
</feed>
