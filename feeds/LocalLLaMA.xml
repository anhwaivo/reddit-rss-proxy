<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-25T07:49:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ljqgxb</id>
    <title>Best tts and stt open source or cheap - NOT real time?</title>
    <updated>2025-06-24T23:30:48+00:00</updated>
    <author>
      <name>/u/dabble_</name>
      <uri>https://old.reddit.com/user/dabble_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seeing a lot of realtime qna when I was browsing and searching the sub, what about not real time? Ideally not insanely slow but I have no need for anything close to real time so higher quality audio would be preferred.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dabble_"&gt; /u/dabble_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljqgxb/best_tts_and_stt_open_source_or_cheap_not_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljqgxb/best_tts_and_stt_open_source_or_cheap_not_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljqgxb/best_tts_and_stt_open_source_or_cheap_not_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T23:30:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljz6sh</id>
    <title>How effective are LLMs at translating heavy context-based languages like Japanese, Korean, Thai, and others?</title>
    <updated>2025-06-25T07:18:39+00:00</updated>
    <author>
      <name>/u/GTurkistane</name>
      <uri>https://old.reddit.com/user/GTurkistane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of these languages rely deeply on cultural nuance, implied subjects, honorifics, and flexible grammar structures that don't map neatly to English or other Indo-European languages. For example:&lt;/p&gt; &lt;p&gt;Japanese often omits the subject and even the object, relying entirely on context.&lt;/p&gt; &lt;p&gt;Korean speech changes based on social hierarchy and uses multiple speech levels.&lt;/p&gt; &lt;p&gt;Thai and Vietnamese rely on particles, tone, and implied relationships to carry meaning.&lt;/p&gt; &lt;p&gt;So Can LLMs accurately interpret and preserve the intended meaning when so much depends on whatâ€™s not said?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTurkistane"&gt; /u/GTurkistane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljz6sh/how_effective_are_llms_at_translating_heavy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljz6sh/how_effective_are_llms_at_translating_heavy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljz6sh/how_effective_are_llms_at_translating_heavy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T07:18:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljup39</id>
    <title>LM Studio alternative for remote APIs?</title>
    <updated>2025-06-25T02:56:27+00:00</updated>
    <author>
      <name>/u/TrickyWidget</name>
      <uri>https://old.reddit.com/user/TrickyWidget</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. I need something that does all the things that LM Studio does, except for remote APIs instead of local.&lt;/p&gt; &lt;p&gt;I see things like Chatbox and SillyTavern, but I need something far more developer-oriented. Set all API parameters, system message, etc.&lt;/p&gt; &lt;p&gt;Any suggestions?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TrickyWidget"&gt; /u/TrickyWidget &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljup39/lm_studio_alternative_for_remote_apis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljup39/lm_studio_alternative_for_remote_apis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljup39/lm_studio_alternative_for_remote_apis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T02:56:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljuul0</id>
    <title>Using public to provide a Ai model for free?</title>
    <updated>2025-06-25T03:04:12+00:00</updated>
    <author>
      <name>/u/Pale_Ad_6029</name>
      <uri>https://old.reddit.com/user/Pale_Ad_6029</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently came upon this &lt;a href="https://mindcraft.riqvip.dev/andy-docs"&gt;https://mindcraft.riqvip.dev/andy-docs&lt;/a&gt; , it's a llama 8b finetuned for minecraft. The way it's being hosted interested me its relying on people hosting it for themselves and letting others use that compute power. Would there be potential to this with other larger models? I know this has been done in the past but never seen it succeed much&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pale_Ad_6029"&gt; /u/Pale_Ad_6029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljuul0/using_public_to_provide_a_ai_model_for_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljuul0/using_public_to_provide_a_ai_model_for_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljuul0/using_public_to_provide_a_ai_model_for_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T03:04:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljye3o</id>
    <title>Built an AI Notes Assistant Using Mistral 7B Instruct â€“ Feedback Welcome!</title>
    <updated>2025-06-25T06:26:39+00:00</updated>
    <author>
      <name>/u/anonymously_geek</name>
      <uri>https://old.reddit.com/user/anonymously_geek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljye3o/built_an_ai_notes_assistant_using_mistral_7b/"&gt; &lt;img alt="Built an AI Notes Assistant Using Mistral 7B Instruct â€“ Feedback Welcome!" src="https://preview.redd.it/p9b6u09to09f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1839f31598c39b80c04a798bc9235c6370138774" title="Built an AI Notes Assistant Using Mistral 7B Instruct â€“ Feedback Welcome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been building an AI-powered website called NexNotes AI, and wanted to share a bit of my journey here for folks working with open models.&lt;/p&gt; &lt;p&gt;Iâ€™m currently using Mistral 7B Instruct (via Together AI) to handle summarization ,flashcards, Q&amp;amp;A over user notes, article content,, and PDFs. Itâ€™s been surprisingly effective for structured outputs like:&lt;/p&gt; &lt;p&gt;TL;DR summaries of long documents&lt;/p&gt; &lt;p&gt;Extracting question-answer pairs from messy transcripts&lt;/p&gt; &lt;p&gt;Generating flashcards from textbook dumps&lt;/p&gt; &lt;p&gt;Since Togetherâ€™s free tier gives 60 RPM and sometimes throttles under load, Iâ€™ve recently added a fallback to Groq for overflow traffic (also using Mistral 7B or Mixtral when needed). The routing logic just switches providers based on rate-limiting headers.&lt;/p&gt; &lt;p&gt;So far, itâ€™s running smoothly, and Groqâ€™s speed is ðŸ”¥ â€” especially noticeable on longer inputs.&lt;/p&gt; &lt;p&gt;If you're building something similar or working with local/hosted open models, I'd love:&lt;/p&gt; &lt;p&gt;Tips on better prompting for Mistral 7B&lt;/p&gt; &lt;p&gt;Whether anyone here has self-hosted Mistral and seen better results&lt;/p&gt; &lt;p&gt;Any suggestions on better rate-limit handling across providers&lt;/p&gt; &lt;p&gt;Also, if anyone wants to check it out or give feedback,here's the link --&amp;gt; &lt;a href="https://nexnotes-ai.pages.dev"&gt;nexnotes ai &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anonymously_geek"&gt; /u/anonymously_geek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p9b6u09to09f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljye3o/built_an_ai_notes_assistant_using_mistral_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljye3o/built_an_ai_notes_assistant_using_mistral_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T06:26:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljyhkc</id>
    <title>Suggestions to build local voice assistant</title>
    <updated>2025-06-25T06:32:42+00:00</updated>
    <author>
      <name>/u/prashv</name>
      <uri>https://old.reddit.com/user/prashv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AIM&lt;/h1&gt; &lt;p&gt;I am looking to build a local running voice assistant that acts as a full time assistant with memory that helps me for the following: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Help me with my work related tasks (coding/business/analysis/mails/taking notes) &lt;ul&gt; &lt;li&gt;I should be able to attach media(s) and share it with my model/assistant&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Offer personalized suggestions for productivity depending on my personality/ambitions/areas of improvement&lt;/li&gt; &lt;li&gt;Acts as a therapist/counselor/friend with whom i can discuss personal emotions/thoughts&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Questions:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Is there any open source voice assistant already that offers the above&lt;/li&gt; &lt;li&gt;Any pointers/resources on how to build one?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any help or suggestions are welcome. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prashv"&gt; /u/prashv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyhkc/suggestions_to_build_local_voice_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyhkc/suggestions_to_build_local_voice_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyhkc/suggestions_to_build_local_voice_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T06:32:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnoj7</id>
    <title>AMD Instinct MI60 (32gb VRAM) "llama bench" results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected</title>
    <updated>2025-06-24T21:32:35+00:00</updated>
    <author>
      <name>/u/FantasyMaster85</name>
      <uri>https://old.reddit.com/user/FantasyMaster85</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt; &lt;img alt="AMD Instinct MI60 (32gb VRAM) &amp;quot;llama bench&amp;quot; results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected" src="https://b.thumbs.redditmedia.com/IWP60MgSnWzXR7H6EGZicI90kN9NpZLeyDsSansVnlA.jpg" title="AMD Instinct MI60 (32gb VRAM) &amp;quot;llama bench&amp;quot; results for 10 models - Qwen3 30B A3B Q4_0 resulted in: pp512 - 1,165 t/s | tg128 68 t/s - Overall very pleased and resulted in a better outcome for my use case than I even expected" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just completed a new build and (finally) have everything running as I wanted it to when I spec'd out the build. I'll be making a separate post about that as I'm now my own sovereign nation state for media, home automation (including voice activated commands), security cameras and local AI which I'm thrilled about...but, like I said, that's for a separate post.&lt;/p&gt; &lt;p&gt;This one is with regard to the MI60 GPU which I'm very happy with given my use case. I bought two of them on eBay, got one for right around $300 and the other for just shy of $500. Turns out I only need one as I can fit both of the models I'm using (one for HomeAssistant and the other for Frigate security camera feed processing) onto the same GPU with more than acceptable results. I might keep the second one for other models, but for the time being it's not installed. &lt;strong&gt;EDIT:&lt;/strong&gt; Forgot to mention I'm running Ubuntu 24.04 on the server.&lt;/p&gt; &lt;p&gt;For HomeAssistant I get results back in less than two seconds for voice activated commands like &amp;quot;it's a little dark in the living room and the cats are meowing at me because they're hungry&amp;quot; (it brightens the lights and feeds the cats, obviously). For Frigate it takes about 10 seconds after a camera has noticed an object of interest to return back what was observed (here is a copy/paste of an example of data returned from one of my camera feeds: &amp;quot;&lt;em&gt;Person detected. The person is a man wearing a black sleeveless top and red shorts. He is standing on the deck holding a drink. Given their casual demeanor this does not appear to be suspicious.&lt;/em&gt;&amp;quot;&lt;/p&gt; &lt;p&gt;Notes about the setup for the GPU, for some reason I'm unable to get the powercap set to anything higher than 225w (I've got a 1000w PSU, I've tried the physical switch on the card, I've looked for different vbios versions for the card and can't locate any...it's frustrating, but is what it is...it's supposed to be a 300tdp card). I was able to slightly increase it because while it won't allow me to change the powercap to anything higher, I was able to set the &amp;quot;overdrive&amp;quot; to allow for a 20% increase. With the cooling shroud for the GPU (photo at bottom of post) even at full bore, the GPU has never gone over 64 degrees Celsius&lt;/p&gt; &lt;p&gt;Here are some &amp;quot;llama-bench&amp;quot; results of various models that I was testing before settling on the two I'm using (noted below):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q4_K_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q4_K - Medium | 4.58 GiB | 8.03 B | ROCm | 99 | pp512 | 581.33 Â± 0.16 | | llama 8B Q4_K - Medium | 4.58 GiB | 8.03 B | ROCm | 99 | tg128 | 64.82 Â± 0.04 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/DeepSeek-R1-0528-Qwen3-8B-UD-Q8_K_XL.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3 8B Q8_0 | 10.08 GiB | 8.19 B | ROCm | 99 | pp512 | 587.76 Â± 1.04 | | qwen3 8B Q8_0 | 10.08 GiB | 8.19 B | ROCm | 99 | tg128 | 43.50 Â± 0.18 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Hermes-3-Llama-3.1-8B.Q8_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Hermes-3-Llama-3.1-8B.Q8_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q8_0 | 7.95 GiB | 8.03 B | ROCm | 99 | pp512 | 582.56 Â± 0.62 | | llama 8B Q8_0 | 7.95 GiB | 8.03 B | ROCm | 99 | tg128 | 52.94 Â± 0.03 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Meta-Llama-3-8B-Instruct.Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Meta-Llama-3-8B-Instruct.Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 8B Q4_0 | 4.33 GiB | 8.03 B | ROCm | 99 | pp512 | 1214.07 Â± 1.93 | | llama 8B Q4_0 | 4.33 GiB | 8.03 B | ROCm | 99 | tg128 | 70.56 Â± 0.12 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_0 | 12.35 GiB | 23.57 B | ROCm | 99 | pp512 | 420.61 Â± 0.18 | | llama 13B Q4_0 | 12.35 GiB | 23.57 B | ROCm | 99 | tg128 | 31.03 Â± 0.01 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_K - Medium | 13.34 GiB | 23.57 B | ROCm | 99 | pp512 | 188.13 Â± 0.03 | | llama 13B Q4_K - Medium | 13.34 GiB | 23.57 B | ROCm | 99 | tg128 | 27.37 Â± 0.03 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Mistral-Small-3.1-24B-Instruct-2503-UD-IQ2_M.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Mistral-Small-3.1-24B-Instruct-2503-UD-IQ2_M.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B IQ2_M - 2.7 bpw | 8.15 GiB | 23.57 B | ROCm | 99 | pp512 | 257.37 Â± 0.04 | | llama 13B IQ2_M - 2.7 bpw | 8.15 GiB | 23.57 B | ROCm | 99 | tg128 | 17.65 Â± 0.02 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;nexusraven-v2-13b.Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/nexusraven-v2-13b.Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | llama 13B Q4_0 | 6.86 GiB | 13.02 B | ROCm | 99 | pp512 | 704.18 Â± 0.29 | | llama 13B Q4_0 | 6.86 GiB | 13.02 B | ROCm | 99 | tg128 | 52.75 Â± 0.07 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Q4_0.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Qwen3-30B-A3B-Q4_0.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_0 | 16.18 GiB | 30.53 B | ROCm | 99 | pp512 | 1165.52 Â± 4.04 | | qwen3moe 30B.A3B Q4_0 | 16.18 GiB | 30.53 B | ROCm | 99 | tg128 | 68.26 Â± 0.13 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-32B-Q4_1.gguf&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin$ ./llama-bench -m /models/Qwen3-32B-Q4_1.gguf ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3 32B Q4_1 | 19.21 GiB | 32.76 B | ROCm | 99 | pp512 | 270.18 Â± 0.14 | | qwen3 32B Q4_1 | 19.21 GiB | 32.76 B | ROCm | 99 | tg128 | 21.59 Â± 0.01 | build: 8d947136 (5700) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a photo of the build for anyone interested (i9-14900k, 96gb RAM, total of 11 drives, a mix of NVME, HDD and SSD):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4uumjneh1y8f1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e928321bdce578095eea2c8a5a2a782f061bd5d0"&gt;https://preview.redd.it/4uumjneh1y8f1.jpg?width=3024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e928321bdce578095eea2c8a5a2a782f061bd5d0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FantasyMaster85"&gt; /u/FantasyMaster85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnoj7/amd_instinct_mi60_32gb_vram_llama_bench_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljrt7g</id>
    <title>WebBench: A real-world benchmark for Browser Agents</title>
    <updated>2025-06-25T00:32:43+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrt7g/webbench_a_realworld_benchmark_for_browser_agents/"&gt; &lt;img alt="WebBench: A real-world benchmark for Browser Agents" src="https://preview.redd.it/h8nloj5oxy8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79bf85459d92b50a112d7ba2230da0e95b42fc14" title="WebBench: A real-world benchmark for Browser Agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;WebBench is an open, task-oriented benchmark designed to measure how effectively browser agents handle complex, realistic web workflows. It includes 2,454 tasks across 452 live websites selected from the global top-1000 by traffic.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Halluminate/WebBench"&gt;https://github.com/Halluminate/WebBench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h8nloj5oxy8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrt7g/webbench_a_realworld_benchmark_for_browser_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrt7g/webbench_a_realworld_benchmark_for_browser_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:32:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljrwrq</id>
    <title>Does anyone else find Dots really impressive?</title>
    <updated>2025-06-25T00:37:22+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Dots and I find it really impressive. It's my current favorite model. It's knowledgeable, uncensored and has a bit of attitude. Its uncensored in that it will not only talk about TS, it will do so in great depth. If you push it about something, it'll show some attitude by being sarcastic. I like that. It's more human.&lt;/p&gt; &lt;p&gt;The only thing that baffles me about Dots is since it was trained on Rednote, why does it speak English so well? Rednote is in Chinese.&lt;/p&gt; &lt;p&gt;What do others think about it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrwrq/does_anyone_else_find_dots_really_impressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrwrq/does_anyone_else_find_dots_really_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljrwrq/does_anyone_else_find_dots_really_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:37:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljz16o</id>
    <title>OMG i can finally post something here.</title>
    <updated>2025-06-25T07:08:25+00:00</updated>
    <author>
      <name>/u/GTurkistane</name>
      <uri>https://old.reddit.com/user/GTurkistane</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have tried to post multiple times in this subreddit and it is always automatically removed saying &amp;quot;awating moderator approve&amp;quot; or something similar and it was never approved, i tried contacting the old mods and no one replied, i learned then that the old &amp;quot;mods&amp;quot; was literally one person with multiple automods, who was also a mod in almost every LLM or AI subreddit and he never really does anything, so i made a post about it to criticize him and get the sub attention but it was in the &amp;quot;awating moderator approve&amp;quot; and never approved so i just gave up.&lt;/p&gt; &lt;p&gt;Thanks &lt;a href="/u/HOLUPREDICTIONS"&gt;u/HOLUPREDICTIONS&lt;/a&gt; !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTurkistane"&gt; /u/GTurkistane &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljz16o/omg_i_can_finally_post_something_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljz16o/omg_i_can_finally_post_something_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljz16o/omg_i_can_finally_post_something_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T07:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm2n2</id>
    <title>Polaris: A Post-training recipe for scaling RL on Advanced ReasonIng models</title>
    <updated>2025-06-24T20:28:55+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ChenxinAn-fdu/POLARIS"&gt;Here is the link.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have no idea what it is but it was released a few days ago and has an intriguing concept so I decided to post here to see if anyone knows about this. It seems pretty new but its some sort of post-training RL with a unique approach that claims a Qwen3-4b performance boost that surpasses Claude-4-Opus, Grok-3-Beta, and o3-mini-high.&lt;/p&gt; &lt;p&gt;Take it with a grain of salt. I am not in any way affiliated with this project. Someone simply recommended it to me so I posted it here to gather your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm2n2/polaris_a_posttraining_recipe_for_scaling_rl_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:28:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljw728</id>
    <title>NeuralTranslate: Nahuatl to Spanish LLM! (Gemma 3 27b fine-tune)</title>
    <updated>2025-06-25T04:15:40+00:00</updated>
    <author>
      <name>/u/Azuriteh</name>
      <uri>https://old.reddit.com/user/Azuriteh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! After quite a long time there's a new release from my open-source series of models: NeuralTranslate!&lt;/p&gt; &lt;p&gt;This time I full fine-tuned Gemma 3 27b on a Nahuatl-Spanish dataset. It comes with 3 versions: v1, v1.1 &amp;amp; v1.2. v1 is the epoch 4 checkpoint for the model, v1.1 is for epoch 9 &amp;amp; v1.2 is for epoch 10. I've seen great results with the v1.2 version and the demo for the model actually uses that one! But there might be some overfitting... I haven't thoroughly tested the checkpoints yet. v1 is the main release and shouldn't be presenting signs of overfitting from my limited testing, though!&lt;/p&gt; &lt;p&gt;Here is the demo: &lt;a href="https://huggingface.co/spaces/Thermostatic/neuraltranslate-27b-mt-nah-es"&gt;https://huggingface.co/spaces/Thermostatic/neuraltranslate-27b-mt-nah-es&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are the weights:&lt;/p&gt; &lt;p&gt;- v1: &lt;a href="https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1"&gt;https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- v1.1: &lt;a href="https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1.1"&gt;https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- v1.2: &lt;a href="https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1.2"&gt;https://huggingface.co/Thermostatic/neuraltranslate-27b-mt-nah-es-v1.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've contacted a few knowledgeable nahuatl speakers and it seems that the dataset itself is archaic, so sadly the model itself it's not as good as I'd wish I wanted, but hopefully I can overcome those issues in future releases! Currently working in creating the v1 of NeuralTranslate English to Spanish and will be releasing it shortly :)&lt;/p&gt; &lt;p&gt;I fine-tuned the model using a B200 with the help of Unsloth (4-bit full fine-tuning is a game changer). You can easily recreate my workflow with my public repo for training LLMs in QLoRa &amp;amp; Full fine-tune with Unsloth too: &lt;a href="https://github.com/Sekinal/neuraltranslate-nahuatl/tree/master"&gt;https://github.com/Sekinal/neuraltranslate-nahuatl/tree/master&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hopefully this isn't taken as spam, I'm really not trying to make a profit nor anything like that, I just think the model itself or my workflow would be of help for a lot of people and this is a really exciting project I wanted to share!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Azuriteh"&gt; /u/Azuriteh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljw728/neuraltranslate_nahuatl_to_spanish_llm_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljw728/neuraltranslate_nahuatl_to_spanish_llm_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljw728/neuraltranslate_nahuatl_to_spanish_llm_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T04:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljogsx</id>
    <title>LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs</title>
    <updated>2025-06-24T22:05:08+00:00</updated>
    <author>
      <name>/u/BumbleSlob</name>
      <uri>https://old.reddit.com/user/BumbleSlob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"&gt; &lt;img alt="LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs" src="https://external-preview.redd.it/ZSkXOQ0Ftmzf9m07Ydba1-71lECRPh1WZMhCFovef6Y.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1fdb319a25ca00eba0456ee1f02c9bf5308cdb5e" title="LinusTechTips reviews Chinese 4090s with 48Gb VRAM, messes with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just thought it might be fun for the community to see one of the largest tech YouTubers introducing their audience to local LLMs.&lt;/p&gt; &lt;p&gt;Lots of newbie mistakes in their messing with Open WebUI and Ollama but hopefully it encourages some of their audience to learn more. For anyone who saw the video and found their way here, welcome! Feel free to ask questions about getting started.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BumbleSlob"&gt; /u/BumbleSlob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/HZgQp-WDebU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljogsx/linustechtips_reviews_chinese_4090s_with_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljs4e7</id>
    <title>All of our posts for the last week:</title>
    <updated>2025-06-25T00:47:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs4e7/all_of_our_posts_for_the_last_week/"&gt; &lt;img alt="All of our posts for the last week:" src="https://preview.redd.it/0feqhgvc0z8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb98aa33f9a72ba846bb3609af050401518880f2" title="All of our posts for the last week:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0feqhgvc0z8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs4e7/all_of_our_posts_for_the_last_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs4e7/all_of_our_posts_for_the_last_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljo4ns</id>
    <title>New Moondream 2B VLM update, with visual reasoning</title>
    <updated>2025-06-24T21:51:07+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-2025-06-21-release"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4ns/new_moondream_2b_vlm_update_with_visual_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljo4ns/new_moondream_2b_vlm_update_with_visual_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljxa2e</id>
    <title>Gemini CLI: your open-source AI agent</title>
    <updated>2025-06-25T05:18:00+00:00</updated>
    <author>
      <name>/u/adefa</name>
      <uri>https://old.reddit.com/user/adefa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljxa2e/gemini_cli_your_opensource_ai_agent/"&gt; &lt;img alt="Gemini CLI: your open-source AI agent" src="https://external-preview.redd.it/v_nU-59VjAFg3tUf3ktH0OR1eDLLCpt7sTIO-4lpiic.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66dc977cf68889558dd1e0a18ef318dff22dc727" title="Gemini CLI: your open-source AI agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Really generous free tier&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adefa"&gt; /u/adefa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/technology/developers/introducing-gemini-cli/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljxa2e/gemini_cli_your_opensource_ai_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljxa2e/gemini_cli_your_opensource_ai_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T05:18:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljp29d</id>
    <title>So, what do people think about the new Mistral Small 3.2?</title>
    <updated>2025-06-24T22:30:10+00:00</updated>
    <author>
      <name>/u/TacticalRock</name>
      <uri>https://old.reddit.com/user/TacticalRock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering why the sub was so quiet lately, but alas, what're your thoughts so far?&lt;/p&gt; &lt;p&gt;I for one welcome the decreased repetition, solid &amp;quot;minor&amp;quot; update.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TacticalRock"&gt; /u/TacticalRock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljp29d/so_what_do_people_think_about_the_new_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljp29d/so_what_do_people_think_about_the_new_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljp29d/so_what_do_people_think_about_the_new_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnmj9</id>
    <title>Google researcher requesting feedback on the next Gemma.</title>
    <updated>2025-06-24T21:30:18+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt; &lt;img alt="Google researcher requesting feedback on the next Gemma." src="https://a.thumbs.redditmedia.com/YXztzxUAkpa8OQtPRt3lxinca8NVcah5DIxz1ZPOgn4.jpg" title="Google researcher requesting feedback on the next Gemma." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kr52i2mn0y8f1.png?width=700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f654b4d8fc807a8722055201e8c097168452937f"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/osanseviero/status/1937453755261243600"&gt;https://x.com/osanseviero/status/1937453755261243600&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm gpu poor. 8-12B models are perfect for me. What are yout thoughts ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnmj9/google_researcher_requesting_feedback_on_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljr1wn</id>
    <title>Where is OpenAI's open source model?</title>
    <updated>2025-06-24T23:57:16+00:00</updated>
    <author>
      <name>/u/_Vedr</name>
      <uri>https://old.reddit.com/user/_Vedr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did I miss something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Vedr"&gt; /u/_Vedr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljr1wn/where_is_openais_open_source_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T23:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljs95d</id>
    <title>ThermoAsk: getting an LLM to set its own temperature</title>
    <updated>2025-06-25T00:54:24+00:00</updated>
    <author>
      <name>/u/tycho_brahes_nose_</name>
      <uri>https://old.reddit.com/user/tycho_brahes_nose_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"&gt; &lt;img alt="ThermoAsk: getting an LLM to set its own temperature" src="https://preview.redd.it/t8az5arc1z8f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40ab4b4271e74985945a33ea726d1e36e0b0897b" title="ThermoAsk: getting an LLM to set its own temperature" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got an LLM to dynamically adjust its own sampling temperature.&lt;/p&gt; &lt;p&gt;I wrote a blog post on how I did this and why dynamic temperature adjustment might be a valuable ability for a language model to possess: &lt;a href="http://amanvir.com/blog/getting-an-llm-to-set-its-own-temperature"&gt;amanvir.com/blog/getting-an-llm-to-set-its-own-temperature&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: LLMs can struggle with prompts that inherently require large changes in sampling temperature for sensible or accurate responses. This includes simple prompts like &amp;quot;pick a random number from &amp;lt;some range&amp;gt;&amp;quot; and more complex stuff like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Solve the following math expression: &amp;quot;1 + 5 * 3 - 4 / 2&amp;quot;. Then, write a really abstract poem that contains the answer to this expression.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Tackling these prompts with a &amp;quot;default&amp;quot; temperature value will not lead to good responses. To solve this problem, I had the idea of allowing LLMs to request changes to their own temperature based on the task they were dealing with. To my knowledge, this is the first time such a system has been proposed, so I thought I'd use the opportunity to give this technique a name: &lt;strong&gt;ThermoAsk&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I've created a basic implementation of ThermoAsk that relies on Ollama's Python SDK and Qwen2.5-7B: &lt;a href="https://github.com/amanvirparhar/thermoask"&gt;github.com/amanvirparhar/thermoask&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'd love to hear your thoughts on this approach!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tycho_brahes_nose_"&gt; /u/tycho_brahes_nose_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8az5arc1z8f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljs95d/thermoask_getting_an_llm_to_set_its_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T00:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljnhca</id>
    <title>Made an LLM Client for the PS Vita</title>
    <updated>2025-06-24T21:24:23+00:00</updated>
    <author>
      <name>/u/ajunior7</name>
      <uri>https://old.reddit.com/user/ajunior7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt; &lt;img alt="Made an LLM Client for the PS Vita" src="https://external-preview.redd.it/Y283aGV6aXd6eDhmMfIP8BrPficmhyY5KB42Ptrwyms9E-ke6lpIPgzOipjX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40daff1e17d68cd71479175d661e93123af22f55" title="Made an LLM Client for the PS Vita" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, awhile back I had ported llama2.c on the PS Vita for on-device inference using the TinyStories 260K &amp;amp; 15M checkpoints. Was a cool and fun concept to work on, but it wasn't too practical in the end.&lt;/p&gt; &lt;p&gt;Since then, I have made a full fledged LLM client for the Vita instead! You can even use the camera to take photos to send to models that support vision. In this demo I gave it an endpoint to test out vision and reasoning models, and I'm happy with how it all turned out. It isn't perfect, as LLMs like to display messages in fancy ways like using TeX and markdown formatting, so it shows that in its raw text. The Vita can't even do emojis!&lt;/p&gt; &lt;p&gt;You can download the vpk in the releases section of my repo. Throw in an endpoint and try it yourself! (If using an API key, I hope you are very patient in typing that out manually)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/callbacked/vela"&gt;https://github.com/callbacked/vela&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ajunior7"&gt; /u/ajunior7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qunyr1jwzx8f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljnhca/made_an_llm_client_for_the_ps_vita/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T21:24:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljpo64</id>
    <title>I gave the same silly task to ~70 models that fit on 32GB of VRAM - thousands of times (resharing my post from /r/LocalLLM)</title>
    <updated>2025-06-24T22:56:20+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd posted this over at &lt;a href="/r/LocalLLM"&gt;/r/LocalLLM&lt;/a&gt; and Some people thought I presented this too much as serious research - it wasn't, it was much closer to a bored rainy day activity. So here's the post I've been waiting to make on &lt;a href="/r/LocalLLaMA"&gt;/r/LocalLLaMA&lt;/a&gt; for some time, simplified as casually as possible:&lt;/p&gt; &lt;p&gt;Quick recap - &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lbfinu/26_quants_that_fit_on_32gb_vs_10000token_needle/"&gt;here is the original post&lt;/a&gt; from a few weeks ago where users suggested I greatly expand the scope of this little game. &lt;a href="https://old.reddit.com/r/LocalLLM/comments/1liy7ku/i_thousands_of_tests_on_104_different_ggufs_10k/"&gt;Here is the post on /r/LocalLLM&lt;/a&gt; yesterday that I imagine some of you saw. I hope you don't mind the cross-post - but &lt;em&gt;THIS&lt;/em&gt; is the subreddit that I really wanted to bounce this off of and yesterday it was going through a change-of-management :-)&lt;/p&gt; &lt;p&gt;To be as brief/casual as possible: I broke HG Well's &lt;em&gt;&amp;quot;The Time Machine&amp;quot;&lt;/em&gt; again with a sentence that was correct English, but contextually nonsense, and asked a bunch of quantized LLM's (all that fit with 16k context on 32GB of VRAM). I did this multiple times at all temperatures from 0.0 to 0.9 in steps of 0.1 . For models with optional reasoning I split thinking mode on and off.&lt;/p&gt; &lt;h2&gt;What should you take from this?&lt;/h2&gt; &lt;p&gt;nothing at all! I'm hoping to get a better feel for how quantization works on some of my favorite models, so will take a little thing I do during my day and repeat it thousands and thousands of times to see if patterns emerge. I share this dataset with you for fun. I have my takeaways, I'd be interested to hear yours. My biggest takeaway from this is that I built a little framework of scripts for myself that will run and evaluate these sorts of tests at whatever scale I set them to.&lt;/p&gt; &lt;h2&gt;The Results&lt;/h2&gt; &lt;p&gt;Without further ado, the results. The 'Score' column is a percentage of correct answers.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Quant&lt;/th&gt; &lt;th&gt;Reasoning&lt;/th&gt; &lt;th&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Meta Llama Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.2_3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_8B_Instruct&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_70B_Instruct&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_4_Scout_17B&lt;/td&gt; &lt;td&gt;iq1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_4_Scout_17B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nvidia Nemotron Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_Nemotron_8B_UltraLong&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.1_Nemotron_8B_UltraLong&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama_3.3_Nemotron_Super_49B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Mistral Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral_Small_24B_2503&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Microsoft Phi Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Phi_4&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Alibaba Qwen Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_14B_Instruct&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_Coder_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5_Coder_32B_Instruct&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;QwQ_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_14B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A3B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_30B_A6B_16_Extreme&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Google Gemma Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_12B_IT&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma_3_27B_IT&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Deepseek (Distill) Family&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Qwen3_8B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek_R1_Distill_Qwen_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Other&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;iq3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cogito&lt;em&gt;v1_Preview&lt;/em&gt;&lt;em&gt;Qwen_14B&lt;/em&gt;&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;thinking&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepHermes_3_Mistral_24B_Preview&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;nothink&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;iq4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;q5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM_4_32B&lt;/td&gt; &lt;td&gt;q6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljpo64/i_gave_the_same_silly_task_to_70_models_that_fit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T22:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljm3pb</id>
    <title>LocalLlama is saved!</title>
    <updated>2025-06-24T20:30:08+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LocalLlama has been many folk's favorite place to be for everything AI, so it's good to see a new moderator taking the reins!&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/HOLUPREDICTIONS"&gt;u/HOLUPREDICTIONS&lt;/a&gt; for taking the reins!&lt;/p&gt; &lt;p&gt;More detail here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR - the previous moderator (we appreciate their work) unfortunately left the subreddit, and unfortunately deleted new comments and posts - it's now lifted!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljm3pb/localllama_is_saved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:30:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljlr5b</id>
    <title>Subreddit back in business</title>
    <updated>2025-06-24T20:16:36+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt; &lt;img alt="Subreddit back in business" src="https://preview.redd.it/1sx7mwusnx8f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f5a6313e8a4b034a44e79151a371760d959973" title="Subreddit back in business" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As most of you folks I'm also not sure what happened but I'm attaching screenshot of the last actions taken by the previous moderator before deleting their account &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1sx7mwusnx8f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-24T20:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ljyo2p</id>
    <title>Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)</title>
    <updated>2025-06-25T06:44:26+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt; &lt;img alt="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" src="https://external-preview.redd.it/MDRyeGJ6bmJvMDlmMdx7LrexgFcEoZTqX8Yp_PzSREeGDqUB-Qd2XY93v_7d.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c71ebd7c03a7ccb476c3ff52d6b9e5cc00e65722" title="Jan-nano-128k: A 4B Model with a Super-Long Context Window (Still Outperforms 671B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone it's me from Menlo Research again,&lt;/p&gt; &lt;p&gt;Today, I'd like to introduce our latest model: &lt;strong&gt;Jan-nano-128k&lt;/strong&gt; - this model is fine-tuned on &lt;strong&gt;Jan-nano&lt;/strong&gt; (which is a qwen3 finetune), improve performance when enable YaRN scaling &lt;strong&gt;(instead of having degraded performance)&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It can uses tools continuously, repeatedly. &lt;/li&gt; &lt;li&gt;It can perform deep research &lt;strong&gt;VERY VERY DEEP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Extremely persistence (please pick the right MCP as well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Again, we are not trying to beat Deepseek-671B models, we just want to see how far this current model can go. To our surprise, &lt;strong&gt;it is going very very far.&lt;/strong&gt; Another thing, we have spent all the resource on this version of Jan-nano so.... &lt;/p&gt; &lt;p&gt;&lt;strong&gt;We pushed back the technical report release! But it's coming ...sooon!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can find the model at:&lt;br /&gt; &lt;a href="https://huggingface.co/Menlo/Jan-nano-128k"&gt;https://huggingface.co/Menlo/Jan-nano-128k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have gguf at:&lt;br /&gt; &lt;strong&gt;We are converting the GGUF check in comment section&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This model will require &lt;strong&gt;YaRN Scaling&lt;/strong&gt; supported from inference engine, we already configure it in the model, but your inference engine will need to be able to handle YaRN scaling. Please run the model in l&lt;strong&gt;lama.server or Jan app&lt;/strong&gt; (these are from our team, we tested them, just it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;SimpleQA:&lt;/strong&gt;&lt;br /&gt; - OpenAI o1: 42.6&lt;br /&gt; - Grok 3: 44.6&lt;br /&gt; - 03: 49.4&lt;br /&gt; - Claude-3.7-Sonnet: 50.0&lt;br /&gt; - Gemini-2.5 pro: 52.9&lt;br /&gt; &lt;strong&gt;- baseline-with-MCP: 59.2&lt;/strong&gt;&lt;br /&gt; - ChatGPT-4.5: 62.5&lt;br /&gt; &lt;strong&gt;- deepseek-671B-with-MCP: 78.2&lt;/strong&gt; (we benchmark using openrouter)&lt;br /&gt; - jan-nano-v0.4-with-MCP: 80.7&lt;br /&gt; &lt;strong&gt;- jan-nano-128k-with-MCP: 83.2&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/909kwwnbo09f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ljyo2p/jannano128k_a_4b_model_with_a_superlong_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-25T06:44:26+00:00</published>
  </entry>
</feed>
