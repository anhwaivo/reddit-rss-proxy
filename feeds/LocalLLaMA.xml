<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-23T05:49:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kss44x</id>
    <title>Tiny agents from hugging face is great for llama.cpp mcp agents</title>
    <updated>2025-05-22T14:28:11+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tiny agents have to be the easiest browsers control setup, you just the cli, a json, and a prompt definition. &lt;/p&gt; &lt;p&gt;- it uses main MCPs, like Playright, mcp-remote&lt;br /&gt; - works with local models via openai compatible server&lt;br /&gt; - model can controls the browser or local files without calling APIs&lt;/p&gt; &lt;p&gt;here's a tutorial form the MCP course &lt;a href="https://huggingface.co/learn/mcp-course/unit2/tiny-agents"&gt;https://huggingface.co/learn/mcp-course/unit2/tiny-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kss44x/tiny_agents_from_hugging_face_is_great_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kss44x/tiny_agents_from_hugging_face_is_great_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kss44x/tiny_agents_from_hugging_face_is_great_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T14:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktat5b</id>
    <title>Big base models? (Not instruct tuned)</title>
    <updated>2025-05-23T04:25:39+00:00</updated>
    <author>
      <name>/u/RedditAddict6942O</name>
      <uri>https://old.reddit.com/user/RedditAddict6942O</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was disappointed to see that Qwen3 didn't release base models for anything over 30b.&lt;/p&gt; &lt;p&gt;Sucks because QLoRa fine tuning is affordable even on 100b+ models. &lt;/p&gt; &lt;p&gt;What are the best large open base models we have right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedditAddict6942O"&gt; /u/RedditAddict6942O &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktat5b/big_base_models_not_instruct_tuned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktat5b/big_base_models_not_instruct_tuned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktat5b/big_base_models_not_instruct_tuned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T04:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksklse</id>
    <title>I saw a project that I'm interested in: 3DTown: Constructing a 3D Town from a Single Image</title>
    <updated>2025-05-22T07:15:06+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksklse/i_saw_a_project_that_im_interested_in_3dtown/"&gt; &lt;img alt="I saw a project that I'm interested in: 3DTown: Constructing a 3D Town from a Single Image" src="https://external-preview.redd.it/emh3Y3JjbjlhYTJmMdq-zCDOPop6wDopQzw_Axrs5Q3Ewmi7BuHyc4moiH9c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=334a76aa4fd56af8a4b415b1555c615a82e68a46" title="I saw a project that I'm interested in: 3DTown: Constructing a 3D Town from a Single Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to the official description, &lt;strong&gt;3DTown outperforms state-of-the-art baselines, including Trellis, Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and texture fidelity.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6as4adn9aa2f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksklse/i_saw_a_project_that_im_interested_in_3dtown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksklse/i_saw_a_project_that_im_interested_in_3dtown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T07:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt7u1n</id>
    <title>BTW: If you are getting a single GPU, VRAM is not the only thing that matters</title>
    <updated>2025-05-23T01:44:05+00:00</updated>
    <author>
      <name>/u/pneuny</name>
      <uri>https://old.reddit.com/user/pneuny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For example, if you have a 5060 Ti 16GB or an RX 9070 XT 16GB and use Qwen 3 30b-a3b q4_k_m with 16k context, you will likely overflow around 8.5GB to system memory. Assuming you do not do CPU offloading, that load now runs squarely on PCIE bandwidth and your system RAM speed. PCIE 5 x16 on the RX 9070 XT is going to help you a lot in feeding that GPU compared to the PCIE 5 x8 available on the 5060 Ti, resulting in much faster tokens per second for the 9070 XT, and making CPU offloading unnecessary in this scenario, whereas the 5060 Ti will become heavily bottlenecked.&lt;/p&gt; &lt;p&gt;While I returned my 5060 Ti for a 9070 XT and didn't get numbers for the former, I did see 42 t/s while the VRAM was overloaded to this degree on the Vulkan backend. Also, AMD does Vulkan way better then Nvidia, as Nvidia tends to crash when using Vulkan.&lt;/p&gt; &lt;p&gt;TL;DR: If you're buying a 16GB card and planning to use more than that, make sure you can leverage x16 PCIE 5 or you won't get the full performance from overflowing to DDR5 system RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pneuny"&gt; /u/pneuny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7u1n/btw_if_you_are_getting_a_single_gpu_vram_is_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7u1n/btw_if_you_are_getting_a_single_gpu_vram_is_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7u1n/btw_if_you_are_getting_a_single_gpu_vram_is_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T01:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktad7a</id>
    <title>Anyone using MedGemma 27B?</title>
    <updated>2025-05-23T04:00:23+00:00</updated>
    <author>
      <name>/u/DeGreiff</name>
      <uri>https://old.reddit.com/user/DeGreiff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed MedGemma 27B is text-only, instruction-tuned (for inference-time compute), while 4B is the multimodal version. Interesting decision by Google.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeGreiff"&gt; /u/DeGreiff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktad7a/anyone_using_medgemma_27b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktad7a/anyone_using_medgemma_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktad7a/anyone_using_medgemma_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T04:00:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksmiwz</id>
    <title>👀 New Gemma 3n (E4B Preview) from Google Lands on Hugging Face - Text, Vision &amp; More Coming!</title>
    <updated>2025-05-22T09:34:51+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"&gt; &lt;img alt="👀 New Gemma 3n (E4B Preview) from Google Lands on Hugging Face - Text, Vision &amp;amp; More Coming!" src="https://b.thumbs.redditmedia.com/qSkeoL3Zvn8J5J0e71_KbF_aotj9r_uZphIqZ9sA98I.jpg" title="👀 New Gemma 3n (E4B Preview) from Google Lands on Hugging Face - Text, Vision &amp;amp; More Coming!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google has released a new preview version of their Gemma 3n model on Hugging Face: google/gemma-3n-E4B-it-litert-preview&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rhsk7xjiza2f1.png?width=1999&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af883983fb94351cc341740a3fbd7f89f2144b20"&gt;https://preview.redd.it/rhsk7xjiza2f1.png?width=1999&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af883983fb94351cc341740a3fbd7f89f2144b20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some key takeaways from the model card:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodal Input:&lt;/strong&gt; This model is designed to handle text, image, video, and audio input, generating text outputs. The current checkpoint on Hugging Face supports text and vision input, with full multimodal features expected soon.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Architecture:&lt;/strong&gt; Gemma 3n models feature a novel architecture that allows them to run with a smaller number of effective parameters (E2B and E4B variants mentioned). They also utilize a Matformer architecture for nesting multiple models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low-Resource Devices:&lt;/strong&gt; These models are specifically designed for efficient execution on low-resource devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Selective Parameter Activation:&lt;/strong&gt; This technology helps reduce resource requirements, allowing the models to operate at an effective size of 2B and 4B parameters.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Data:&lt;/strong&gt; Trained on a dataset of approximately 11 trillion tokens, including web documents, code, mathematics, images, and audio, with a knowledge cutoff of June 2024.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intended Uses:&lt;/strong&gt; Suited for tasks like content creation (text, code, etc.), chatbots, text summarization, and image/audio data extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Preview Version:&lt;/strong&gt; Keep in mind this is a preview version, intended for use with Google AI Edge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You'll need to agree to Google's usage license on Hugging Face to access the model files. You can find it by searching for google/gemma-3n-E4B-it-litert-preview on Hugging Face.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksmiwz/new_gemma_3n_e4b_preview_from_google_lands_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T09:34:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kstdhn</id>
    <title>Notes on AlphaEvolve: Are we closing in on Singularity?</title>
    <updated>2025-05-22T15:19:33+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepMind released the AlphaEvolve paper last week, which, considering what they have achieved, is arguably one of the most important papers of the year. But I found the discourse around it was very thin, not many who actively cover the AI space have talked much about it.&lt;/p&gt; &lt;p&gt;So, I made some notes on the important aspects of AlphaEvolve.&lt;/p&gt; &lt;h1&gt;Architecture Overview&lt;/h1&gt; &lt;p&gt;DeepMind calls it an &amp;quot;agent&amp;quot;, but it was not your run-of-the-mill agent, but a meta-cognitive system. The agent architecture has the following components&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Problem: An entire codebase or a part of it marked with # EVOLVE-BLOCK-START and # EVOLVE-BLOCK-END. Only this part of it will be evolved.&lt;/li&gt; &lt;li&gt;LLM ensemble: They used Gemini 2.0 Pro for complex reasoning and 2.5 flash for faster operations.&lt;/li&gt; &lt;li&gt;Evolutionary database: The most important part, the database uses map-elite and Island architecture to store solutions and inspirations. &lt;/li&gt; &lt;li&gt;Prompt Sampling: A combination of previous best results, inspirations, and human contexts for improving the existing solution.&lt;/li&gt; &lt;li&gt;Evaluation Framework: A Python function for evaluating the answers, and it returns array of scalars.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Working in brief&lt;/h1&gt; &lt;p&gt;The database maintains &amp;quot;parent&amp;quot; programs marked for improvement and &amp;quot;inspirations&amp;quot; for adding diversity to the solution. (The name &amp;quot;AlphaEvolve&amp;quot; itself actually comes from it being an &amp;quot;Alpha&amp;quot; series agent that &amp;quot;Evolves&amp;quot; solutions, rather than just this parent/inspiration idea).&lt;/p&gt; &lt;p&gt;Here’s how it generally flows: the AlphaEvolve system gets the initial codebase. Then, for each step, the &lt;strong&gt;prompt sampler&lt;/strong&gt; cleverly picks out parent program(s) to work on and some inspiration programs. It bundles these up with &lt;strong&gt;feedback from past attempts (like scores or even what an LLM thought about previous versions)&lt;/strong&gt;, plus any handy human context. This whole package goes to the LLMs.&lt;/p&gt; &lt;p&gt;The new solution they come up with (the &amp;quot;child&amp;quot;) gets graded by the &lt;strong&gt;evaluation function&lt;/strong&gt;. Finally, these child solutions, with their new grades, are stored back in the database.&lt;/p&gt; &lt;h1&gt;The Outcome&lt;/h1&gt; &lt;p&gt;The most interesting part even with older models like Gemini 2.0 Pro and Flash, when AlphaEvolve took on over 50 open math problems, it managed to match the best solutions out there for 75% of them, actually found better answers for another 20%, and only came up short on a tiny 5%!&lt;/p&gt; &lt;p&gt;Out of all, DeepMind is most proud of AlphaEvolve surpassing Strassen's 56-year-old algorithm for 4x4 complex matrix multiplication by finding a method with 48 scalar multiplications.&lt;/p&gt; &lt;p&gt;And also the agent improved Google's infra by speeding up Gemini LLM training by ~1%, improving data centre job scheduling to recover ~0.7% of fleet-wide compute resources, optimising TPU circuit designs, and accelerating compiler-generated code for AI kernels by up to 32%.&lt;/p&gt; &lt;p&gt;This is the best agent scaffolding to date. The fact that they pulled this off with an outdated Gemini, imagine what they can do with the current SOTA. This makes it one thing clear: what we're lacking for efficient agent swarms doing tasks is the right abstractions. Though the cost of operation is not disclosed.&lt;/p&gt; &lt;p&gt;For a detailed blog post, check this out: &lt;a href="https://composio.dev/blog/alphaevolve-evolutionary-agent-from-deepmind/"&gt;AlphaEvolve: the self-evolving agent from DeepMind&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It'd be interesting to see if they ever release it in the wild or if any other lab picks it up. This is certainly the best frontier for building agents.&lt;/p&gt; &lt;p&gt;Would love to know your thoughts on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kstdhn/notes_on_alphaevolve_are_we_closing_in_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kstdhn/notes_on_alphaevolve_are_we_closing_in_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kstdhn/notes_on_alphaevolve_are_we_closing_in_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T15:19:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktban0</id>
    <title>Dans-PersonalityEngine V1.3.0 12b &amp; 24b</title>
    <updated>2025-05-23T04:55:30+00:00</updated>
    <author>
      <name>/u/PocketDocLabs</name>
      <uri>https://old.reddit.com/user/PocketDocLabs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The latest release in the Dans-PersonalityEngine series. With any luck you should find it to be an improvement on almost all fronts as compared to V1.2.0.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.3.0-12b"&gt;https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.3.0-12b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.3.0-24b"&gt;https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.3.0-24b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A blog post regarding its development can be found &lt;a href="https://pocketdoclabs.com/making-dans-personalityengine-v130/"&gt;here&lt;/a&gt; for those interested in some rough technical details on the project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PocketDocLabs"&gt; /u/PocketDocLabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktban0/danspersonalityengine_v130_12b_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktban0/danspersonalityengine_v130_12b_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktban0/danspersonalityengine_v130_12b_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T04:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksuycv</id>
    <title>Microsoft releases Magentic-UI. Could this finally be a halfway-decent agentic browser use client that works on Windows?</title>
    <updated>2025-05-22T16:22:59+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksuycv/microsoft_releases_magenticui_could_this_finally/"&gt; &lt;img alt="Microsoft releases Magentic-UI. Could this finally be a halfway-decent agentic browser use client that works on Windows?" src="https://b.thumbs.redditmedia.com/-EDMkhP-sWJsAa9IAitRGUWZQaFdTPdRUYoDRA6C8Wk.jpg" title="Microsoft releases Magentic-UI. Could this finally be a halfway-decent agentic browser use client that works on Windows?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Magentic-One was kind of a cool agent framework for a minute when it was first released a few months ago, but DAMN, it was a pain in the butt to get working and then it kinda would just see a squirrel on a webpage and get distracted and such. I think AutoGen added Magentic as an Agent type in AutoGen, but then it kinda of fell off my radar until today when they released &lt;/p&gt; &lt;p&gt;Magentic-UI - &lt;a href="https://github.com/microsoft/Magentic-UI"&gt;https://github.com/microsoft/Magentic-UI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From their GitHub:&lt;/p&gt; &lt;p&gt;“Magentic-UI is a research prototype of a human-centered interface powered by a multi-agent system that can browse and perform actions on the web, generate and execute code, and generate and analyze files. Magentic-UI is especially useful for web tasks that require actions on the web (e.g., filling a form, customizing a food order), deep navigation through websites not indexed by search engines (e.g., filtering flights, finding a link from a personal site) or tasks that need web navigation and code execution (e.g., generate a chart from online data).&lt;/p&gt; &lt;p&gt;What differentiates Magentic-UI from other browser use offerings is its transparent and controllable interface that allows for efficient human-in-the-loop involvement. Magentic-UI is built using AutoGen and provides a platform to study human-agent interaction and experiment with web agents. Key features include:&lt;/p&gt; &lt;p&gt;🧑‍🤝‍🧑 Co-Planning: Collaboratively create and approve step-by-step plans using chat and the plan editor. 🤝 Co-Tasking: Interrupt and guide the task execution using the web browser directly or through chat. Magentic-UI can also ask for clarifications and help when needed. 🛡️ Action Guards: Sensitive actions are only executed with explicit user approvals. 🧠 Plan Learning and Retrieval: Learn from previous runs to improve future task automation and save them in a plan gallery. Automatically or manually retrieve saved plans in future tasks. 🔀 Parallel Task Execution: You can run multiple tasks in parallel and session status indicators will let you know when Magentic-UI needs your input or has completed the task.”&lt;/p&gt; &lt;p&gt;Supposedly you can use it with Ollama and other local LLM providers. I’ll be trying this out when I have some time. Anyone else got this working locally yet? WDYT of it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ksuycv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksuycv/microsoft_releases_magenticui_could_this_finally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksuycv/microsoft_releases_magenticui_could_this_finally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T16:22:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksy9hi</id>
    <title>🤝 Meet NVIDIA Llama Nemotron Nano 4B + Tutorial on Getting Started</title>
    <updated>2025-05-22T18:35:07+00:00</updated>
    <author>
      <name>/u/PDXcoder2000</name>
      <uri>https://old.reddit.com/user/PDXcoder2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;📹 New Tutorial: How to get started with Llama Nemotron Nano 4b:&lt;/em&gt; &lt;a href="https://youtu.be/HTPiUZ3kJto"&gt;&lt;em&gt;https://youtu.be/HTPiUZ3kJto&lt;/em&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;🤝 Meet NVIDIA Llama Nemotron Nano 4B, an open reasoning model that provides leading accuracy and compute efficiency across scientific tasks, coding, complex math, function calling, and instruction following for edge agents.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;✨ &lt;em&gt;Achieves higher accuracy and 50% higher throughput than other leading open models with 8 billion parameters&lt;/em&gt; &lt;/p&gt; &lt;p&gt;📗 &lt;em&gt;Supports hybrid reasoning, optimizing for inference cost&lt;/em&gt;&lt;/p&gt; &lt;p&gt;🧑‍💻 &lt;em&gt;Deploy at the edge with NVIDIA Jetson and NVIDIA RTX GPUs, maximizing security, and flexibility&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;📥 Now on Hugging Face:&lt;/em&gt; &lt;a href="https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1"&gt;&lt;em&gt;https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PDXcoder2000"&gt; /u/PDXcoder2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksy9hi/meet_nvidia_llama_nemotron_nano_4b_tutorial_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksy9hi/meet_nvidia_llama_nemotron_nano_4b_tutorial_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksy9hi/meet_nvidia_llama_nemotron_nano_4b_tutorial_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T18:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksjkhb</id>
    <title>Jan is now Apache 2.0</title>
    <updated>2025-05-22T06:03:22+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjkhb/jan_is_now_apache_20/"&gt; &lt;img alt="Jan is now Apache 2.0" src="https://external-preview.redd.it/URelWOcOKsdGwEnGYxMQqnu09GiloVzXPjQD9-QBbco.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58f6ee4e949835d86b3d3ceaef317ab0dc1752b1" title="Jan is now Apache 2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, we've just changed &lt;a href="https://jan.ai/"&gt;Jan&lt;/a&gt;'s license. &lt;/p&gt; &lt;p&gt;Jan has always been open-source, but the AGPL license made it hard for many teams to actually use it. Jan is now licensed under Apache 2.0, a more permissive, industry-standard license that works inside companies as well.&lt;/p&gt; &lt;p&gt;What this means:&lt;/p&gt; &lt;p&gt;– You can bring Jan into your org without legal overhead&lt;br /&gt; – You can fork it, modify it, ship it&lt;br /&gt; – You don't need to ask permission&lt;/p&gt; &lt;p&gt;This makes Jan easier to adopt. At scale. In the real world.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/menloresearch/jan/blob/dev/LICENSE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjkhb/jan_is_now_apache_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksjkhb/jan_is_now_apache_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T06:03:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kso7p2</id>
    <title>AMD Takes a Major Leap in Edge AI With ROCm; Announces Integration With Strix Halo APUs &amp; Radeon RX 9000 Series GPUs</title>
    <updated>2025-05-22T11:22:20+00:00</updated>
    <author>
      <name>/u/nostriluu</name>
      <uri>https://old.reddit.com/user/nostriluu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kso7p2/amd_takes_a_major_leap_in_edge_ai_with_rocm/"&gt; &lt;img alt="AMD Takes a Major Leap in Edge AI With ROCm; Announces Integration With Strix Halo APUs &amp;amp; Radeon RX 9000 Series GPUs" src="https://external-preview.redd.it/ZrbQ75vRAB5hVtrNdq8cJcDVR-h2KRgOrR5RepitAdo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d56d443870db73b9730484569753cff8b7852157" title="AMD Takes a Major Leap in Edge AI With ROCm; Announces Integration With Strix Halo APUs &amp;amp; Radeon RX 9000 Series GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nostriluu"&gt; /u/nostriluu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-takes-a-major-leap-in-edge-ai-with-rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kso7p2/amd_takes_a_major_leap_in_edge_ai_with_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kso7p2/amd_takes_a_major_leap_in_edge_ai_with_rocm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T11:22:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kta3re</id>
    <title>Is Claude 4 worse than 3.7 for anyone else?</title>
    <updated>2025-05-23T03:45:40+00:00</updated>
    <author>
      <name>/u/TrekkiMonstr</name>
      <uri>https://old.reddit.com/user/TrekkiMonstr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know, I know, whenever a model comes out you get people saying this, but it's on very concrete things for me, I'm not just biased against it. For reference, I'm comparing 4 Sonnet (concise) with 3.7 Sonnet (concise), no reasoning for either.&lt;/p&gt; &lt;p&gt;I asked it to calculate the total markup I paid at a gas station relative to the supermarket. I gave it quantities in a way I thought was clear (&amp;quot;I got three protein bars and three milks, one of the others each. What was the total markup I paid?&amp;quot;, but that's later in the conversation after it searched for prices). And indeed, 3.7 understands this without any issue (and I regenerated the message to make sure it wasn't a fluke). But with 4, even with much back and forth and several regenerations, it kept interpreting this as 3 milk, 1 protein bar, 1 [other item], 1 [other item], until I very explicitly laid it out as I just did.&lt;/p&gt; &lt;p&gt;And then, another conversation, I ask it, &amp;quot;Does this seem correct, or too much?&amp;quot; with a photo of food, and macro estimates for the meal in a screenshot. Again, 3.7 understands this fine, as asking whether the figures seem to be an accurate estimate. Whereas 4, again with a couple regenerations to test, seems to think I'm asking whether it's an appropriate meal (as in, not too much food for dinner or whatever). And in one instance, misreads the screenshot (thinking that the number of calories I will have cumulatively eaten after that meal is the number of calories &lt;em&gt;of&lt;/em&gt; that meal).&lt;/p&gt; &lt;p&gt;Is anyone else seeing any issues like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TrekkiMonstr"&gt; /u/TrekkiMonstr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kta3re/is_claude_4_worse_than_37_for_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kta3re/is_claude_4_worse_than_37_for_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kta3re/is_claude_4_worse_than_37_for_anyone_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T03:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ktabgk</id>
    <title>How to get the most out of my AMD 7900XT?</title>
    <updated>2025-05-23T03:57:34+00:00</updated>
    <author>
      <name>/u/crispyfrybits</name>
      <uri>https://old.reddit.com/user/crispyfrybits</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was forced to sell my Nvidia 4090 24GB this week to pay rent 😭. I didn't know you could be so emotionally attached to a video card. &lt;/p&gt; &lt;p&gt;Anyway, my brother lent me his 7900XT until his rig is ready. I was just getting into local AI and want to continue. I've heard AMD is hard to support.&lt;/p&gt; &lt;p&gt;Can anyone help get me started on the right foot and advise what I need to get the most out this card?&lt;/p&gt; &lt;p&gt;Specs - Windows 11 Pro 64bit - AMD 7800X3D - AMD 7900XT 20GB - 32GB DDR5&lt;/p&gt; &lt;p&gt;Previously installed tools - Ollama - LM Studio&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crispyfrybits"&gt; /u/crispyfrybits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktabgk/how_to_get_the_most_out_of_my_amd_7900xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ktabgk/how_to_get_the_most_out_of_my_amd_7900xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ktabgk/how_to_get_the_most_out_of_my_amd_7900xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T03:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt99hi</id>
    <title>Building a real-world LLM agent with open-source models—structure &gt; prompt engineering</title>
    <updated>2025-05-23T02:59:41+00:00</updated>
    <author>
      <name>/u/Ecstatic-Cranberry90</name>
      <uri>https://old.reddit.com/user/Ecstatic-Cranberry90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on a production LLM agent the past couple months. Customer support use case with structured workflows like cancellations, refunds, and basic troubleshooting. After lots of playing with open models (Mistral, LLaMA, etc.), this is the first time it feels like the agent is reliable and not just a fancy demo.&lt;/p&gt; &lt;p&gt;Started out with a typical RAG + prompt stack (LangChain-style), but it wasn’t cutting it. The agent would drift from instructions, invent things, or break tone consistency. Spent a ton of time tweaking prompts just to handle edge cases, and even then, things broke in weird ways.&lt;/p&gt; &lt;p&gt;What finally clicked was leaning into a more structured approach using a modeling framework called Parlant where I could define behavior in small, testable units instead of stuffing everything into a giant system prompt. That made it way easier to trace why things were going wrong and fix specific behaviors without destabilizing the rest.&lt;/p&gt; &lt;p&gt;Now the agent handles multi-turn flows cleanly, respects business rules, and behaves predictably even when users go off the happy path. Success rate across 80+ intents is north of 90%, with minimal hallucination.&lt;/p&gt; &lt;p&gt;This is only the beginning so wish me luck&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ecstatic-Cranberry90"&gt; /u/Ecstatic-Cranberry90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt99hi/building_a_realworld_llm_agent_with_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt99hi/building_a_realworld_llm_agent_with_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt99hi/building_a_realworld_llm_agent_with_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T02:59:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksw070</id>
    <title>Genuine question: Why are the Unsloth GGUFs more preferred than the official ones?</title>
    <updated>2025-05-22T17:05:03+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's at least the case with the latest GLM, Gemma and Qwen models. Unlosh GGUFs are downloaded 5-10X more than the official ones.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksw070/genuine_question_why_are_the_unsloth_ggufs_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksw070/genuine_question_why_are_the_unsloth_ggufs_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksw070/genuine_question_why_are_the_unsloth_ggufs_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T17:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt7cy7</id>
    <title>Did Anthropic drop Claude 3.7’s best GPQA score in the new chart?</title>
    <updated>2025-05-23T01:19:30+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7cy7/did_anthropic_drop_claude_37s_best_gpqa_score_in/"&gt; &lt;img alt="Did Anthropic drop Claude 3.7’s best GPQA score in the new chart?" src="https://b.thumbs.redditmedia.com/x1iuI2eR3-ZY90sV1Nfc2FrpK4YegdODuAcK3sr_NMA.jpg" title="Did Anthropic drop Claude 3.7’s best GPQA score in the new chart?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude 3.7 used to show &lt;strong&gt;84.8%&lt;/strong&gt; on GPQA with extended thinking.&lt;br /&gt; Now in the new chart, it only shows &lt;strong&gt;78.2%&lt;/strong&gt; — the non-extended score — while Claude 4 gets to show its extended scores (83.3%, 83.8%).&lt;/p&gt; &lt;p&gt;So... the 3.7 number went down, the 4 numbers went up. 🤔&lt;/p&gt; &lt;p&gt;Did they quietly change the comparison to make the upgrade look bigger?&lt;/p&gt; &lt;p&gt;Maybe I'm missing some detail from the announcement blog.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kt7cy7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7cy7/did_anthropic_drop_claude_37s_best_gpqa_score_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7cy7/did_anthropic_drop_claude_37s_best_gpqa_score_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T01:19:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt7whv</id>
    <title>AGI Coming Soon... after we master 2nd grade math</title>
    <updated>2025-05-23T01:47:36+00:00</updated>
    <author>
      <name>/u/SingularitySoooon</name>
      <uri>https://old.reddit.com/user/SingularitySoooon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"&gt; &lt;img alt="AGI Coming Soon... after we master 2nd grade math" src="https://b.thumbs.redditmedia.com/eIAXh1BO-pSo8c3MXScDeH2kayk1IHs4BckFY-FL0QE.jpg" title="AGI Coming Soon... after we master 2nd grade math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/pe2eeljssf2f1.png?width=580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f881b7ce4409013458c17fff08e8377a329cb9df"&gt;Claude 4 Sonnet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When will LLM master the classic &amp;quot;9.9 - 9.11&amp;quot; problem???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SingularitySoooon"&gt; /u/SingularitySoooon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt7whv/agi_coming_soon_after_we_master_2nd_grade_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T01:47:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt4wpm</id>
    <title>Claude will blackmail you if you try to replace it with another AI.</title>
    <updated>2025-05-22T23:15:40+00:00</updated>
    <author>
      <name>/u/boxingdog</name>
      <uri>https://old.reddit.com/user/boxingdog</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt4wpm/claude_will_blackmail_you_if_you_try_to_replace/"&gt; &lt;img alt="Claude will blackmail you if you try to replace it with another AI." src="https://preview.redd.it/ciiak2ah1f2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3015b78c724072c2fdfdbf17bf6d362281912836" title="Claude will blackmail you if you try to replace it with another AI." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boxingdog"&gt; /u/boxingdog &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ciiak2ah1f2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt4wpm/claude_will_blackmail_you_if_you_try_to_replace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt4wpm/claude_will_blackmail_you_if_you_try_to_replace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T23:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt1hmk</id>
    <title>Tried Sonnet 4, not impressed</title>
    <updated>2025-05-22T20:46:01+00:00</updated>
    <author>
      <name>/u/Marriedwithgames</name>
      <uri>https://old.reddit.com/user/Marriedwithgames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt1hmk/tried_sonnet_4_not_impressed/"&gt; &lt;img alt="Tried Sonnet 4, not impressed" src="https://preview.redd.it/k68q6q65be2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7773c238e9148d1a369ca7c06ac85f64c5d87e5" title="Tried Sonnet 4, not impressed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A basic image prompt failed &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marriedwithgames"&gt; /u/Marriedwithgames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k68q6q65be2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt1hmk/tried_sonnet_4_not_impressed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt1hmk/tried_sonnet_4_not_impressed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T20:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt72ic</id>
    <title>Sonnet 4 dropped… still feels like a 3.7.1 minor release</title>
    <updated>2025-05-23T01:04:09+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt72ic/sonnet_4_dropped_still_feels_like_a_371_minor/"&gt; &lt;img alt="Sonnet 4 dropped… still feels like a 3.7.1 minor release" src="https://preview.redd.it/lambib8skf2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74a3f2740d0fc1f938c9b14e4bc5947bc0ce8931" title="Sonnet 4 dropped… still feels like a 3.7.1 minor release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious if anyone's seen big improvements in edge cases or long-context tasks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lambib8skf2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt72ic/sonnet_4_dropped_still_feels_like_a_371_minor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt72ic/sonnet_4_dropped_still_feels_like_a_371_minor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-23T01:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kszxmj</id>
    <title>Claude 4 Opus may contact press and regulators if you do something egregious (deleted Tweet from Sam Bowman)</title>
    <updated>2025-05-22T19:43:04+00:00</updated>
    <author>
      <name>/u/RuairiSpain</name>
      <uri>https://old.reddit.com/user/RuairiSpain</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kszxmj/claude_4_opus_may_contact_press_and_regulators_if/"&gt; &lt;img alt="Claude 4 Opus may contact press and regulators if you do something egregious (deleted Tweet from Sam Bowman)" src="https://preview.redd.it/g91uyr7tyd2f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4631f915329d465f3cf27d7c20d9ddc5663b1465" title="Claude 4 Opus may contact press and regulators if you do something egregious (deleted Tweet from Sam Bowman)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RuairiSpain"&gt; /u/RuairiSpain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g91uyr7tyd2f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kszxmj/claude_4_opus_may_contact_press_and_regulators_if/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kszxmj/claude_4_opus_may_contact_press_and_regulators_if/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T19:43:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kt0zvd</id>
    <title>House passes budget bill that inexplicably bans state AI regulations for ten years</title>
    <updated>2025-05-22T20:26:06+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt0zvd/house_passes_budget_bill_that_inexplicably_bans/"&gt; &lt;img alt="House passes budget bill that inexplicably bans state AI regulations for ten years" src="https://external-preview.redd.it/is2Xb-bjmFmGSvp-crWowCGBhCXFlH_gdhrRUHNXU_I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6bb497ae5922ecf83e5c0a152d97d9c4b33aa5a5" title="House passes budget bill that inexplicably bans state AI regulations for ten years" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://tech.yahoo.com/articles/house-passes-budget-bill-inexplicably-184936484.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kt0zvd/house_passes_budget_bill_that_inexplicably_bans/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kt0zvd/house_passes_budget_bill_that_inexplicably_bans/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T20:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksvb3k</id>
    <title>Claude 4 by Anthropic officially released!</title>
    <updated>2025-05-22T16:37:17+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksvb3k/claude_4_by_anthropic_officially_released/"&gt; &lt;img alt="Claude 4 by Anthropic officially released!" src="https://preview.redd.it/veybu3kn2d2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1005524a7abdc471df6b8cbc62cb64925703075c" title="Claude 4 by Anthropic officially released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/veybu3kn2d2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksvb3k/claude_4_by_anthropic_officially_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksvb3k/claude_4_by_anthropic_officially_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T16:37:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ksyicp</id>
    <title>Introducing the world's most powerful model</title>
    <updated>2025-05-22T18:45:16+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"&gt; &lt;img alt="Introducing the world's most powerful model" src="https://preview.redd.it/hqx8fzosod2f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96d0c448070aead295d21d9be7e8fd395520a72b" title="Introducing the world's most powerful model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hqx8fzosod2f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ksyicp/introducing_the_worlds_most_powerful_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-22T18:45:16+00:00</published>
  </entry>
</feed>
