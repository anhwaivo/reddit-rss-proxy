<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-17T13:40:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jdcm1u</id>
    <title>What app to use on an old Android 8.1.0 phone to connect to my LLM on my laptop?</title>
    <updated>2025-03-17T13:27:32+00:00</updated>
    <author>
      <name>/u/ExtremePresence3030</name>
      <uri>https://old.reddit.com/user/ExtremePresence3030</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and how to do it?&lt;/p&gt; &lt;p&gt;I use koboldcpp as my llm server app.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtremePresence3030"&gt; /u/ExtremePresence3030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdcm1u/what_app_to_use_on_an_old_android_810_phone_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdcm1u/what_app_to_use_on_an_old_android_810_phone_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdcm1u/what_app_to_use_on_an_old_android_810_phone_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T13:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd8v0i</id>
    <title>How fast is Threadripper 5995WX for Inference instead of a GPU?</title>
    <updated>2025-03-17T09:46:52+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am thinking that I can buy a Threadripper 5995WX and wait until the prices of the GPUs stabilize.&lt;br /&gt; I am based in China, and I found prices for this processor are relatively goo USD1200-1800.&lt;br /&gt; My question is how fast can this processor generate tokens for model like 70B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8v0i/how_fast_is_threadripper_5995wx_for_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8v0i/how_fast_is_threadripper_5995wx_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8v0i/how_fast_is_threadripper_5995wx_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T09:46:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdacqe</id>
    <title>back end for RTX 5090</title>
    <updated>2025-03-17T11:25:30+00:00</updated>
    <author>
      <name>/u/arivar</name>
      <uri>https://old.reddit.com/user/arivar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've recently added a RTX 5090 to my setup (now 4090 + 5090) and I just can't find a back end that supports it yet. I tried aphrodite engine, vllm and even text web generation UI standalone. The problem is that only the current nightly version of pytorch supports cuda 12.8 (which is the only one that supports the 5090). What are you guys using? Any tips here?&lt;/p&gt; &lt;p&gt;I use arch linux btw.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arivar"&gt; /u/arivar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdacqe/back_end_for_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdacqe/back_end_for_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdacqe/back_end_for_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:25:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcx3gh</id>
    <title>A dataset of 7k flux-generated hands with various finger counts – great for training/testing VLMs on finger counting task</title>
    <updated>2025-03-16T22:06:46+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx3gh/a_dataset_of_7k_fluxgenerated_hands_with_various/"&gt; &lt;img alt="A dataset of 7k flux-generated hands with various finger counts – great for training/testing VLMs on finger counting task" src="https://external-preview.redd.it/ERLssQL8T7-JL3OKLYdPbyrlra2Ql3KExBAqXUCjlS4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bed78071d6ce48dd87ba242c0a51c3442cbe679e" title="A dataset of 7k flux-generated hands with various finger counts – great for training/testing VLMs on finger counting task" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/taesiri/FluxHands-FingerCount"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx3gh/a_dataset_of_7k_fluxgenerated_hands_with_various/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx3gh/a_dataset_of_7k_fluxgenerated_hands_with_various/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T22:06:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcre0y</id>
    <title>RTX 3060 vs RTX 3090: LLM Performance on 7B, 14B, 32B, 70B Models</title>
    <updated>2025-03-16T17:58:30+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcre0y/rtx_3060_vs_rtx_3090_llm_performance_on_7b_14b/"&gt; &lt;img alt="RTX 3060 vs RTX 3090: LLM Performance on 7B, 14B, 32B, 70B Models" src="https://external-preview.redd.it/5KAhHFD5rwW2nNDpKI_LcYCfDf4tB7OyGvSX5OWqLeA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a690ac8c11dc8c1aceccb4206ce1d8bb8f32874" title="RTX 3060 vs RTX 3090: LLM Performance on 7B, 14B, 32B, 70B Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/VGyKwi9Rfhk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcre0y/rtx_3060_vs_rtx_3090_llm_performance_on_7b_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcre0y/rtx_3060_vs_rtx_3090_llm_performance_on_7b_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T17:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd9ttx</id>
    <title>PSA: c4ai-command-a-03-2025 seems to be trained for reasoning / "thinking"</title>
    <updated>2025-03-17T10:53:07+00:00</updated>
    <author>
      <name>/u/CheatCodesOfLife</name>
      <uri>https://old.reddit.com/user/CheatCodesOfLife</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tested &lt;a href="https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF"&gt;c4ai-command-a-03-2025-GGUF&lt;/a&gt; Q4_K with this simple prompt (very crude, I'm sure there's a lot of room for improvement) system prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Think about your response within &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags before responding to the user. There's no need for structure or formatting, take as long as you need. When you're ready, write the final response outside the thinking tags. The user will only see the final response.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It even did the QwQ/R1-style reasoning with &amp;quot;wait...&amp;quot; within the tags, and it managed to solve a problem that no other local model I've tried could solve.&lt;/p&gt; &lt;p&gt;Without the system prompt, it just gave me the usual incorrect response that other models like Mistral-Large and QwQ provide.&lt;/p&gt; &lt;p&gt;Give it a try!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheatCodesOfLife"&gt; /u/CheatCodesOfLife &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9ttx/psa_c4aicommanda032025_seems_to_be_trained_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9ttx/psa_c4aicommanda032025_seems_to_be_trained_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9ttx/psa_c4aicommanda032025_seems_to_be_trained_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T10:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcxb9w</id>
    <title>R2R v3.5.0 Release Notes</title>
    <updated>2025-03-16T22:16:28+00:00</updated>
    <author>
      <name>/u/docsoc1</name>
      <uri>https://old.reddit.com/user/docsoc1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxb9w/r2r_v350_release_notes/"&gt; &lt;img alt="R2R v3.5.0 Release Notes" src="https://external-preview.redd.it/NErnoInmUcnZ4qVCAEDCbRxra-qc_bCUEu34Ma-sqH0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb8d8b0e950c0928d1cbab06e4e58a4350e51c11" title="R2R v3.5.0 Release Notes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to announce R2R v3.5.0, featuring our new Deep Research API and significant improvements to our RAG capabilities.&lt;/p&gt; &lt;h1&gt;🚀 Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Deep Research API: Multi-step reasoning system that fetches data from your knowledge base and the internet to deliver comprehensive, context-aware answers&lt;/li&gt; &lt;li&gt;Enhanced RAG Agent: More robust with new web search and scraping capabilities&lt;/li&gt; &lt;li&gt;Real-time Streaming: Server-side event streaming for visibility into the agent's thinking process and tool usage&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;✨ Key Features&lt;/h1&gt; &lt;h1&gt;Research Capabilities&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Research Agent: Specialized mode with advanced reasoning and computational tools&lt;/li&gt; &lt;li&gt;Extended Thinking: Toggle reasoning capabilities with optimized Claude model support&lt;/li&gt; &lt;li&gt;Improved Citations: Real-time citation identification with precise source attribution&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;New Tools&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Web Tools: Search external APIs and scrape web pages for up-to-date information&lt;/li&gt; &lt;li&gt;Research Tools: Reasoning, critique, and Python execution for complex analysis&lt;/li&gt; &lt;li&gt;RAG Tool: Leverage underlying RAG capabilities within the research agent&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;💡 Usage Examples&lt;/h1&gt; &lt;h1&gt;Basic RAG Mode&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;response = client.retrieval.agent( query=&amp;quot;What does deepseek r1 imply for the future of AI?&amp;quot;, generation_config={ &amp;quot;model&amp;quot;: &amp;quot;anthropic/claude-3-7-sonnet-20250219&amp;quot;, &amp;quot;extended_thinking&amp;quot;: True, &amp;quot;thinking_budget&amp;quot;: 4096, &amp;quot;temperature&amp;quot;: 1, &amp;quot;max_tokens_to_sample&amp;quot;: 16000, &amp;quot;stream&amp;quot;: True }, rag_tools=[&amp;quot;search_file_descriptions&amp;quot;, &amp;quot;search_file_knowledge&amp;quot;, &amp;quot;get_file_content&amp;quot;, &amp;quot;web_search&amp;quot;, &amp;quot;web_scrape&amp;quot;], mode=&amp;quot;rag&amp;quot; ) # Process the streaming events for event in response: if isinstance(event, ThinkingEvent): print(f&amp;quot;🧠 Thinking: {event.data.delta.content[0].payload.value}&amp;quot;) elif isinstance(event, ToolCallEvent): print(f&amp;quot;🔧 Tool call: {event.data.name}({event.data.arguments})&amp;quot;) elif isinstance(event, ToolResultEvent): print(f&amp;quot;📊 Tool result: {event.data.content[:60]}...&amp;quot;) elif isinstance(event, CitationEvent): print(f&amp;quot;📑 Citation: {event.data}&amp;quot;) elif isinstance(event, MessageEvent): print(f&amp;quot;💬 Message: {event.data.delta.content[0].payload.value}&amp;quot;) elif isinstance(event, FinalAnswerEvent): print(f&amp;quot;✅ Final answer: {event.data.generated_answer[:100]}...&amp;quot;) print(f&amp;quot; Citations: {len(event.data.citations)} sources referenced&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Research Mode&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;response = client.retrieval.agent( query=&amp;quot;Analyze the philosophical implications of DeepSeek R1&amp;quot;, generation_config={ &amp;quot;model&amp;quot;: &amp;quot;anthropic/claude-3-opus-20240229&amp;quot;, &amp;quot;extended_thinking&amp;quot;: True, &amp;quot;thinking_budget&amp;quot;: 8192, &amp;quot;temperature&amp;quot;: 0.2, &amp;quot;max_tokens_to_sample&amp;quot;: 32000, &amp;quot;stream&amp;quot;: True }, research_tools=[&amp;quot;rag&amp;quot;, &amp;quot;reasoning&amp;quot;, &amp;quot;critique&amp;quot;, &amp;quot;python_executor&amp;quot;], mode=&amp;quot;research&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For more details, visit our &lt;a href="https://github.com/SciPhi-AI/R2R/"&gt;Github&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jcxb9w/video/kmbjp35ro7pe1/player"&gt;EDIT - Adding a video.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/docsoc1"&gt; /u/docsoc1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxb9w/r2r_v350_release_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxb9w/r2r_v350_release_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcxb9w/r2r_v350_release_notes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T22:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd9zsr</id>
    <title>UPDATE: Tool calling support for QwQ-32B using LangChain’s ChatOpenAI</title>
    <updated>2025-03-17T11:03:00+00:00</updated>
    <author>
      <name>/u/lc19-</name>
      <uri>https://old.reddit.com/user/lc19-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;QwQ-32B Support&lt;/strong&gt; ✅&lt;/p&gt; &lt;p&gt;I've updated my repo with a new tutorial for tool calling support for QwQ-32B using LangChain’s ChatOpenAI (via OpenRouter) using both the Python and JavaScript/TypeScript version of my package (Note: LangChain's ChatOpenAI does not currently support tool calling for QwQ-32B).&lt;/p&gt; &lt;p&gt;I noticed OpenRouter's QwQ-32B API is a little unstable (likely due to model was only added about a week ago) and returning empty responses. So I have updated the package to keep looping until a non-empty response is returned. If you have previously downloaded the package, please update the package via &lt;code&gt;pip install --upgrade taot&lt;/code&gt; or &lt;code&gt;npm update taot-ts&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can also use the TAoT package for tool calling support for QwQ-32B on Nebius AI which uses LangChain's ChatOpenAI. Alternatively, you can also use Groq where their team have already provided tool calling support for QwQ-32B using LangChain's ChatGroq.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;OpenAI Agents SDK? Not Yet!&lt;/strong&gt; ❌&lt;/p&gt; &lt;p&gt;I checked out the OpenAI Agents SDK framework for tool calling support for non-OpenAI models (&lt;a href="https://openai.github.io/openai-agents-python/models/"&gt;https://openai.github.io/openai-agents-python/models/&lt;/a&gt;) and they don't support tool calling for DeepSeek-R1 (or any models available through OpenRouter) yet. &lt;strong&gt;So there you go!&lt;/strong&gt; 😉&lt;/p&gt; &lt;p&gt;Check it out my updates here: Python: &lt;a href="https://github.com/leockl/tool-ahead-of-time"&gt;https://github.com/leockl/tool-ahead-of-time&lt;/a&gt;&lt;/p&gt; &lt;p&gt;JavaScript/TypeScript: &lt;a href="https://github.com/leockl/tool-ahead-of-time-ts"&gt;https://github.com/leockl/tool-ahead-of-time-ts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please give my GitHub repos a star if this was helpful ⭐&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lc19-"&gt; /u/lc19- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9zsr/update_tool_calling_support_for_qwq32b_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9zsr/update_tool_calling_support_for_qwq32b_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9zsr/update_tool_calling_support_for_qwq32b_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:03:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd8lwp</id>
    <title>What’s the smallest, most effective model for function calling and AI agents?</title>
    <updated>2025-03-17T09:27:53+00:00</updated>
    <author>
      <name>/u/DiogoSnows</name>
      <uri>https://old.reddit.com/user/DiogoSnows</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for a compact, highly efficient model that performs well on function calling. For now I’m thinking &amp;lt;= 4b parameters (do you consider that small?)&lt;/p&gt; &lt;p&gt;Does anyone know of any dedicated leaderboards or benchmarks that compare smaller models in this area?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DiogoSnows"&gt; /u/DiogoSnows &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8lwp/whats_the_smallest_most_effective_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8lwp/whats_the_smallest_most_effective_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8lwp/whats_the_smallest_most_effective_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T09:27:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd7jll</id>
    <title>Gemma3 recommended settings on Ollama v0.6.1 (Open WebUI)</title>
    <updated>2025-03-17T08:03:38+00:00</updated>
    <author>
      <name>/u/Tx3hc78</name>
      <uri>https://old.reddit.com/user/Tx3hc78</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So for Gemma3, it is recommended to use the following settings:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 1.0 top_k = 64 top_p = 0.95 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But for ollama it was recommended to keep using only&lt;/p&gt; &lt;pre&gt;&lt;code&gt;temperature = 0.1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With new version of ollama &lt;strong&gt;v0.6.1&lt;/strong&gt; they improved handling of &lt;code&gt;temperature&lt;/code&gt; and &lt;code&gt;top_k&lt;/code&gt; so what are we supposed to change back to general recommended values now?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Improved sampling parameters such as &lt;code&gt;temperature&lt;/code&gt; and &lt;code&gt;top_k&lt;/code&gt; to behave similar to other implementations&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;There is no mention for &lt;code&gt;top_p&lt;/code&gt; so should we set that to &lt;code&gt;0.95&lt;/code&gt; as well?&lt;/p&gt; &lt;p&gt;On the Gemma3 model page from &lt;a href="https://ollama.com/library/gemma3"&gt;ollama's website&lt;/a&gt; the &lt;code&gt;temperature&lt;/code&gt; parameter is still set to &lt;code&gt;0.1&lt;/code&gt;.&lt;br /&gt; Also do you set the &lt;code&gt;stop&lt;/code&gt; (Stop Sequence) parameter to &lt;code&gt;&amp;quot;&amp;lt;end_of_turn&amp;gt;&amp;quot;&lt;/code&gt; as well? Like it says from ollama website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tx3hc78"&gt; /u/Tx3hc78 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7jll/gemma3_recommended_settings_on_ollama_v061_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7jll/gemma3_recommended_settings_on_ollama_v061_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7jll/gemma3_recommended_settings_on_ollama_v061_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T08:03:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd0p8a</id>
    <title>Taking prompt suggestions for a new version of EQ-Bench creative writing benchmark</title>
    <updated>2025-03-17T00:57:20+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA, creator of EQ-Bench here.&lt;/p&gt; &lt;p&gt;Many people have criticised the prompts in the current creative writing eval as, variously, &amp;quot;garbage&amp;quot; and &amp;quot;complete slop&amp;quot;. This is fair, and honestly I used chatgpt to make most of those prompts.&lt;/p&gt; &lt;p&gt;This time around there will be less of that. Give me your suggestions for prompts which:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;separate good writers from bad writers&lt;/li&gt; &lt;li&gt;you'd actually like to read for manual vibe checking&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Two slightly different questions because I may include prompts that are useful to humans but not include them in scoring.&lt;/p&gt; &lt;p&gt;The prototype is already much more discriminative between the top models (which is the reason I'm making a new version -- it was saturating).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0p8a/taking_prompt_suggestions_for_a_new_version_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0p8a/taking_prompt_suggestions_for_a_new_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0p8a/taking_prompt_suggestions_for_a_new_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T00:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd7u63</id>
    <title>[Open Source] Deploy and run voice AI models with one click on MacOS</title>
    <updated>2025-03-17T08:26:54+00:00</updated>
    <author>
      <name>/u/Heybud221</name>
      <uri>https://old.reddit.com/user/Heybud221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Ilikepizza2/localspeech-AI"&gt;LocalSpeech&lt;/a&gt; is an open source project that I created to make it easy to run and deploy Voice AI models on MacOS in an openai compliant api server along with an API playground. Currently it supports Zonos, Spark, Whisper and Kokoro. Had been away for the weekend so I am still working on adding support for Sesame CSM. &lt;/p&gt; &lt;p&gt;Currently learning MLOps to make it reliable for prod. I don't have a good GPU machine for linux, so I am not able to test but I want this to be compatible with linux too. If you have one and are willing to assist, PRs would be welcome :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heybud221"&gt; /u/Heybud221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7u63/open_source_deploy_and_run_voice_ai_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7u63/open_source_deploy_and_run_voice_ai_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd7u63/open_source_deploy_and_run_voice_ai_models_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T08:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jctquk</id>
    <title>Introducing Mochi, a finetuned version of Moshi.</title>
    <updated>2025-03-16T19:38:29+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/DavidBrowne17/Muchi"&gt;https://huggingface.co/DavidBrowne17/Muchi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I finetuned a version of Moshi, using a modified version of this repo &lt;a href="https://github.com/yangdongchao/RSTnet"&gt;https://github.com/yangdongchao/RSTnet&lt;/a&gt; it still has some of the issues with intelligence but it seems better to me. Using that repo we can also finetune new moshi style models using other smarter LLMs than the helium model that moshi is based on. There is no moat.&lt;/p&gt; &lt;p&gt;Edit: Renamed to Muchi as there is already an AI named Mochi&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jct1lk</id>
    <title>PR for native Windows support was just submitted to vLLM</title>
    <updated>2025-03-16T19:08:31+00:00</updated>
    <author>
      <name>/u/Nextil</name>
      <uri>https://old.reddit.com/user/Nextil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;User SystemPanic just &lt;a href="https://github.com/vllm-project/vllm/pull/14891"&gt;submitted a PR&lt;/a&gt; to the vLLM repo adding native Windows support. Before now it was only possible to run on Linux/WSL. This should make it significantly easier to run new models (especially VLMs) on Windows. No builds that I can see but it includes build instructions. The patched repo is &lt;a href="https://github.com/SystemPanic/vllm-windows/tree/vllm-windows"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The PR mentions submitting a FlashInfer PR adding Windows support, but that doesn't appear to have been done as of writing so it might not be possible to build just yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nextil"&gt; /u/Nextil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd8cw3</id>
    <title>Is there a better service than Grok for NSFW 'Creative Writing'?</title>
    <updated>2025-03-17T09:08:28+00:00</updated>
    <author>
      <name>/u/PangurBanTheCat</name>
      <uri>https://old.reddit.com/user/PangurBanTheCat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've found it to be largely unfiltered, while also very capable. However every so often it just outright refuses requests, or the service will just... start writing really poorly? It's also a bit spendy at $30/month. &lt;/p&gt; &lt;p&gt;I want something that's intelligent and capable of handling maybe a short-novels worth of context. I enjoy writing things that are maybe a few chapters worth. &lt;/p&gt; &lt;p&gt;So far the only service I've encountered that can handle my needs is Grok. It's fairly intelligent, it writes well as long as it's prompted to do so, it has good understanding of what or who characters are and how they should respond in certain situations, and it can handle a pretty decent amount of context. &lt;/p&gt; &lt;p&gt;Just wondering if there's anything better. Or at least cheaper while offering the same. Or even just more unfiltered as the occasional refusals do get annoying.&lt;/p&gt; &lt;p&gt;Edit: Oh, also, if you push Grok too much to try to get past it's refusals, it will literally just delete the entire session. So. Yeah, fuck that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PangurBanTheCat"&gt; /u/PangurBanTheCat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8cw3/is_there_a_better_service_than_grok_for_nsfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8cw3/is_there_a_better_service_than_grok_for_nsfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd8cw3/is_there_a_better_service_than_grok_for_nsfw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T09:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcqy6c</id>
    <title>We have Deep Research at home</title>
    <updated>2025-03-16T17:39:04+00:00</updated>
    <author>
      <name>/u/atineiatte</name>
      <uri>https://old.reddit.com/user/atineiatte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"&gt; &lt;img alt="We have Deep Research at home" src="https://external-preview.redd.it/NA_JTAjwBAYLbzLjIgJ3Q_k4TmFsR5MWHCoiYKiIQJ8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9bd51d7c05f78cbad725a12ad69bc6ff6fe2ec" title="We have Deep Research at home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atineiatte"&gt; /u/atineiatte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/atineiatte/deep-research-at-home"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T17:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdc0hq</id>
    <title>Mathematics for Machine Learning: 417 page pdf ebook</title>
    <updated>2025-03-17T12:58:19+00:00</updated>
    <author>
      <name>/u/Sporeboss</name>
      <uri>https://old.reddit.com/user/Sporeboss</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sporeboss"&gt; /u/Sporeboss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mml-book.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdc0hq/mathematics_for_machine_learning_417_page_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdc0hq/mathematics_for_machine_learning_417_page_pdf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T12:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd0zpf</id>
    <title>Token Explorer - A simple interface for quickly exploring and modifying the token generation process!</title>
    <updated>2025-03-17T01:12:19+00:00</updated>
    <author>
      <name>/u/CountBayesie</name>
      <uri>https://old.reddit.com/user/CountBayesie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spend &lt;em&gt;a lot&lt;/em&gt; of my time working on the logit end of LLMs and have long wanted a way to more quickly and interactively understand what LLMs are doing during the token generation process and how that might help us improve prompting and better understand these models!&lt;/p&gt; &lt;p&gt;So to scratch that itch I put together &lt;a href="https://github.com/willkurt/token-explorer"&gt;Token Explorer&lt;/a&gt;. It's an open source Python tool with a simple interface that allows you to visually step through the token generation process.&lt;/p&gt; &lt;p&gt;Features include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Simple keyboard interface (WASD + arrow keys).&lt;/li&gt; &lt;li&gt;Ability to select which token is chosen at each step.&lt;/li&gt; &lt;li&gt;Likewise, the ability to backtrack and try a new path.&lt;/li&gt; &lt;li&gt;Fork prompts and iterate them to explore and compare alternative sampling possibilities.&lt;/li&gt; &lt;li&gt;Visualization layers allow you to see the &lt;em&gt;probability&lt;/em&gt; of each token at time generation and the &lt;em&gt;entropy&lt;/em&gt; of tokens in the prompt/generation so far.&lt;/li&gt; &lt;li&gt;Load prompts from a plain text file.&lt;/li&gt; &lt;li&gt;Defaults to &lt;code&gt;Qwen/Qwen2.5-0.5B&lt;/code&gt; so can be run on most hardware.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The caveat, of course, is that this &lt;em&gt;is&lt;/em&gt; just a quick weekend project so it's a bit rough around the edges. The current setup is absolutely not built for performance so trying long prompts and large models might cause some issues.&lt;/p&gt; &lt;p&gt;Nonethless, I thought people might appreciate the ability to experiment with the internal sampling process of LLMs. I've already had a lot of fun testing out whether or not the LLM can still get the correct answer to math questions if you intentionally make it choose low probability tokens! It's also interesting to look at prompts and see where the model is the most uncertain and how changing that can impact downstream success!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CountBayesie"&gt; /u/CountBayesie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0zpf/token_explorer_a_simple_interface_for_quickly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0zpf/token_explorer_a_simple_interface_for_quickly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd0zpf/token_explorer_a_simple_interface_for_quickly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T01:12:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdawqj</id>
    <title>What is the difference between an AI agent and a background job calling LLM API?</title>
    <updated>2025-03-17T11:58:44+00:00</updated>
    <author>
      <name>/u/superloser48</name>
      <uri>https://old.reddit.com/user/superloser48</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi - I am a programmer and I use LLMs extensively for work. For coding and for data cleaning - I have found LLMs INSANELY helpful.&lt;/p&gt; &lt;p&gt;But I am struggling to understand the &lt;strong&gt;difference between using an AI agent vs calling the LLMs' API in a background job&lt;/strong&gt; (cron). My code currently runs in cron jobs and passes PDFs to LLMs' API to OCR for dirty PDFs. (eg. we have a lot of PDF submissions on our website).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is not a loaded question or a diss on AI agents.&lt;/strong&gt; Would love it if someone could point what can be done differently in a AI agent vs a background job. I am curious if I can reduce my codebase size for data cleaning.&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superloser48"&gt; /u/superloser48 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdawqj/what_is_the_difference_between_an_ai_agent_and_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdawqj/what_is_the_difference_between_an_ai_agent_and_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdawqj/what_is_the_difference_between_an_ai_agent_and_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd616a</id>
    <title>Why are audio (tts/stt) models so much smaller in size than general llms?</title>
    <updated>2025-03-17T06:07:10+00:00</updated>
    <author>
      <name>/u/Heybud221</name>
      <uri>https://old.reddit.com/user/Heybud221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLMs have possible outputs comprising of words(text) but speech models require words as well as phenomes. Shouldn't they be larger?&lt;/p&gt; &lt;p&gt;From what I think, it is because they don't have the understanding (technically, llms also don't &amp;quot;understand&amp;quot; words) as much as LLMs. Is that correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heybud221"&gt; /u/Heybud221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd616a/why_are_audio_ttsstt_models_so_much_smaller_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd616a/why_are_audio_ttsstt_models_so_much_smaller_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd616a/why_are_audio_ttsstt_models_so_much_smaller_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T06:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdasng</id>
    <title>Heads up if you're using Gemma 3 vision</title>
    <updated>2025-03-17T11:52:09+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"&gt; &lt;img alt="Heads up if you're using Gemma 3 vision" src="https://external-preview.redd.it/YXiABCYSLmR9qQ-LeZnYrdVC2XA7zhm-nWxrhAxjA3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f558a293cfea6675b1b3428038710d15adc358d8" title="Heads up if you're using Gemma 3 vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a quick heads up for anyone using Gemma 3 in &lt;strong&gt;LM Studio&lt;/strong&gt; or &lt;strong&gt;Koboldcpp&lt;/strong&gt;, its vision capabilities aren't fully functional within those interfaces, resulting in degraded quality. (I do not know about Open WebUI as I'm not using it).&lt;/p&gt; &lt;p&gt;I believe a lot of users potentially have used vision without realizing it has been more or less crippled, not showcasing Gemma 3's full potential. However, when you do &lt;strong&gt;not&lt;/strong&gt; use vision for details or texts, the degraded accuracy is often not noticeable and works quite good, for example with general artwork and landscapes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Koboldcpp&lt;/strong&gt; resizes images before being processed by Gemma 3, which particularly distorts details, perhaps most noticeable with smaller text. While Koboldcpp &lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.81.1"&gt;version 1.81&lt;/a&gt; (released January 7th) expanded supported resolutions and aspect ratios, the resizing still affects vision quality negatively, resulting in degraded accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt; is behaving more odd, initial image input sent to Gemma 3 is relatively accurate (but still somewhat crippled, probably because it's doing re-scaling here as well), but subsequent regenerations using the same image or starting new chats with new images results in &lt;em&gt;significantly&lt;/em&gt; degraded output, most noticeable images with finer details such as characters in far distance or text.&lt;/p&gt; &lt;p&gt;When I send images to Gemma 3 directly (not through these UIs), its accuracy becomes much better, especially for details and texts.&lt;/p&gt; &lt;p&gt;Below is a collage (I can't upload multiple images on Reddit) demonstrating how vision quality degrades even more when doing a regeneration or starting a new chat in LM Studio.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q0r0w0jli8pe1.jpg?width=414&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2ace1de458ee966030714ca8b80111156a3e28bb"&gt;https://preview.redd.it/q0r0w0jli8pe1.jpg?width=414&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2ace1de458ee966030714ca8b80111156a3e28bb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdasng/heads_up_if_youre_using_gemma_3_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd87wv</id>
    <title>underwhelming MCP Vs hype</title>
    <updated>2025-03-17T08:58:04+00:00</updated>
    <author>
      <name>/u/ashutrv</name>
      <uri>https://old.reddit.com/user/ashutrv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My early thoughts on MCPs :&lt;/p&gt; &lt;p&gt;As I see the current state of hype, the experience is underwhelming:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Confusing targeting — developers and non devs both.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For devs — it’s straightforward coding agent basically just llm.txt , so why would I use MCP isn’t clear.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For non devs — It’s like tools that can be published by anyone and some setup to add config etc. But the same stuff has been tried by ChatGPT GPTs as well last year where anyone can publish their tools as GPTs, which in my experience didn’t work well.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;There’s isn’t a good client so far and the clients UIs not being open source makes the experience limited as in our case, no client natively support video upload and playback.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Installing MCPs on local machines can have setup issues later with larger MCPs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I feel the hype isn’t organic and fuelled by Anthropic. I was expecting MCP ( being a protocol ) to have deeper developer value for agentic workflows and communication standards then just a wrapper over docker and config files.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let’s imagine a world with lots of MCPs — how would I choose which one to install and why, how would it rank similar servers? Are they imagining it like a ecosystem like App store where my main client doesn’t change but I am able to achieve any tasks that I do with a SaaS product.&lt;/p&gt; &lt;p&gt;We tried a simple task — &lt;code&gt;&amp;quot;take the latest video on Gdrive and give me a summary&amp;quot;&lt;/code&gt; For this the steps were not easy:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Go through Gdrive MCP and setup documentation — Gdrive MCP has 11 step setup process.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;VideoDB MCP has 1 step setup process.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall 12, 13 step to do a basic task.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ashutrv"&gt; /u/ashutrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd87wv/underwhelming_mcp_vs_hype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd87wv/underwhelming_mcp_vs_hype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd87wv/underwhelming_mcp_vs_hype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T08:58:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcx69i</id>
    <title>Text an LLM at +61493035885</title>
    <updated>2025-03-16T22:10:18+00:00</updated>
    <author>
      <name>/u/benkaiser</name>
      <uri>https://old.reddit.com/user/benkaiser</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a basic service running on an old Android phone + cheap prepaid SIM card to allow people to send a text and receive a response from Llama 3.1 8B. I felt the need when we recently lost internet access during a tropical cyclone but SMS was still working.&lt;/p&gt; &lt;p&gt;Full details in the blog post: &lt;a href="https://benkaiser.dev/text-an-llm/"&gt;https://benkaiser.dev/text-an-llm/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benkaiser"&gt; /u/benkaiser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcx69i/text_an_llm_at_61493035885/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T22:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1jd9xjj</id>
    <title>Gemma 3 is now available for free on HuggingChat!</title>
    <updated>2025-03-17T10:59:36+00:00</updated>
    <author>
      <name>/u/SensitiveCranberry</name>
      <uri>https://old.reddit.com/user/SensitiveCranberry</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9xjj/gemma_3_is_now_available_for_free_on_huggingchat/"&gt; &lt;img alt="Gemma 3 is now available for free on HuggingChat!" src="https://external-preview.redd.it/fWy5OWPAOAEc-6PWhgSCmMIjuAf3liLshl-njJSXolI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9dc784c22ed0ab0a891d88c523a62c0663cf6cf" title="Gemma 3 is now available for free on HuggingChat!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SensitiveCranberry"&gt; /u/SensitiveCranberry &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hf.co/chat/models/google/gemma-3-27b-it"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9xjj/gemma_3_is_now_available_for_free_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jd9xjj/gemma_3_is_now_available_for_free_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T10:59:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jdaq7x</id>
    <title>3x RTX 5090 watercooled in one desktop</title>
    <updated>2025-03-17T11:48:07+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"&gt; &lt;img alt="3x RTX 5090 watercooled in one desktop" src="https://preview.redd.it/zsu6kw5pm8pe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=313ede5ac5797a563ccfc2f875620a34ab784cbe" title="3x RTX 5090 watercooled in one desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zsu6kw5pm8pe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jdaq7x/3x_rtx_5090_watercooled_in_one_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-17T11:48:07+00:00</published>
  </entry>
</feed>
