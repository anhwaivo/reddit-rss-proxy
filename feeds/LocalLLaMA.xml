<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-30T11:48:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kbbdra</id>
    <title>uhh.. what?</title>
    <updated>2025-04-30T08:21:04+00:00</updated>
    <author>
      <name>/u/MigorRortis96</name>
      <uri>https://old.reddit.com/user/MigorRortis96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have no idea what's going on with qwen3 but I've never seen this type of hallucinating before. I noticed also that the smaller models locally seem to overthink and repeat stuff infinitely.&lt;/p&gt; &lt;p&gt;235b does not do this, and neither does any of the qwen2.5 models including the 0.5b one&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.qwen.ai/s/49cf72ca-7852-4d99-8299-5e4827d925da?fev=0.0.86"&gt;https://chat.qwen.ai/s/49cf72ca-7852-4d99-8299-5e4827d925da?fev=0.0.86&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit 1: it seems that saying &amp;quot;xyz is not the answer&amp;quot; leads it to continue rather than producing a stop token. I don't think this is a sampling bug but rather poor training which leads it to continue if no &amp;quot;answer&amp;quot; has been found. it may not be able to &amp;quot;not know&amp;quot; something. this is backed up by a bunch of other posts on here on infinite thinking, looping and getting confused.&lt;/p&gt; &lt;p&gt;I tried it on my app via deepinfra and it's ability to follow instructions and produce json is extremely poor. qwen 2.5 7b does a better job than 235b via deepinfra &amp;amp; alibaba &lt;/p&gt; &lt;p&gt;really hope I'm wrong&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MigorRortis96"&gt; /u/MigorRortis96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbdra/uhh_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbdra/uhh_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbdra/uhh_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T08:21:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb2d7z</id>
    <title>codename "LittleLLama". 8B llama 4 incoming</title>
    <updated>2025-04-29T23:35:44+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb2d7z/codename_littlellama_8b_llama_4_incoming/"&gt; &lt;img alt="codename &amp;quot;LittleLLama&amp;quot;. 8B llama 4 incoming" src="https://external-preview.redd.it/i1zk2WxsO1Z-ctZBJ8kCoA7z8bnswfkJFDSk_5B34jo.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac196252196a0daf71dbc62e063cb102c4d40cab" title="codename &amp;quot;LittleLLama&amp;quot;. 8B llama 4 incoming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=rYXeQbTuVl0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb2d7z/codename_littlellama_8b_llama_4_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb2d7z/codename_littlellama_8b_llama_4_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T23:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbdi9l</id>
    <title>Raspberry Pi 5: a small comparison between Qwen3 0.6B and Microsoft's new BitNet model</title>
    <updated>2025-04-30T10:53:05+00:00</updated>
    <author>
      <name>/u/privacyparachute</name>
      <uri>https://old.reddit.com/user/privacyparachute</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdi9l/raspberry_pi_5_a_small_comparison_between_qwen3/"&gt; &lt;img alt="Raspberry Pi 5: a small comparison between Qwen3 0.6B and Microsoft's new BitNet model" src="https://b.thumbs.redditmedia.com/6lnRiaUi2Hx1wJ6i_Smnb5wkjtY8gFBP5W7g2zDSZzI.jpg" title="Raspberry Pi 5: a small comparison between Qwen3 0.6B and Microsoft's new BitNet model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been doing some quick tests today, and wanted to share my results. I was testing this for a local voice assistant feature. The Raspberry Pi has 4Gb of memory, and is running a &lt;a href="https://www.candlesmarthome.com"&gt;smart home controller&lt;/a&gt; at the same time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen 3 0.6B, Q4 gguf&lt;/strong&gt; &lt;em&gt;using llama.cpp&lt;/em&gt;&lt;br /&gt; - 0.6GB in size&lt;br /&gt; - Uses 600MB of memory&lt;br /&gt; - About 20 tokens per second&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;`./llama-cli -m qwen3_06B_Q4.gguf -c 4096 -cnv -t 4`&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0k8pgez1cyxe1.png?width=2644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=374a7543adcf213a1835a5b8cd39c4c25bf4a0f4"&gt;https://preview.redd.it/0k8pgez1cyxe1.png?width=2644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=374a7543adcf213a1835a5b8cd39c4c25bf4a0f4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;BitNet-b1.58-2B-4T&lt;/strong&gt; &lt;em&gt;using&lt;/em&gt; &lt;a href="https://www.bijanbowen.com/bitnet-b1-58-on-raspberry-pi-4b/"&gt;BitNet&lt;/a&gt; &lt;em&gt;(Microsoft's fork of llama.cpp)&lt;/em&gt;&lt;br /&gt; - 1.2GB in size&lt;br /&gt; - Uses 300MB of memory (!)&lt;br /&gt; - About 7 tokens per second&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mzftb1x4cyxe1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a739aae47625710b9378a37b0ffac5cc030ab11f"&gt;https://preview.redd.it/mzftb1x4cyxe1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a739aae47625710b9378a37b0ffac5cc030ab11f&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;`python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p &amp;quot;Hello from BitNet on Pi5!&amp;quot; -cnv -t 4 -c 4096`&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The low memory use of the BitNet model seems pretty impressive? But what I don't understand is why the BitNet model is relatively slow. Is there a way to improve performance of the BitNet model? Or is Qwen 3 just that fast?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/privacyparachute"&gt; /u/privacyparachute &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdi9l/raspberry_pi_5_a_small_comparison_between_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdi9l/raspberry_pi_5_a_small_comparison_between_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdi9l/raspberry_pi_5_a_small_comparison_between_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T10:53:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kau30f</id>
    <title>Qwen3 vs Gemma 3</title>
    <updated>2025-04-29T17:43:50+00:00</updated>
    <author>
      <name>/u/Sadman782</name>
      <uri>https://old.reddit.com/user/Sadman782</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After playing around with Qwen3, I’ve got mixed feelings. It’s actually pretty solid in math, coding, and reasoning. The hybrid reasoning approach is impressive — it really shines in that area.&lt;/p&gt; &lt;p&gt;But compared to Gemma, there are a few things that feel lacking:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multilingual support&lt;/strong&gt; isn’t great. Gemma 3 12B does better than Qwen3 14B, 30B MoE, and maybe even the 32B dense model in my language.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Factual knowledge&lt;/strong&gt; is really weak — even worse than LLaMA 3.1 8B in some cases. Even the biggest Qwen3 models seem to struggle with facts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No vision capabilities.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ever since Qwen 2.5, I was hoping for better factual accuracy and multilingual capabilities, but unfortunately, it still falls short. But it’s a solid step forward overall. The range of sizes and especially the 30B MoE for speed are great. Also, the hybrid reasoning is genuinely impressive.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What’s your experience been like?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: The poor SimpleQA/Knowledge result has been confirmed here: &lt;a href="https://x.com/nathanhabib1011/status/1917230699582751157"&gt;https://x.com/nathanhabib1011/status/1917230699582751157&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sadman782"&gt; /u/Sadman782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kau30f/qwen3_vs_gemma_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kau30f/qwen3_vs_gemma_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kau30f/qwen3_vs_gemma_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T17:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbdzo2</id>
    <title>Qwen3 32B leading LiveBench / IF / story_generation</title>
    <updated>2025-04-30T11:22:01+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdzo2/qwen3_32b_leading_livebench_if_story_generation/"&gt; &lt;img alt="Qwen3 32B leading LiveBench / IF / story_generation" src="https://preview.redd.it/u3wxgjiaiyxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db301bb03849c31d1716dcac3b89a7a8e6e05f4a" title="Qwen3 32B leading LiveBench / IF / story_generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://livebench.ai/#/?IF=as"&gt;https://livebench.ai/#/?IF=as&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u3wxgjiaiyxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdzo2/qwen3_32b_leading_livebench_if_story_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdzo2/qwen3_32b_leading_livebench_if_story_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T11:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kalkgi</id>
    <title>I just realized Qwen3-30B-A3B is all I need for local LLM</title>
    <updated>2025-04-29T11:26:53+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After I found out that the new Qwen3-30B-A3B MoE is really slow in Ollama, I decided to try LM Studio instead, and it's working as expected, over 100+ tk/s on a power-limited 4090.&lt;/p&gt; &lt;p&gt;After testing it more, I suddenly realized: this one model is all I need! &lt;/p&gt; &lt;p&gt;I tested translation, coding, data analysis, video subtitle and blog summarization, etc. It performs really well on all categories and is super fast. Additionally, it's very VRAM efficient—I still have 4GB VRAM left after maxing out the context length (Q8 cache enabled, Unsloth Q4 UD gguf).&lt;/p&gt; &lt;p&gt;I used to switch between multiple models of different sizes and quantization levels for different tasks, which is why I stuck with Ollama because of its easy model switching. I also keep using an older version of Open WebUI because the managing a large amount of models is much more difficult in the latest version.&lt;/p&gt; &lt;p&gt;Now all I need is LM Studio, the latest Open WebUI, and Qwen3-30B-A3B. I can finally free up some disk space and move my huge model library to the backup drive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kalkgi/i_just_realized_qwen330ba3b_is_all_i_need_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kalkgi/i_just_realized_qwen330ba3b_is_all_i_need_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kalkgi/i_just_realized_qwen330ba3b_is_all_i_need_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T11:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbamoh</id>
    <title>dnakov/anon-kode GitHub repo taken down by Anthropic</title>
    <updated>2025-04-30T07:25:00+00:00</updated>
    <author>
      <name>/u/Economy-Fact-8362</name>
      <uri>https://old.reddit.com/user/Economy-Fact-8362</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub repo dnakov/anon-kode has been hit with a DMCA takedown from Anthropic.&lt;/p&gt; &lt;p&gt;Link to the notice: &lt;a href="https://github.com/github/dmca/blob/master/2025/04/2025-04-28-anthropic.md"&gt;https://github.com/github/dmca/blob/master/2025/04/2025-04-28-anthropic.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo is no longer publicly accessible and all forks have been taken down.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy-Fact-8362"&gt; /u/Economy-Fact-8362 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbamoh/dnakovanonkode_github_repo_taken_down_by_anthropic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbamoh/dnakovanonkode_github_repo_taken_down_by_anthropic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbamoh/dnakovanonkode_github_repo_taken_down_by_anthropic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T07:25:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb59p2</id>
    <title>China's Huawei develops new AI chip, seeking to match Nvidia, WSJ reports</title>
    <updated>2025-04-30T02:00:29+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb59p2/chinas_huawei_develops_new_ai_chip_seeking_to/"&gt; &lt;img alt="China's Huawei develops new AI chip, seeking to match Nvidia, WSJ reports" src="https://external-preview.redd.it/_MhQV-xUqiO3Y0IIwXyN_CLCzP6Uu0GLDq4JaPxeNyI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=101f90eb751f4582a08ef73c287e17ff0e2987c8" title="China's Huawei develops new AI chip, seeking to match Nvidia, WSJ reports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/04/27/chinas-huawei-develops-new-ai-chip-seeking-to-match-nvidia-wsj-reports.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb59p2/chinas_huawei_develops_new_ai_chip_seeking_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb59p2/chinas_huawei_develops_new_ai_chip_seeking_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T02:00:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb1qz6</id>
    <title>INTELLECT-2 finished training today</title>
    <updated>2025-04-29T23:06:45+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb1qz6/intellect2_finished_training_today/"&gt; &lt;img alt="INTELLECT-2 finished training today" src="https://external-preview.redd.it/2TaHsZn-6uiirSACnSyMrnV5EoZxDaj8F8MVB4S1kQw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8f21a2effbdc013afb553d8655b0b5e6a67982c" title="INTELLECT-2 finished training today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://app.primeintellect.ai/intelligence/intellect-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb1qz6/intellect2_finished_training_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb1qz6/intellect2_finished_training_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T23:06:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1katz6u</id>
    <title>No new models in LlamaCon announced</title>
    <updated>2025-04-29T17:39:27+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1katz6u/no_new_models_in_llamacon_announced/"&gt; &lt;img alt="No new models in LlamaCon announced" src="https://external-preview.redd.it/XRPU2B1-WyXMxWy34TlfRr2ItA7lCciwL2CfvgMnsfE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=353ac7ecc5aa40634908575f505f14f208cf6093" title="No new models in LlamaCon announced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess it wasn’t good enough &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/llamacon-llama-news/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1katz6u/no_new_models_in_llamacon_announced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1katz6u/no_new_models_in_llamacon_announced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T17:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb7dqt</id>
    <title>Xiaomi MiMo - MiMo-7B-RL</title>
    <updated>2025-04-30T03:53:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb7dqt/xiaomi_mimo_mimo7brl/"&gt; &lt;img alt="Xiaomi MiMo - MiMo-7B-RL" src="https://external-preview.redd.it/QhG2hxjpDpBYc1nPejzCvz7HJ-lDjRJrjJUX5Q5zU8w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a19f7779daa5666d3a2ff3dafca48ef50e0c785e" title="Xiaomi MiMo - MiMo-7B-RL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-7B-RL"&gt;https://huggingface.co/XiaomiMiMo/MiMo-7B-RL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short Summary by Qwen3-30B-A3B:&lt;/strong&gt;&lt;br /&gt; This work introduces &lt;em&gt;MiMo-7B&lt;/em&gt;, a series of reasoning-focused language models trained from scratch, demonstrating that small models can achieve exceptional mathematical and code reasoning capabilities, even outperforming larger 32B models. Key innovations include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pre-training optimizations&lt;/strong&gt;: Enhanced data pipelines, multi-dimensional filtering, and a three-stage data mixture (25T tokens) with &lt;em&gt;Multiple-Token Prediction&lt;/em&gt; for improved reasoning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Post-training techniques&lt;/strong&gt;: Curated 130K math/code problems with rule-based rewards, a difficulty-driven code reward for sparse tasks, and data re-sampling to stabilize RL training.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RL infrastructure&lt;/strong&gt;: A &lt;em&gt;Seamless Rollout Engine&lt;/em&gt; accelerates training/validation by 2.29×/1.96×, paired with robust inference support. MiMo-7B-RL matches OpenAI’s o1-mini on reasoning tasks, with all models (base, SFT, RL) open-sourced to advance the community’s development of powerful reasoning LLMs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rhbeynh1awxe1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78ac27cfa4b73b3fcc1cb591f7a1a7b314700ec2"&gt;https://preview.redd.it/rhbeynh1awxe1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78ac27cfa4b73b3fcc1cb591f7a1a7b314700ec2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb7dqt/xiaomi_mimo_mimo7brl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb7dqt/xiaomi_mimo_mimo7brl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb7dqt/xiaomi_mimo_mimo7brl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T03:53:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbdk08</id>
    <title>GitHub - XiaomiMiMo/MiMo: MiMo: Unlocking the Reasoning Potential of Language Model – From Pretraining to Posttraining</title>
    <updated>2025-04-30T10:56:08+00:00</updated>
    <author>
      <name>/u/marcocastignoli</name>
      <uri>https://old.reddit.com/user/marcocastignoli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdk08/github_xiaomimimomimo_mimo_unlocking_the/"&gt; &lt;img alt="GitHub - XiaomiMiMo/MiMo: MiMo: Unlocking the Reasoning Potential of Language Model – From Pretraining to Posttraining" src="https://external-preview.redd.it/U_NJ00xovJr4fKj6AJwQ35Odzc6w6jc272-TXHXNUC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=471cbfb82ac86afc15e99dceb63f180024f5a0b3" title="GitHub - XiaomiMiMo/MiMo: MiMo: Unlocking the Reasoning Potential of Language Model – From Pretraining to Posttraining" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marcocastignoli"&gt; /u/marcocastignoli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/XiaomiMiMo/MiMo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdk08/github_xiaomimimomimo_mimo_unlocking_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdk08/github_xiaomimimomimo_mimo_unlocking_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T10:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb8yyw</id>
    <title>DFloat11: Lossless LLM Compression for Efficient GPU Inference</title>
    <updated>2025-04-30T05:31:23+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/LeanModels/DFloat11"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb8yyw/dfloat11_lossless_llm_compression_for_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb8yyw/dfloat11_lossless_llm_compression_for_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T05:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaodxu</id>
    <title>Qwen3 Unsloth Dynamic GGUFs + 128K Context + Bug Fixes</title>
    <updated>2025-04-29T13:48:38+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Localllama"&gt;r/Localllama&lt;/a&gt;! We've uploaded Dynamic 2.0 GGUFs and quants for Qwen3. &lt;strong&gt;ALL Qwen3&lt;/strong&gt; models now benefit from &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;Dynamic 2.0&lt;/a&gt; format.&lt;/p&gt; &lt;p&gt;We've also &lt;strong&gt;fixed all chat template &amp;amp; loading issues.&lt;/strong&gt; They now work properly on all inference engines (llama.cpp, Ollama, LM Studio, Open WebUI etc.)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;These bugs came from incorrect chat template implementations, &lt;strong&gt;not&lt;/strong&gt; the Qwen team. We've informed them, and they’re helping fix it in places like llama.cpp. Small bugs like this happen all the time, and it was through your guy's feedback that we were able to catch this. Some GGUFs defaulted to using the &lt;code&gt;chat_ml&lt;/code&gt; template, so they seemed to work but it's actually incorrect. All our uploads are now corrected.&lt;/li&gt; &lt;li&gt;Context length has been extended from 32K to &lt;strong&gt;128K&lt;/strong&gt; using native YaRN.&lt;/li&gt; &lt;li&gt;Some &lt;strong&gt;235B-A22B&lt;/strong&gt; quants aren't compatible with iMatrix + Dynamic 2.0 despite many testing. We're uploaded as many standard GGUF sizes as possible and left a few of the iMatrix + Dynamic 2.0 that do work.&lt;/li&gt; &lt;li&gt;Thanks to your feedback, we now added Q4_NL, Q5.1, Q5.0, Q4.1, and Q4.0 formats.&lt;/li&gt; &lt;li&gt;ICYMI: Dynamic 2.0 sets new benchmarks for KL Divergence and 5-shot MMLU, making it the best performing quants for running LLMs. &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;See benchmarks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;We also uploaded Dynamic safetensors for fine-tuning/deployment. Fine-tuning is technically supported in Unsloth, but please wait for the official announcement coming very soon.&lt;/li&gt; &lt;li&gt;We made a detailed guide on how to run Qwen3 (including 235B-A22B) with official settings: &lt;a href="https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune"&gt;https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen3 - Official Settings:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Setting&lt;/th&gt; &lt;th align="left"&gt;Non-Thinking Mode&lt;/th&gt; &lt;th align="left"&gt;Thinking Mode&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Temperature&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.7&lt;/td&gt; &lt;td align="left"&gt;0.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Min_P&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.0 (optional, but 0.01 works well; llama.cpp default is 0.1)&lt;/td&gt; &lt;td align="left"&gt;0.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Top_P&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.8&lt;/td&gt; &lt;td align="left"&gt;0.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;TopK&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Qwen3 - Unsloth Dynamic 2.0 Uploads -with optimal configs:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Qwen3 variant&lt;/th&gt; &lt;th align="left"&gt;GGUF&lt;/th&gt; &lt;th align="left"&gt;GGUF (128K Context)&lt;/th&gt; &lt;th align="left"&gt;Dynamic 4-bit Safetensor&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;0.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-0.6B-GGUF"&gt;0.6B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-0.6B-128K-GGUF"&gt;0.6B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit"&gt;0.6B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;1.7B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-1.7B-GGUF"&gt;1.7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-1.7B-128K-GGUF"&gt;1.7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-1.7B-unsloth-bnb-4bit"&gt;1.7B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;4B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-4B-GGUF"&gt;4B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-4B-128K-GGUF"&gt;4B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-4B-unsloth-bnb-4bit"&gt;4B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-8B-GGUF"&gt;8B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF"&gt;8B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-8B-unsloth-bnb-4bit"&gt;8B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;14B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-14B-GGUF"&gt;14B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-14B-128K-GGUF"&gt;14B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-14B-unsloth-bnb-4bit"&gt;14B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;30B-A3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF"&gt;30B-A3B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-128K-GGUF"&gt;30B-A3B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-GGUF"&gt;32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF"&gt;32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-unsloth-bnb-4bit"&gt;32B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Also wanted to give a huge shoutout to the Qwen team for helping us and the open-source community with their incredible team support! And of course thank you to you all for reporting and testing the issues with us! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaodxu/qwen3_unsloth_dynamic_ggufs_128k_context_bug_fixes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaodxu/qwen3_unsloth_dynamic_ggufs_128k_context_bug_fixes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaodxu/qwen3_unsloth_dynamic_ggufs_128k_context_bug_fixes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T13:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb5v6h</id>
    <title>Thoughts on Mistral.rs</title>
    <updated>2025-04-30T02:31:02+00:00</updated>
    <author>
      <name>/u/EricBuehler</name>
      <uri>https://old.reddit.com/user/EricBuehler</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I'm the developer of &lt;a href="https://github.com/EricLBuehler/mistral.rs"&gt;mistral.rs&lt;/a&gt;, and I wanted to gauge community interest and feedback.&lt;/p&gt; &lt;p&gt;Do you use mistral.rs? Have you heard of mistral.rs?&lt;/p&gt; &lt;p&gt;Please let me know! I'm open to any feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EricBuehler"&gt; /u/EricBuehler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb5v6h/thoughts_on_mistralrs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb5v6h/thoughts_on_mistralrs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb5v6h/thoughts_on_mistralrs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T02:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaqhxy</id>
    <title>Llama 4 reasoning 17b model releasing today</title>
    <updated>2025-04-29T15:18:43+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqhxy/llama_4_reasoning_17b_model_releasing_today/"&gt; &lt;img alt="Llama 4 reasoning 17b model releasing today" src="https://preview.redd.it/hy71sz0sjsxe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ff1abe7a5307a69a23eb4ca02f7f5cbb5bafaf4" title="Llama 4 reasoning 17b model releasing today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hy71sz0sjsxe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqhxy/llama_4_reasoning_17b_model_releasing_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqhxy/llama_4_reasoning_17b_model_releasing_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T15:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb97ys</id>
    <title>ubergarm/Qwen3-235B-A22B-GGUF over 140 tok/s PP and 10 tok/s TG quant for gaming rigs!</title>
    <updated>2025-04-30T05:47:24+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb97ys/ubergarmqwen3235ba22bgguf_over_140_toks_pp_and_10/"&gt; &lt;img alt="ubergarm/Qwen3-235B-A22B-GGUF over 140 tok/s PP and 10 tok/s TG quant for gaming rigs!" src="https://external-preview.redd.it/cYA9q4-I6muEuEtTYSLVR3CPWT7qR_y4KkbLTSvSOXY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b11d509cf500d20e1a15b7a569f5bf8fcc448a5f" title="ubergarm/Qwen3-235B-A22B-GGUF over 140 tok/s PP and 10 tok/s TG quant for gaming rigs!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just cooked up an experimental ik_llama.cpp exclusive 3.903 BPW quant blend for Qwen3-235B-A22B that delivers good quality and speed on a high end gaming rig fitting full 32k context in under 120 GB (V)RAM e.g. 24GB VRAM + 2x48GB DDR5 RAM.&lt;/p&gt; &lt;p&gt;Just benchmarked over 140 tok/s prompt processing and 10 tok/s generation on my 3090TI FE + AMD 9950X 96GB RAM DDR5-6400 gaming rig (see comment for graph).&lt;/p&gt; &lt;p&gt;Keep in mind this quant is *not* supported by mainline llama.cpp, ollama, koboldcpp, lm studio etc. I'm not releasing those as mainstream quality quants are available from bartowski, unsloth, mradermacher, et al.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb97ys/ubergarmqwen3235ba22bgguf_over_140_toks_pp_and_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb97ys/ubergarmqwen3235ba22bgguf_over_140_toks_pp_and_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T05:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kay93z</id>
    <title>You can run Qwen3-30B-A3B on a 16GB RAM CPU-only PC!</title>
    <updated>2025-04-29T20:36:39+00:00</updated>
    <author>
      <name>/u/Foxiya</name>
      <uri>https://old.reddit.com/user/Foxiya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got the Qwen3-30B-A3B model in q4 running on my CPU-only PC using llama.cpp, and honestly, I’m blown away by how well it's performing. I'm running the q4 quantized version of the model, and despite having just 16GB of RAM and no GPU, I’m consistently getting more than 10 tokens per second.&lt;/p&gt; &lt;p&gt;I wasnt expecting much given the size of the model and my relatively modest hardware setup. I figured it would crawl or maybe not even load at all, but to my surprise, it's actually snappy and responsive for many tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foxiya"&gt; /u/Foxiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kay93z/you_can_run_qwen330ba3b_on_a_16gb_ram_cpuonly_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kay93z/you_can_run_qwen330ba3b_on_a_16gb_ram_cpuonly_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kay93z/you_can_run_qwen330ba3b_on_a_16gb_ram_cpuonly_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T20:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbazrd</id>
    <title>Qwen3 on LiveBench</title>
    <updated>2025-04-30T07:52:20+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbazrd/qwen3_on_livebench/"&gt; &lt;img alt="Qwen3 on LiveBench" src="https://b.thumbs.redditmedia.com/IlWChU1967WBI_0P25pC4bLTg5smTk6gYbPMDSLkzTM.jpg" title="Qwen3 on LiveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://livebench.ai/#/"&gt;https://livebench.ai/#/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9wg6nkargxxe1.png?width=925&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d9016c13d45318a17731b376fb4e39f640251aa"&gt;https://preview.redd.it/9wg6nkargxxe1.png?width=925&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d9016c13d45318a17731b376fb4e39f640251aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n6nx96prgxxe1.png?width=947&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b4d9a9ac2f50f9bd95667bc088b3d388536d09b"&gt;https://preview.redd.it/n6nx96prgxxe1.png?width=947&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b4d9a9ac2f50f9bd95667bc088b3d388536d09b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bqf2671sgxxe1.png?width=940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e465c0584d3a19b3dd372b86397f737ac8d04e5c"&gt;https://preview.redd.it/bqf2671sgxxe1.png?width=940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e465c0584d3a19b3dd372b86397f737ac8d04e5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3mi1zmhxgxxe1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=069841c9dada3aaf5c340977f6e0db382e868c53"&gt;https://preview.redd.it/3mi1zmhxgxxe1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=069841c9dada3aaf5c340977f6e0db382e868c53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/28rqjpuxgxxe1.png?width=1048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dacdc258ff44e6eabcb2bc94c555a7e84d36662b"&gt;https://preview.redd.it/28rqjpuxgxxe1.png?width=1048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dacdc258ff44e6eabcb2bc94c555a7e84d36662b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbazrd/qwen3_on_livebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbazrd/qwen3_on_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbazrd/qwen3_on_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T07:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbbz1n</id>
    <title>New model DeepSeek-Prover-V2-671B</title>
    <updated>2025-04-30T09:06:09+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbz1n/new_model_deepseekproverv2671b/"&gt; &lt;img alt="New model DeepSeek-Prover-V2-671B" src="https://preview.redd.it/v83oaiaztxxe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73be969d15c5e7a4e0f77d22ddd89e84ed0cccd6" title="New model DeepSeek-Prover-V2-671B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;link: &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B/tree/main"&gt;https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B/tree/main&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v83oaiaztxxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbz1n/new_model_deepseekproverv2671b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbz1n/new_model_deepseekproverv2671b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T09:06:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbaecl</id>
    <title>Honestly, THUDM might be the new star on the horizon (creators of GLM-4)</title>
    <updated>2025-04-30T07:07:51+00:00</updated>
    <author>
      <name>/u/dampflokfreund</name>
      <uri>https://old.reddit.com/user/dampflokfreund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've read many comments here saying that THUDM/GLM-4-32B-0414 is better than the latest Qwen 3 models and I have to agree. The 9B is also very good and fits in just 6 GB VRAM at IQ4_XS. These GLM-4 models have crazy efficient attention (less VRAM usage for context than any other model I've tried.)&lt;/p&gt; &lt;p&gt;It does better in my tests, I like its personality and writing style more and imo it also codes better. &lt;/p&gt; &lt;p&gt;I didn't expect these pretty unknown model creators to beat Qwen 3 to be honest, so if they keep it up they might have a chance to become the next DeepSeek. &lt;/p&gt; &lt;p&gt;There's nice room for improvement, like native multimodality, hybrid reasoning and better multilingual support (it leaks chinese characters sometimes, sadly)&lt;/p&gt; &lt;p&gt;What are your experiences with these models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dampflokfreund"&gt; /u/dampflokfreund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbaecl/honestly_thudm_might_be_the_new_star_on_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbaecl/honestly_thudm_might_be_the_new_star_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbaecl/honestly_thudm_might_be_the_new_star_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T07:07:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbbt74</id>
    <title>DeepSeek-Prover-V2-671B is released</title>
    <updated>2025-04-30T08:54:11+00:00</updated>
    <author>
      <name>/u/Thin_Ad7360</name>
      <uri>https://old.reddit.com/user/Thin_Ad7360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thin_Ad7360"&gt; /u/Thin_Ad7360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbt74/deepseekproverv2671b_is_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbt74/deepseekproverv2671b_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbt74/deepseekproverv2671b_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T08:54:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbbcp8</id>
    <title>deepseek-ai/DeepSeek-Prover-V2-671B · Hugging Face</title>
    <updated>2025-04-30T08:18:47+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbcp8/deepseekaideepseekproverv2671b_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-Prover-V2-671B · Hugging Face" src="https://external-preview.redd.it/MfS0kbAv6ZSxumHFhRuKL1EVBrJ457E-QoycmpgMTBk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff61d01135c5b0a1b9141534d8a5e2c46dbfa952" title="deepseek-ai/DeepSeek-Prover-V2-671B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbcp8/deepseekaideepseekproverv2671b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbcp8/deepseekaideepseekproverv2671b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T08:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb6bbl</id>
    <title>New study from Cohere shows Lmarena (formerly known as Lmsys Chatbot Arena) is heavily rigged against smaller open source model providers and favors big companies like Google, OpenAI and Meta</title>
    <updated>2025-04-30T02:54:14+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb6bbl/new_study_from_cohere_shows_lmarena_formerly/"&gt; &lt;img alt="New study from Cohere shows Lmarena (formerly known as Lmsys Chatbot Arena) is heavily rigged against smaller open source model providers and favors big companies like Google, OpenAI and Meta" src="https://b.thumbs.redditmedia.com/b4ReiD50vW0GnRbxWwbT4dVjH2h_fZBzjjqsVwguaHU.jpg" title="New study from Cohere shows Lmarena (formerly known as Lmsys Chatbot Arena) is heavily rigged against smaller open source model providers and favors big companies like Google, OpenAI and Meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Meta tested over 27 private variants, Google 10 to select the best performing one. \&lt;/li&gt; &lt;li&gt;OpenAI and Google get the majority of data from the arena (~40%). &lt;/li&gt; &lt;li&gt;All closed source providers get more frequently featured in the battles.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2504.20879"&gt;https://arxiv.org/abs/2504.20879&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kb6bbl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb6bbl/new_study_from_cohere_shows_lmarena_formerly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb6bbl/new_study_from_cohere_shows_lmarena_formerly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T02:54:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb3gox</id>
    <title>Technically Correct, Qwen 3 working hard</title>
    <updated>2025-04-30T00:29:20+00:00</updated>
    <author>
      <name>/u/poli-cya</name>
      <uri>https://old.reddit.com/user/poli-cya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb3gox/technically_correct_qwen_3_working_hard/"&gt; &lt;img alt="Technically Correct, Qwen 3 working hard" src="https://preview.redd.it/dudbg02v9vxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40a85bf16e35037679b06a0406d1230e4e6050e5" title="Technically Correct, Qwen 3 working hard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poli-cya"&gt; /u/poli-cya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dudbg02v9vxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb3gox/technically_correct_qwen_3_working_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb3gox/technically_correct_qwen_3_working_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T00:29:20+00:00</published>
  </entry>
</feed>
