<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-25T01:36:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1myigna</id>
    <title>"Why are you all so worried whenever the big companies talk about LLM safety? What's the worst that could happen?"</title>
    <updated>2025-08-24T01:08:43+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myigna/why_are_you_all_so_worried_whenever_the_big/"&gt; &lt;img alt="&amp;quot;Why are you all so worried whenever the big companies talk about LLM safety? What's the worst that could happen?&amp;quot;" src="https://external-preview.redd.it/amthMTBncThhdmtmMeYkHvQl6ANcbp9DAX5oa2nUyz5pQDo1cq9KjrP_m95D.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23c6d3e5b22e0c35924b85610790740771d73e6b" title="&amp;quot;Why are you all so worried whenever the big companies talk about LLM safety? What's the worst that could happen?&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r0ym4gq8avkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myigna/why_are_you_all_so_worried_whenever_the_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myigna/why_are_you_all_so_worried_whenever_the_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T01:08:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1myb09v</id>
    <title>Google and Anthropic struggle to keep marketshare as everyone else catches up</title>
    <updated>2025-08-23T19:44:10+00:00</updated>
    <author>
      <name>/u/ObnoxiouslyVivid</name>
      <uri>https://old.reddit.com/user/ObnoxiouslyVivid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"&gt; &lt;img alt="Google and Anthropic struggle to keep marketshare as everyone else catches up" src="https://preview.redd.it/35p1pim9ntkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e48d4b2543aa0cd859924de94edd03937a9fc35a" title="Google and Anthropic struggle to keep marketshare as everyone else catches up" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Data from last 6 months on OpenRouter compared to now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObnoxiouslyVivid"&gt; /u/ObnoxiouslyVivid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/35p1pim9ntkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myb09v/google_and_anthropic_struggle_to_keep_marketshare/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T19:44:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytkpp</id>
    <title>Do you still use mikupad or is there a replacement?</title>
    <updated>2025-08-24T11:52:58+00:00</updated>
    <author>
      <name>/u/aeroumbria</name>
      <uri>https://old.reddit.com/user/aeroumbria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mikupad was my go-to tool for generating text with the option to show alternative tokens. This is especially useful for getting a feel of a model's preferences, writing stories, hacking context, or just working with non-conversational tasks in general. However, it has not been updated for a while, and although still fully functional, I actually had to revert to an earlier commit to make alternative tokens work, as the last commit broke the function, and the prospect of this function breaking again with no fix is not reassuring. Has anyone found a good alternative for mikupad, or is it still the best tool we have for now? &lt;/p&gt; &lt;p&gt;In case this is not clear enough, by &amp;quot;alternative tokens&amp;quot; I mean the ability to see the top K options at each step of the generation, and in mikupad you can even click any of them and restart generation using the selected choice as the last input.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aeroumbria"&gt; /u/aeroumbria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytkpp/do_you_still_use_mikupad_or_is_there_a_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytkpp/do_you_still_use_mikupad_or_is_there_a_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytkpp/do_you_still_use_mikupad_or_is_there_a_replacement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:52:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytbfz</id>
    <title>Accuracy recovery adapter with self-generated data (magpie-style)</title>
    <updated>2025-08-24T11:39:03+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLama"&gt;r/LocalLLama&lt;/a&gt;! Wanted to share a technique that's been working really well for recovering performance after INT4 quantization. &lt;/p&gt; &lt;p&gt;Typically, quantizing the LLM to INT4 (unlike say INT8) for inference can incur some accuracy loss. Instead of accepting the quality loss, we used the FP16 model as a teacher to train a tiny LoRA adapter (rank=16) for the quantized model. The cool part: the model generates its own training data using the Magpie technique so no external datasets needed. This is critical because we want to remain as much as possible in the distribution of the model's natural responses. &lt;/p&gt; &lt;p&gt;Last year Apple's foundational models paper (&lt;a href="https://arxiv.org/pdf/2407.21075"&gt;https://arxiv.org/pdf/2407.21075&lt;/a&gt;) had proposed a similar technique and found &amp;quot;By using accuracy-recovery LoRA adapters with only rank 16, Alpaca win rate can be improved by 7-18%, GMS8K accuracy is boosted by 5-10%.&amp;quot; (page 47). &lt;/p&gt; &lt;p&gt;We saw similar results on Qwen3-0.6B:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Perplexity: 2.40 → 2.09 (only 5.7% degradation from FP16 baseline)&lt;/li&gt; &lt;li&gt;Memory: Only 0.28GB vs 1.0GB for FP16 (75% reduction)&lt;/li&gt; &lt;li&gt;Speed: 3.0x faster inference than FP16&lt;/li&gt; &lt;li&gt;Quality: Generates correct, optimized code solutions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://colab.research.google.com/github/codelion/ellora/blob/main/Ellora_Recipe_1_Self_Distillation_For_Quantization_Recovery.ipynb"&gt;Colab notebook with full implementation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/codelion/Qwen3-0.6B-accuracy-recovery-lora"&gt;Pre-trained adapter on HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/codelion/ellora"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the implementation or help anyone trying to replicate this. The key insight is that quantization errors are systematic and learnable - a small adapter can bridge the gap without negating the benefits of quantization.&lt;/p&gt; &lt;p&gt;Has anyone else experimented with self-distillation for quantization recovery? Would love to hear about different approaches!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytbfz/accuracy_recovery_adapter_with_selfgenerated_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytbfz/accuracy_recovery_adapter_with_selfgenerated_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytbfz/accuracy_recovery_adapter_with_selfgenerated_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4vwu</id>
    <title>What is the smallest model that rivals GPT-3.5?</title>
    <updated>2025-08-24T19:25:09+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I was recently looking at an old project of mine that i did as my bachelor's thesis back in Q2 2023 where i created a multi-agent system using one of the first versions of langchain and GPT-3.5. &lt;/p&gt; &lt;p&gt;This made me think about all the progress that we've made in the LLM world in such a short period of time, especially in the open-source space.&lt;/p&gt; &lt;p&gt;So, as the title suggests, What do you think is the smallest, open-source model that is &lt;em&gt;generally&lt;/em&gt; as good or better than GPT-3.5? I'm' not talking about a specific task, but general knowledge, intelligence and capability of completing a wide array of tasks. My guess would be something in the 30B parameter count, such as Qwen3-32B. Maybe with reasoning this number could go even lower, but i personally think it's a bit like cheating because we didn't have reasoning back in Q2 2023. &lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4vwu/what_is_the_smallest_model_that_rivals_gpt35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4vwu/what_is_the_smallest_model_that_rivals_gpt35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4vwu/what_is_the_smallest_model_that_rivals_gpt35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:25:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mymyfu</id>
    <title>A timeline of LLM Context Windows, Over the past 5 years. (done right this time)</title>
    <updated>2025-08-24T05:09:43+00:00</updated>
    <author>
      <name>/u/jack-ster</name>
      <uri>https://old.reddit.com/user/jack-ster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1mymyfu/video/hi8umq5ehwkf1/player"&gt;https://reddit.com/link/1mymyfu/video/hi8umq5ehwkf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/CD9QEbCZ"&gt;https://pastebin.com/CD9QEbCZ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jack-ster"&gt; /u/jack-ster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mymyfu/a_timeline_of_llm_context_windows_over_the_past_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T05:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz0tqc</id>
    <title>MALM: A Modular Adapter-based Language Model (paper + Hugging Face link)</title>
    <updated>2025-08-24T16:53:04+00:00</updated>
    <author>
      <name>/u/TimesLast_</name>
      <uri>https://old.reddit.com/user/TimesLast_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I just finished writing a short paper about a new idea I call MALM, a Modular Adapter-based Language Model.&lt;/p&gt; &lt;p&gt;The core idea is simple: instead of training giant multilingual LLMs, I propose keeping one small, sharp Core Language Model (reasoning in English), and delegating translation to lightweight, swappable Specialized Translation Adapters (STAs).&lt;/p&gt; &lt;p&gt;This means:&lt;/p&gt; &lt;p&gt;- Smaller, cheaper models&lt;/p&gt; &lt;p&gt;- Easy to add new languages&lt;/p&gt; &lt;p&gt;- Better for edge devices and low-resource settings&lt;/p&gt; &lt;p&gt;Example flow:&lt;br /&gt; ```&lt;br /&gt; User: &amp;quot;Translate 'my name is Adam' into German.&amp;quot;&lt;br /&gt; CLM → &amp;lt;to:de&amp;gt; my name is Adam &amp;lt;/to&amp;gt;&lt;br /&gt; STA → &amp;quot;Mein Name ist Adam&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Read the full paper here: &lt;a href="https://huggingface.co/TimesLast/MALM"&gt;https://huggingface.co/TimesLast/MALM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, especially on how this could be extended beyond translation (math, code, multimodal adapters, etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TimesLast_"&gt; /u/TimesLast_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0tqc/malm_a_modular_adapterbased_language_model_paper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0tqc/malm_a_modular_adapterbased_language_model_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0tqc/malm_a_modular_adapterbased_language_model_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T16:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1myvpqv</id>
    <title>Open Source Tool for Manga translation</title>
    <updated>2025-08-24T13:33:30+00:00</updated>
    <author>
      <name>/u/New_Blueberry9858</name>
      <uri>https://old.reddit.com/user/New_Blueberry9858</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are some paid tools for manga translation, like INKR studio, but turns out to be pretty expensive. Thus our team at curify-ai worked on our custom manga translation tool and decided to open source the prototype at : &lt;a href="https://huggingface.co/spaces/Curify/manga_translation"&gt;https://huggingface.co/spaces/Curify/manga_translation&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The prototype features the following:&lt;br /&gt; a. Horizontally cropping skinny manga images to improve its visibility.&lt;/p&gt; &lt;p&gt;b. Using PaddleOCR to detect text and use a polygon based approach for inpaint. Still need to improve OCR and inpainting method, Qwen might be a good candidate.&lt;/p&gt; &lt;p&gt;c. Translate with Microsoft translator and allow customization of translated text.&lt;/p&gt; &lt;p&gt;d. Render the translated image.&lt;/p&gt; &lt;p&gt;It's still work in progress, welcome to use and suggest improvements. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Blueberry9858"&gt; /u/New_Blueberry9858 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myvpqv/open_source_tool_for_manga_translation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myvpqv/open_source_tool_for_manga_translation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myvpqv/open_source_tool_for_manga_translation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T13:33:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz644g</id>
    <title>PCIe Bifurcation x4x4x4x4 Question</title>
    <updated>2025-08-24T20:12:17+00:00</updated>
    <author>
      <name>/u/ducksaysquackquack</name>
      <uri>https://old.reddit.com/user/ducksaysquackquack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: has anybody run into problems running pcie x16 to x4x4x4x4 on consumer hardware?&lt;/p&gt; &lt;p&gt;current setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;9800x3d (28 total pcie lanes, 24 usable lanes with 4 going to chipset)&lt;/li&gt; &lt;li&gt;64gb ddr5-6000&lt;/li&gt; &lt;li&gt;MSI x670e Mag Tomahawk WIFI board&lt;/li&gt; &lt;li&gt;5090 in pcie 5.0 x16 slot (cpu)&lt;/li&gt; &lt;li&gt;4090 in pcie 4.0 x4 slot (cpu)&lt;/li&gt; &lt;li&gt;3090ti in pcie 4.0 x2 slot (chipset)&lt;/li&gt; &lt;li&gt;Corsair HX1500i psu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;i have two 3060 12gb that i have laying around and would like to add to the system, if anything just for the sake of using them instead of sitting in box. i would like to pick up two 3090 off fb market, but i'm not really trying to spend $500-$600 each for what folks are asking in my area. and since i already had these 3060 sitting around, why not use them. &lt;/p&gt; &lt;p&gt;i don't believe i'll have power issues since right now, aida64 sensor panel shows the hx1500i hitting max 950w during inference. psu connects via usb for power monitoring. i can't imagine the 3060 using more than 150w each, since they're only 1x8-pin each.&lt;/p&gt; &lt;p&gt;bios shows x16 slot can do either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;x8x8&lt;/li&gt; &lt;li&gt;x8x4x4&lt;/li&gt; &lt;li&gt;x4x4x4x4&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;also, all i can find are $20-$50 bifurcation cards that are pcie 3.0, would dropping to gen3 be an issue during inference?&lt;/p&gt; &lt;p&gt;i'd like to have 5090/4090/3090ti/3060 on the bifurcation card and second 3060 on the pcie secondary x16 slot. hopefully add 3090 down the line if they price drop after the new supers release later this year.&lt;/p&gt; &lt;p&gt;if this is not worth it, then it's no biggie. i just like tinkering.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ducksaysquackquack"&gt; /u/ducksaysquackquack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz644g/pcie_bifurcation_x4x4x4x4_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz644g/pcie_bifurcation_x4x4x4x4_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz644g/pcie_bifurcation_x4x4x4x4_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T20:12:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz0640</id>
    <title>Best small local llm for coding</title>
    <updated>2025-08-24T16:28:21+00:00</updated>
    <author>
      <name>/u/Low-Palpitation-4724</name>
      <uri>https://old.reddit.com/user/Low-Palpitation-4724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;br /&gt; I am looking for good small llm for coding. By small i mean somewhere around 10b parameters like gemma3:12b or codegemma. I like them both but first one is not specifically coding model and second one is a year old. Does anyone have some suggestions about other good models or a place that benchmarks those? I am talking about those small models because i use them on gpu with 12gb vram or even laptop with 8.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low-Palpitation-4724"&gt; /u/Low-Palpitation-4724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0640/best_small_local_llm_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0640/best_small_local_llm_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz0640/best_small_local_llm_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mybft5</id>
    <title>grok 2 weights</title>
    <updated>2025-08-23T20:00:52+00:00</updated>
    <author>
      <name>/u/HatEducational9965</name>
      <uri>https://old.reddit.com/user/HatEducational9965</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt; &lt;img alt="grok 2 weights" src="https://external-preview.redd.it/4tfHT9vpFrwHCpX5cn0_tHyoUS8M6oeQ7jwWbePCicw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9576154cc1820a09f2c9b345d4d88427c3729b9a" title="grok 2 weights" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HatEducational9965"&gt; /u/HatEducational9965 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/xai-org/grok-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mybft5/grok_2_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T20:00:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mypokb</id>
    <title>GPT OSS 20b is Impressive at Instruction Following</title>
    <updated>2025-08-24T07:56:56+00:00</updated>
    <author>
      <name>/u/crodjer</name>
      <uri>https://old.reddit.com/user/crodjer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have found GPT OSS 20b to be consistently great at following complex instructions. For instance, it did performed perfectly with a test prompt I used: &lt;a href="https://github.com/crodjer/glaince/tree/main/cipher#results"&gt;https://github.com/crodjer/glaince/tree/main/cipher#results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All other models in the same size (Gemma 3, Qwen 3, Mistral Small) make the same mistake, resulting them to deviate from expectation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crodjer"&gt; /u/crodjer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mypokb/gpt_oss_20b_is_impressive_at_instruction_following/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T07:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mytpf1</id>
    <title>Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)</title>
    <updated>2025-08-24T11:59:59+00:00</updated>
    <author>
      <name>/u/Mass2018</name>
      <uri>https://old.reddit.com/user/Mass2018</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"&gt; &lt;img alt="Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)" src="https://b.thumbs.redditmedia.com/x4bULEyzYY3EEBanwjUi7dZiywWjI4EP8X8WNQAbYyk.jpg" title="Apple M3 Ultra w/28-Core CPU, 60-Core GPU (256GB RAM) Running Deepseek-R1-UD-IQ1_S (140.23GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of discussion recently about the performance of the Apple studios with large models, so I thought I'd share actual data from about a month of usage in our household.&lt;/p&gt; &lt;p&gt;This is mainly used by the non-me part of our household, so it sits nice and stable and just runs Deepseek 24/7, where my personal rig is constantly being swapped between different things that I'm working on.&lt;/p&gt; &lt;p&gt;The Apple Studio replaced the 10xP100 rig I had previously built for this purpose, and I have to say for what we're using it for it's been a godsend. It's much, much faster, can load larger models, has a much lower power footprint, and it was just... so easy to get it up and running. Honestly, it felt a bit like cheating after the hell that the P100 rig put me through.&lt;/p&gt; &lt;p&gt;Anyway, actual numbers:&lt;/p&gt; &lt;p&gt;|| || |Total logged requests:|161| |Context Average:|643.72| |Average Prompt Eval Tokens/Second:|64.73 tokens/second| |Average Tokens Generated:|343.16| |Average Tokens Generated/Second:|13.97 tokens/second|&lt;/p&gt; &lt;p&gt;My personal opinion is if all you're going to do is inferencing, it's a great option. I absolutely loathe the Mac GUI, and my constant attempt to control-c/control-v is infuriating, but other than that... NO RAGRETS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mass2018"&gt; /u/Mass2018 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mytpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mytpf1/apple_m3_ultra_w28core_cpu_60core_gpu_256gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T11:59:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mzaeee</id>
    <title>InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing</title>
    <updated>2025-08-24T23:05:11+00:00</updated>
    <author>
      <name>/u/Dull-Ad-1708</name>
      <uri>https://old.reddit.com/user/Dull-Ad-1708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/MeiGen-AI/InfiniteTalk"&gt;https://github.com/MeiGen-AI/InfiniteTalk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/vBgoXVW.mp4"&gt;https://i.imgur.com/vBgoXVW.mp4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Ad-1708"&gt; /u/Dull-Ad-1708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzaeee/infinitetalk_audiodriven_video_generation_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzaeee/infinitetalk_audiodriven_video_generation_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzaeee/infinitetalk_audiodriven_video_generation_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T23:05:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz2di5</id>
    <title>I tried fine-tuning Gemma-3-270m and prepared for deployments</title>
    <updated>2025-08-24T17:50:40+00:00</updated>
    <author>
      <name>/u/codes_astro</name>
      <uri>https://old.reddit.com/user/codes_astro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google recently released &lt;strong&gt;Gemma3-270M&lt;/strong&gt; model, which is one of the smallest open models out there.&lt;br /&gt; Model weights are available on Hugging Face and its size is ~550MB and there were some testing where it was being used on phones.&lt;/p&gt; &lt;p&gt;It’s one of the perfect models for fine-tuning, so I put it to the test using the official Colab notebook and an NPC game dataset.&lt;/p&gt; &lt;p&gt;I put everything together as a written guide in my newsletter and also as a small demo video while performing the steps.&lt;/p&gt; &lt;p&gt;I have skipped the fine-tuning part in the guide because you can find the official notebook on the release blog to test using Hugging Face Transformers. I did the same locally on my notebook.&lt;/p&gt; &lt;p&gt;Gemma3-270M is so small that fine-tuning and testing were finished in just a few minutes (~15). Then I used a open source tool called KitOps to package it together for secure production deployments.&lt;/p&gt; &lt;p&gt;I was trying to see if fine-tuning this small model is fast and efficient enough to be used in production environments or not. The steps I covered are mainly for devs looking for secure deployment of these small models for real apps. (example covered is very basic and done on Mac mini M4)&lt;/p&gt; &lt;p&gt;Steps I took are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Importing a Hugging Face Model&lt;/li&gt; &lt;li&gt;Fine-Tuning the Model&lt;/li&gt; &lt;li&gt;Initializing the Model with KitOps&lt;/li&gt; &lt;li&gt;Packaging the model and related files after fine-tuning&lt;/li&gt; &lt;li&gt;Push to a Hub to get security scans done and container deployments.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;watch the demo video – &lt;a href="https://youtu.be/8SKV_m5XV6o"&gt;here&lt;/a&gt;&lt;br /&gt; take a look at the guide – &lt;a href="https://mranand.substack.com/p/you-can-fine-tune-gemma3-270m-in"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codes_astro"&gt; /u/codes_astro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz2di5/i_tried_finetuning_gemma3270m_and_prepared_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T17:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz42eu</id>
    <title>Qwen3-Coder-480B Q4_0 on 6x7900xtx</title>
    <updated>2025-08-24T18:54:17+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt; &lt;img alt="Qwen3-Coder-480B Q4_0 on 6x7900xtx" src="https://a.thumbs.redditmedia.com/OtUhAjy3dNMyywvnZbc9ZIPzO4CmV_Rfiexy6H6qaR8.jpg" title="Qwen3-Coder-480B Q4_0 on 6x7900xtx" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Running Qwen3-Coder-480B Q4_0 on 6x7900xtx with 7 token/s&lt;/strong&gt; output speed, did you have any suggestion or ideas to speed up it?&lt;/p&gt; &lt;p&gt;Maybe you know smart-offloading specific layers?&lt;/p&gt; &lt;p&gt;I launch it with this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./lama-hip-0608/build/bin/llama-server \ --model 480B-A35B_Q4_0/Qwen3-Coder-480B-A35B-Instruct-Q4_0-00001-of-00006.gguf \ --main-gpu 0 \ --temp 0.65 \ --top-k 20 \ --min-p 0.0 \ --top-p 0.95 \ --gpu-layers 48 \ --ctx-size 4000 \ --host 0.0.0.0 \ --port ${PORT} \ --parallel 1 \ --tensor-split 24,24,24,24,24,24 \ --jinja \ --mlock \ --flash-attn \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ -ot &amp;quot;.ffn_(down)_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz42eu/qwen3coder480b_q4_0_on_6x7900xtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T18:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1myjzmn</id>
    <title>There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)</title>
    <updated>2025-08-24T02:26:33+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt; &lt;img alt="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" src="https://preview.redd.it/2t25pwj6ovkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8c8abd5ee1bf8381408ed5b298fc42879b01bd1" title="There are at least 15 open source models I could find that can be run on a consumer GPU and which are better than Grok 2 (according to Artificial Analysis)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And they have better licenses, less restrictions. What exactly is the point of Grok 2 then? I appreciate open source effort, but wouldn't it make more sense to open source a competitive model that can at least be run locally by most people?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2t25pwj6ovkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myjzmn/there_are_at_least_15_open_source_models_i_could/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T02:26:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1myx4l5</id>
    <title>Which local model are you currently using the most? What’s your main use case, and why do you find it good?</title>
    <updated>2025-08-24T14:32:16+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myx4l5/which_local_model_are_you_currently_using_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T14:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mza0wy</id>
    <title>Made Chatterbox TTS a bit faster again on CUDA (155it/s on 3090)</title>
    <updated>2025-08-24T22:49:20+00:00</updated>
    <author>
      <name>/u/RSXLV</name>
      <uri>https://old.reddit.com/user/RSXLV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/rsxdalv/chatterbox/tree/faster"&gt;https://github.com/rsxdalv/chatterbox/tree/faster&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Previous version discussion: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lfnn7b/optimized_chatterbox_tts_up_to_24x_nonbatched/&lt;/a&gt; (hopefully most of the old questions will become obsolete)&lt;/p&gt; &lt;p&gt;Disclaimer - for batched generation in dedicated deployments Chatterbox-VLLM should be the better choice.&lt;/p&gt; &lt;p&gt;I have mostly exhausted the options for speeding up almost vanilla HF Transformers' Llama with torch. Inductor, Triton, Max Autotune, different cache sizes etc, and they are available in the codebase. In the end, manually capturing cuda-graphs was the fastest. The model should be able to run around 230 it/s with fused kernels and better code. (I was unable to remedy the kv_cache code to enable cuda graph capture with torch.compile's max autotune.) Besides the speed, the main benefit is that setting a small cache size is no longer necessary, neither are max_new_tokens important. I plan to make it compile by default to facilitate drop-in use in other projects. Since the main effort is exhausted, I will keep on updating incrementally - for example, speeding up the s3gen (which is now a bottleneck).&lt;/p&gt; &lt;h1&gt;Results for 1500 cache size with BFloat16&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:02&amp;lt;00:04, 159.15it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 2.05 seconds 156.29 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([2, 188, 1024]) Sampling: 32%|███▏ | 320/1000 [00:01&amp;lt;00:03, 170.52it/s] Stopping at 321 because EOS token was generated Generated 321 tokens in 1.88 seconds 170.87 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([2, 339, 1024]) Sampling: 62%|██████▏ | 620/1000 [00:04&amp;lt;00:02, 154.58it/s] Stopping at 621 because EOS token was generated Generated 621 tokens in 4.01 seconds 154.69 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([2, 46, 1024]) Sampling: 4%|▍ | 40/1000 [00:00&amp;lt;00:05, 182.08it/s] Stopping at 41 because EOS token was generated Generated 41 tokens in 0.22 seconds 184.94 it/s &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Disabling classifier free guidance (cfg_weight=0)&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 169.38it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.89 seconds 158.95 it/s Estimated token count: 304 Input embeds shape before padding: torch.Size([1, 187, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 194.04it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.55 seconds 193.66 it/s Estimated token count: 606 Input embeds shape before padding: torch.Size([1, 338, 1024]) Sampling: 100%|██████████| 300/300 [00:01&amp;lt;00:00, 182.28it/s] Stopping at 300 because max_new_tokens reached Generated 300 tokens in 1.65 seconds 182.22 it/s Estimated token count: 20 Input embeds shape before padding: torch.Size([1, 45, 1024]) Sampling: 20%|██ | 60/300 [00:00&amp;lt;00:01, 208.54it/s] Stopping at 61 because EOS token was generated Generated 61 tokens in 0.29 seconds 210.54 it/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Current code example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def t3_to(model: ChatterboxTTS, dtype): model.t3.to(dtype=dtype) model.conds.t3.to(dtype=dtype) torch.cuda.empty_cache() return model # Most new GPUs would work the fastest with this, but not all. t3_to(model, torch.bfloat16) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, warmup&amp;quot;) audio = model.generate(&amp;quot;fast generation using cudagraphs-manual, full speed&amp;quot;) # Extra options: audio = model.generate( text, t3_params={ # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;eager&amp;quot;, # slower - default # &amp;quot;initial_forward_pass_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # speeds up set up # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-manual&amp;quot;, # fastest - default # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;eager&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;inductor-strided&amp;quot;, # &amp;quot;generate_token_backend&amp;quot;: &amp;quot;cudagraphs-strided&amp;quot;, # &amp;quot;stride_length&amp;quot;: 4, # &amp;quot;strided&amp;quot; options compile &amp;lt;1-2-3-4&amp;gt; iteration steps together, which improves performance by reducing memory copying issues in torch.compile # &amp;quot;skip_when_1&amp;quot;: True, # skips Top P when it's set to 1.0 # &amp;quot;benchmark_t3&amp;quot;: True, # Synchronizes CUDA to get the real it/s } ) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RSXLV"&gt; /u/RSXLV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mza0wy/made_chatterbox_tts_a_bit_faster_again_on_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T22:49:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1myz59l</id>
    <title>Seed-OSS is insanely good</title>
    <updated>2025-08-24T15:50:03+00:00</updated>
    <author>
      <name>/u/I-cant_even</name>
      <uri>https://old.reddit.com/user/I-cant_even</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It took a day for me to get it running but *wow* this model is good. I had been leaning heavily on a 4bit 72B Deepseek R1 Distill but it had some regularly frustrating failure modes.&lt;/p&gt; &lt;p&gt;I was prepping to finetune my own model to address my needs but now it's looking like I can remove refusals and run Seed-OSS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I-cant_even"&gt; /u/I-cant_even &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myz59l/seedoss_is_insanely_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T15:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1myz6f1</id>
    <title>Fast CUDA DFloat11 decoding kernel</title>
    <updated>2025-08-24T15:51:15+00:00</updated>
    <author>
      <name>/u/No_Dimension41</name>
      <uri>https://old.reddit.com/user/No_Dimension41</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago, I came across the amazing work on &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k7o89n/we_compress_any_bf16_model_to_70_size_during/"&gt;DFloat11&lt;/a&gt;, which achieves lossless output while shrinking models to 70% of their original size by compressing the exponent bits of BF16. It is a great work. However, I found a problem: it decompresses an entire tensor into VRAM, and then perform computations separately, which severely impacts the model's decoding speed. According to some &lt;a href="https://github.com/LeanModels/DFloat11/issues/7"&gt;issues&lt;/a&gt; on GitHub, it only reaches about 1/3 of the native BF16 speed. Furthermore, the author hasn't released the code for encoding the models, and the decoding kernel is provided in a nearly unreadable PTX format.&lt;/p&gt; &lt;p&gt;So, I decided to write my own implementation. I used the Huffman coding and LUT-based decoding algorithms described in their &lt;a href="https://arxiv.org/abs/2504.11651"&gt;paper&lt;/a&gt;, but I &lt;strong&gt;fused the Huffman decoding process and the GEMV operation into a single kernel&lt;/strong&gt;. This avoids unnecessary memory bandwidth overhead and dramatically speeds up decoding.&lt;/p&gt; &lt;p&gt;With a batch size of 1, my implementation can now reach about &lt;strong&gt;90% of native BF16 speed&lt;/strong&gt; on regular GPUs. On some VRAM bandwidth-constrained GPUs, like the RTX 4060 Ti, it can even &lt;strong&gt;surpass native BF16 speed&lt;/strong&gt; because the compressed weights reduce the demand on VRAM bandwidth.&lt;/p&gt; &lt;p&gt;Here's a simple benchmark for generating 256 tokens:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Raw BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Compressed BF16 Time&lt;/th&gt; &lt;th align="left"&gt;Raw / Compressed Size&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5 7B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;14.98s&lt;/td&gt; &lt;td align="left"&gt;13.02s&lt;/td&gt; &lt;td align="left"&gt;14.19 / 10.99 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;6.66s&lt;/td&gt; &lt;td align="left"&gt;7.23s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 8B&lt;/td&gt; &lt;td align="left"&gt;RTX 4060Ti&lt;/td&gt; &lt;td align="left"&gt;OOM&lt;/td&gt; &lt;td align="left"&gt;14.11s&lt;/td&gt; &lt;td align="left"&gt;15.26 / 11.52 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;RTX A6000&lt;/td&gt; &lt;td align="left"&gt;7.75s&lt;/td&gt; &lt;td align="left"&gt;8.24s&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Of course, there are still areas for improvement. Due to the extra padding required by the CUDA kernel's layout, the current compression rate is slightly lower than the original DFloat11, achieving around 75%-80%. Additionally, support for uncommon tensor shapes and batch sizes greater than 1 is currently limited.&lt;/p&gt; &lt;p&gt;For more information, please visit my GitHub repository: &lt;a href="https://github.com/lszxb/bf16_huffman_infer"&gt;https://github.com/lszxb/bf16_huffman_infer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Dimension41"&gt; /u/No_Dimension41 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myz6f1/fast_cuda_dfloat11_decoding_kernel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T15:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1myrdtb</id>
    <title>Mistral Large soon?</title>
    <updated>2025-08-24T09:45:24+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt; &lt;img alt="Mistral Large soon?" src="https://preview.redd.it/m9zk5bipuxkf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52ff9d33632d0268f989230460a6dbd3328b7244" title="Mistral Large soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source &lt;a href="https://mistral.ai/news/mistral-medium-3"&gt;https://mistral.ai/news/mistral-medium-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9zk5bipuxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myrdtb/mistral_large_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T09:45:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz6som</id>
    <title>Almost done with the dashboard for local llama.cpp agents</title>
    <updated>2025-08-24T20:38:43+00:00</updated>
    <author>
      <name>/u/PayBetter</name>
      <uri>https://old.reddit.com/user/PayBetter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt; &lt;img alt="Almost done with the dashboard for local llama.cpp agents" src="https://b.thumbs.redditmedia.com/7LaV7Jli4Sm51VyrQaQKuYWfN3-w_vEMntHaCP24k1w.jpg" title="Almost done with the dashboard for local llama.cpp agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This won't be for sale and will be released as open source with a non commercial license. No code will be released until after the hackathon I've entered is over next month.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PayBetter"&gt; /u/PayBetter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz6som"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz6som/almost_done_with_the_dashboard_for_local_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T20:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1myqkqh</id>
    <title>Elmo is providing</title>
    <updated>2025-08-24T08:54:37+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt; &lt;img alt="Elmo is providing" src="https://preview.redd.it/n6p9jpdvlxkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e03cd4c5782959f5dca22ea135d42d7032a20b59" title="Elmo is providing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n6p9jpdvlxkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1myqkqh/elmo_is_providing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T08:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mz4hrg</id>
    <title>All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th</title>
    <updated>2025-08-24T19:10:09+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt; &lt;img alt="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" src="https://b.thumbs.redditmedia.com/fUU-BLlYX-WkpMfx3LdfGqjKydfcxu7DsHg7PwU2cQk.jpg" title="All of the top 15 OS models on Design Arena come from China. The best non-Chinese model is GPT OSS 120B, ranked at 16th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;China is not only the main competitor to the US in the overall AI race, but dominating the open-source landscape. Out of the open source models listed on &lt;a href="https://www.designarena.ai/"&gt;Design Arena&lt;/a&gt; (a UI/UX and frontend benchmark for LLMs), Chinese models take up all of the top 15 spots with the first non-Chinese model making its appearing at #16 as GPT OSS 120B, developed by Open AI. &lt;/p&gt; &lt;p&gt;It's really remarkable what DeepSeek, Zhipu, Kimi, and Qwen have been able to do while staying OS. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mz4hrg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mz4hrg/all_of_the_top_15_os_models_on_design_arena_come/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-24T19:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
