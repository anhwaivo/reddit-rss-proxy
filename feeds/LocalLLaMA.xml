<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-12T18:07:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1in0938</id>
    <title>I made Iris: A fully-local realtime voice chatbot!</title>
    <updated>2025-02-11T14:49:16+00:00</updated>
    <author>
      <name>/u/Born_Search2534</name>
      <uri>https://old.reddit.com/user/Born_Search2534</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt; &lt;img alt="I made Iris: A fully-local realtime voice chatbot!" src="https://external-preview.redd.it/stLjrcu85AgTrwW4zDhH8loF7eayJD3_hD2XyXmIgUw.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65bd986bcf0e0f5ab0f244100b8cb83d8e1266a9" title="I made Iris: A fully-local realtime voice chatbot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Born_Search2534"&gt; /u/Born_Search2534 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=XK-37m-p11k&amp;amp;feature=shared"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in0938/i_made_iris_a_fullylocal_realtime_voice_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T14:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1inpkb0</id>
    <title>My little setup</title>
    <updated>2025-02-12T12:23:38+00:00</updated>
    <author>
      <name>/u/droomurray</name>
      <uri>https://old.reddit.com/user/droomurray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my i5 NUC with 64GB Ram and a Thunderbolt 3 connected 3090 running Ubuntu with the relevant CUDA drivers etc. All seems to work fine. So far I have experimented with docker running ollama / openwebui and ComfeyUI. &lt;/p&gt; &lt;p&gt;Keen to understand what LLM's people use and the difference between them in terms of the size / names etc. Should I be using vllm not ollama ? What UI do people use ? Any pointers etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/droomurray"&gt; /u/droomurray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inpkb0/my_little_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inpkb0/my_little_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inpkb0/my_little_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T12:23:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1inbili</id>
    <title>UK and US refuse to sign international AI declaration</title>
    <updated>2025-02-11T22:32:41+00:00</updated>
    <author>
      <name>/u/Durian881</name>
      <uri>https://old.reddit.com/user/Durian881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"&gt; &lt;img alt="UK and US refuse to sign international AI declaration" src="https://external-preview.redd.it/4F7TAGCOz8Rfg3WqXnrQP8MWV7RA9T-cLoTH3Je-EA8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbfa17295cbd542f368737b59db21a9f78ea0823" title="UK and US refuse to sign international AI declaration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durian881"&gt; /u/Durian881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bbc.com/news/articles/c8edn0n58gwo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inbili/uk_and_us_refuse_to_sign_international_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T22:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1invsz3</id>
    <title>Is there a Local LLaMA that can analyze and give feedback on a long audio note that I recorded?</title>
    <updated>2025-02-12T17:04:48+00:00</updated>
    <author>
      <name>/u/thebestisyetocome</name>
      <uri>https://old.reddit.com/user/thebestisyetocome</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to analyze an audio recording of myself during a therapeutic psychedelic trip, but I'm nervous about feeding it straight to chatGPT for legal reasons. Is there a local LLaMA that I can run on either gpt4all or LM Studio that can do this for my on my own computer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebestisyetocome"&gt; /u/thebestisyetocome &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1invsz3/is_there_a_local_llama_that_can_analyze_and_give/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1invsz3/is_there_a_local_llama_that_can_analyze_and_give/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1invsz3/is_there_a_local_llama_that_can_analyze_and_give/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T17:04:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1inuz15</id>
    <title>New Paper: Can frontier models self-explore and discover their own capabilities in an open-ended way?</title>
    <updated>2025-02-12T16:31:25+00:00</updated>
    <author>
      <name>/u/MolassesWeak2646</name>
      <uri>https://old.reddit.com/user/MolassesWeak2646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inuz15/new_paper_can_frontier_models_selfexplore_and/"&gt; &lt;img alt="New Paper: Can frontier models self-explore and discover their own capabilities in an open-ended way?" src="https://b.thumbs.redditmedia.com/cCqUW5PzBXZNNIKacrpoJ2CvhpFZbMst_eKRZR04zfs.jpg" title="New Paper: Can frontier models self-explore and discover their own capabilities in an open-ended way?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; Automated Capability Discovery via Model Self-Exploration&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cong Lu, Shengran Hu, Jeff Clune.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2502.07577"&gt;https://arxiv.org/abs/2502.07577&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data. It remains challenging to precisely characterize even a fraction of the full spectrum of capabilities and potential risks in any new model. Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models. We introduce Automated Capability Discovery (ACD), a framework that designates one foundation model as a scientist to systematically propose open-ended tasks probing the abilities of a subject model (potentially itself). By combining frontier models with ideas from the field of open-endedness, ACD automatically and systematically uncovers both surprising capabilities and failures in the subject model. We demonstrate ACD across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically reveals thousands of capabilities that would be challenging for any single team to uncover. We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations. By leveraging foundation models' ability to both create tasks and self-evaluate, ACD is a significant step toward scalable, automated evaluation of novel AI systems.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vz3a7aygjqie1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c41d7c3f1100de512b1fd6e0ce21db74fd75c67"&gt;https://preview.redd.it/vz3a7aygjqie1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c41d7c3f1100de512b1fd6e0ce21db74fd75c67&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MolassesWeak2646"&gt; /u/MolassesWeak2646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inuz15/new_paper_can_frontier_models_selfexplore_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inuz15/new_paper_can_frontier_models_selfexplore_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inuz15/new_paper_can_frontier_models_selfexplore_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T16:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1in69s3</id>
    <title>4x3090 in a 4U case, don't recommend it</title>
    <updated>2025-02-11T18:58:29+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt; &lt;img alt="4x3090 in a 4U case, don't recommend it" src="https://a.thumbs.redditmedia.com/78a1YQ1sn0cHJ3q5f1VyWRySQ3DMLohcp9folBrA-j0.jpg" title="4x3090 in a 4U case, don't recommend it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1in69s3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in69s3/4x3090_in_a_4u_case_dont_recommend_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T18:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1in83vw</id>
    <title>Chonky Boi has arrived</title>
    <updated>2025-02-11T20:12:07+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt; &lt;img alt="Chonky Boi has arrived" src="https://external-preview.redd.it/YKjiYoZG5DJKHF8InRyOIM7iTEJQimQj1eCDwySpqqE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=69fcf80dcc8f1be1b6b900fa7ba68bf7b62ee469" title="Chonky Boi has arrived" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/kh64WJy.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in83vw/chonky_boi_has_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:12:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1injegi</id>
    <title>LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!</title>
    <updated>2025-02-12T05:05:27+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.07374"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1injegi/llms_can_easily_learn_to_reason_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1injegi/llms_can_easily_learn_to_reason_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T05:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1in9vsb</id>
    <title>NYT: Vance speech at EU AI summit</title>
    <updated>2025-02-11T21:24:49+00:00</updated>
    <author>
      <name>/u/Mediocre_Tree_5690</name>
      <uri>https://old.reddit.com/user/Mediocre_Tree_5690</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt; &lt;img alt="NYT: Vance speech at EU AI summit" src="https://preview.redd.it/vjltv4twukie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6ab17775dd480a87f8fde7e76f52035c4a8b6cc" title="NYT: Vance speech at EU AI summit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://archive.is/eWNry"&gt;https://archive.is/eWNry&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's an archive link in case anyone wants to read the article. Macron spoke about lighter regulation at the AI summit as well. Are we thinking safetyism is finally on its way out? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Tree_5690"&gt; /u/Mediocre_Tree_5690 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vjltv4twukie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in9vsb/nyt_vance_speech_at_eu_ai_summit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T21:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1inieoe</id>
    <title>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</title>
    <updated>2025-02-12T04:07:22+00:00</updated>
    <author>
      <name>/u/ekaesmem</name>
      <uri>https://old.reddit.com/user/ekaesmem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt; &lt;img alt="Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling" src="https://b.thumbs.redditmedia.com/xPf7D_ti2_9HmqPFloVt-vKCywazKffYdOe7zEWXDAg.jpg" title="Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2vt00wzoumie1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fef3cc679cc29305e31e0eb234f1b990c0f2fca"&gt;https://preview.redd.it/2vt00wzoumie1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fef3cc679cc29305e31e0eb234f1b990c0f2fca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.06703"&gt;[2502.06703] Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ekaesmem"&gt; /u/ekaesmem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inieoe/can_1b_llm_surpass_405b_llm_rethinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T04:07:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoewc</id>
    <title>OLMoE-0125 &amp; iOS App from allenai</title>
    <updated>2025-02-12T11:03:49+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoewc/olmoe0125_ios_app_from_allenai/"&gt; &lt;img alt="OLMoE-0125 &amp;amp; iOS App from allenai" src="https://b.thumbs.redditmedia.com/2W3p9PcVbwGzJeMR11PCMqP_IHoAoR2nsQAUErncanY.jpg" title="OLMoE-0125 &amp;amp; iOS App from allenai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OLMoE-0125:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/olmoe-january-2025-67992134f9ebea0a941706ca"&gt;https://huggingface.co/collections/allenai/olmoe-january-2025-67992134f9ebea0a941706ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;iOS App:&lt;/p&gt; &lt;p&gt;&lt;a href="https://allenai.org/blog/olmoe-app"&gt;https://allenai.org/blog/olmoe-app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1inoewc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoewc/olmoe0125_ios_app_from_allenai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoewc/olmoe0125_ios_app_from_allenai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1imyp19</id>
    <title>If you want my IT department to block HF, just say so.</title>
    <updated>2025-02-11T13:35:20+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt; &lt;img alt="If you want my IT department to block HF, just say so." src="https://preview.redd.it/h1dbbwhxiiie1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1bafacbc3514f10c5b939ade16c607722c1d9b0" title="If you want my IT department to block HF, just say so." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1dbbwhxiiie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1imyp19/if_you_want_my_it_department_to_block_hf_just_say/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T13:35:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1inunwk</id>
    <title>Example: Binding llama.cpp in C++ apps</title>
    <updated>2025-02-12T16:18:45+00:00</updated>
    <author>
      <name>/u/derjanni</name>
      <uri>https://old.reddit.com/user/derjanni</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inunwk/example_binding_llamacpp_in_c_apps/"&gt; &lt;img alt="Example: Binding llama.cpp in C++ apps" src="https://external-preview.redd.it/xJa--3IItTS1vor9uoL1mumel9Hy3TfDJajORXHxGxs.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=894c6ae4ab424db4f92655c16e52e716296e66b9" title="Example: Binding llama.cpp in C++ apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &amp;quot;simple&amp;quot; example app in llama.cpp is not really self explanatory and doesn't build out of the box. I fiddled around with it a little to make it work on my machine. Long story short, here's the working source: &lt;a href="https://github.com/jankammerath/LlamaCppSample"&gt;https://github.com/jankammerath/LlamaCppSample&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4z3cff8bgqie1.png?width=1705&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b0befbab53a0e6619795019bdd07292a7e50c96"&gt;llama.cpp with TinyDolphin compiled on macOS (C++)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm posting this here as it took me ages to figure out how to build it properly as part of the research for my article &amp;quot;&lt;a href="https://programmers.fyi/the-resurgance-of-cpp-with-llama-cuda-metal"&gt;The Resurgence of C++ through Llama.cpp, CUDA &amp;amp; Metal&lt;/a&gt;&amp;quot;. Turns out, the CMake build script in the simple example folder in the llama.cpp repostiroy lacks the necessary ggml libraries.&lt;/p&gt; &lt;p&gt;Also ensure to build llama.cpp with the &lt;code&gt;-DBUILD_SHARED_LIBS=ON&lt;/code&gt; flags on.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -B build -DBUILD_SHARED_LIBS=ON cmake --build build --config Release &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There's also a llama.cpp in homebrew (for the mac users like me), but I don't know what commit or version that currently is. Seems like it does have all the necessary headers and libs. So if you don't feel like compiling llama.cpp yourself, they should be good to go.&lt;/p&gt; &lt;p&gt;I hope this helps someone avoid diggung around for hours like I did.&lt;/p&gt; &lt;p&gt;Cheers, Jan&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/derjanni"&gt; /u/derjanni &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inunwk/example_binding_llamacpp_in_c_apps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inunwk/example_binding_llamacpp_in_c_apps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inunwk/example_binding_llamacpp_in_c_apps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T16:18:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1in8vya</id>
    <title>EU mobilizes $200 billion in AI race against US and China</title>
    <updated>2025-02-11T20:44:31+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt; &lt;img alt="EU mobilizes $200 billion in AI race against US and China" src="https://external-preview.redd.it/X_db72AfOkvUPacuwPMCLwkrfkqSycSFXdfcaogRMnw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6dff9e983d7903f62f20823b94ae4fd34bac0f8" title="EU mobilizes $200 billion in AI race against US and China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/news/609930/eu-200-billion-investment-ai-development"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in8vya/eu_mobilizes_200_billion_in_ai_race_against_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T20:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1inqb6n</id>
    <title>Letting LLMs using an IDE’s debugger</title>
    <updated>2025-02-12T13:04:42+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just built an experimental VSCode extension called LLM Debugger. It’s a proof-of-concept that lets a large language model take charge of debugging. Instead of only looking at the static code, the LLM also gets to see the live runtime state—actual variable values, function calls, branch decisions, and more. The idea is to give it enough context to help diagnose issues faster and even generate synthetic data from running programs.&lt;/p&gt; &lt;p&gt;Here’s what it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Active Debugging: It integrates with Node.js debug sessions to gather runtime info (like variable states and stack traces).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Automated Breakpoints: It automatically sets and manages breakpoints based on both code analysis and LLM suggestions.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;LLM Guidance: With live debugging context, the LLM can suggest actions like stepping through code or adjusting breakpoints in real time.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I built this out of curiosity to see if combining static code with runtime data could help LLMs solve bugs more effectively. It’s rough around the edges and definitely not production-ready&lt;/p&gt; &lt;p&gt;I’m not planning on maintaining it further. But I thought it was a fun experiment and wanted to share it with you all.&lt;/p&gt; &lt;p&gt;Check out the attached video demo to see it in action. Would love to hear your thoughts and any feedback you might have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inqb6n/letting_llms_using_an_ides_debugger/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inqb6n/letting_llms_using_an_ides_debugger/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inqb6n/letting_llms_using_an_ides_debugger/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T13:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1in7nka</id>
    <title>ChatGPT 4o feels straight up stupid after using o1 and DeepSeek for awhile</title>
    <updated>2025-02-11T19:54:09+00:00</updated>
    <author>
      <name>/u/Getabock_</name>
      <uri>https://old.reddit.com/user/Getabock_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And to think I used to be really impressed with 4o. Crazy. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Getabock_"&gt; /u/Getabock_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1in7nka/chatgpt_4o_feels_straight_up_stupid_after_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T19:54:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1inu52f</id>
    <title>Is more VRAM always better?</title>
    <updated>2025-02-12T15:57:36+00:00</updated>
    <author>
      <name>/u/kxzzm</name>
      <uri>https://old.reddit.com/user/kxzzm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone.&lt;br /&gt; Im not interested in training big LLM models but I do want to use simpler models for tasks like reading CSV data, analyzing simple data etc.&lt;/p&gt; &lt;p&gt;Im on a tight budget and need some advice regards running LLM locally. &lt;/p&gt; &lt;p&gt;Is an RTX 3060 with 12GB VRAM better than a newer model with only 8GB?&lt;br /&gt; Does VRAM size matter more, or is speed just as important? &lt;/p&gt; &lt;p&gt;From what I understand, more VRAM helps run models with less quantization, but for quantized models, speed is more important. Am I right?&lt;/p&gt; &lt;p&gt;I couldn't find a clear answer online, so any help would be appreciated. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kxzzm"&gt; /u/kxzzm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inu52f/is_more_vram_always_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inu52f/is_more_vram_always_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inu52f/is_more_vram_always_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T15:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1inrhd9</id>
    <title>The light based computer that supports pytorch</title>
    <updated>2025-02-12T14:02:29+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey a funny one today! A pick at light-based computer with a startup called Q.ANT They refactor 90's CMOS foundery to make ai chips using light. Their chips are already on there ways to datacenter. &lt;a href="https://youtu.be/2xE4bopeXhw?feature=shared"&gt;https://youtu.be/2xE4bopeXhw?feature=shared&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inrhd9/the_light_based_computer_that_supports_pytorch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inrhd9/the_light_based_computer_that_supports_pytorch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inrhd9/the_light_based_computer_that_supports_pytorch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T14:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ino284</id>
    <title>Here we go. Attending nvidia DIGITS webinar. Hope to get some info :)</title>
    <updated>2025-02-12T10:33:29+00:00</updated>
    <author>
      <name>/u/uti24</name>
      <uri>https://old.reddit.com/user/uti24</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ino284/here_we_go_attending_nvidia_digits_webinar_hope/"&gt; &lt;img alt="Here we go. Attending nvidia DIGITS webinar. Hope to get some info :)" src="https://preview.redd.it/cte975mgroie1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f1eb752eac584c3cdc9b974d150c70db7f42d16" title="Here we go. Attending nvidia DIGITS webinar. Hope to get some info :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uti24"&gt; /u/uti24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cte975mgroie1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ino284/here_we_go_attending_nvidia_digits_webinar_hope/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ino284/here_we_go_attending_nvidia_digits_webinar_hope/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T10:33:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1inn034</id>
    <title>Phi-4, but pruned and unsafe</title>
    <updated>2025-02-12T09:13:06+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some things just start on a &lt;strong&gt;whim&lt;/strong&gt;. This is the story of &lt;strong&gt;Phi-Lthy4&lt;/strong&gt;, pretty much:&lt;/p&gt; &lt;p&gt;&amp;gt; yo sicarius can you make phi-4 smarter?&lt;br /&gt; nope. but i can still make it better.&lt;br /&gt; &amp;gt; wdym??&lt;br /&gt; well, i can yeet a couple of layers out of its math brain, and teach it about the wonders of love and intimate relations. maybe. idk if its worth it.&lt;br /&gt; &amp;gt; lol its all synth data in the pretrain. many before you tried.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;fine. ill do it.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;But... why?&lt;/h1&gt; &lt;p&gt;The trend it seems, is to make AI models more &lt;strong&gt;assistant-oriented&lt;/strong&gt;, use as much &lt;strong&gt;synthetic data&lt;/strong&gt; as possible, be more &lt;strong&gt;'safe'&lt;/strong&gt;, and be more &lt;strong&gt;benchmaxxed&lt;/strong&gt; (hi qwen). Sure, this makes great assistants, but &lt;strong&gt;sanitized&lt;/strong&gt; data (like in the &lt;strong&gt;Phi&lt;/strong&gt; model series case) butchers &lt;strong&gt;creativity&lt;/strong&gt;. Not to mention that the previous &lt;strong&gt;Phi 3.5&lt;/strong&gt; wouldn't even tell you how to &lt;strong&gt;kill a process&lt;/strong&gt; and so on and so forth...&lt;/p&gt; &lt;p&gt;This little side project took about &lt;strong&gt;two weeks&lt;/strong&gt; of on-and-off fine-tuning. After about &lt;strong&gt;1B tokens&lt;/strong&gt; or so, I lost track of how much I trained it. The idea? A &lt;strong&gt;proof of concept&lt;/strong&gt; of sorts to see if sheer will (and 2xA6000) will be enough to shape a model to &lt;strong&gt;any&lt;/strong&gt; parameter size, behavior or form.&lt;/p&gt; &lt;p&gt;So I used mergekit to perform a crude &lt;strong&gt;LLM brain surgery&lt;/strong&gt;— and yeeted some &lt;strong&gt;useless&lt;/strong&gt; neurons that dealt with math. How do I know that these exact neurons dealt with math? Because &lt;strong&gt;ALL&lt;/strong&gt; of Phi's neurons dealt with math. Success was guaranteed.&lt;/p&gt; &lt;p&gt;Is this the best Phi-4 &lt;strong&gt;11.9B&lt;/strong&gt; RP model in the &lt;strong&gt;world&lt;/strong&gt;? It's quite possible, simply because tuning &lt;strong&gt;Phi-4&lt;/strong&gt; for RP is a completely stupid idea, both due to its pretraining data, &amp;quot;limited&amp;quot; context size of &lt;strong&gt;16k&lt;/strong&gt;, and the model's MIT license.&lt;/p&gt; &lt;p&gt;Surprisingly, it's &lt;strong&gt;quite good at RP&lt;/strong&gt;, turns out it didn't need those 8 layers after all. It could probably still solve a basic math question, but I would strongly recommend using a calculator for such tasks. Why do we want LLMs to do basic math anyway?&lt;/p&gt; &lt;p&gt;Oh, regarding &lt;strong&gt;censorship&lt;/strong&gt;... Let's just say it's... &lt;strong&gt;Phi-lthy&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The BEST Phi-4 Roleplay&lt;/strong&gt; finetune in the &lt;strong&gt;world&lt;/strong&gt; (Not that much of an achievement here, Phi roleplay finetunes can probably be counted on a &lt;strong&gt;single hand&lt;/strong&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compact size &amp;amp; fully healed from the brain surgery&lt;/strong&gt; Only &lt;strong&gt;11.9B&lt;/strong&gt; parameters. &lt;strong&gt;Phi-4&lt;/strong&gt; wasn't that hard to run even at &lt;strong&gt;14B&lt;/strong&gt;, now with even fewer brain cells, your new phone could probably run it easily. (&lt;strong&gt;SD8Gen3&lt;/strong&gt; and above recommended).&lt;/li&gt; &lt;li&gt;Strong &lt;strong&gt;Roleplay &amp;amp; Creative writing&lt;/strong&gt; abilities. This really surprised me. &lt;strong&gt;Actually good&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Writes and roleplays &lt;strong&gt;quite uniquely&lt;/strong&gt;, probably because of lack of RP\writing slop in the &lt;strong&gt;pretrain&lt;/strong&gt;. Who would have thought?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart&lt;/strong&gt; assistant with &lt;strong&gt;low refusals&lt;/strong&gt; - It kept some of the smarts, and our little Phi-Lthy here will be quite eager to answer your naughty questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quite good&lt;/strong&gt; at following the &lt;strong&gt;character card&lt;/strong&gt;. Finally, it puts its math brain to some productive tasks. Gooner technology is becoming more popular by the day.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-lthy4"&gt;https://huggingface.co/SicariusSicariiStuff/Phi-lthy4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inn034/phi4_but_pruned_and_unsafe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T09:13:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoxu6</id>
    <title>Tested lot of small models for coding and i was surprised how good is (nvidia/AceInstruct-7B), idk why no one talking about it</title>
    <updated>2025-02-12T11:49:06+00:00</updated>
    <author>
      <name>/u/solomars3</name>
      <uri>https://old.reddit.com/user/solomars3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It feels like this one flew over the radar, idk if its just a fintune or not, but usually what i do when testing small models for coding i start with : (Make a single html modern calculator)&lt;/p&gt; &lt;p&gt;Just to see if its gonna give me a good looking one, most models struggle to make each button in its own place, layout usualy bad, AceInstruct-7B does good job&lt;/p&gt; &lt;p&gt;After that i use my second prompt:&lt;/p&gt; &lt;p&gt;(Make a windows app using python that has a simple interface, With three buttons, when you click the first button it turns green When you click second button it turns blue When you click third button, it turns red, buttons its self change color)&lt;/p&gt; &lt;p&gt;Again simple but most small models struggle, AceInstruct-7B does it and it follow changes pretty well, like if you ask it to make changes, it will do so and give you the updated code without making weird changes that cause errors,&lt;/p&gt; &lt;p&gt;Just wanted to share this, and there is a 72B version too, ill try to find a way to test it for coding, but i think it will be insane &lt;/p&gt; &lt;p&gt;Edit :&lt;/p&gt; &lt;p&gt;The AceInstruct family, which includes AceInstruct-1.5B, 7B, and 72B, is Improved using Qwen. These models are fine-tuned on Qwen2.5-Base using general SFT datasets. These same datasets are also used in the training of AceMath-Instruct. Different from AceMath-Instruct which is specialized for math questions, AceInstruct is versatile and can be applied to a wide range of domains. Benchmark evaluations across coding, mathematics, and general knowledge tasks demonstrate that AceInstruct delivers performance comparable to Qwen2.5-Instruct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solomars3"&gt; /u/solomars3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoxu6/tested_lot_of_small_models_for_coding_and_i_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoxu6/tested_lot_of_small_models_for_coding_and_i_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoxu6/tested_lot_of_small_models_for_coding_and_i_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:49:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1inos01</id>
    <title>Some details on Project Digits from PNY presentation</title>
    <updated>2025-02-12T11:32:04+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"&gt; &lt;img alt="Some details on Project Digits from PNY presentation" src="https://b.thumbs.redditmedia.com/pzMMeiqVpng-Evo7PS_VS5BolvYAdkUVtZ-ex05okEA.jpg" title="Some details on Project Digits from PNY presentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These are my meeting notes, unedited:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;• Only 19 people attended the presentation?!!! Some left mid-way.. • Presentation by PNY DGX EMEA lead • PNY takes Nvidia DGX ecosystemto market • Memory is DDR5x, 128GB &amp;quot;initially&amp;quot; ○ No comment on memory speed or bandwidth. ○ The memory is on the same fabric, connected to CPU and GPU. ○ &amp;quot;we don't have the specific bandwidth specification&amp;quot; • Also include a dual port QSFP networking, includes a Mellanox chip, supports infiniband and ethernet. Expetced at least 100gb/port, not yet confirmed by Nvidia. • Brand new ARM processor built for the Digits, never released before product (processor, not core). • Real product pictures, not rendering. • &amp;quot;what makes it special is the software stack&amp;quot; • Will run a Ubuntu based OS. Software stack shared with the rest of the nvidia ecosystem. • Digits is to be the first product of a new line within nvidia. • No dedicated power connector could be seen, USB-C powered? ○ &amp;quot;I would assume it is USB-C powered&amp;quot; • Nvidia indicated two maximum can be stacked. There is a possibility to cluster more. ○ The idea is to use it as a developer kit, not or production workloads. • &amp;quot;hopefully May timeframe to market&amp;quot;. • Cost: circa $3k RRP. Can be more depending on software features required, some will be paid. • &amp;quot;significantly more powerful than what we've seen on Jetson products&amp;quot; ○ &amp;quot;exponentially faster than Jetson&amp;quot; ○ &amp;quot;everything you can run on DGX, you can run on this, obviously slower&amp;quot; ○ Targeting universities and researchers. • &amp;quot;set expectations:&amp;quot; ○ It's a workstation ○ It can work standalone, or can be connected to another device to offload processing. ○ Not a replacement for a &amp;quot;full-fledged&amp;quot; multi-GPU workstation &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A few of us pushed on how the performance compares to a RTX 5090. No clear answer given beyond talking about 5090 not designed for enterprise workload, and power consumption&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1inos01"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inos01/some_details_on_project_digits_from_pny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1inmkbc</id>
    <title>agentica-org/DeepScaleR-1.5B-Preview</title>
    <updated>2025-02-12T08:39:47+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt; &lt;img alt="agentica-org/DeepScaleR-1.5B-Preview" src="https://preview.redd.it/3fm88arb7oie1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=094bb1f83ed48f6b26b3ca5b52f7cdfb742b34e0" title="agentica-org/DeepScaleR-1.5B-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3fm88arb7oie1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inmkbc/agenticaorgdeepscaler15bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T08:39:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1inch7r</id>
    <title>A new paper demonstrates that LLMs could "think" in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows.</title>
    <updated>2025-02-11T23:14:51+00:00</updated>
    <author>
      <name>/u/tehbangere</name>
      <uri>https://old.reddit.com/user/tehbangere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt; &lt;img alt="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." src="https://external-preview.redd.it/lsXw1VKNR0EoTFYgDUro5o8By4n9gHC7i_cxDktIeuo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7f243d34bc596be68af0031b70b22b21c475830" title="A new paper demonstrates that LLMs could &amp;quot;think&amp;quot; in latent space, effectively decoupling internal reasoning from visible context tokens. This breakthrough suggests that even smaller models can achieve remarkable performance without relying on extensive context windows." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tehbangere"&gt; /u/tehbangere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2502.05171"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-11T23:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1inoui5</id>
    <title>AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory</title>
    <updated>2025-02-12T11:36:29+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt; &lt;img alt="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" src="https://external-preview.redd.it/qxSKCWeduksNqEDRWvwQaww7R41JuTdE_uY1z8NDX_M.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b97591e394a959b1d54b453c3148692e6cab6ca" title="AMD reportedly working on gaming Radeon RX 9070 XT GPU with 32GB memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-reportedly-working-on-gaming-radeon-rx-9000-gpu-with-32gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1inoui5/amd_reportedly_working_on_gaming_radeon_rx_9070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-12T11:36:29+00:00</published>
  </entry>
</feed>
