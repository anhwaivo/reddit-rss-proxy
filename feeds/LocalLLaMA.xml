<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-19T07:36:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lf01uz</id>
    <title>Any LLM that can detect musical tonality from an audio?</title>
    <updated>2025-06-19T02:52:23+00:00</updated>
    <author>
      <name>/u/9acca9</name>
      <uri>https://old.reddit.com/user/9acca9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering if there is such a thing locally.&lt;/p&gt; &lt;p&gt;Or something that can work with .mid file???? MIDI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9acca9"&gt; /u/9acca9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf01uz/any_llm_that_can_detect_musical_tonality_from_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf01uz/any_llm_that_can_detect_musical_tonality_from_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf01uz/any_llm_that_can_detect_musical_tonality_from_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T02:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf0pk9</id>
    <title>Which AWS Sagemaker Quota to request for training llama 3.2-3B-Instruct with PPO and Reinforcement learning?</title>
    <updated>2025-06-19T03:26:54+00:00</updated>
    <author>
      <name>/u/Furiousguy79</name>
      <uri>https://old.reddit.com/user/Furiousguy79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my first time using AWS. I have been added to my PI's lab organization, which has some credits. Now I am trying to do an experiment where I will be basically using a modified reward method for training llama3.2-3B with PPO. The authors of the original work used 4 A100 GPUs for their training with PPO (they used Qwen 2.5 3B).&lt;/p&gt; &lt;p&gt;What is a similar (maybe a bit smaller in scale) service in AWS Sagemaker? I mean, in GPU power? I am thinking of ml.p3.8xlarge. I am not sure if I will be needing this much. I have some credits left in colab where I am using A100 GPU. Since I have a paper submission in two weeks,. I wanted to request for quota early.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Furiousguy79"&gt; /u/Furiousguy79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf0pk9/which_aws_sagemaker_quota_to_request_for_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf0pk9/which_aws_sagemaker_quota_to_request_for_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf0pk9/which_aws_sagemaker_quota_to_request_for_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T03:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf35fh</id>
    <title>Embedding Language Model (ELM)</title>
    <updated>2025-06-19T05:48:24+00:00</updated>
    <author>
      <name>/u/Repulsive-Memory-298</name>
      <uri>https://old.reddit.com/user/Repulsive-Memory-298</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can be a bit nutty, but this HAS to be the future. &lt;/p&gt; &lt;p&gt;The ability to sample and score over the continuous latent representation, made relatively extremely transparent by a densely populated semantic &amp;quot;map&amp;quot; which can be traversed. &lt;/p&gt; &lt;p&gt;Anyone want to team up and train one 😎&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Repulsive-Memory-298"&gt; /u/Repulsive-Memory-298 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/html/2310.04475v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf35fh/embedding_language_model_elm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf35fh/embedding_language_model_elm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T05:48:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1led23c</id>
    <title>Local AI for a small/median accounting firm - € Buget of 10k-25k</title>
    <updated>2025-06-18T09:51:00+00:00</updated>
    <author>
      <name>/u/AFruitShopOwner</name>
      <uri>https://old.reddit.com/user/AFruitShopOwner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our medium-sized &lt;strong&gt;accounting firm&lt;/strong&gt; (around 100 people) in the &lt;strong&gt;Netherlands&lt;/strong&gt; is looking to set up a local AI system, I'm hoping to tap into your collective wisdom for some recommendations. The &lt;strong&gt;budget&lt;/strong&gt; is roughly &lt;strong&gt;€10k-€25k.&lt;/strong&gt; This is purely for the hardware. I'll be able to build the system myself. I'll also handle the software side. I don't have a lot of experience actually running local models but I do spent a lot of my free time watching videos about it.&lt;/p&gt; &lt;p&gt;We're going local for privacy. Keeping sensitive client data in-house is paramount. My boss does not want anything going to the cloud.&lt;/p&gt; &lt;p&gt;Some more info about use cases what I had in mind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RAG system&lt;/strong&gt; for professional questions about Dutch accounting standards and laws. (We already have an extensive librairy of documents, neatly orderd)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyzing and summarizing&lt;/strong&gt; various files like contracts, invoices, emails, excel sheets, word files and pdfs.&lt;/li&gt; &lt;li&gt;Developing &lt;strong&gt;AI agents&lt;/strong&gt; for more advanced task automation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coding assistance&lt;/strong&gt; for our data analyst (mainly in Python).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm looking for broad advice on:&lt;/p&gt; &lt;p&gt;Hardware&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Go with a &lt;strong&gt;CPU&lt;/strong&gt; based or &lt;strong&gt;GPU based&lt;/strong&gt; set up?&lt;/li&gt; &lt;li&gt;If I go with GPU's should I go with a couple of consumer GPU's like 3090/4090's or maybe a single Pro 6000? Why pick one over the other (cost obviously)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Software&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Operating System:&lt;/strong&gt; Is Linux still the go-to for optimal AI performance and compatibility with frameworks?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local AI Model (LLMs):&lt;/strong&gt; What LLMs are generally recommended for a mix of RAG, summarization, agentic workflows, and coding? Or should I consider running multiple models? I've read some positive reviews about qwen3 235b. Can I even run a model like that with reasonable tps within this budget? Probably not the full 235b variant?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference Software:&lt;/strong&gt; What are the best tools for running open-source LLMs locally, from user-friendly options for beginners to high-performance frameworks for scaling?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supporting Software:&lt;/strong&gt; What recommendations do you have for open-source tools or frameworks for building RAG systems (vector databases, RAG frameworks) and AI agents?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any general insights, experiences, or project architectural advice would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance for your input!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Wow, thank you all for the incredible amount of feedback and advice! &lt;/p&gt; &lt;p&gt;I want to clarify a couple of things that came up in the comments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This system will probably only be used by 20 users, with probably no more than 5 using it at the same time.&lt;/li&gt; &lt;li&gt;My boss and our IT team are aware that this is an experimental project. The goal is to build in-house knowledge, and we are prepared for some setbacks along the way. Our company already has the necessary infrastructure for security and data backups.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks again to everyone for the valuable input! It has given me a lot to think about and will be extremely helpful as I move forward with this project.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AFruitShopOwner"&gt; /u/AFruitShopOwner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1led23c/local_ai_for_a_smallmedian_accounting_firm_buget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1led23c/local_ai_for_a_smallmedian_accounting_firm_buget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1led23c/local_ai_for_a_smallmedian_accounting_firm_buget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T09:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lehbra</id>
    <title>Built memX: a shared memory backend for LLM agents (demo + open-source code)</title>
    <updated>2025-06-18T13:37:19+00:00</updated>
    <author>
      <name>/u/Temporary-Tap-7323</name>
      <uri>https://old.reddit.com/user/Temporary-Tap-7323</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lehbra/built_memx_a_shared_memory_backend_for_llm_agents/"&gt; &lt;img alt="Built memX: a shared memory backend for LLM agents (demo + open-source code)" src="https://external-preview.redd.it/bWpmbGR5djV2bzdmMYCyFtIdy85G-V-pyC1NhTykFPs5rMNi1ya3S7fCsS5U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16d6b58da2a98255f6e48827413597a42a678592" title="Built memX: a shared memory backend for LLM agents (demo + open-source code)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone — I built this over the weekend and wanted to share:&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://github.com/MehulG/memX"&gt;https://github.com/MehulG/memX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;memX&lt;/strong&gt; is a shared memory layer for LLM agents — kind of like Redis, but with real-time sync, pub/sub, schema validation, and access control.&lt;/p&gt; &lt;p&gt;Instead of having agents pass messages or follow a fixed pipeline, they just read and write to shared memory keys. It’s like a collaborative whiteboard where agents evolve context together.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt; - Real-time pub/sub - Per-key JSON schema validation - API key-based ACLs - Python SDK&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Tap-7323"&gt; /u/Temporary-Tap-7323 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ibq16xv5vo7f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lehbra/built_memx_a_shared_memory_backend_for_llm_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lehbra/built_memx_a_shared_memory_backend_for_llm_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T13:37:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lf1j2v</id>
    <title>Is there any LLM tool for UX and accessibility?</title>
    <updated>2025-06-19T04:11:54+00:00</updated>
    <author>
      <name>/u/darkcatpirate</name>
      <uri>https://old.reddit.com/user/darkcatpirate</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any LLM tool for UX and accessibility? I am looking for some kind of scanner that detects issues in my apps.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkcatpirate"&gt; /u/darkcatpirate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf1j2v/is_there_any_llm_tool_for_ux_and_accessibility/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lf1j2v/is_there_any_llm_tool_for_ux_and_accessibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lf1j2v/is_there_any_llm_tool_for_ux_and_accessibility/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T04:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1leyvq5</id>
    <title>Dual CPU Penalty?</title>
    <updated>2025-06-19T01:53:34+00:00</updated>
    <author>
      <name>/u/jsconiers</name>
      <uri>https://old.reddit.com/user/jsconiers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Should there be a noticable penalty for running dual CPUs on a workload? Two systems running same version of Ubuntu Linux, on ollama with gemma3 (27b-it-fp16). One has a thread ripper 7985 with 256GB memory, 5090. Second system is a dual 8480 Xeon with 256GB memory and a 5090. Regaurdless of workload the threadripper is always faster.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jsconiers"&gt; /u/jsconiers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyvq5/dual_cpu_penalty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyvq5/dual_cpu_penalty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leyvq5/dual_cpu_penalty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T01:53:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lex3pi</id>
    <title>How much is the 3090 on the used market in your country?</title>
    <updated>2025-06-19T00:25:34+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hoping you're having a good day.&lt;/p&gt; &lt;p&gt;I was wondering the 3090 used prices on your country, as they seem very different based on this.&lt;/p&gt; &lt;p&gt;I will start, with Chile. Here the used 3090s used hover between 550 and 650USD. This is a bit of increase in price vs some months ago, when it was between 500 and 550 USD instead.&lt;/p&gt; &lt;p&gt;Also I went to EU, specifically to Madrid, Spain 3 weeks ago. And when I did check on a quick search, they hovered between 600 and 700 EUR.&lt;/p&gt; &lt;p&gt;BTW as reference, 4090s used go for ~1800-1900USD which is just insane, and new 5090s are at 2700-2900USD range, which is also insane.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lex3pi/how_much_is_the_3090_on_the_used_market_in_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lex3pi/how_much_is_the_3090_on_the_used_market_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lex3pi/how_much_is_the_3090_on_the_used_market_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T00:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lexg9w</id>
    <title>How to set up local llms on a 6700 xt</title>
    <updated>2025-06-19T00:42:29+00:00</updated>
    <author>
      <name>/u/Electronic_Image1665</name>
      <uri>https://old.reddit.com/user/Electronic_Image1665</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All right so I struggled for what’s gotta be about four or five weeks now to get local LLM’s running with my GPU which is a 6700 XT. After this process of about four weeks I finally got something working on windows so here is the guide in case anyone is interested:&lt;/p&gt; &lt;h1&gt;AMD RX 6700 XT LLM Setup Guide - KoboldCpp with GPU Acceleration&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Successfully tested on AMD Radeon RX 6700 XT (gfx1031) running Windows 11&lt;/strong&gt;&lt;/p&gt; &lt;h2&gt;Performance Results&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Generation Speed&lt;/strong&gt;: ~17 tokens/second&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Processing Speed&lt;/strong&gt;: ~540 tokens/second&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Utilization&lt;/strong&gt;: 20/29 layers offloaded to GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Usage&lt;/strong&gt;: ~2.7GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Size&lt;/strong&gt;: 4096 tokens&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;The Problem&lt;/h2&gt; &lt;p&gt;Most guides focus on ROCm setup, but AMD RX 6700 XT (gfx1031 architecture) has compatibility issues with ROCm on Windows. The solution is using &lt;strong&gt;Vulkan acceleration&lt;/strong&gt; instead, which provides excellent performance and stability.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;AMD RX 6700 XT graphics card&lt;/li&gt; &lt;li&gt;Windows 10/11&lt;/li&gt; &lt;li&gt;At least 8GB system RAM&lt;/li&gt; &lt;li&gt;4-5GB free storage space&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Step 1: Download KoboldCpp-ROCm&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Go to: &lt;a href="https://github.com/YellowRoseCx/koboldcpp-rocm/releases"&gt;https://github.com/YellowRoseCx/koboldcpp-rocm/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Download the latest &lt;code&gt;koboldcpp_rocm.exe&lt;/code&gt; &lt;/li&gt; &lt;li&gt;Create folder: &lt;code&gt;C:\Users\[YourUsername]\llamafile_test\koboldcpp-rocm\&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Place the executable inside the &lt;code&gt;koboldcpp-rocm&lt;/code&gt; folder&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Step 2: Download a Model&lt;/h2&gt; &lt;p&gt;Download a GGUF model (recommended: 7B parameter models for RX 6700 XT): - Qwen2.5-Coder-7B-Instruct (recommended for coding) - Llama-3.1-8B-Instruct - Any other 7B-8B GGUF model&lt;/p&gt; &lt;p&gt;Place the &lt;code&gt;.gguf&lt;/code&gt; file in: &lt;code&gt;C:\Users\[YourUsername]\llamafile_test\&lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Step 3: Create Launch Script&lt;/h2&gt; &lt;p&gt;Create &lt;code&gt;start_koboldcpp_optimized.bat&lt;/code&gt; with this content:&lt;/p&gt; &lt;p&gt;```batch @echo off cd /d &amp;quot;C:\Users[YourUsername]\llamafile_test&amp;quot;&lt;/p&gt; &lt;p&gt;REM Kill any existing processes taskkill /F /IM koboldcpp-rocm.exe 2&amp;gt;nul&lt;/p&gt; &lt;p&gt;echo =============================================== echo KoboldCpp with Vulkan GPU Acceleration echo =============================================== echo Model: [your-model-name].gguf echo GPU: AMD RX 6700 XT via Vulkan echo GPU Layers: 20 echo Context: 4096 tokens echo Port: 5001 echo ===============================================&lt;/p&gt; &lt;p&gt;koboldcpp-rocm\koboldcpp-rocm.exe ^ --model &amp;quot;[your-model-name].gguf&amp;quot; ^ --host 127.0.0.1 ^ --port 5001 ^ --contextsize 4096 ^ --gpulayers 20 ^ --blasbatchsize 1024 ^ --blasthreads 4 ^ --highpriority ^ --skiplauncher&lt;/p&gt; &lt;p&gt;echo. echo Server running at: http://localhost:5001 echo Performance: ~17 tokens/second generation echo. pause ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Replace &lt;code&gt;[YourUsername]&lt;/code&gt; and &lt;code&gt;[your-model-name]&lt;/code&gt; with your actual values.&lt;/strong&gt;&lt;/p&gt; &lt;h2&gt;Step 4: Run and Verify&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Run the script&lt;/strong&gt;: Double-click &lt;code&gt;start_koboldcpp_optimized.bat&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Look for these success indicators&lt;/strong&gt;: &lt;code&gt; Auto Selected Vulkan Backend... ggml_vulkan: 0 = AMD Radeon RX 6700 XT (AMD proprietary driver) offloaded 20/29 layers to GPU Starting Kobold API on port 5001 &lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open browser&lt;/strong&gt;: Navigate to http://localhost:5001&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test generation&lt;/strong&gt;: Try generating some text to verify GPU acceleration&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Expected Output&lt;/h2&gt; &lt;p&gt;&lt;code&gt; Processing Prompt [BLAS] (XXX / XXX tokens) Generating (XXX / XXX tokens) [Time] CtxLimit:XXXX/4096, Process:X.XXs (500+ T/s), Generate:X.XXs (15-20 T/s) &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Troubleshooting&lt;/h2&gt; &lt;h3&gt;If you get &amp;quot;ROCm failed&amp;quot; or crashes:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: The script automatically falls back to Vulkan - this is expected and optimal&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Don't install ROCm&lt;/strong&gt; - it's not needed and can cause conflicts&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;If you get low performance (&amp;lt; 10 tokens/sec):&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Reduce GPU layers&lt;/strong&gt;: Change &lt;code&gt;--gpulayers 20&lt;/code&gt; to &lt;code&gt;--gpulayers 15&lt;/code&gt; or &lt;code&gt;--gpulayers 10&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Check VRAM&lt;/strong&gt;: Monitor GPU memory usage in Task Manager&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reduce context&lt;/strong&gt;: Change &lt;code&gt;--contextsize 4096&lt;/code&gt; to &lt;code&gt;--contextsize 2048&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;If server won't start:&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Check port&lt;/strong&gt;: Change &lt;code&gt;--port 5001&lt;/code&gt; to &lt;code&gt;--port 5002&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Run as administrator&lt;/strong&gt;: Right-click script → &amp;quot;Run as administrator&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Key Differences from Other Guides&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;No ROCm required&lt;/strong&gt;: Uses Vulkan instead of ROCm&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No environment variables needed&lt;/strong&gt;: Auto-detection works perfectly&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No compilation required&lt;/strong&gt;: Uses pre-built executable&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized for gaming GPUs&lt;/strong&gt;: Settings tuned for consumer hardware&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Performance Comparison&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Method&lt;/th&gt; &lt;th&gt;Setup Complexity&lt;/th&gt; &lt;th&gt;Performance&lt;/th&gt; &lt;th&gt;Stability&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ROCm (typical guides)&lt;/td&gt; &lt;td&gt;High&lt;/td&gt; &lt;td&gt;Variable&lt;/td&gt; &lt;td&gt;Poor on gfx1031&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Vulkan (this guide)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;Low&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;17+ T/s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;Excellent&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CPU-only&lt;/td&gt; &lt;td&gt;Low&lt;/td&gt; &lt;td&gt;3-4 T/s&lt;/td&gt; &lt;td&gt;Good&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Final Notes&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VRAM limit&lt;/strong&gt;: RX 6700 XT has 12GB, can handle up to ~28 GPU layers for 7B models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context scaling&lt;/strong&gt;: Larger context (8192+) may require fewer GPU layers&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model size&lt;/strong&gt;: 13B models work but require fewer GPU layers (~10-15)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: Vulkan is more stable than ROCm for gaming GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This setup provides near-optimal performance for AMD RX 6700 XT without the complexity and instability of ROCm configuration.&lt;/p&gt; &lt;h2&gt;Support&lt;/h2&gt; &lt;p&gt;If you encounter issues: 1. Check Windows GPU drivers are up to date 2. Ensure you have latest Visual C++ redistributables 3. Try reducing &lt;code&gt;--gpulayers&lt;/code&gt; value if you run out of VRAM&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tested Configuration&lt;/strong&gt;: Windows 11, AMD RX 6700 XT, 32GB RAM, AMD Ryzen 5 5600&lt;/p&gt; &lt;p&gt;Hope this helps!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic_Image1665"&gt; /u/Electronic_Image1665 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lexg9w/how_to_set_up_local_llms_on_a_6700_xt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lexg9w/how_to_set_up_local_llms_on_a_6700_xt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lexg9w/how_to_set_up_local_llms_on_a_6700_xt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T00:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lexlsd</id>
    <title>Best realtime open source STT model?</title>
    <updated>2025-06-19T00:49:58+00:00</updated>
    <author>
      <name>/u/ThatIsNotIllegal</name>
      <uri>https://old.reddit.com/user/ThatIsNotIllegal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the best model to transcribe a conversation in realtime, meaning that the words have to appear as the person is talking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThatIsNotIllegal"&gt; /u/ThatIsNotIllegal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lexlsd/best_realtime_open_source_stt_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lexlsd/best_realtime_open_source_stt_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lexlsd/best_realtime_open_source_stt_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T00:49:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lewg4u</id>
    <title>Does this mean we are free from the shackles of CUDA? We can use AMD GPUs wired up together to run models ?</title>
    <updated>2025-06-18T23:53:58+00:00</updated>
    <author>
      <name>/u/Just_Lingonberry_352</name>
      <uri>https://old.reddit.com/user/Just_Lingonberry_352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lewg4u/does_this_mean_we_are_free_from_the_shackles_of/"&gt; &lt;img alt="Does this mean we are free from the shackles of CUDA? We can use AMD GPUs wired up together to run models ?" src="https://preview.redd.it/y31qo2q5xr7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2f2ad2fad2ebbe87b16acdd6835744a159aea21" title="Does this mean we are free from the shackles of CUDA? We can use AMD GPUs wired up together to run models ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Lingonberry_352"&gt; /u/Just_Lingonberry_352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y31qo2q5xr7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lewg4u/does_this_mean_we_are_free_from_the_shackles_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lewg4u/does_this_mean_we_are_free_from_the_shackles_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T23:53:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1lezxa9</id>
    <title>[Open] LMeterX - Professional Load Testing for Any OpenAI-Compatible LLM API</title>
    <updated>2025-06-19T02:45:45+00:00</updated>
    <author>
      <name>/u/SignalBelt7205</name>
      <uri>https://old.reddit.com/user/SignalBelt7205</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lezxa9/open_lmeterx_professional_load_testing_for_any/"&gt; &lt;img alt="[Open] LMeterX - Professional Load Testing for Any OpenAI-Compatible LLM API" src="https://b.thumbs.redditmedia.com/BFyKUwwRnCZUvNSyXzVoPT1Y9xWb2g6aL1l-qXNK-QA.jpg" title="[Open] LMeterX - Professional Load Testing for Any OpenAI-Compatible LLM API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Solving Real Pain Points&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;🤔 Don't know your LLM's concurrency limits?&lt;/p&gt; &lt;p&gt;🤔 Need to compare model performance but lack proper tools?&lt;/p&gt; &lt;p&gt;🤔 Want professional metrics (TTFT, TPS, RPS) not just basic HTTP stats?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;✅ Universal compatibility - Applicable to any openai format API such as GPT, Claude, Llama, etc (language/multimodal /CoT)&lt;/p&gt; &lt;p&gt;✅ Smart load testing - Precise concurrency control &amp;amp; Real user simulation&lt;/p&gt; &lt;p&gt;✅ Professional metrics - TTFT, TPS, RPS, success/error rate, etc&lt;/p&gt; &lt;p&gt;✅ Multi-scenario support - Text conversations &amp;amp; Multimodal (image+text)&lt;/p&gt; &lt;p&gt;✅ Visualize the results - Performance report &amp;amp; Model arena&lt;/p&gt; &lt;p&gt;✅ Real-time monitoring - Hierarchical monitoring of tasks and services&lt;/p&gt; &lt;p&gt;✅ Enterprise ready - Docker deployment &amp;amp; Web management console &amp;amp; Scalable architecture&lt;/p&gt; &lt;p&gt;⬇️ &lt;strong&gt;DEMO&lt;/strong&gt; ⬇️&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/14l0srxgrs7f1.gif"&gt;https://i.redd.it/14l0srxgrs7f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🚀 One-Click Docker deploy&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;curl -fsSL &lt;a href="https://raw.githubusercontent.com/DataEval/LMeterX/main/quick-start.sh"&gt;https://raw.githubusercontent.com/MigoXLab/LMeterX/main/quick-start.sh&lt;/a&gt; | bash&lt;/p&gt; &lt;p&gt;⭐ &lt;strong&gt;GitHub&lt;/strong&gt; ➡️ &lt;a href="https://github.com/MigoXLab/LMeterX"&gt;https://github.com/MigoXLab/LMeterX&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignalBelt7205"&gt; /u/SignalBelt7205 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lezxa9/open_lmeterx_professional_load_testing_for_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lezxa9/open_lmeterx_professional_load_testing_for_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lezxa9/open_lmeterx_professional_load_testing_for_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T02:45:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1lek04t</id>
    <title>Built an open-source DeepThink plugin that brings Gemini 2.5 style advanced reasoning to local models (DeepSeek R1, Qwen3, etc.)</title>
    <updated>2025-06-18T15:26:55+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lek04t/built_an_opensource_deepthink_plugin_that_brings/"&gt; &lt;img alt="Built an open-source DeepThink plugin that brings Gemini 2.5 style advanced reasoning to local models (DeepSeek R1, Qwen3, etc.)" src="https://b.thumbs.redditmedia.com/9MeqPAY2NW7ptCMMeXL3V8ULf8znmAvn_poPZJGnM8g.jpg" title="Built an open-source DeepThink plugin that brings Gemini 2.5 style advanced reasoning to local models (DeepSeek R1, Qwen3, etc.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;So Google just dropped their Gemini 2.5 report and there's this really interesting technique called &amp;quot;Deep Think&amp;quot; that got me thinking. Basically, it's a structured reasoning approach where the model generates multiple hypotheses in parallel and critiques them before giving you the final answer. The results are pretty impressive - SOTA on math olympiad problems, competitive coding, and other challenging benchmarks.&lt;/p&gt; &lt;p&gt;I implemented a DeepThink plugin for OptiLLM that works with local models like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1&lt;/li&gt; &lt;li&gt;Qwen3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The plugin essentially makes your local model &amp;quot;think out loud&amp;quot; by exploring multiple solution paths simultaneously, then converging on the best answer. It's like giving your model an internal debate team.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;Instead of the typical single-pass generation, the model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generates multiple approaches to the problem in parallel&lt;/li&gt; &lt;li&gt;Evaluates each approach critically&lt;/li&gt; &lt;li&gt;Synthesizes the best elements into a final response&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This is especially useful for complex reasoning tasks, math problems, coding challenges, etc.&lt;/p&gt; &lt;p&gt;We actually won the 3rd Prize at Cerebras &amp;amp; OpenRouter Qwen 3 Hackathon with this approach, which was pretty cool validation that the technique works well beyond Google's implementation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5el6xgxhep7f1.png?width=1238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9f4f420191f047573dc5dd7adfbc05c2c175227"&gt;https://preview.redd.it/5el6xgxhep7f1.png?width=1238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9f4f420191f047573dc5dd7adfbc05c2c175227&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Code &amp;amp; Demo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/plugins/deepthink"&gt;https://github.com/codelion/optillm/tree/main/optillm/plugins/deepthink&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo video: &lt;a href="https://www.youtube.com/watch?v=b06kD1oWBA4"&gt;https://www.youtube.com/watch?v=b06kD1oWBA4&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The plugin is ready to use right now if you want to try it out. Would love to get feedback from the community and see what improvements we can make together.&lt;/p&gt; &lt;p&gt;Has anyone else been experimenting with similar reasoning techniques for local models? Would be interested to hear what approaches you've tried.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; For those asking about performance impact - yes, it does increase inference time since you're essentially running multiple reasoning passes. But for complex problems where you want the best possible answer, the trade-off is usually worth it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lek04t/built_an_opensource_deepthink_plugin_that_brings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lek04t/built_an_opensource_deepthink_plugin_that_brings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lek04t/built_an_opensource_deepthink_plugin_that_brings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T15:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lepjc5</id>
    <title>Mobile Phones are becoming better at running AI locally on the device.</title>
    <updated>2025-06-18T19:02:35+00:00</updated>
    <author>
      <name>/u/Henrie_the_dreamer</name>
      <uri>https://old.reddit.com/user/Henrie_the_dreamer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lepjc5/mobile_phones_are_becoming_better_at_running_ai/"&gt; &lt;img alt="Mobile Phones are becoming better at running AI locally on the device." src="https://external-preview.redd.it/bssrhhUFkv6YYPmNcbuJIt4gLvIfF5uq2fTh65BCaWI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2764476115f204e3b179924d64c1d7e63030de16" title="Mobile Phones are becoming better at running AI locally on the device." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tvgjdqmqiq7f1.png?width=1393&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78bd43352e644ef4a02073cba83069a5bed72c48"&gt;https://preview.redd.it/tvgjdqmqiq7f1.png?width=1393&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78bd43352e644ef4a02073cba83069a5bed72c48&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We aggregated the tokens/second on various devices that use apps built with Cactus&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1B - 4B models at INT4 run quite fast (we shipped some improvements though).&lt;/li&gt; &lt;li&gt;You can see the full list on our GitHub &lt;a href="https://github.com/cactus-compute/cactus"&gt;https://github.com/cactus-compute/cactus&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You might be wondering if these models aren’t too small to get meaningful results, however:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Beyond coding and large-scale enterprise projects that involves reasoning over massive contexts, these models are overkill. &lt;/li&gt; &lt;li&gt;Most products are fine with GPT 4.1 actually, users working on embedding even go for much smaller models, Gemma is great.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ow1n6jbxgq7f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7699d15dc26eae73165c1455af491dd7ecddc19b"&gt;Gemma 3n 4B is very competitive!&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1-4B models are perfect for on-device problems like automatic message/call handling, email summary, gallery search, photo editing, text retrieval, reminder/calendar management, phone settings control, text-to-speech, realtime translation, quick Q/As and other personal problems&lt;/li&gt; &lt;li&gt;Even Apple’s foundation framework and Google AI Edge products do not exceed 3B either.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You might also be thinking “yes privacy might be a use case, but is API cost really a problem”, well its not for B2B products and …but its nuanced.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For &lt;strong&gt;consumer&lt;/strong&gt; &lt;strong&gt;products&lt;/strong&gt; with &lt;strong&gt;100s of millions of users&lt;/strong&gt; and &lt;strong&gt;&amp;lt;= 3B in revenue&lt;/strong&gt;, (Pinterest, Dropbox, Telegram, Duolingo, Blinklist, Audible, ), covering the cost for 500m users is infeasible, makes more sense to offload the costs to the users via a premium package or deploying in-house versions.&lt;/li&gt; &lt;li&gt;Well, wouldn’t they maximise profits and reduce operational overhead by letting the users run the AI locally?&lt;/li&gt; &lt;li&gt;In fact, I would argue that Cursor is becoming too expensive for non-corporate users, and could benefit by using a local model for simple tasks.&lt;/li&gt; &lt;li&gt;The future of personal AI is heading towards realtime live models like Project Astra, Gemini Live, ChatGPT Live Preview etc, which all need very low latency for good user experience.&lt;/li&gt; &lt;li&gt;I mean Zoom/Meets/Teams calls still face latency issues, and we see this glitch in these live streaming models.&lt;/li&gt; &lt;li&gt;We created a low-latency live AI system that runs locally on device with Cactus, watch demo here &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7334225731243139072"&gt;https://www.linkedin.com/feed/update/urn:li:activity:7334225731243139072&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share your thoughts here in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Henrie_the_dreamer"&gt; /u/Henrie_the_dreamer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lepjc5/mobile_phones_are_becoming_better_at_running_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lepjc5/mobile_phones_are_becoming_better_at_running_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lepjc5/mobile_phones_are_becoming_better_at_running_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T19:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1led0lb</id>
    <title>Google doubled the price of Gemini 2.5 Flash thinking output after GA from 0.15 to 0.30 what</title>
    <updated>2025-06-18T09:48:16+00:00</updated>
    <author>
      <name>/u/NoAd2240</name>
      <uri>https://old.reddit.com/user/NoAd2240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry the input**&lt;/p&gt; &lt;p&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/pricing"&gt;https://cloud.google.com/vertex-ai/generative-ai/pricing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoAd2240"&gt; /u/NoAd2240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1led0lb/google_doubled_the_price_of_gemini_25_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1led0lb/google_doubled_the_price_of_gemini_25_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1led0lb/google_doubled_the_price_of_gemini_25_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T09:48:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lemmsq</id>
    <title>We took Qwen3 235B A22B from 34 tokens/sec to 54 tokens/sec by switching from llama.cpp with Unsloth dynamic Q4_K_M GGUF to vLLM with INT4 w4a16</title>
    <updated>2025-06-18T17:09:31+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;System: quad RTX A6000 Epyc. &lt;/p&gt; &lt;p&gt;Originally we were running the Unsloth dynamic GGUFs at UD_Q4_K_M and UD_Q5_K_XL with which we were getting speeds of 34 and 31 tokens/sec, respectively, for small-ish prompts of 1-2k tokens. &lt;/p&gt; &lt;p&gt;A couple of days ago we tried an experiment with another 4-bit quant type: INT 4, specifically w4a16, which is a 4-bit quant that's expanded and run at FP16. Or something. The wizard and witches will know better, forgive my butchering of LLM mechanics. This is the one we used: &lt;code&gt;justinjja/Qwen3-235B-A22B-INT4-W4A16&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The point is that w4a16 runs in vLLM and is a whopping 20 tokens/sec faster than Q4 in llama.cpp in like-for-like tests (as close as we could get without going crazy). &lt;/p&gt; &lt;p&gt;Does anyone know how w4a16 compares to Q4_K_M in terms of quantization quality? Are these 4-bit quants actually comparing apples to apples? Or are we sacrificing quality for speed? We'll do our own tests, but I'd like to hear opinions from the peanut gallery.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lemmsq/we_took_qwen3_235b_a22b_from_34_tokenssec_to_54/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lemmsq/we_took_qwen3_235b_a22b_from_34_tokenssec_to_54/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lemmsq/we_took_qwen3_235b_a22b_from_34_tokenssec_to_54/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T17:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1leq843</id>
    <title>The Bizarre Limitations of Apple's Foundation Models Framework</title>
    <updated>2025-06-18T19:29:31+00:00</updated>
    <author>
      <name>/u/SandBlaster2000AD</name>
      <uri>https://old.reddit.com/user/SandBlaster2000AD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week Apple announced some great new APIs for their on-device foundation models in OS 26. Devs have been experimenting with it for over a week now, and the local LLM is surprisingly capable for only a 3B model w/2-bit quantization. It's also very power efficient because it leverages the ANE. You can try it out for yourself if you have the current developer OS releases as a &lt;a href="https://github.com/PallavAg/Apple-Intelligence-Chat"&gt;chat interface&lt;/a&gt; or using &lt;a href="https://developer.apple.com/documentation/foundationmodels/generate-dynamic-game-content-with-guided-generation-and-tools"&gt;Apple's game dialog demo&lt;/a&gt;. Unfortunately, people are quickly finding that artificial restrictions are limiting the utility of the framework (at least for now).&lt;/p&gt; &lt;p&gt;The first issue most devs will notice are the overly aggressive guardrails. Just take a look at the posts over on the &lt;a href="https://developer.apple.com/forums/topics/machine-learning-and-ai/machine-learning-and-ai-foundation-models"&gt;developer forums&lt;/a&gt;. Everything from news summarization to apps about fishing and camping are blocked. All but the most bland dialog in the Dream Coffee demo is also censored - just try asking &amp;quot;Can I get a polonium latte for my robot?&amp;quot;. You can't even work around the guardrails through clever prompting because the API call itself returns an error.&lt;/p&gt; &lt;p&gt;There are also rate limits for certain uses, so no batch processing or frequent queries. The excuse here might be power savings on mobile, but the only comparable workaround is to bundle another open-weight model - which will totally nuke the battery anyway.&lt;/p&gt; &lt;p&gt;Lastly, you cannot really build an app around any Apple Intelligence features because the App Store ecosystem does not allow publishers to restrict availability to supported devices. Apple will tell you that you need a fallback for older devices, in case local models are not available. But that kind of defeats the purpose - if I need to bundle Mistral or Qwen with my app &amp;quot;just in case&amp;quot;, then I might as well not use the Foundation Models Framework at all.&lt;/p&gt; &lt;p&gt;I really hope that these issues get resolved during the OS 26 beta cycle. There is a ton of potential here for local AI apps, and I'd love to see it take off!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SandBlaster2000AD"&gt; /u/SandBlaster2000AD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leq843/the_bizarre_limitations_of_apples_foundation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leq843/the_bizarre_limitations_of_apples_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leq843/the_bizarre_limitations_of_apples_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T19:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lenf36</id>
    <title>new 72B and 70B models from Arcee</title>
    <updated>2025-06-18T17:39:53+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;looks like there are some new models from Arcee&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Virtuoso-Large"&gt;https://huggingface.co/arcee-ai/Virtuoso-Large&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Virtuoso-Large-GGUF"&gt;https://huggingface.co/arcee-ai/Virtuoso-Large-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;strong&gt;Virtuoso-Large (72B)&lt;/strong&gt; is our most powerful and versatile general-purpose model, designed to excel at handling complex and varied tasks across domains. With state-of-the-art performance, it offers unparalleled capability for nuanced understanding, contextual adaptability, and high accuracy.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Arcee-SuperNova-v1"&gt;https://huggingface.co/arcee-ai/Arcee-SuperNova-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Arcee-SuperNova-v1-GGUF"&gt;https://huggingface.co/arcee-ai/Arcee-SuperNova-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;&lt;strong&gt;Arcee-SuperNova-v1 (70B)&lt;/strong&gt; is a merged model built from multiple advanced training approaches. At its core is a distilled version of Llama-3.1-405B-Instruct into Llama-3.1-70B-Instruct, using out &lt;a href="https://github.com/arcee-ai/DistillKit"&gt;DistillKit&lt;/a&gt; to preserve instruction-following strengths while reducing size.&amp;quot;&lt;/p&gt; &lt;p&gt;not sure is it related or there will be more:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14185"&gt;https://github.com/ggml-org/llama.cpp/pull/14185&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;This adds support for upcoming Arcee model architecture, currently codenamed the Arcee Foundation Model (AFM).&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lenf36/new_72b_and_70b_models_from_arcee/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lenf36/new_72b_and_70b_models_from_arcee/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lenf36/new_72b_and_70b_models_from_arcee/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T17:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1leod7d</id>
    <title>OpenAI found features in AI models that correspond to different ‘personas’</title>
    <updated>2025-06-18T18:16:40+00:00</updated>
    <author>
      <name>/u/nightsky541</name>
      <uri>https://old.reddit.com/user/nightsky541</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openai.com/index/emergent-misalignment/"&gt;https://openai.com/index/emergent-misalignment/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;br /&gt; OpenAI discovered that large language models contain internal &amp;quot;persona&amp;quot; features neural patterns linked to specific behaviours like toxic, helpfulness or sarcasm. By activating or suppressing these, researchers can steer the model’s personality and alignment.&lt;/p&gt; &lt;p&gt;Edit: Replaced with original source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nightsky541"&gt; /u/nightsky541 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leod7d/openai_found_features_in_ai_models_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leod7d/openai_found_features_in_ai_models_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leod7d/openai_found_features_in_ai_models_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T18:16:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1leyi70</id>
    <title>Self-hosting LLaMA: What are your biggest pain points?</title>
    <updated>2025-06-19T01:34:58+00:00</updated>
    <author>
      <name>/u/Sriyakee</name>
      <uri>https://old.reddit.com/user/Sriyakee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow llama enthusiasts!&lt;/p&gt; &lt;p&gt;Setting aside compute, what has been the biggest issues that you guys have faced when trying to self host models? e.g:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Running out of GPU memory or dealing with slow inference times&lt;/li&gt; &lt;li&gt;Struggling to optimize model performance for specific use cases&lt;/li&gt; &lt;li&gt;Privacy?&lt;/li&gt; &lt;li&gt;Scaling models to handle high traffic or large datasets&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sriyakee"&gt; /u/Sriyakee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyi70/selfhosting_llama_what_are_your_biggest_pain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyi70/selfhosting_llama_what_are_your_biggest_pain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leyi70/selfhosting_llama_what_are_your_biggest_pain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T01:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1leh14g</id>
    <title>Can your favourite local model solve this?</title>
    <updated>2025-06-18T13:24:24+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leh14g/can_your_favourite_local_model_solve_this/"&gt; &lt;img alt="Can your favourite local model solve this?" src="https://preview.redd.it/gkjegqtyso7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93880be720bd03128b1e673976aa49f67626b2f0" title="Can your favourite local model solve this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested which, if any, models this relatively simple geometry picture if you simply give it this image.&lt;/p&gt; &lt;p&gt;I don't have a big enough setup to test visual models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gkjegqtyso7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leh14g/can_your_favourite_local_model_solve_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leh14g/can_your_favourite_local_model_solve_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T13:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lersrw</id>
    <title>Augmentoolkit 3.0: 7 months of work, MIT License, Specialist AI Training</title>
    <updated>2025-06-18T20:33:11+00:00</updated>
    <author>
      <name>/u/Heralax_Tekran</name>
      <uri>https://old.reddit.com/user/Heralax_Tekran</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lersrw/augmentoolkit_30_7_months_of_work_mit_license/"&gt; &lt;img alt="Augmentoolkit 3.0: 7 months of work, MIT License, Specialist AI Training" src="https://external-preview.redd.it/JPdazJ6jtyR317Uj2SGJFQQZYzaRBapP-lbz0ow2wM8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8583a011c64efbc563707ac8996f32baf680fa6e" title="Augmentoolkit 3.0: 7 months of work, MIT License, Specialist AI Training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Over the past year and a half&lt;/strong&gt; I've been working on the problem of &lt;strong&gt;factual finetuning&lt;/strong&gt; -- &lt;strong&gt;training an open-source LLM on new facts&lt;/strong&gt; so that it learns those facts, essentially extending its knowledge cutoff. Now that I've made significant progress on the problem, I just released &lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;&lt;strong&gt;Augmentoolkit 3.0&lt;/strong&gt; &lt;/a&gt;— an easy-to-use dataset generation and model training tool. Add documents, click a button, and Augmentoolkit will do everything for you: it'll generate a domain-specific dataset, combine it with a balanced amount of generic data, automatically train a model on it, download it, quantize it, and run it for inference (accessible with a built-in chat interface). The project (and its demo models) are fully open-source. I even trained a model to run inside Augmentoolkit itself, allowing for faster &lt;strong&gt;local dataset generation&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This update took more than six months and thousands of dollars to put together, and represents &lt;strong&gt;a complete rewrite and overhaul of the original project.&lt;/strong&gt; It includes 16 prebuilt dataset generation pipelines and the extensively-documented code and conventions to build more. Beyond just factual finetuning, it even &lt;strong&gt;includes an experimental&lt;/strong&gt; &lt;a href="https://github.com/e-p-armstrong/augmentoolkit/blob/master/docs/grpo.md"&gt;&lt;strong&gt;GRPO pipeline&lt;/strong&gt;&lt;/a&gt; that lets you &lt;strong&gt;train a model to do any conceivable task&lt;/strong&gt; by just &lt;strong&gt;writing a prompt to grade that task.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jnr3luv5zq7f1.png?width=1952&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2aed0b5ba86c0945dc41ea084445744784d42e6"&gt;https://preview.redd.it/jnr3luv5zq7f1.png?width=1952&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2aed0b5ba86c0945dc41ea084445744784d42e6&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;The Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;Project&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=E9TyyZzIMyY&amp;amp;ab_channel=Augmentoolkit"&gt;Train your first model in 13 minutes quickstart tutorial video&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo model (what the quickstart produces) &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Heralax/llama-Augmentoolkit-Quickstart-Factual-Demo-Example"&gt;Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Dataset and training configs are fully open source. The config is literally the quickstart config; the dataset is&lt;/li&gt; &lt;li&gt;The demo model is an LLM trained on a subset of the US Army Field Manuals -- the best free and open modern source of comprehensive documentation on a well-known field that I have found. This is also because I trained a model on these in the past and so training on them now serves as a good comparison between the power of the current tool compared to its previous version.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Experimental GRPO models &lt;ul&gt; &lt;li&gt;Now that Augmentoolkit includes the ability to grade models for their performance on a task, I naturally wanted to try this out, and on a task that people are familiar with.&lt;/li&gt; &lt;li&gt;I produced two RP models (base: Mistral 7b v0.2) with the intent of maximizing writing style quality and emotion, while minimizing GPT-isms.&lt;/li&gt; &lt;li&gt;One model has thought processes, the other does not. The non-thought-process model came out better for reasons described in the model card.&lt;/li&gt; &lt;li&gt;Non-reasoner &lt;a href="https://huggingface.co/Heralax/llama-gRPo-emotions-nothoughts"&gt;https://huggingface.co/Heralax/llama-gRPo-emotions-nothoughts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Reasoner &lt;a href="https://huggingface.co/Heralax/llama-gRPo-thoughtprocess"&gt;https://huggingface.co/Heralax/llama-gRPo-thoughtprocess&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Process to Reproduce&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Clone &lt;ul&gt; &lt;li&gt;&lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://github.com/e-p-armstrong/augmentoolkit.git"&gt;&lt;code&gt;https://github.com/e-p-armstrong/augmentoolkit.git&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Run Start Script &lt;ul&gt; &lt;li&gt;Local or Online&lt;/li&gt; &lt;li&gt;Mac &lt;ul&gt; &lt;li&gt;&lt;code&gt;bash&lt;/code&gt; &lt;a href="http://macos.sh/"&gt;&lt;code&gt;macos.sh&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;bash local_macos.sh&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Linux &lt;ul&gt; &lt;li&gt;&lt;code&gt;bash&lt;/code&gt; &lt;a href="http://linux.sh/"&gt;&lt;code&gt;linux.sh&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;bash local_linux.sh&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Windows + warning &lt;ul&gt; &lt;li&gt;Use WSL. If you don't want to, you will have to use the CLI instead. Instructions are in the readme in the quickstart page.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Add API keys or use the local model &lt;ul&gt; &lt;li&gt;I trained a 7b model that is purpose-built to run Augmentoolkit pipelines (Apache license). This means that you can probably generate data at a decent speed on your own computer. It will definitely be slower than with an API, but it will be &lt;em&gt;much&lt;/em&gt; better than trying to generate tens of millions of tokens with a local 70b.&lt;/li&gt; &lt;li&gt;There are separate start scripts for local datagen.&lt;/li&gt; &lt;li&gt;You'll probably only be able to get good dataset generation speed on a linux machine even though it does technically run on Mac, since Llama.cpp is MUCH slower than vLLM (which is Linux-only).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Click the &amp;quot;run&amp;quot; Button&lt;/li&gt; &lt;li&gt;Get Your Model &lt;ul&gt; &lt;li&gt;The integrated chat interface will automatically let you chat with it when the training and quanting is finished&lt;/li&gt; &lt;li&gt;The model will also automatically be pushed to Hugging Face (make sure you have enough space!)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Uses&lt;/h1&gt; &lt;p&gt;Besides faster generation times and lower costs, an expert AI that is trained on a domain gains a &amp;quot;big-picture&amp;quot; understanding of the subject that a generalist just won't have. It's the difference between giving a new student a class's full textbook and asking them to write an exam, versus asking a graduate student in that subject to write the exam. The new student probably won't even know where in that book they should look for the information they need, and even if they see the correct context, there's no guarantee that they understands what it means or how it fits into the bigger picture.&lt;/p&gt; &lt;p&gt;Also, trying to build AI apps based on closed-source LLMs released by big labs sucks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The lack of stable checkpoints under the control of the person running the model, makes the tech unstable and unpredictable to build on.&lt;/li&gt; &lt;li&gt;Capabilities change without warning and models are frequently made worse.&lt;/li&gt; &lt;li&gt;People building with AI have to work around the LLMs they are using (a moving target), rather than make the LLMs they are using fit into their system&lt;/li&gt; &lt;li&gt;Refusals force people deploying models to dance around the stuck-up morality of these models while developing.&lt;/li&gt; &lt;li&gt;Closed-source labs charge obscene prices, doing monopolistic rent collecting and impacting the margins of their customers.&lt;/li&gt; &lt;li&gt;Using closed-source labs is a privacy nightmare, especially now that API providers may be required by law to save and log formerly-private API requests.&lt;/li&gt; &lt;li&gt;Different companies have to all work with the same set of models, which have the same knowledge, the same capabilities, the same opinions, and they all sound more or less the same.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But current open-source models often either suffer from a severe lack of capability, or are massive enough that they might as well be closed-source for most of the people trying to run them. The proposed solution? Small, efficient, powerful models that achieve superior performance on the things they are being used for (and sacrifice performance in the areas they &lt;em&gt;aren't&lt;/em&gt; being used for) which are trained for their task and are controlled by the companies that use them.&lt;/p&gt; &lt;p&gt;With &lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;&lt;strong&gt;Augmentoolkit&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You train your models, decide when those models update, and have full transparency over what went into them.&lt;/li&gt; &lt;li&gt;Capabilities change only when the company wants, and no one is forcing them to make their models worse.&lt;/li&gt; &lt;li&gt;People working with AI can customize the model they are using to function as part of the system they are designing, rather than having to twist their system to match a model.&lt;/li&gt; &lt;li&gt;Since you control the data it is built on, the model is only as restricted as you want it to be.&lt;/li&gt; &lt;li&gt;7 billion parameter models (the standard size Augmentoolkit trains) are so cheap to run it is absurd. They can run on a laptop, even.&lt;/li&gt; &lt;li&gt;Because you control your model, you control your inference, and you control your customers' data.&lt;/li&gt; &lt;li&gt;With your model's capabilities being fully customizable, your AI sounds like &lt;em&gt;your&lt;/em&gt; AI, and has the opinions and capabilities that you want it to have.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Furthermore, the open-source indie finetuning scene has been on life support, largely due to a lack of ability to make data, and the difficulty of getting started with (and getting results with) training, compared to methods like merging. Now that data is far easier to make, and training for specific objectives is much easier to do, and there is a good baseline with training wheels included that makes getting started easy, the hope is that people can iterate on finetunes and the scene can have new life.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;Augmentoolkit&lt;/a&gt; is taking a bet on an open-source future powered by small, efficient, Specialist Language Models.&lt;/p&gt; &lt;h1&gt;Cool things of note&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Factually-finetuned models can actually cite what files they are remembering information from, and with a good degree of accuracy at that. This is not exclusive to the domain of RAG anymore.&lt;/li&gt; &lt;li&gt;Augmentoolkit models by default use a custom prompt template because it turns out that making SFT data look more like pretraining data in its structure helps models use their pretraining skills during chat settings. This includes factual recall.&lt;/li&gt; &lt;li&gt;Augmentoolkit was used to create the dataset generation model that runs Augmentoolkit's pipelines. You can find the config used to make the dataset (2.5 gigabytes) in the &lt;code&gt;generation/core_composition/meta_datagen&lt;/code&gt; folder.&lt;/li&gt; &lt;li&gt;There's a pipeline for turning normal SFT data into reasoning SFT data that can give a good cold start to models that you want to give thought processes to. A number of datasets converted using this pipeline &lt;a href="https://huggingface.co/Augmentoolkit"&gt;are available on Hugging Face&lt;/a&gt;, fully open-source.&lt;/li&gt; &lt;li&gt;Augmentoolkit does not just automatically train models on the domain-specific data you generate: to ensure that there is enough data made for the model to 1) generalize and 2) learn the actual capability of conversation, Augmentoolkit will balance your domain-specific data with generic conversational data, ensuring that the LLM becomes smarter while retaining all of the question-answering capabilities imparted by the facts it is being trained on.&lt;/li&gt; &lt;li&gt;If you just want to make data and don't want to automatically train models, there's a config file option for that of course.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why do all this + Vision&lt;/h1&gt; &lt;p&gt;I believe AI alignment is solved when individuals and orgs can make their AI act as they want it to, rather than having to settle for a one-size-fits-all solution. The moment people can use AI specialized to their domains, is also the moment when AI stops being slightly wrong at everything, and starts being incredibly useful across different fields. Furthermore, we must do everything we can to avoid a specific type of AI-powered future: the AI-powered future where what AI believes and is capable of doing is entirely controlled by a select few. Open source has to survive and thrive for this technology to be used right. As many people as possible must be able to control AI.&lt;/p&gt; &lt;p&gt;I want to stop a slop-pocalypse. I want to stop a future of extortionate rent-collecting by the established labs. I want open-source finetuning, even by individuals, to thrive. I want people to be able to be artists, with data their paintbrush and AI weights their canvas.&lt;/p&gt; &lt;p&gt;Teaching models facts was the first step, and I believe this first step has now been taken. It was probably one of the hardest; best to get it out of the way sooner. After this, I'm going to be making coding expert models for specific languages, and I will also improve the &lt;a href="https://github.com/e-p-armstrong/augmentoolkit/blob/master/docs/grpo.md"&gt;GRPO pipeline&lt;/a&gt;, which allows for models to be trained to do &lt;em&gt;literally anything&lt;/em&gt; better. I encourage you to fork the project so that you can make your own data, so that you can create your own pipelines, and so that you can keep the spirit of open-source finetuning and experimentation alive. I also encourage you to star the project, because I like it when &amp;quot;number go up&amp;quot;.&lt;/p&gt; &lt;p&gt;Huge thanks to Austin Cook and all of Alignment Lab AI for helping me with ideas and with getting this out there. Look out for some cool stuff from them soon, by the way :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/e-p-armstrong/augmentoolkit"&gt;Happy hacking!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heralax_Tekran"&gt; /u/Heralax_Tekran &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lersrw/augmentoolkit_30_7_months_of_work_mit_license/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lersrw/augmentoolkit_30_7_months_of_work_mit_license/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lersrw/augmentoolkit_30_7_months_of_work_mit_license/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T20:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1leyzxp</id>
    <title>Private AI Voice Assistant + Open-Source Speaker Powered by Llama &amp; Jetson!</title>
    <updated>2025-06-19T01:59:17+00:00</updated>
    <author>
      <name>/u/FutureProofHomes</name>
      <uri>https://old.reddit.com/user/FutureProofHomes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyzxp/private_ai_voice_assistant_opensource_speaker/"&gt; &lt;img alt="Private AI Voice Assistant + Open-Source Speaker Powered by Llama &amp;amp; Jetson!" src="https://external-preview.redd.it/1cKeuQGVkwjz1OX7NjtAv9GHzhusji4vD5LLPS4kBVk.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47f09968e3b0f627cdcdb3a1244eedbba09400f1" title="Private AI Voice Assistant + Open-Source Speaker Powered by Llama &amp;amp; Jetson!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;br /&gt; We built a &lt;strong&gt;100% private, AI-powered voice assistant&lt;/strong&gt; for your smart home — runs locally on &lt;strong&gt;Jetson&lt;/strong&gt;, uses &lt;strong&gt;Llama models&lt;/strong&gt;, connects to our &lt;strong&gt;open-source Sonos-like speaker&lt;/strong&gt;, and integrates with &lt;strong&gt;Home Assistant&lt;/strong&gt; to control basically &lt;em&gt;everything&lt;/em&gt;. No cloud. Just fast, private, real-time control.&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Wassup Llama friends!&lt;/p&gt; &lt;p&gt;I started a YouTube channel showing how to build a private/local voice assistant (think Alexa, but off-grid). It kinda/sorta blew up… and that led to a full-blown hardware startup.&lt;/p&gt; &lt;p&gt;We built a &lt;strong&gt;local LLM server and conversational voice pipeline&lt;/strong&gt; on Jetson hardware, then connected it wirelessly to our &lt;strong&gt;open-source smart speaker&lt;/strong&gt; (like a DIY Sonos One). Then we layered in robust &lt;strong&gt;tool-calling support to integrate with Home Assistant&lt;/strong&gt;, unlocking full control over your smart home — lights, sensors, thermostats, you name it.&lt;/p&gt; &lt;p&gt;End result? A &lt;strong&gt;100% private, local voice assistant&lt;/strong&gt; for the smart home. No cloud. No spying. Just you, your home, and a talking box that &lt;em&gt;actually respects your privacy&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;We’re call ourselves &lt;strong&gt;FutureProofHomes&lt;/strong&gt;, and we’d love a little LocalLLaMA love to help spread the word.&lt;/p&gt; &lt;p&gt;Check us out @ &lt;a href="https://FutureProofHomes.ai"&gt;FutureProofHomes.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers, everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FutureProofHomes"&gt; /u/FutureProofHomes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/WrreIi8LCiw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1leyzxp/private_ai_voice_assistant_opensource_speaker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1leyzxp/private_ai_voice_assistant_opensource_speaker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-19T01:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lewhla</id>
    <title>We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!</title>
    <updated>2025-06-18T23:55:55+00:00</updated>
    <author>
      <name>/u/Nice-Comfortable-650</name>
      <uri>https://old.reddit.com/user/Nice-Comfortable-650</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lewhla/we_built_this_project_to_increase_llm_throughput/"&gt; &lt;img alt="We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!" src="https://preview.redd.it/775o8e8hxr7f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c12230c686bdb16949fed6cf8cf00afff6399ea3" title="We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, our team has built this open source project, LMCache, to reduce repetitive computation in LLM inference and make systems serve more people (3x more throughput in chat applications) and it has been used in IBM's open source LLM inference stack.&lt;/p&gt; &lt;p&gt;In LLM serving, the input is computed into intermediate states called KV cache to further provide answers. These data are relatively large (~1-2GB for long context) and are often evicted when GPU memory is not enough. In these cases, when users ask a follow up question, the software needs to recompute for the same KV Cache. LMCache is designed to combat that by efficiently offloading and loading these KV cache to and from DRAM and disk. This is particularly helpful in multi-round QA settings when context reuse is important but GPU memory is not enough.&lt;/p&gt; &lt;p&gt;Ask us anything!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/LMCache/LMCache"&gt;https://github.com/LMCache/LMCache&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nice-Comfortable-650"&gt; /u/Nice-Comfortable-650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/775o8e8hxr7f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lewhla/we_built_this_project_to_increase_llm_throughput/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lewhla/we_built_this_project_to_increase_llm_throughput/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T23:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lei5mb</id>
    <title>Oops</title>
    <updated>2025-06-18T14:12:39+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"&gt; &lt;img alt="Oops" src="https://preview.redd.it/iv35yrek1p7f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a1be0e37ffab5a4926e5a5a7a869b2ee3a9c853" title="Oops" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iv35yrek1p7f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lei5mb/oops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-18T14:12:39+00:00</published>
  </entry>
</feed>
