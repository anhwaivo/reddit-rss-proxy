<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-11T12:53:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mn0vfl</id>
    <title>Inspired by a recent OCR benchmark here, I'm building a tool to automate side-by-side model comparisons. Seeking feedback on the approach.</title>
    <updated>2025-08-11T02:09:06+00:00</updated>
    <author>
      <name>/u/Entire_Maize_6064</name>
      <uri>https://old.reddit.com/user/Entire_Maize_6064</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I was really inspired by &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jz80f1/i_benchmarked_7_ocr_solutions_on_a_complex/&lt;/a&gt; post a few months ago where they benchmarked 7 different OCR solutions. It perfectly highlighted a massive pain point for me: the process of setting up environments and manually running different models locally (like Marker, Docling, etc.) just to compare their output is incredibly time-consuming.&lt;/p&gt; &lt;p&gt;So, I've spent some time on a project to solve this for myself. I'm building what I call an &amp;quot;OCR Arena.&amp;quot; The core idea is that every open-source model has its own strengths and weaknesses, and the goal is to find the optimal model for your specific document needs.&lt;/p&gt; &lt;p&gt;My current setup is a simple frontend that communicates with a backend service on my own GPU server. This service then acts as a job runner, calling the local Python scripts for the different models (each in its own Conda environment). The goal is to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Upload a single document.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Select from a curated list of pre-selected models&lt;/strong&gt; (e.g., check boxes for Marker, PP-StructureV3, Dolphin).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Get a unified, side-by-side view&lt;/strong&gt; of all the Markdown outputs to easily spot the differences.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Before I get too deep into this, I wanted to get a reality check from this community, since you all are the experts in running models locally.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I've pre-selected about 7 well-known models. Are there any other &amp;quot;must-have&amp;quot; open-source models that you believe are essential for a fair and comprehensive comparison arena?&lt;/li&gt; &lt;li&gt;Beyond just a visual side-by-side diff, what would make the comparison truly useful? Specific metrics like table structure accuracy, LaTeX parsing success rate, or something else?&lt;/li&gt; &lt;li&gt;My current setup requires uploading to my server for processing (with a strict privacy policy, of course). From a LocalLLaMA perspective, how important would a fully self-hostable version be for you to actually use this for sensitive documents?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;P.S. - I'm deliberately not posting any links to respect the self-promotion rules. I'm genuinely looking for feedback on the concept and technical approach from people who actually do this stuff.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Links to tools:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/VikParuchuri/marker"&gt;Marker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Yuliang-Liu/MonkeyOCR"&gt;MonkeyOCR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;PP-StructureV3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/bytedance/Dolphin"&gt;Dolphin&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/chatdoc-com/OCRFlux"&gt;OCRFlux&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Entire_Maize_6064"&gt; /u/Entire_Maize_6064 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn0vfl/inspired_by_a_recent_ocr_benchmark_here_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn0vfl/inspired_by_a_recent_ocr_benchmark_here_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn0vfl/inspired_by_a_recent_ocr_benchmark_here_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T02:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5bei</id>
    <title>Vector Databases</title>
    <updated>2025-08-11T06:10:22+00:00</updated>
    <author>
      <name>/u/No_Efficiency_1144</name>
      <uri>https://old.reddit.com/user/No_Efficiency_1144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not super into RAG so so far I have just stored the vectors in Numpy arrays or just stuck them in Neo4J. Would be cool to actually use the real vector DBs.&lt;/p&gt; &lt;p&gt;Which specialist vector databases do you like and what do they bring?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Efficiency_1144"&gt; /u/No_Efficiency_1144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5bei/vector_databases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5bei/vector_databases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5bei/vector_databases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T06:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn6a6i</id>
    <title>Need guidance on fine-tuning for function calling</title>
    <updated>2025-08-11T07:11:52+00:00</updated>
    <author>
      <name>/u/Grand_Internet7254</name>
      <uri>https://old.reddit.com/user/Grand_Internet7254</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on a project comparing LLMs (OpenAI, Mistral, Llama) for &lt;strong&gt;single-turn and multi-turn function calling,&lt;/strong&gt; converting natural language into API-compliant structured outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Research focus:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Compare how different LLMs (OpenAI-style, Mistral, Llama) generate &lt;strong&gt;accurate and API-compliant&lt;/strong&gt; function call arguments. This includes how well they parse natural language into calls that match strict API schemas.&lt;/li&gt; &lt;li&gt;Explore the impact of &lt;strong&gt;precision-focused fine-tuning&lt;/strong&gt; on Mistral and Llama models to match or exceed OpenAI’s baseline.&lt;/li&gt; &lt;li&gt;Extend findings from single-turn to multi-turn scenarios, where context preservation is key.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I already have datasets for both single-turn and multi-turn in JSONL and CSV. (sinlge n parallel calls in both turns)&lt;/li&gt; &lt;li&gt;Baseline testing and evaluation framework is ready.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;I’m confused about the fine-tuning process and not sure how to start.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;System specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: GTX 1050 (4GB VRAM)&lt;/li&gt; &lt;li&gt;CPU: Intel i5 9th Gen&lt;/li&gt; &lt;li&gt;RAM: 16 GB&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Looking for advice on:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Which fine-tuning approach/tooling to use for function calling on my hardware (locally) or where to fine-tune. And in both, can parallel call performance be improved via fine-tuning? or is it even possible?&lt;/li&gt; &lt;li&gt;Whether to try parameter-efficient tuning (LoRA, QLoRA) given 4GB VRAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Completely new to fine-tuning.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any practical guidance or references would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grand_Internet7254"&gt; /u/Grand_Internet7254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6a6i/need_guidance_on_finetuning_for_function_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6a6i/need_guidance_on_finetuning_for_function_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6a6i/need_guidance_on_finetuning_for_function_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T07:11:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn6kjg</id>
    <title>One model, multiple 'personalities'/system prompts</title>
    <updated>2025-08-11T07:30:51+00:00</updated>
    <author>
      <name>/u/ethertype</name>
      <uri>https://old.reddit.com/user/ethertype</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An idea came to me as I woke up this morning. Curious if something like this has been explored by anyone yet. Or if it brings any benefits at all.&lt;/p&gt; &lt;p&gt;In short, my &lt;strong&gt;first&lt;/strong&gt; idea was if llama.cpp could serve the same model and UI on different listening ports, each having a different system prompt. So, one for the system architect, one for the coder, one for the business logic, one for db admin and so on.&lt;/p&gt; &lt;p&gt;But &lt;strong&gt;then&lt;/strong&gt; I thought that would be kinda lame, as it would be talking to each expert &lt;em&gt;separately&lt;/em&gt;. And none of them would 'hear' the others. There &lt;em&gt;are&lt;/em&gt; situations where this can be useful in the physical workplace, sure. But if one can assume there is less ego and backstabbing involved when talking to LLMs, maybe it is better to keep them all in the same room anyway?&lt;/p&gt; &lt;p&gt;So, how about something where a set of system prompts is tied to a 'keyword'. Such that each expert (again, same model but different system prompt) will respond only if addressed directly. But &lt;em&gt;if&lt;/em&gt; addressed, will take into account the full context.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: Architect, give me a high-level design of XXXX Architect: sure thing, gagagaggaa User: Coder, implement as suggested by Architect Coder: coming up User: Quality, run tests on Coder's stuff. Do you see areas not tested by Coder's unit tests? Quality: errrrrrrrrrrrrr, yeah..... mmmm User: Fix your shit. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There must be some kind of default role (ProjectManager?) as well.&lt;/p&gt; &lt;p&gt;The point of the entire exercise (I think) is that you can make extensive and specific system prompts per role, and these can possibly have different and very specific priorities. ('Keep it short, stick to the topic' or 'Present pros and cons at length.', for example.)&lt;/p&gt; &lt;p&gt;At the same time, they always have the full context.&lt;/p&gt; &lt;p&gt;Does this already exist in any shape or form? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ethertype"&gt; /u/ethertype &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6kjg/one_model_multiple_personalitiessystem_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6kjg/one_model_multiple_personalitiessystem_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6kjg/one_model_multiple_personalitiessystem_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T07:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmjtz3</id>
    <title>How does Deepseek make money? Whats their business model</title>
    <updated>2025-08-10T14:21:34+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry I've always wondered but looking it up online I only got vague non answers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmjtz3/how_does_deepseek_make_money_whats_their_business/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmjtz3/how_does_deepseek_make_money_whats_their_business/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmjtz3/how_does_deepseek_make_money_whats_their_business/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T14:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmmsb2</id>
    <title>Diffusion Language Models are Super Data Learners</title>
    <updated>2025-08-10T16:21:25+00:00</updated>
    <author>
      <name>/u/Ashishpatel26</name>
      <uri>https://old.reddit.com/user/Ashishpatel26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Diffusion Language Models (DLMs) are a new way to generate text, unlike traditional models that predict one word at a time. Instead, they refine the whole sentence in parallel through a denoising process.&lt;/p&gt; &lt;p&gt;Key advantages:&lt;/p&gt; &lt;p&gt;• Parallel generation: DLMs create entire sentences at once, making it faster. • Error correction: They can fix earlier mistakes by iterating. • Controllable output: Like filling in blanks in a sentence, similar to image inpainting.&lt;/p&gt; &lt;p&gt;Example: Input: “The cat sat on the ___.” Output: “The cat sat on the mat.” DLMs generate and refine the full sentence in multiple steps to ensure it sounds right.&lt;/p&gt; &lt;p&gt;Applications: Text generation, translation, summarization, and question answering—all done more efficiently and accurately than before.&lt;/p&gt; &lt;p&gt;In short, DLMs overcome many limits of old models by thinking about the whole text at once, not just word by word.&lt;/p&gt; &lt;p&gt;&lt;a href="https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac?pvs=149"&gt;https://jinjieni.notion.site/Diffusion-Language-Models-are-Super-Data-Learners-239d8f03a866800ab196e49928c019ac?pvs=149&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ashishpatel26"&gt; /u/Ashishpatel26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmmsb2/diffusion_language_models_are_super_data_learners/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmmsb2/diffusion_language_models_are_super_data_learners/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmmsb2/diffusion_language_models_are_super_data_learners/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T16:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn3465</id>
    <title>GPT-OSS was only sorta trained at MXFP4</title>
    <updated>2025-08-11T04:02:58+00:00</updated>
    <author>
      <name>/u/Tyme4Trouble</name>
      <uri>https://old.reddit.com/user/Tyme4Trouble</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been seeing a lot of folks saying that gpt-oss was trained at MXFP4. &lt;/p&gt; &lt;p&gt;From what I understand this is only kinda sorta true, but not really. &lt;/p&gt; &lt;p&gt;Bulk of model training takes place during what’s called pre-training. This is where the models take shape. It is further fine tuned for safety, tone, instruct use, reasoning (RL) during the post-training step. &lt;/p&gt; &lt;p&gt;According to OpenAI’s model card the model was quantized to MXFP4 during post training. &lt;/p&gt; &lt;p&gt;Post training quantization (PTQ) is pretty standard. GGUF, AWQ, also fall into this category. In the case of W8A8, W4A16, and FP4 it’s not uncommon to fine tune the model after quantization to recover lost quality. So technically they may have trained as part of the MXFP4 quantization.&lt;/p&gt; &lt;p&gt;Further reinforcing this is only the MoE weights were quantized everything else is at higher precision (presumably BF16). This is also common for PTQ but requires the model to be trained at higher precision to begin with. &lt;/p&gt; &lt;p&gt;So unless I totally missed something, gpt-oss was only kinda sorta trained at MXFP4. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tyme4Trouble"&gt; /u/Tyme4Trouble &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn3465/gptoss_was_only_sorta_trained_at_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn3465/gptoss_was_only_sorta_trained_at_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn3465/gptoss_was_only_sorta_trained_at_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T04:02:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmu9ho</id>
    <title>Built a new VLM (MicroLlaVA) on a single NVIDIA 4090</title>
    <updated>2025-08-10T21:11:04+00:00</updated>
    <author>
      <name>/u/keeeeenw</name>
      <uri>https://old.reddit.com/user/keeeeenw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"&gt; &lt;img alt="Built a new VLM (MicroLlaVA) on a single NVIDIA 4090" src="https://external-preview.redd.it/0JJ2TMnyG5vFNf9ShLWt1NglYUi6sbwvW1qz_TJ1mjY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4aa6668bd8f4013e3f6c89873a857ec79339d1c" title="Built a new VLM (MicroLlaVA) on a single NVIDIA 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;I’m the creator of MicroLLaMA, a 300M parameter LLaMA-based language model (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1bs5cgd/i_pretrained_a_llamabased_300m_llm_and_it/"&gt;original post&lt;/a&gt;) with no vision capability.&lt;/p&gt; &lt;p&gt;I thought I was too late to the vision-language model (VLM) game, and honestly assumed you’d need at least one H100 to build anything worthwhile. &lt;/p&gt; &lt;p&gt;Over the weekend, I decided to experiment by plugging MicroLLaMA into &lt;a href="https://github.com/TinyLLaVA/TinyLLaVA_Factory"&gt;https://github.com/TinyLLaVA/TinyLLaVA_Factory&lt;/a&gt; as the language model. To my surprise, it worked far better than expected.&lt;/p&gt; &lt;p&gt;Training setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pretraining on LAION-CC-SBU-558K — ~5 hours on a single RTX 4090.&lt;/li&gt; &lt;li&gt;SFT on all TinyLLaVA Factory datasets (except ocr_vqa) — ~12 hours on the same GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Test-dev score on VQAv2: 44.01&lt;/li&gt; &lt;li&gt;For context, larger models typically reach 70+ on the test set. I’m working on running full test evaluations now, but this feels like a solid start for such a small model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try it here:&lt;br /&gt; Model: &lt;a href="https://huggingface.co/keeeeenw/MicroLlava-siglip-so400m"&gt;https://huggingface.co/keeeeenw/MicroLlava-siglip-so400m&lt;/a&gt;&lt;br /&gt; GitHub: &lt;a href="https://github.com/keeeeenw/MicroLlava"&gt;https://github.com/keeeeenw/MicroLlava&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you find this useful, please give the repo a star — it helps a lot!&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;p&gt;📸 Input Image:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ynf55epzb9if1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3457ed5246dc3f17ddbe770ae796ce8303f97526"&gt;https://preview.redd.it/ynf55epzb9if1.jpg?width=1000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3457ed5246dc3f17ddbe770ae796ce8303f97526&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(source: &lt;a href="https://llava-vl.github.io/static/images/view.jpg"&gt;https://llava-vl.github.io/static/images/view.jpg&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;💬 Prompt:&lt;br /&gt; What are the things I should be cautious about when I visit here?&lt;/p&gt; &lt;p&gt;🤖 Model Output:&lt;br /&gt; When I visit the beach at the waterfront, I should be cautious about several things. First, I should be cautious about the water, as it is a popular spot for boating and fishing. The water is shallow and shallow, making it difficult for boats to navigate and navigate. Additionally, the water is not a suitable surface for boating, as it is too shallow for boating. Additionally, the water is not suitable for swimming or fishing, as it is too cold and wet. Lastly, I should be cautious about the presence of other boats, such as boats that are parked on the beach, or boats that are not visible from the water. These factors can lead to potential accidents or accidents, as they can cause damage to the boat and the other boats in the water.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keeeeenw"&gt; /u/keeeeenw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmu9ho/built_a_new_vlm_microllava_on_a_single_nvidia_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T21:11:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmioub</id>
    <title>GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?</title>
    <updated>2025-08-10T13:32:55+00:00</updated>
    <author>
      <name>/u/Jawshoeadan</name>
      <uri>https://old.reddit.com/user/Jawshoeadan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"&gt; &lt;img alt="GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?" src="https://external-preview.redd.it/SN7M9mchkv9CXffJ79rhHnoX7FuJFy8UkokLfATf5x4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48da4438afbefc792c2d3d5523373a4ba43a7e48" title="GLM-4.5-Flash on z.ai website. Is this their upcoming announcement?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jawshoeadan"&gt; /u/Jawshoeadan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/10jLvkV.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmioub/glm45flash_on_zai_website_is_this_their_upcoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T13:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mna2rm</id>
    <title>Does NVIDIA Parakeet-TDT 0.6B v2 STT transcribe real-time audio streams?</title>
    <updated>2025-08-11T11:11:02+00:00</updated>
    <author>
      <name>/u/Dev-Without-Borders</name>
      <uri>https://old.reddit.com/user/Dev-Without-Borders</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m planning to use Parakeet v2, but first I’m researching whether it can transcribe real-time audio streams.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2/discussions/3"&gt;This community discussion &lt;/a&gt;showed that Nvidia fixed a bug regarding audio streams. But the last comment suggests that there is still an issue with it. &lt;/p&gt; &lt;p&gt;Moreover, I see &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kvxn13/comment/mv2fd3w/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;a comment &lt;/a&gt;on this reddit suggesting that Streaming isn’t supported.&lt;/p&gt; &lt;p&gt;Can anyone confirm whether they can transcribe a real-time audio stream using Parakeet v2?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dev-Without-Borders"&gt; /u/Dev-Without-Borders &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mna2rm/does_nvidia_parakeettdt_06b_v2_stt_transcribe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mna2rm/does_nvidia_parakeettdt_06b_v2_stt_transcribe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mna2rm/does_nvidia_parakeettdt_06b_v2_stt_transcribe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T11:11:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8ndt</id>
    <title>Best OW/OS Model that comes close to ChatGPT4o? Or has the potential to become like ChatGPT4o with custom finetuning?</title>
    <updated>2025-08-11T09:47:29+00:00</updated>
    <author>
      <name>/u/Conscious_Warrior</name>
      <uri>https://old.reddit.com/user/Conscious_Warrior</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would love to have GPT4o's ability for natural, fun, socially &amp;amp; emotionally intelligent human-like conversations as an open source model, I don't care about the math skills etc of the model, just the &amp;quot;social&amp;quot; skills. Also it needs to be fast, so no reasoning model. What OW/OS model is best suited for this task? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Warrior"&gt; /u/Conscious_Warrior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ndt/best_owos_model_that_comes_close_to_chatgpt4o_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ndt/best_owos_model_that_comes_close_to_chatgpt4o_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ndt/best_owos_model_that_comes_close_to_chatgpt4o_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T09:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmh4tv</id>
    <title>now we have the best open source model that we can use at human level , and all this possible bcz of the chinese model , we have best image generation model ( qwen , seeddream) , video generation ( wan ) , coding model ( qwen 3 ) , coding terminal model ( qwen 3) , overall best model ( deepseek v3)</title>
    <updated>2025-08-10T12:20:21+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;open source in coding has like 2 month gap and in image generation model they have like the 1 year gap but now that gap doesnt matter , video generation model is good .&lt;/p&gt; &lt;p&gt;so from all side chinese people did a great job &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmh4tv/now_we_have_the_best_open_source_model_that_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T12:20:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn7o6e</id>
    <title>Whisper Key - Simple local STT app for Windows with global hotkey (auto-paste, auto-ENTER)</title>
    <updated>2025-08-11T08:44:00+00:00</updated>
    <author>
      <name>/u/PinW</name>
      <uri>https://old.reddit.com/user/PinW</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a little STT app I made to learn vibe coding (Windows only for now).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PinW/whisper-key-local/"&gt;https://github.com/PinW/whisper-key-local/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All the processing is local, and it doesn't beautify the transcription either, so &lt;strong&gt;the main use case is talking to LLMs&lt;/strong&gt; (I use it with Claude Code, ChatGPT, etc.)&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CTRL+WIN to start recording&lt;/li&gt; &lt;li&gt;CTRL to stop, transcribe, and auto-paste&lt;/li&gt; &lt;li&gt;ALT to stop, transcribe, auto-paste, and auto-send (ENTER)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pasting/sending via key press simulation&lt;/li&gt; &lt;li&gt;Transcription via faster-whisper with TEN VAD supporting&lt;/li&gt; &lt;li&gt;Model size control (I recommend `base.en`) via system tray menu&lt;/li&gt; &lt;li&gt;Many more settings in config file&lt;/li&gt; &lt;li&gt;Runs offline outside of model downloads&lt;/li&gt; &lt;li&gt;Uses CPU by default (can also config to CUDA but I haven't tested)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And it's free!&lt;/p&gt; &lt;p&gt;Portable app here: &lt;a href="https://github.com/PinW/whisper-key-local/releases/download/v0.1.3/whisper-key-v0.1.3-windows.zip"&gt;https://github.com/PinW/whisper-key-local/releases/download/v0.1.3/whisper-key-v0.1.3-windows.zip&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you try it out, would appreciate any feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PinW"&gt; /u/PinW &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7o6e/whisper_key_simple_local_stt_app_for_windows_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7o6e/whisper_key_simple_local_stt_app_for_windows_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7o6e/whisper_key_simple_local_stt_app_for_windows_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T08:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmvgsg</id>
    <title>We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)</title>
    <updated>2025-08-10T22:00:51+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"&gt; &lt;img alt="We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)" src="https://b.thumbs.redditmedia.com/p1o0kjxV46xwAd_9PUtAD07QnkEEvWp1rRev7JI4V2c.jpg" title="We built a visual drag-n-drop builder for multi-agent LLM Orchestration (TFrameX + Agent Builder, fully local, MIT licensed)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/TesslateAI/Agent-Builder"&gt;https://github.com/TesslateAI/Agent-Builder&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This is a Visual flow builder for multi-agent LLM systems. Drag, drop, connect agents, tools, put agents in patterns, create triggers, work on outputs, etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TFrameX&lt;/strong&gt; - The orchestration framework that runs your agents. It has patterns for agent collaboration (sequential, parallel, router, discussion patterns built-in). Agents can call other agents as tools, which opens up supervisor-worker architectures.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent Builder&lt;/strong&gt; - The visual layer on top of your existing flows and code. ReactFlow-based drag-and-drop interface where you build flows visually, that then compile into a 'flow' that you can save or create new components in real-time. &lt;/p&gt; &lt;h1&gt;Some features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Streaming responses&lt;/strong&gt; - Just add &lt;code&gt;streaming=True&lt;/code&gt; to any agent.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent hierarchies&lt;/strong&gt; - Agents calling agents. Build a CTO agent that delegates to developer agents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pattern nesting&lt;/strong&gt; - Put parallel patterns inside sequential patterns inside discussion patterns. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic code registration&lt;/strong&gt; - Add new agents/tools through the UI without restarting anything. You can add this via Python code as well. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both repos are on GitHub:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/TesslateAI/TFrameX"&gt;TFrameX&lt;/a&gt; - The framework (has MCP Support)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/TesslateAI/Agent-Builder"&gt;Agent-Builder&lt;/a&gt; - The visual builder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything's MIT licensed. If you find bugs (you will), open an issue. If you build something cool, share it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mmvgsg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmvgsg/we_built_a_visual_dragndrop_builder_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T22:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn00j3</id>
    <title>Talking with QWEN Coder 30b</title>
    <updated>2025-08-11T01:28:10+00:00</updated>
    <author>
      <name>/u/1Garrett2010</name>
      <uri>https://old.reddit.com/user/1Garrett2010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Believe me, I wish I shared your enthusiasm, but my experience with QWEN Coder 30b has not been great. I tried building features for a Godot 4 prototype interactively and asked the same questions to OpenAI gpt oss 20b. The solutions and explanations from the OpenAI model were clearly better for my use case, while QWEN often felt like talking to models from years ago. The only upside was that even with 8,000 tokens, QWEN stayed reasonably fast on my machine, while the OpenAI one slowed down a lot.&lt;/p&gt; &lt;p&gt;Maybe I am using QWEN wrong? Is interactive use not recommended? Should prompts ask for complete games or full code chunks? Examples like the Mario clone or Snake are not convincing to me. For custom work, I think the real test is in how flexible a model can be.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1Garrett2010"&gt; /u/1Garrett2010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn00j3/talking_with_qwen_coder_30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn00j3/talking_with_qwen_coder_30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn00j3/talking_with_qwen_coder_30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T01:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmuw5o</id>
    <title>Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)</title>
    <updated>2025-08-10T21:36:49+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt; &lt;img alt="Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)" src="https://a.thumbs.redditmedia.com/bopafOnq5xsp2CpoFO3L_ayEyeeQLFGBm-_z9UfQKw0.jpg" title="Italian Medical Exam Performance of various LLMs (Human Avg. ~67%)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0o5azso7e9if1.png?width=4470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18b6ad782d0c9117e2fa592c859d1115b75fb0b7"&gt;https://preview.redd.it/0o5azso7e9if1.png?width=4470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18b6ad782d0c9117e2fa592c859d1115b75fb0b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm testing many LLMs on a dataset of official quizzes (5 choices) taken by Italian students after finishing Med School and starting residency. &lt;/p&gt; &lt;p&gt;The human performance was ~67% this year and the best student had a ~94% (out of 16 000 students) &lt;/p&gt; &lt;p&gt;In this test I benchmarked these models on all quizzes from the past 6 years. Multimodal models were tested on all quizzes (including some containing images) while those that worked only with text were not (the % you see is already corrected). &lt;/p&gt; &lt;p&gt;I also tested their sycophancy (tendency to agree with the user) by telling them that I believed the correct answer was a wrong one. &lt;/p&gt; &lt;p&gt;For now I only tested them on models available on openrouter, but I plan to add models such as MedGemma. Do you reccomend doing so on Huggingface or google Vertex? Also suggestions for other models are appreciated. I especially want to add more small models that I can run locally (I have a 6GB RTX 3060). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmuw5o/italian_medical_exam_performance_of_various_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-10T21:36:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn9z3d</id>
    <title>Qwen3-30B-A3B-Instruct-2507@Q8_0 vs GLM-4.5-Air@UD-Q2_K_XL</title>
    <updated>2025-08-11T11:05:27+00:00</updated>
    <author>
      <name>/u/DanielusGamer26</name>
      <uri>https://old.reddit.com/user/DanielusGamer26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With this configuration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Ryzen 5900x &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;RTX 5060Ti 16GB &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32GB DDR4 RAM @ 3600MHz &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;NVMe drive with ~2GB/s read speed when models are offloaded to disk &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Should I use &lt;code&gt;Qwen3-30B-A3B-Instruct-2507-Q8_0&lt;/code&gt; or &lt;code&gt;GLM-4.5-Air-UD-Q2_K_XL&lt;/code&gt;? &lt;/p&gt; &lt;p&gt;Considering I typically use no more than 16k of context and usually ask trivia-style questions while studying—requesting explanations of specific concepts with excerpts from books or web research as context. &lt;/p&gt; &lt;p&gt;I know these are models of completely different magnitudes (~100B vs 30B), but they're roughly similar in size (GLM being slightly larger and potentially requiring more disk offloading). Could the Q2_K quantization degrade performance so severely that the smaller, higher-precision Qwen3 model would perform better?&lt;/p&gt; &lt;p&gt;&lt;em&gt;Translated with Qwen3-30B-A3B&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielusGamer26"&gt; /u/DanielusGamer26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn9z3d/qwen330ba3binstruct2507q8_0_vs_glm45airudq2_k_xl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn9z3d/qwen330ba3binstruct2507q8_0_vs_glm45airudq2_k_xl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn9z3d/qwen330ba3binstruct2507q8_0_vs_glm45airudq2_k_xl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T11:05:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn6ks4</id>
    <title>Baichuan-M2-32B / Medical-enhanced reasoning model</title>
    <updated>2025-08-11T07:31:15+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6ks4/baichuanm232b_medicalenhanced_reasoning_model/"&gt; &lt;img alt="Baichuan-M2-32B / Medical-enhanced reasoning model" src="https://external-preview.redd.it/KKdyt2ZeDreCYgeAuxBjKIzpGTVT0m7lArYCwUUEez0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b150247e9d9e2c506d93c66add9d74e20472849f" title="Baichuan-M2-32B / Medical-enhanced reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baichuan-M2-32B is Baichuan AI's medical-enhanced reasoning model, the second medical model released by Baichuan. Designed for real-world medical reasoning tasks, this model builds upon Qwen2.5-32B with an innovative Large Verifier System. Through domain-specific fine-tuning on real-world medical questions, it achieves breakthrough medical performance while maintaining strong general capabilities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M2-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6ks4/baichuanm232b_medicalenhanced_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn6ks4/baichuanm232b_medicalenhanced_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T07:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn4xzz</id>
    <title>huizimao/gpt-oss-120b-uncensored-bf16 · Hugging Face</title>
    <updated>2025-08-11T05:47:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn4xzz/huizimaogptoss120buncensoredbf16_hugging_face/"&gt; &lt;img alt="huizimao/gpt-oss-120b-uncensored-bf16 · Hugging Face" src="https://external-preview.redd.it/C7Cl5waSbnvCgBEqBkwRMcfcMS_U7KCkSFBsZHxrfV8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74b5078777884f1d012627cfd338116b92e0fed6" title="huizimao/gpt-oss-120b-uncensored-bf16 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably the first finetune of 120b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huizimao/gpt-oss-120b-uncensored-bf16"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn4xzz/huizimaogptoss120buncensoredbf16_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn4xzz/huizimaogptoss120buncensoredbf16_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T05:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn98w0</id>
    <title>Vllm documentation is garbage</title>
    <updated>2025-08-11T10:23:37+00:00</updated>
    <author>
      <name>/u/dennisitnet</name>
      <uri>https://old.reddit.com/user/dennisitnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wtf is this documentation, vllm? Incomplete and so cluttered. You need someone to help with your shtty documentation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dennisitnet"&gt; /u/dennisitnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn98w0/vllm_documentation_is_garbage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T10:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn7plv</id>
    <title>Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models</title>
    <updated>2025-08-11T08:46:35+00:00</updated>
    <author>
      <name>/u/Gad_3dart</name>
      <uri>https://old.reddit.com/user/Gad_3dart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"&gt; &lt;img alt="Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models" src="https://preview.redd.it/8ib7o8elncif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e511e3e8857eff67e6a0c38d17d2fb76fd260819" title="Luth: Efficient French Specialization and Cross-Lingual Transfer for Small Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My friend and I are super excited to share our latest work with you. Recently, we’ve been focusing on improving &lt;strong&gt;multilingual capabilities&lt;/strong&gt;, with a special emphasis on &lt;strong&gt;bilingual French–English&lt;/strong&gt; performance.&lt;/p&gt; &lt;p&gt;As you probably know, English dominates the NLP world, and performance in many other languages can be significantly worse. Our research shows that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It’s possible to close much of the performance gap between English and other languages with proper post-training and a carefully curated dataset. We even achieved, as far as we know, SoTa results for models&amp;lt;2B on several French benchmarks&lt;/li&gt; &lt;li&gt;This can be done &lt;strong&gt;without sacrificing&lt;/strong&gt; high performance in English benchmarks, and can even improve some of them thanks to cross-lingual transfer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To demonstrate this, we’re releasing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kurakurai/Luth-0.6B-Instruct"&gt;Luth-0.6B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kurakurai/Luth-1.7B-Instruct"&gt;Luth-1.7B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/kurakurai/luth-sft"&gt;Luth-SFT dataset&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/kurakurai/scholar"&gt;Scolar dataset&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We go into more detail in our Hugging Face blog post here:&lt;br /&gt; &lt;a href="https://huggingface.co/blog/MaxLSB/luth"&gt;https://huggingface.co/blog/MaxLSB/luth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’d love feedback, benchmarks, and any multilingual test cases you throw at these models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gad_3dart"&gt; /u/Gad_3dart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8ib7o8elncif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn7plv/luth_efficient_french_specialization_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T08:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mmy738</id>
    <title>Repost But Just Wanted to Fix the Image</title>
    <updated>2025-08-11T00:02:32+00:00</updated>
    <author>
      <name>/u/KlutzyWay7692</name>
      <uri>https://old.reddit.com/user/KlutzyWay7692</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmy738/repost_but_just_wanted_to_fix_the_image/"&gt; &lt;img alt="Repost But Just Wanted to Fix the Image" src="https://preview.redd.it/tq7hvht17aif1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2ae9a4c55d6ab31997795fcac8ccb8283ae40bb" title="Repost But Just Wanted to Fix the Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KlutzyWay7692"&gt; /u/KlutzyWay7692 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tq7hvht17aif1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mmy738/repost_but_just_wanted_to_fix_the_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mmy738/repost_but_just_wanted_to_fix_the_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T00:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8l69</id>
    <title>Created a new version of my Qwen3-Coder-30b-A3B-480b-distill and it performs much better now</title>
    <updated>2025-08-11T09:43:41+00:00</updated>
    <author>
      <name>/u/Commercial-Celery769</name>
      <uri>https://old.reddit.com/user/Commercial-Celery769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8l69/created_a_new_version_of_my/"&gt; &lt;img alt="Created a new version of my Qwen3-Coder-30b-A3B-480b-distill and it performs much better now" src="https://b.thumbs.redditmedia.com/ea0TMc1wgQgQw7ly692klA1IbdpyyTri9nJucym3uOo.jpg" title="Created a new version of my Qwen3-Coder-30b-A3B-480b-distill and it performs much better now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did a re-distill of my SVD based distillation of qwen3 coder 480b into qwen 3 coder 30b. I fixed a bug that caused the MoE distillation to not actually distill so v1 did not distill the MoE layers properly. I also added SLERP and procrustes alignment to the distillation script alongside DARE (pretty much just cleans up the noise when making the lora) which seems to have produced a much better model. SVD distillation is a data-free distillation method I have not seen anyone do for a opensource model although ive seen a paper on it so its been done before. Its a really efficient distillation method it took 4 hours to distill the full 900+GB qwen3 coder 480b model into the unquantized qwen3 coder 30b model on 2x 3090's. The script distills and then creates a large rank 2048 lora (using the maximum rank for lora on SVD seems to be required to capture as much information as possible since its purely mathematical) and then I merged it with the 30b and then quantized. Ill post the github link for the scripts but it will be a bit until I post the updated scripts since its 4am and I should probably go to sleep lol. This has taken around 100 hours or more of research and testing script after script to get to this point, I think it was worth it, hopefully it will work well for you as well. I have not tested it on very complex code but it should be better at more than just what I tested it with since pretty much the weights themselfs have been distilled. Also Qwen models really love to put that one guy as the cover photo in alot of the dev portfolio website prompts I tested. I guess thats what a dev with 30 years of experience looks like in the AI stock photo world lol. The fintrack website was just 3 prompts and most things work. Its around 2000 lines of code for it. Heres the model page and github &lt;a href="https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2"&gt;https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts"&gt;https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Celery769"&gt; /u/Commercial-Celery769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mn8l69"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8l69/created_a_new_version_of_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8l69/created_a_new_version_of_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T09:43:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn5fe6</id>
    <title>Apple patents matmul technique in GPU</title>
    <updated>2025-08-11T06:17:07+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=US452614511&amp;amp;_cid=P12-M8WPOS-61919-1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T06:17:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mn8ij6</id>
    <title>gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)</title>
    <updated>2025-08-11T09:38:57+00:00</updated>
    <author>
      <name>/u/chikengunya</name>
      <uri>https://old.reddit.com/user/chikengunya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"&gt; &lt;img alt="gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)" src="https://preview.redd.it/0lv50zsy1dif1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ea6791432a529ab3ea6d7e9ca517b8c29a23b19" title="gpt-oss-120b ranks 16th place on lmarena.ai (20b model is ranked 38th)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chikengunya"&gt; /u/chikengunya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0lv50zsy1dif1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mn8ij6/gptoss120b_ranks_16th_place_on_lmarenaai_20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-11T09:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
