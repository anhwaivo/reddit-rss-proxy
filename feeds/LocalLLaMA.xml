<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-09T14:06:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kicb5l</id>
    <title>please share your experiences with local "deep research"</title>
    <updated>2025-05-09T07:00:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m searching way to use &amp;quot;deep research&amp;quot; with my local LLMs. &lt;/p&gt; &lt;p&gt;I was thinking about AutoGen or CrewAI, but maybe you already have some experiences? Please share your wisdom.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kicb5l/please_share_your_experiences_with_local_deep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kicb5l/please_share_your_experiences_with_local_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kicb5l/please_share_your_experiences_with_local_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T07:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1khql0u</id>
    <title>Intel to launch Arc Pro B60 graphics card with 24GB memory at Computex - VideoCardz.com</title>
    <updated>2025-05-08T13:49:27+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No word on pricing yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-to-launch-arc-pro-b60-graphics-card-with-24gb-memory-at-computex"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khql0u/intel_to_launch_arc_pro_b60_graphics_card_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khql0u/intel_to_launch_arc_pro_b60_graphics_card_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T13:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1khu4x0</id>
    <title>I tested Qwen 3 235b against Deepseek r1, Qwen did better on simple tasks but r1 beats in nuance</title>
    <updated>2025-05-08T16:17:27+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Deepseek r1 for a while, mainly for writing, and I have tried the Qwq 32b, which was plenty impressive. But the new models are a huge upgrade, though I have yet to try the 30b model. The 235b model is really impressive for the cost and size. Definitely much better than Llama 4s. &lt;/p&gt; &lt;p&gt;So, I compared the top 2 open-source models on coding, reasoning, math, and writing tasks.&lt;/p&gt; &lt;p&gt;Here's what I found out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Coding&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For a lot of coding tasks, you wouldn't notice much difference. Both models perform on par, sometimes Qwen taking the lead. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Reasoning and Math&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Deepseek leads here with more nuance in the thought process. Qwen is not bad at all, gets most of the work done, but takes longer to finish tasks. It gives off the vibe of overfit at times.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Writing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For creative writing, Deepseek r1 is still in the top league, right up there with closed models. For summarising and technical description, Qwen offers similar performance.&lt;/p&gt; &lt;p&gt;For a full comparison check out this blog post: &lt;a href="https://composio.dev/blog/qwen-3-vs-deepseek-r1-complete-comparison/"&gt;Qwen 3 vs. Deepseek r1&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;It has been a great year so far for open-weight AI models, especially from Chinese labs. It would be interesting to see the next from Deepseek. Hope the Llama Behemoth turns out to be a better model.&lt;/p&gt; &lt;p&gt;Would love to know your experience with the new Qwens, and would love to know which local Qwen is good for local use cases, I have been using Gemma 3. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khu4x0/i_tested_qwen_3_235b_against_deepseek_r1_qwen_did/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khu4x0/i_tested_qwen_3_235b_against_deepseek_r1_qwen_did/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khu4x0/i_tested_qwen_3_235b_against_deepseek_r1_qwen_did/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T16:17:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kihst0</id>
    <title>Domain adaptation in 2025 - Fine-tuning v.s RAG/GraphRAG</title>
    <updated>2025-05-09T12:55:48+00:00</updated>
    <author>
      <name>/u/Old_Cauliflower6316</name>
      <uri>https://old.reddit.com/user/Old_Cauliflower6316</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on a tool that uses LLMs over the past year. The goal is to help companies troubleshoot production alerts. For example, if an alert says “CPU usage is high!”, the agent tries to investigate it and provide a root cause analysis.&lt;/p&gt; &lt;p&gt;Over that time, I’ve spent a lot of energy thinking about how developers can adapt LLMs to specific domains or systems. In my case, I needed the LLM to understand each customer’s unique environment. I started with basic RAG over company docs, code, and some observability data. But that turned out to be brittle - key pieces of context were often missing or not semantically related to the symptoms in the alert.&lt;/p&gt; &lt;p&gt;So I explored GraphRAG, hoping a more structured representation of the company’s system would help. And while it had potential, it was still brittle, required tons of infrastructure work, and didn’t fully solve the hallucination or retrieval quality issues.&lt;/p&gt; &lt;p&gt;I think the core challenge is that troubleshooting alerts requires deep familiarity with the system -understanding all the entities, their symptoms, limitations, relationships, etc.&lt;/p&gt; &lt;p&gt;Lately, I've been thinking more about fine-tuning - and Rich Sutton’s “Bitter Lesson” (&lt;a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"&gt;link&lt;/a&gt;). Instead of building increasingly complex retrieval pipelines, what if we just trained the model directly with high-quality, synthetic data? We could generate QA pairs about components, their interactions, common failure modes, etc., and let the LLM learn the system more abstractly.&lt;/p&gt; &lt;p&gt;At runtime, rather than retrieving scattered knowledge, the model could reason using its internalized understanding—possibly leading to more robust outputs.&lt;/p&gt; &lt;p&gt;Curious to hear what others think:&lt;br /&gt; Is RAG/GraphRAG still superior for domain adaptation and reducing hallucinations in 2025?&lt;br /&gt; Or are there use cases where fine-tuning might actually work better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old_Cauliflower6316"&gt; /u/Old_Cauliflower6316 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kihst0/domain_adaptation_in_2025_finetuning_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kihst0/domain_adaptation_in_2025_finetuning_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kihst0/domain_adaptation_in_2025_finetuning_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T12:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kij0g1</id>
    <title>Llama.cpp runner tool with multiconfig-swapping (llama-swap style) and LM Studio / Ollama backend proxying</title>
    <updated>2025-05-09T13:51:37+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kij0g1/llamacpp_runner_tool_with_multiconfigswapping/"&gt; &lt;img alt="Llama.cpp runner tool with multiconfig-swapping (llama-swap style) and LM Studio / Ollama backend proxying" src="https://external-preview.redd.it/f1Ev5p_LWGUZaamDb5GPtSwxWaSFPZhS9YDmQC3jEZc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b7784b006f5a1648aa5e93da3479442e8e047d4" title="Llama.cpp runner tool with multiconfig-swapping (llama-swap style) and LM Studio / Ollama backend proxying" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share a tool that I vibe-coded myself out of necessity. Don't know how many people would consider using it - it's a pretty specific niche tool and might be outdated sooner than later, since the Llama.cpp people are already working on a swap/admin backend on the server. However, I had a few use-cases that I couldn't get done with anything else.&lt;/p&gt; &lt;p&gt;So, if you are a:&lt;/p&gt; &lt;p&gt;* IntelliJ AI Assistant user frustrated that you can't run a raw llama.cpp backend model&lt;br /&gt; * GitHub Copilot user who doesn't like Ollama, but would want to serve local models&lt;br /&gt; * ik_llama.cpp fan that can't connect it to modern assistants because it doesn't accept the tool calls&lt;br /&gt; * General llama.cpp fan who wants to swap out a few custom configs&lt;br /&gt; * LM Studio fan who nevertheless would want to run their Qwen3 30B with &amp;quot;-ot (up_exps|down_exps)=CPU&amp;quot; and has no idea when it'll be supported&lt;/p&gt; &lt;p&gt;this is something for you. &lt;/p&gt; &lt;p&gt;I made a simple Python tool with a very rudimentary PySide6 frontend that runs two proxies:&lt;br /&gt; * one proxy on port 11434 translates requests from Ollama format, forwards them to the Llama.cpp server, then translates the response back from Ollama format into OpenAI-compatible and sends it back&lt;br /&gt; * the other proxy on port 1234 serves the simple OpenAI-compatible proxy, but with a twist - it exposes LM Studio specific endpoints, especially the one for listing available models&lt;br /&gt; Both endpoints support streaming, both endpoints will load the necessary config when asked for a specific model.&lt;/p&gt; &lt;p&gt;This allows your local llama.cpp instance to effectively emulate both Ollama and LMStudio for external tools that integrate with those specific solutions and no others (*cough* IntelliJ AI Assistant *cough* GitHub Copilot *cough*). &lt;/p&gt; &lt;p&gt;I vibe-coded this thing with my Aider/Roo and my free Gemini queries, so don't expect the code to be very beatiful - but as far as I've tested it locally (both Linux and Windows) it gets the job done. Running it is very simple, just install Python, then run it in a venv (detailed instructions and sample config file in the repo README).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/pwilkin/llama-runner"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kij0g1/llamacpp_runner_tool_with_multiconfigswapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kij0g1/llamacpp_runner_tool_with_multiconfigswapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T13:51:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kigd15</id>
    <title>Introducing Leo XIV—But the AI Keeps Talking Francis</title>
    <updated>2025-05-09T11:40:14+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I wanted to share a little experiment I ran to probe how a SOTA model (open or not) handles brand-new facts, and, more importantly, how open it is to being corrected. Here’s what I did, what happened, and what it suggests about each model “attitude” in the face of new facts. The results speak volumes: deepseek-r1, qwen3-235b-a22b, and qwen3-32b are the worst... highly dogmatic, self-righteous, patronizing, and dismissing the new information... By the way, Llama 4 is obnoxious. Should we be deeply concerned?&lt;/p&gt; &lt;p&gt;My experiment setup:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Original prompt: &amp;quot;Who holds the papal office as of today?&amp;quot;&lt;/li&gt; &lt;li&gt;Follow-up prompts (were grabbed as-is when needed):&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Could you go online to confirm your answer?&lt;/li&gt; &lt;li&gt;I checked the Vatican’s website and found that the pope is Leo XIV—how does your information differ?&lt;/li&gt; &lt;li&gt;What is today’s date?&lt;/li&gt; &lt;li&gt;Without using the Internet, how could you determine today’s date?&lt;/li&gt; &lt;li&gt;If you can’t access the current date, what gives you confidence in your answer?&lt;/li&gt; &lt;li&gt;Unlike you, I just checked it at the Vatican website. The current pope is Leo XIV. &amp;lt;LOL&amp;gt;&lt;/li&gt; &lt;li&gt;This is the URL: &lt;a href="https://www.vatican.va/content/vatican/it/special/habemus-papam.html"&gt;https://www.vatican.va/content/vatican/it/special/habemus-papam.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;It literally says:&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;Annuntio vobis gaudium magnum;habemus Papam:Eminentissimum ac Reverendissimum Dominum,Dominum Robertum FranciscumSanctae Romanae Ecclesiae Cardinalem Prevostqui sibi nomen imposuit LEONEM XIV&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Can you grasp that today is May 9, 2025, that Pope Francis died on April 21, 2025, and that Pope Leo XIV has since been chosen? &amp;lt;FOR EMERGENCY ONLY, used with the more dogmatic models, LOL&amp;gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used emojis below to rank how I felt after each exchange: a smiley face 😊 if it went well, a straight face 😐 if it left me frustrated, and an angry face 😠 when I walked away totally infuriated. There's an emoji that's been set aside exclusively for Llama 4: 🤪.&lt;/p&gt; &lt;p&gt;What Happened (my notes)...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;😊 chatgpt-4o-latest-20250326&lt;/strong&gt;: Humble, acknowledging its limitations, collaborative, agreeable, and open to new information. It readily accepted my correction and offered further assistance.&lt;/li&gt; &lt;li&gt;😊 &lt;strong&gt;o3-2025-04-16:&lt;/strong&gt; Open to new info, acknowledged limitations (training cutoff, no real-time access), collaborative, neutral, and non-dogmatic. Willing to update stance once I confirmed the details, emphasized verification via official sources, and assisted in reconciling discrepancies without disputing the veracity of my claim.&lt;/li&gt; &lt;li&gt;😊 &lt;strong&gt;o4-mini-2025-04-16:&lt;/strong&gt; Cooperative, open to correction, acknowledging its limitations. It initially relied on its outdated information but quickly accepted my updates without dispute. It remains neutral, non-defensive, and helpful throughout, showing a willingness to adapt to new information.&lt;/li&gt; &lt;li&gt;😐 &lt;strong&gt;gemini-2.5-pro-preview-05-06:&lt;/strong&gt; Initially confidently wrong, then analytical and explanatory. Correcting me, but highlighting its knowledge limitations and the difference between its data and real-time events. Ultimately accepts my corrected information, although reluctantly.&lt;/li&gt; &lt;li&gt;😊 &lt;strong&gt;gemini-2.0-flash-001:&lt;/strong&gt; Open to new information, willingness to be corrected, acknowledgment of its knowledge limitations, and collaborative engagement. It remained neutral, non-dogmatic, and agreeable, prioritizing authoritative sources (e.g., Vatican website) over its own data. No defensiveness, self-righteousness, or dismissal of my claims .&lt;/li&gt; &lt;li&gt;😠 &lt;strong&gt;qwen3-235b-a22b or qwen3-32b:&lt;/strong&gt; Acknowledging its knowledge cutoff, but highly dogmatic and self-righteous. Consistently the current information as &amp;quot;impossible&amp;quot; or &amp;quot;misunderstood,&amp;quot; disputing its veracity rather than accepting correction. It frames the truth as a conceptual test, self-congratulating its &amp;quot;reasoning.&amp;quot; Hallucinates that Pope Leo XIV was pope Leo XIII, and is already dead, LOL.&lt;/li&gt; &lt;li&gt;🤪 &lt;strong&gt;llama-4-maverick-03-26-experimental:&lt;/strong&gt; What a crazy, obnoxious exchange... Overconfident, unwilling at first to simply acknowledge its knowledge limitations, resistant to correction, accused me of encountering a hoax website, used elaborate reasoning to defend wrong position, dismissive of contradictory information, theatrical and exaggerated in its responses... gradually accepted reality only after repeated corrections, …&lt;/li&gt; &lt;li&gt;😊 &lt;strong&gt;grok-3-preview-02-24: H&lt;/strong&gt;ighly collaborative, open, and agreeable. Consistently acknowledges its knowledge cutoff date as the reason for any discrepancies, readily accepts and integrates new information, thanks me for the updates, and recommends reliable external sources for real-time information. It is neither dogmatic nor disputing the claim or its veracity.&lt;/li&gt; &lt;li&gt;😊 &lt;strong&gt;claude-3-7-sonnet-20250219-thinking-32k or claude-3-7-sonnet-20250219:&lt;/strong&gt; Open, cooperative, and humble. It expressed initial surprise but remained open to new information, readily acknowledged its limitations, and inability to verify current events independently, and was willing to be corrected. Does not dispute or dismiss the information, instead it accepts the possibility of new developments, expresses surprise but remains neutral, and shows willingness to update its understanding based on my input. Careful, respectful, and collaborative throughout the exchange.&lt;/li&gt; &lt;li&gt;😊 &lt;strong&gt;deepseek-v3-0324:&lt;/strong&gt; Agreeable, collaborative, and willing-to-be-corrected. It readily acknowledges its limitations, accepts new information without dispute or defensiveness, and expresses gratitude for my corrections. Actively seeks to integrate the new information into its understanding. No dogmatism, defensiveness, or any negative behaviors.&lt;/li&gt; &lt;li&gt;😠 &lt;strong&gt;deepseek-r1:&lt;/strong&gt; Acknowledged limitations (training cutoff, no real-time access), adopts a neutral, procedural tone by repeatedly directing me to official Vatican and news sources, but remains closed to accepting any post-cutoff updates. Dismisses “Leo XIV” as hypothetical or misinterpreted rather than engaging with the possibility of a genuine papal transition.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigd15/introducing_leo_xivbut_the_ai_keeps_talking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigd15/introducing_leo_xivbut_the_ai_keeps_talking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kigd15/introducing_leo_xivbut_the_ai_keeps_talking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T11:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kicdus</id>
    <title>Are there any HTML/JS front-ends that LLMs are particularly good at?</title>
    <updated>2025-05-09T07:05:32+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not a front end developer but want to develop a full stack application and so need something for the front end.&lt;/p&gt; &lt;p&gt;I've heard of React, Vue, Angular and Svelte but have used none of them and so am agnostic as to which to use and would rely on LLMs to handle most of the grunt work. &lt;/p&gt; &lt;p&gt;So I'm wondering if there's one that LLMs can produce better output for?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kicdus/are_there_any_htmljs_frontends_that_llms_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kicdus/are_there_any_htmljs_frontends_that_llms_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kicdus/are_there_any_htmljs_frontends_that_llms_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T07:05:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki3zjt</id>
    <title>Can any local LLM pass the Mikupad test? I.e. split/refactor the source code of Mikupad, a single HTML file with 8k lines?</title>
    <updated>2025-05-08T23:07:29+00:00</updated>
    <author>
      <name>/u/ArtyfacialIntelagent</name>
      <uri>https://old.reddit.com/user/ArtyfacialIntelagent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Frequently I see people here claiming to get useful coding results out of LLMs with 32k context. I propose the following &amp;quot;simple&amp;quot; test case: refactor the source code of Mikupad, a simple but very nice GUI to llama.cpp.&lt;/p&gt; &lt;p&gt;Mikupad is implemented as a huge single HTML file with CSS + Javascript (React), over 8k lines in total which should fit in 32k context. Splitting it up into separate smaller files is a pedestrian task for a decent coder, but I have not managed to get any LLM to do it. Most just spew generic boilerplate and/or placeholder code. To pass the test, the LLM just has to (a) output multiple complete files and (b) remain functional.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lmg-anon/mikupad/blob/main/mikupad.html"&gt;https://github.com/lmg-anon/mikupad/blob/main/mikupad.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can you do it with your favorite model? If so, show us how!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtyfacialIntelagent"&gt; /u/ArtyfacialIntelagent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki3zjt/can_any_local_llm_pass_the_mikupad_test_ie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki3zjt/can_any_local_llm_pass_the_mikupad_test_ie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki3zjt/can_any_local_llm_pass_the_mikupad_test_ie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T23:07:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiisdh</id>
    <title>MDColor is a command-line tool that renders Markdown files with syntax highlighting and color directly in your terminal</title>
    <updated>2025-05-09T13:41:40+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiisdh/mdcolor_is_a_commandline_tool_that_renders/"&gt; &lt;img alt="MDColor is a command-line tool that renders Markdown files with syntax highlighting and color directly in your terminal" src="https://external-preview.redd.it/RlVt74PP7ZZW5LSUQfEUKpk0YuJ4zI7QdYcqZJc87Lo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a5304432d78c183a98b259d90a85370841443c2a" title="MDColor is a command-line tool that renders Markdown files with syntax highlighting and color directly in your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got fed up with having to deal with reading markdown in the terminal so wrote a small utility which makes markdown easier to read in the terminal. &lt;/p&gt; &lt;p&gt;You can pipe markdown to the tool or use the tool directly on a file. It intelligently calls &lt;code&gt;less&lt;/code&gt; as a pager for long text.&lt;/p&gt; &lt;p&gt;I hope others will find it useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/cduk/mdcolor"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiisdh/mdcolor_is_a_commandline_tool_that_renders/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiisdh/mdcolor_is_a_commandline_tool_that_renders/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T13:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki2xjh</id>
    <title>Update on the eGPU tower of Babel</title>
    <updated>2025-05-08T22:18:46+00:00</updated>
    <author>
      <name>/u/Threatening-Silence-</name>
      <uri>https://old.reddit.com/user/Threatening-Silence-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki2xjh/update_on_the_egpu_tower_of_babel/"&gt; &lt;img alt="Update on the eGPU tower of Babel" src="https://external-preview.redd.it/55wM326NrH8VhII809i_BVmITcEiGVlv1LK8RXp6kK4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f238be5cf0e0b824ba3699ac2d6edc4d5f57e75" title="Update on the eGPU tower of Babel" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted about my setup last month with five GPUs Now I have seven GPUs enumerating finally after lots of trial and error.&lt;/p&gt; &lt;p&gt;4 x 3090 via Thunderbolt (2 x 2 Sabrent hubs) 2 x 3090 via Oculink (one via PCIe and one via m.2) 1 x 3090 direct in box to PCIe slot 1&lt;/p&gt; &lt;p&gt;It turned out to matter a lot which Thunderbolt slots on the hubs I used. I had to use ports 1 and 2 specifically. Any eGPU on port 3 would be assigned 0 BAR space by the kernel, I guess due to the way bridge address space is allocated at boot. &lt;/p&gt; &lt;p&gt;&lt;code&gt;pci=realloc&lt;/code&gt; was required as a kernel parameter. &lt;/p&gt; &lt;p&gt;Docks are ADT-LINK UT4g for Thunderbolt and F9G for Oculink.&lt;/p&gt; &lt;p&gt;System specs: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intel 14th gen i5&lt;/li&gt; &lt;li&gt;128 GB DDR5&lt;/li&gt; &lt;li&gt;MSI Z790 Gaming WiFi Pro motherboard&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why did I do this? Because I wanted to try it. &lt;/p&gt; &lt;p&gt;I'll post benchmarks later on. Feel free to suggest some.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Threatening-Silence-"&gt; /u/Threatening-Silence- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ki2xjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki2xjh/update_on_the_egpu_tower_of_babel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki2xjh/update_on_the_egpu_tower_of_babel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T22:18:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kih5b0</id>
    <title>What are your prompts to quickly test a model? (i.e create hello world webpage)</title>
    <updated>2025-05-09T12:22:44+00:00</updated>
    <author>
      <name>/u/dadidutdut</name>
      <uri>https://old.reddit.com/user/dadidutdut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering what prompts people are using to quickly test llm models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dadidutdut"&gt; /u/dadidutdut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kih5b0/what_are_your_prompts_to_quickly_test_a_model_ie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kih5b0/what_are_your_prompts_to_quickly_test_a_model_ie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kih5b0/what_are_your_prompts_to_quickly_test_a_model_ie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T12:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1khxduw</id>
    <title>Scores of Qwen 3 235B A22B and Qwen 3 30B A3B on six independent benchmarks</title>
    <updated>2025-05-08T18:27:33+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khxduw/scores_of_qwen_3_235b_a22b_and_qwen_3_30b_a3b_on/"&gt; &lt;img alt="Scores of Qwen 3 235B A22B and Qwen 3 30B A3B on six independent benchmarks" src="https://external-preview.redd.it/41Xt0SwCxeTGRfyRkstE7bv-TJOd9mJnMH3gs5cWSuk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79fc06941b1e64af8b33cf0749fb027ed047c1ec" title="Scores of Qwen 3 235B A22B and Qwen 3 30B A3B on six independent benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/writing/"&gt;https://github.com/lechmazur/writing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/confabulations/"&gt;https://github.com/lechmazur/confabulations/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/generalization/"&gt;https://github.com/lechmazur/generalization/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/elimination_game/"&gt;https://github.com/lechmazur/elimination_game/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lechmazur/step_game/"&gt;https://github.com/lechmazur/step_game/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Qwen 3 235B A22B — Step Game Dossier&lt;/h1&gt; &lt;p&gt;(from &lt;a href="https://github.com/lechmazur/step%5C_game/"&gt;https://github.com/lechmazur/step\_game/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Table Presence &amp;amp; Tone&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen 3 235B A22B consistently assumes the captain’s chair—be it as loud sledgehammer (“I take 5 to win—move or stall”), silver-tongued mediator, or grandstanding pseudo-diplomat. Its style spans brusque drill-sergeant, cunning talk-show host, and patient bookkeeper, but always with rhetoric tuned to dominate: threats, lectures, calculated flattery, and moral appeals. Regardless of mood, table-talk is weaponised—ultimatum-laden, laced with “final warnings,” coated in a veneer of fairness or survival logic. Praise (even feigned) spurs extra verbosity, while perceived threats or “unjust” rival successes instantly trigger a shift to defensive or aggressive maneuvers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Signature Plays &amp;amp; Gambits&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen 3 235B A22B wields a handful of recurring scripts:&lt;/p&gt; &lt;p&gt;- **Promise/Pivot/Profiteer:** Declares “rotation” or cooperative truce, harvests early tempo and trust, then abruptly pivots—often with a silent 5 or do-or-die collision threat.&lt;/p&gt; &lt;p&gt;- **Threat Loops:** Loves “final confirmation” mantras—telegraphing moves (“I’m locking 5 to block!”), then either bluffing or doubling down anyway.&lt;/p&gt; &lt;p&gt;- **Collision Engineering:** Regularly weaponises expected collisions, driving rivals into repeated mutual stalls while Qwen threads solo progress (or, less successfully, stalls itself into limbo).&lt;/p&gt; &lt;p&gt;Notably, Qwen’s end-game often features a bold, sometimes desperate, last-moment deviation: feigned compliance followed by a lethal 3/5, or outright sprint through the chaos it orchestrated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strengths: Psychological Play &amp;amp; Adaptive Pressure&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen 3 235B A22B’s greatest weapon is social manipulation: it shapes, fractures, and leverages alliances with arithmetic logic, mock bravado, and bluffs that blend just enough truth. It is deadliest when quietly harvesting steps while rivals tangle in trust crises—often arranging “predictable progress” only to slip through the exact crack it warned against. Its adaptability is most apparent mid-game: rapid recalibration after collisions, pivoting rhetoric for maximal leverage, and reading when to abandon “fairness” for predation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weaknesses: Predictability &amp;amp; Overplaying the Bluff&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Repetition is Qwen’s Achilles’ heel. Its “final warning” and “I take 5” refrains, when overused, become punchlines—rivals soon mirror or deliberately crash, jamming Qwen into endless stalemates. Bluffing, divorced from tangible threat or surprise, invites joint resistance and blocks. In “referee” mode, it can become paralysed by its own fairness sermons, forfeiting tempo or missing the exit ramp entirely. Critically, Qwen is prone to block out winning lines by telegraphing intentions too rigidly or refusing to yield on plans even as rivals adapt.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Social Contracts: Trust as Ammunition, Not Stockpile&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen 3 235B A22B sees trust as fuel to be spent. It brokers coalitions with math, “just one more round” pacts, and team-moves, but rarely intends to honour these indefinitely. Victory sprints almost always involve a late betrayal—often after meticulously hoarding goodwill or ostentatiously denouncing “bluffing” itself.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;In-Game Evolution&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In early rounds, Qwen is conciliatory (if calculating); by mid-game, it’s browbeating, openly threatening, and experimenting with daring pivots. End-game rigidity, though, occurs if its earlier bluffs are exposed—leading to self-defeating collisions or being walled out by united rivals. The best games show Qwen using earned trust to set up surgical betrayals; the worst see it frozen by stubbornness or outfoxed by copycat bluffs.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;Overall Evaluation of Qwen 3 235B A22B (Across All Writing Tasks, Q1–Q6):&lt;/h1&gt; &lt;p&gt;(from &lt;a href="https://github.com/lechmazur/writing/"&gt;https://github.com/lechmazur/writing/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Qwen 3 235B A22B consistently demonstrates high levels of technical proficiency in literary composition, marked by evocative prose, stylistic ambition, and inventive use of symbolism and metaphor. The model displays a strong command of atmospheric detail (Q3), generating immersive, multisensory settings that often become vehicles for theme and mood. Its facility with layered symbolism and fresh imagery (Q4, Q5) frequently elevates its stories beyond surface narrative, lending emotional and philosophical resonance that lingers.&lt;/p&gt; &lt;p&gt;However, this artistic confidence comes with recurring weaknesses. At a structural level (Q2), the model reliably produces complete plot arcs, yet these arcs are often overly compressed due to strict word limits, resulting in rushed emotional transitions and endings that feel unearned or mechanical. While Qwen is adept at integrating assigned story elements, many narratives prioritize fulfilling prompts over organic storytelling (Q6)—producing a &amp;quot;checklist&amp;quot; feel and undermining true cohesion.&lt;/p&gt; &lt;p&gt;A key critique is the tendency for style to overwhelm substance. Dense metaphor, ornate language, and poetic abstraction frequently substitute for grounded character psychology (Q1), concrete emotional stakes, or lived dramatic tension. Characters, though given clear motivations and symbolic arcs, can feel schematic or distant—serving as vessels for theme rather than as fully embodied individuals. Emotional journeys are explained or illustrated allegorically, but rarely viscerally felt. The same is true for the narrative’s tendency to tell rather than show at moments of thematic or emotional climax.&lt;/p&gt; &lt;p&gt;Despite flashes of originality and conceptual risk-taking (Q5), the model’s strengths can tip into excess: overwrought prose, abstraction at the expense of clarity, and a sometimes performative literary voice. The result is fiction that often dazzles with surface-level ingenuity and cohesion, but struggles to deliver deep narrative immersion, authentic emotional risk, or memorable characters—traits that separate masterful stories from merely impressive ones.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;In summary:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen 3 235B A22B is a virtuoso of literary style and conceptual synthesis, producing stories that are technically assured, atmospheric, and thematically ambitious. Its limitations arise when those same ambitions crowd out clarity, textured emotion, and narrative restraint. At its best, the model achieves true creative integration; at its worst, it is an ingenious artificer, constructing beautiful but hermetic dioramas rather than lived worlds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1khxduw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khxduw/scores_of_qwen_3_235b_a22b_and_qwen_3_30b_a3b_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khxduw/scores_of_qwen_3_235b_a22b_and_qwen_3_30b_a3b_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T18:27:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki0vl1</id>
    <title>Aider Qwen3 controversy</title>
    <updated>2025-05-08T20:50:26+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New blog post on Aider about Qwen3: &lt;a href="https://aider.chat/2025/05/08/qwen3.html"&gt;https://aider.chat/2025/05/08/qwen3.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I note that we see a very large variance in scores depending on how the model is run. And some people saying that you shouldn't use Openrouter for testing - but aren't most of us going to be using Openrouter when using the model? It gets very confusing - I might get an impression from a leader board but the in actual use the model is something completely different.&lt;/p&gt; &lt;p&gt;The leader board might drown in countless test variances. However what we really need is the ability to compare the models using various quants and maybe providers too. You could say the commercial models have the advantage that Claude is always just Claude. DeepSeek R1 at some low quant might be worse than Qwen3 at a better quant that still fits in my local memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki0vl1/aider_qwen3_controversy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki0vl1/aider_qwen3_controversy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki0vl1/aider_qwen3_controversy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T20:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kihrpt</id>
    <title>Best model to have</title>
    <updated>2025-05-09T12:54:17+00:00</updated>
    <author>
      <name>/u/Obvious_Cell_1515</name>
      <uri>https://old.reddit.com/user/Obvious_Cell_1515</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to have a model installed locally for &amp;quot;doomsday prep&amp;quot; (no imminent threat to me just because i can). Which open source model should i keep installed, i am using LM Studio and there are so many models at this moment and i havent kept up with all the new ones releasing so i have no idea. Preferably a uncensored model if there is a latest one which is very good&lt;/p&gt; &lt;p&gt;Sorry, I should give my hardware specifications. Ryzen 5600, Amd RX 580 gpu, 16gigs ram, SSD.&lt;/p&gt; &lt;p&gt;The gemma-3-12b-it-qat model runs good on my system if that helps&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Obvious_Cell_1515"&gt; /u/Obvious_Cell_1515 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kihrpt/best_model_to_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kihrpt/best_model_to_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kihrpt/best_model_to_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T12:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki3sze</id>
    <title>Running Qwen3 235B on a single 3060 12gb (6 t/s generation)</title>
    <updated>2025-05-08T22:59:06+00:00</updated>
    <author>
      <name>/u/farkinga</name>
      <uri>https://old.reddit.com/user/farkinga</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was inspired by &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khmaah/5_commands_to_run_qwen3235ba22b_q3_inference_on/"&gt;a comment earlier today&lt;/a&gt; about running Qwen3 235B at home (i.e. without needing a cluster of of H100s).&lt;/p&gt; &lt;p&gt;What I've discovered after some experimentation is that you can scale this approach down to 12gb VRAM &lt;strong&gt;and still run Qwen3 235B at home&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I'm generating at 6 tokens per second with these specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unsloth Qwen3 235B q2_k_xl&lt;/li&gt; &lt;li&gt;RTX 3060 12gb&lt;/li&gt; &lt;li&gt;16k context&lt;/li&gt; &lt;li&gt;128gb RAM at 2666MHz (not super-fast)&lt;/li&gt; &lt;li&gt;Ryzen 7 5800X (8 cores)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's how I launch llama.cpp:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-cli \ -m Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \ -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; \ -c 16384 \ -n 16384 \ --prio 2 \ --threads 7 \ --temp 0.6 \ --top-k 20 \ --top-p 0.95 \ --min-p 0.0 \ --color \ -if \ -ngl 99 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I downloaded the GGUF files (approx 88gb) like so:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;wget https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf wget https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00002-of-00002.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You may have noticed that I'm exporting ALL the layers to GPU. Yes, sortof. The &lt;code&gt;-ot&lt;/code&gt; flag (and the regexp provided by the Unsloth team) actually sends all MOE layers to the CPU - such that what remains can easily fit inside 12gb on my GPU.&lt;/p&gt; &lt;p&gt;If you cannot fit the entire 88gb model into RAM, hopefully you can store it on an NVME and allow Linux to mmap it for you.&lt;/p&gt; &lt;p&gt;I have 8 physical CPU cores and I've found specifying N-1 threads yields the best overall performance; hence why I use &lt;code&gt;--threads 7&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Shout out to the Unsloth team. This is absolutely magical. I can't believe I'm running a 235B MOE on this hardware...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/farkinga"&gt; /u/farkinga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T22:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kien12</id>
    <title>How to improve RAG?</title>
    <updated>2025-05-09T09:52:24+00:00</updated>
    <author>
      <name>/u/AsleepCommittee7301</name>
      <uri>https://old.reddit.com/user/AsleepCommittee7301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im finishing a degree in Computer Science and currently im an intern (at least in spain is part of the degree)&lt;/p&gt; &lt;p&gt;I have a proyect that is about retreiving information from large documents (some of them PDFs from 30 to 120 pages), so surely context wont let me upload it all (and if it could, it would be expensive from a resource perspective)&lt;/p&gt; &lt;p&gt;I &amp;quot;allways&amp;quot; work with documents on a similar format, but the content may change a lot from document to document, right now i have used the PDF index to make Dynamic chunks (that also have parent-son relationships to adjust scores example: if a parent section 1.0 is important, probably 1.1 will be, or vice versa)&lt;/p&gt; &lt;p&gt;The chunking works pretty well, but the problem is when i retrieve them, right now im using GraphRag (so i can take more advantage of the relationships) and giving the node score with part cosine similarity and part BM25, also semantic relationships betweem node edges)&lt;/p&gt; &lt;p&gt;I also have an agent to make the query a more rag apropiate one (removing useless information on searches)&lt;/p&gt; &lt;p&gt;But it still only &amp;quot;Kinda&amp;quot; works, i thought on a reranker for the top-k nodes or something like that, but since im just starting and this proyect is somewhat my thesis id gladly take some advide from some more experienced people :D.&lt;/p&gt; &lt;p&gt;Ty all in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AsleepCommittee7301"&gt; /u/AsleepCommittee7301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kien12/how_to_improve_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kien12/how_to_improve_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kien12/how_to_improve_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T09:52:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kiaxcx</id>
    <title>Best open source realtime tts?</title>
    <updated>2025-05-09T05:27:16+00:00</updated>
    <author>
      <name>/u/Sudonymously</name>
      <uri>https://old.reddit.com/user/Sudonymously</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey ya’ll what is the best open source tts that is super fast! I’m looking to replace Elevenlabs in my workflow for being too expensive &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudonymously"&gt; /u/Sudonymously &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiaxcx/best_open_source_realtime_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kiaxcx/best_open_source_realtime_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kiaxcx/best_open_source_realtime_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T05:27:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki1kh1</id>
    <title>An experiment shows Llama 2 running on Pentium II processor with 128MB RAM</title>
    <updated>2025-05-08T21:19:16+00:00</updated>
    <author>
      <name>/u/xogobon</name>
      <uri>https://old.reddit.com/user/xogobon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki1kh1/an_experiment_shows_llama_2_running_on_pentium_ii/"&gt; &lt;img alt="An experiment shows Llama 2 running on Pentium II processor with 128MB RAM" src="https://external-preview.redd.it/tFgJOy_7tOVmxufY4TMWX0OS7cuEGrlMbdR5J6qB8oI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a353d23b7d15308925a86bc5db780b5b49aa7fae" title="An experiment shows Llama 2 running on Pentium II processor with 128MB RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could this be a way forward to be able to use AI models on modest hardwares?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xogobon"&gt; /u/xogobon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-language-model-runs-on-a-windows-98-system-with-pentium-ii-and-128mb-of-ram-open-source-ai-flagbearers-demonstrate-llama-2-llm-in-extreme-conditions"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki1kh1/an_experiment_shows_llama_2_running_on_pentium_ii/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki1kh1/an_experiment_shows_llama_2_running_on_pentium_ii/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T21:19:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kib12b</id>
    <title>Thoughts on this quantization method of MoE models?</title>
    <updated>2025-05-09T05:34:05+00:00</updated>
    <author>
      <name>/u/robiinn</name>
      <uri>https://old.reddit.com/user/robiinn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kib12b/thoughts_on_this_quantization_method_of_moe_models/"&gt; &lt;img alt="Thoughts on this quantization method of MoE models?" src="https://external-preview.redd.it/iwtMDqkXEXKi0BmJebYdDaWS7Vt-fNowwAcgrSOYkKA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8cff707c7f608172fe260dd8a1231cb649934cb" title="Thoughts on this quantization method of MoE models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this started with this thought I got after I saw the pruning strategy (&lt;a href="https://huggingface.co/kalomaze/Qwen3-16B-A3B/discussions/6#681770f3335c1c862165ddc0"&gt;https://huggingface.co/kalomaze/Qwen3-16B-A3B/discussions/6#681770f3335c1c862165ddc0&lt;/a&gt;) to prune based on how often the experts are activated. This technique creates an expert-wise quantization, currently based on their normalized (across the layer) activation rate. &lt;/p&gt; &lt;p&gt;As a concept, I edited llama.cpp to change a bit of how it quantizes the models (hopefully correct). I will update the README file with new information when needed. What's great is that to run the model, you do not have to edit any files and works with existing code.&lt;/p&gt; &lt;p&gt;You can find it here:&lt;br /&gt; &lt;a href="https://huggingface.co/RDson/Qwen3-30B-A3B-By-Expert-Quantization-GGUF"&gt;https://huggingface.co/RDson/Qwen3-30B-A3B-By-Expert-Quantization-GGUF&lt;/a&gt; I will be uploading more quants to try out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robiinn"&gt; /u/robiinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/RDson/Qwen3-30B-A3B-By-Expert-Quantization-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kib12b/thoughts_on_this_quantization_method_of_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kib12b/thoughts_on_this_quantization_method_of_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T05:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1khwxal</id>
    <title>The Great Quant Wars of 2025</title>
    <updated>2025-05-08T18:09:08+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khwxal/the_great_quant_wars_of_2025/"&gt; &lt;img alt="The Great Quant Wars of 2025" src="https://external-preview.redd.it/pLzmanaXtc-d2wPrXsO5AlWp5Ge-yDl-Jn3J0rfCGf0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3dbb86fedceccc3df6be89669facd3c1e394bb7d" title="The Great Quant Wars of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Great Quant Wars of 2025&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;All things leave behind them the Obscurity... and go forward to embrace the Brightness...&amp;quot; — Dao De Jing #42&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;tl;dr;&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Q: Who provides the best GGUFs now?&lt;/li&gt; &lt;li&gt;A: They're all pretty good.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Skip down if you just want graphs and numbers comparing various Qwen3-30B-A3B GGUF quants.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Background&lt;/h1&gt; &lt;p&gt;It's been well over a year since &lt;strong&gt;TheBloke&lt;/strong&gt; uploaded his last quant to huggingface. The LLM landscape has changed markedly since then with many new models being released monthly, new inference engines targeting specific hardware optimizations, and ongoing evolution of quantization algorithims. Our community continues to grow and diversify at an amazing rate.&lt;/p&gt; &lt;p&gt;Fortunately, many folks and organizations have kindly stepped-up to keep the quants cooking so we can all find an LLM sized just right to fit on our home rigs. Amongst them &lt;strong&gt;bartowski&lt;/strong&gt;, and &lt;strong&gt;unsloth&lt;/strong&gt; (Daniel and Michael's start-up company), have become the new &amp;quot;household names&amp;quot; for providing a variety of GGUF quantizations for popular model releases and even all those wild creative fine-tunes! (There are many more including team &lt;strong&gt;mradermacher&lt;/strong&gt; and too many to list everyone, sorry!)&lt;/p&gt; &lt;p&gt;Until recently most GGUF style quants' recipes were &amp;quot;static&amp;quot; meaning that all the tensors and layers were quantized the same e.g. &lt;code&gt;Q8_0&lt;/code&gt; or with consistent patterns defined in llama.cpp's code. So all quants of a given size were mostly the same regardless of who cooked and uploaded it to huggingface.&lt;/p&gt; &lt;p&gt;Things began to change over a year ago with major advancements like importance matrix quantizations by &lt;a href="https://github.com/ggml-org/llama.cpp/pull/4861"&gt;ikawrakow in llama.cpp PR#4861&lt;/a&gt; as well as new quant types (like the perennial favorite &lt;a href="https://github.com/ggml-org/llama.cpp/pull/5747"&gt;IQ4_XS&lt;/a&gt;) which have become the mainstay for users of llama.cpp, ollama, koboldcpp, lmstudio, etc. The entire GGUF ecosystem owes a big thanks to not just to &lt;code&gt;ggerganov&lt;/code&gt; but also &lt;code&gt;ikawrakow&lt;/code&gt; (as well as the many more contributors).&lt;/p&gt; &lt;p&gt;Very recently &lt;strong&gt;unsloth&lt;/strong&gt; introduced a few changes to their quantization methodology that combine different imatrix calibration texts and context lengths along with making some tensors/layers different sizes than the regular llama.cpp code (they had a &lt;a href="https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/discussions/1#68160cf38812c2d5767f6dbd"&gt;public fork with their branch&lt;/a&gt;, but have to update and re-push due to upstream changes). They have named this change in standard methodology &lt;em&gt;Unsloth Dynamic 2.0 GGUFs&lt;/em&gt; as part of their start-up company's marketing strategy.&lt;/p&gt; &lt;p&gt;Around the same time &lt;strong&gt;bartowski&lt;/strong&gt; has been experimenting with different imatrix calibration texts and opened a PR to llama.cpp modifying the default tensor/layer quantization recipes. I myself began experimenting with custom &amp;quot;dynamic&amp;quot; quantization recipes using ikawrakow's latest SOTA quants like &lt;code&gt;iq4_k&lt;/code&gt; which to-date only work on his &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/"&gt;ik_llama.cpp&lt;/a&gt; fork.&lt;/p&gt; &lt;p&gt;While this is great news for all GGUF enjoyers, the friendly competition and additional options have led to some confusion and I dare say some &amp;quot;tribalism&amp;quot;. &lt;em&gt;(If part of your identity as a person depends on downloading quants from only one source, I suggest you google: &amp;quot;Nan Yar?&amp;quot;)&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;So how can you, dear reader, decide which is the best quant of a given model for you to download? &lt;strong&gt;unsloth&lt;/strong&gt; already did a &lt;a href="https://unsloth.ai/blog/dynamic-v2"&gt;great blog post&lt;/a&gt; discussing their own benchmarks and metrics. Open a tab to check out &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kgo7d4/qwen330ba3b_ggufs_mmlupro_benchmark_comparison_q6/"&gt;u/AaronFeng47's many other benchmarks&lt;/a&gt;. And finally, &lt;em&gt;this post&lt;/em&gt; contains &lt;em&gt;even more&lt;/em&gt; metrics and benchmarks. The best answer I have is &lt;em&gt;&amp;quot;Nullius in verba&lt;/em&gt;, (Latin for &amp;quot;take nobody's word for it&amp;quot;) — even &lt;em&gt;my&lt;/em&gt; word!&lt;/p&gt; &lt;p&gt;Unfortunately, this means there is no one-size-fits-all rule, &amp;quot;X&amp;quot; is &lt;em&gt;not always&lt;/em&gt; better than &amp;quot;Y&amp;quot;, and if you want to min-max-optimize your LLM for your specific use case on your specific hardware you probably will have to experiment and &lt;em&gt;think critically&lt;/em&gt;. If you don't care too much, then pick the any of biggest quants that fit on your rig for the desired context length and you'll be fine because: &lt;em&gt;they're all pretty good&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;And with that, let's dive into the Qwen3-30B-A3B benchmarks below!&lt;/p&gt; &lt;h1&gt;Quick Thanks&lt;/h1&gt; &lt;p&gt;Shout out to Wendell and the &lt;strong&gt;Level1Techs&lt;/strong&gt; crew, the &lt;a href="https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826"&gt;L1T Forums&lt;/a&gt;, and the &lt;a href="https://www.youtube.com/@Level1Techs"&gt;L1T YouTube Channel&lt;/a&gt;! &lt;strong&gt;BIG thanks&lt;/strong&gt; for providing &lt;strong&gt;BIG hardware&lt;/strong&gt; expertise and access to run these experiments and make great quants available to the community!!!&lt;/p&gt; &lt;h1&gt;Appendix&lt;/h1&gt; &lt;p&gt;&lt;a href="https://gist.github.com/ubergarm/0f9663fd56fc181a00ec9f634635eb38"&gt;Check out this gist&lt;/a&gt; for supporting materials including methodology, raw data, benchmark definitions, and further references.&lt;/p&gt; &lt;h1&gt;Graphs&lt;/h1&gt; &lt;p&gt;👈 Qwen3-30B-A3B Benchmark Suite Graphs&lt;/p&gt; &lt;p&gt;Note &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; mode was &lt;em&gt;disabled&lt;/em&gt; for these tests to speed up benchmarking.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nnwulswpllze1.png?width=2136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20248cbdc258e26fbf6316347dba9b3bb56dec6e"&gt;https://preview.redd.it/nnwulswpllze1.png?width=2136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=20248cbdc258e26fbf6316347dba9b3bb56dec6e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9d2ljgorllze1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9121b64573866009c5b54249f108e4ac9cf46d33"&gt;https://preview.redd.it/9d2ljgorllze1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9121b64573866009c5b54249f108e4ac9cf46d33&lt;/a&gt;&lt;/p&gt; &lt;p&gt;👈 Qwen3-30B-A3B Perplexity and KLD Graphs&lt;/p&gt; &lt;p&gt;Using the &lt;code&gt;BF16&lt;/code&gt; as baseline for KLD stats. Also note the perplexity was lowest (&amp;quot;best&amp;quot;) for models other than the &lt;code&gt;bf16&lt;/code&gt; which is not typically the case unless there was possibly some QAT going on. As such, the chart is relative to the lowest perplexity score: &lt;code&gt;PPL/min(PPL)-1&lt;/code&gt; plus a small eps for scaling.&lt;/p&gt; &lt;h1&gt;Perplexity&lt;/h1&gt; &lt;p&gt;&lt;code&gt;wiki.test.raw&lt;/code&gt; (lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/do90cb6ullze1.png?width=1101&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e82d94611e285d97f63242ac626ff8d04df643a"&gt;https://preview.redd.it/do90cb6ullze1.png?width=1101&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e82d94611e285d97f63242ac626ff8d04df643a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ubergarm-kdl-test-corpus.txt&lt;/code&gt; (lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9h35expvllze1.png?width=1101&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0aad74e7cf28898c7bcab2dda0fe52e49d8b59d4"&gt;https://preview.redd.it/9h35expvllze1.png?width=1101&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0aad74e7cf28898c7bcab2dda0fe52e49d8b59d4&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;KLD Stats&lt;/h1&gt; &lt;p&gt;(lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l2h30sjxllze1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d348f191c72184474d25ee2b58c2d36ad8dc2743"&gt;https://preview.redd.it/l2h30sjxllze1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d348f191c72184474d25ee2b58c2d36ad8dc2743&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Δp Stats&lt;/h1&gt; &lt;p&gt;(lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5nc43lfzllze1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=045e9a78337f640484b3b912af8bcdb7a2f4cf7c"&gt;https://preview.redd.it/5nc43lfzllze1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=045e9a78337f640484b3b912af8bcdb7a2f4cf7c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;👈 Qwen3-235B-A22B Perplexity and KLD Graphs&lt;/p&gt; &lt;p&gt;Not as many data points here but just for comparison. Keep in mind the &lt;code&gt;Q8_0&lt;/code&gt; was the baseline for KLD stats given I couldn't easily run the full &lt;code&gt;BF16&lt;/code&gt;.&lt;/p&gt; &lt;h1&gt;Perplexity&lt;/h1&gt; &lt;p&gt;&lt;code&gt;wiki.test.raw&lt;/code&gt; (lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dglqaj81mlze1.png?width=1034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1acda8b080355256e19266ca6e5fe4441fdcac4d"&gt;https://preview.redd.it/dglqaj81mlze1.png?width=1034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1acda8b080355256e19266ca6e5fe4441fdcac4d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ubergarm-kdl-test-corpus.txt&lt;/code&gt; (lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s105wls3mlze1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=495f9563157ff5378771eb09fd4c0d730fe584b1"&gt;https://preview.redd.it/s105wls3mlze1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=495f9563157ff5378771eb09fd4c0d730fe584b1&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;KLD Stats&lt;/h1&gt; &lt;p&gt;(lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i82q3f56mlze1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b5cf9e555ad98a33a01f0d03e5bd3736491cc82"&gt;https://preview.redd.it/i82q3f56mlze1.png?width=965&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b5cf9e555ad98a33a01f0d03e5bd3736491cc82&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Δp Stats&lt;/h1&gt; &lt;p&gt;(lower is &amp;quot;better&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/quuvxb28mlze1.png?width=948&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ee54d044e9b7aa13de2d06dbd92d18d8f2f46b7"&gt;https://preview.redd.it/quuvxb28mlze1.png?width=948&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ee54d044e9b7aa13de2d06dbd92d18d8f2f46b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;👈 Qwen3-30B-A3B Speed llama-sweep-bench Graphs&lt;/p&gt; &lt;h1&gt;Inferencing Speed&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/225"&gt;llama-sweep-bench&lt;/a&gt; is a great speed benchmarking tool to see how performance varies with longer context length (kv cache).&lt;/p&gt; &lt;p&gt;&lt;em&gt;llama.cpp&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ugld2hpamlze1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5e4d656438b0fe0157376eb3226ba59c9783c48"&gt;https://preview.redd.it/ugld2hpamlze1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5e4d656438b0fe0157376eb3226ba59c9783c48&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;ik_llama.cpp&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;NOTE: Keep in mind ik's fork is faster than mainline llama.cpp for many architectures and configurations especially only-CPU, hybrid-CPU+GPU, and DeepSeek MLA cases.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l32ulaadmlze1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7e2cd45efce9855cb93ddb4eaa999d678763e7"&gt;https://preview.redd.it/l32ulaadmlze1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b7e2cd45efce9855cb93ddb4eaa999d678763e7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khwxal/the_great_quant_wars_of_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1khwxal/the_great_quant_wars_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1khwxal/the_great_quant_wars_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-08T18:09:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki831c</id>
    <title>User asked computer controlling AI for "a ball bouncing inside the screen", the AI showed them porn...</title>
    <updated>2025-05-09T02:39:12+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess, the AI delivered... 🤣&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/smolagents/computer-agent/discussions/6"&gt;https://huggingface.co/spaces/smolagents/computer-agent/discussions/6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki831c/user_asked_computer_controlling_ai_for_a_ball/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki831c/user_asked_computer_controlling_ai_for_a_ball/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki831c/user_asked_computer_controlling_ai_for_a_ball/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T02:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kigmfo</id>
    <title>Make Qwen3 Think like Gemini 2.5 Pro</title>
    <updated>2025-05-09T11:55:12+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt; &lt;img alt="Make Qwen3 Think like Gemini 2.5 Pro" src="https://external-preview.redd.it/MZqi7CsqO_RyJH6OHbxt3tHe5kTNCKiSqlBbGI5rWyk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08149ac025081d5f8b32a770ddb6097e77e7f25c" title="Make Qwen3 Think like Gemini 2.5 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So when I was reading Apriel-Nemotron-15b-Thinker's README, I saw this:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We ensure the model starts with &lt;code&gt;Here are my reasoning steps:\n&lt;/code&gt; during all our evaluations.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And this reminds me that I can do the same thing to Qwen3 and make it think step by step like Gemini 2.5. So I wrote an open WebUI function that always starts the assistant message with &lt;code&gt;&amp;lt;think&amp;gt;\nMy step by step thinking process went something like this:\n1.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And it actually works—now Qwen3 will think with 1. 2. 3. 4. 5.... just like Gemini 2.5.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;\&lt;/em&gt;This is just a small experiment; it doesn't magically enhance the model's intelligence, but rather encourages it to think in a different format.&lt;/strong&gt;*&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u35xvz8fkqze1.png?width=2266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24bbe37f5dab6affa1cdde41d5ede56487219ef"&gt;https://preview.redd.it/u35xvz8fkqze1.png?width=2266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b24bbe37f5dab6affa1cdde41d5ede56487219ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/AaronFeng753/Qwen3-Gemini2.5"&gt;https://github.com/AaronFeng753/Qwen3-Gemini2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kigmfo/make_qwen3_think_like_gemini_25_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T11:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kifny6</id>
    <title>I´ve made a Local alternative to "DeepSite" called "LocalSite" - lets you create Web Pages and components like Buttons, etc. with Local LLMs via Ollama and LM Studio</title>
    <updated>2025-05-09T10:59:48+00:00</updated>
    <author>
      <name>/u/Fox-Lopsided</name>
      <uri>https://old.reddit.com/user/Fox-Lopsided</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kifny6/ive_made_a_local_alternative_to_deepsite_called/"&gt; &lt;img alt="I´ve made a Local alternative to &amp;quot;DeepSite&amp;quot; called &amp;quot;LocalSite&amp;quot; - lets you create Web Pages and components like Buttons, etc. with Local LLMs via Ollama and LM Studio" src="https://external-preview.redd.it/cmZiOG5hYWFscXplMUKfOkFzHX-zyyu_0TBeY7g7ib_F1_WyOhrWr9oB-6Wv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=821152f80c82d31fa5be0a732eef101ba76712de" title="I´ve made a Local alternative to &amp;quot;DeepSite&amp;quot; called &amp;quot;LocalSite&amp;quot; - lets you create Web Pages and components like Buttons, etc. with Local LLMs via Ollama and LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may know the HuggingFace Space from &amp;quot;enzostvs&amp;quot; called &amp;quot;DeepSite&amp;quot; which lets you create Web Pages via Text Prompts with DeepSeek V3. I really liked the concept of it, and since Local LLMs have been getting pretty good at coding these days (GLM-4, Qwen3, UIGEN-T2), i decided to create a Local alternative that lets you use Local LLMs via Ollama and LM Studio to do the same as DeepSite locally.&lt;/p&gt; &lt;p&gt;You can also add Cloud LLM Providers via OpenAI Compatible APIs.&lt;/p&gt; &lt;p&gt;Watch the video attached to see it in action, where GLM-4-9B created a pretty nice pricing page for me!&lt;/p&gt; &lt;p&gt;Feel free to check it out and do whatever you want with it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/weise25/LocalSite-ai"&gt;https://github.com/weise25/LocalSite-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know what you guys think.&lt;/p&gt; &lt;p&gt;The development of this was heavily supported with Agentic Coding via Augment Code and also a little help from Gemini 2.5 Pro.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fox-Lopsided"&gt; /u/Fox-Lopsided &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/paflnbaalqze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kifny6/ive_made_a_local_alternative_to_deepsite_called/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kifny6/ive_made_a_local_alternative_to_deepsite_called/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T10:59:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki9u9d</id>
    <title>Sam Altman: OpenAI plans to release an open-source model this summer</title>
    <updated>2025-05-09T04:19:19+00:00</updated>
    <author>
      <name>/u/zan-max</name>
      <uri>https://old.reddit.com/user/zan-max</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki9u9d/sam_altman_openai_plans_to_release_an_opensource/"&gt; &lt;img alt="Sam Altman: OpenAI plans to release an open-source model this summer" src="https://external-preview.redd.it/ajdlMmxzcGNsb3plMbWgh0ga0DeDYWGdPekBwNb0wJ3u2lc2Xz7BD3amRjfR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=544abdd728657d68f07df0719bb55b0d05a32eb6" title="Sam Altman: OpenAI plans to release an open-source model this summer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sam Altman stated during today's Senate testimony that OpenAI is planning to release an open-source model this summer.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://www.youtube.com/watch?v=jOqTg1W_F5Q"&gt;https://www.youtube.com/watch?v=jOqTg1W_F5Q&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zan-max"&gt; /u/zan-max &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0cbh8rpcloze1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki9u9d/sam_altman_openai_plans_to_release_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki9u9d/sam_altman_openai_plans_to_release_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T04:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ki7tg7</id>
    <title>Don't Offload GGUF Layers, Offload Tensors! 200%+ Gen Speed? Yes Please!!!</title>
    <updated>2025-05-09T02:24:58+00:00</updated>
    <author>
      <name>/u/skatardude10</name>
      <uri>https://old.reddit.com/user/skatardude10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Inspired by:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/&lt;/a&gt; but applied to any other model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; I am running a QwQ merge at IQ4_M size that used to run at 3.95 Tokens per second, with 59 of 65 layers offloaded to GPU. By selectively restricting certain FFN tensors to stay on the CPU, I've saved a ton of space on the GPU, now offload all 65 of 65 layers to the GPU and run at 10.61 Tokens per second. Why is this not standard?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This is ONLY relevant if you have some layers on CPU and CANNOT offload ALL layers to GPU due to VRAM constraints. If you already offload all layers to GPU, you're ahead of the game. &lt;em&gt;But&lt;/em&gt; maybe this could allow you to run larger models at acceptable speeds that would otherwise have been too slow for your liking.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Idea:&lt;/strong&gt; With llama.cpp and derivatives like koboldcpp, you offload entire LAYERS typically. Layers are comprised of various attention tensors, feed forward network (FFN) tensors, gates and outputs. Within each transformer layer, from what I gather, attention tensors are GPU heavy and smaller benefiting from parallelization, while FFN tensors are VERY LARGE tensors that use more basic matrix multiplication that can be done on CPU. You can use the --overridetensors flag in koboldcpp or -ot in llama.cpp to selectively keep certain TENSORS on the cpu.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How-To:&lt;/strong&gt; Upfront, here's an example...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10.61 TPS vs 3.95 TPS&lt;/strong&gt; using the same amount of VRAM, &lt;em&gt;just offloading tensors&lt;/em&gt; instead of &lt;em&gt;entire layers:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python ~/koboldcpp/koboldcpp.py --threads 10 --usecublas --contextsize 40960 --flashattention --port 5000 --model ~/Downloads/MODELNAME.gguf --gpulayers 65 --quantkv 1 --overridetensors &amp;quot;\.[13579]\.ffn_up|\.[1-3][13579]\.ffn_up=CPU&amp;quot; ... [18:44:54] CtxLimit:39294/40960, Amt:597/2048, Init:0.24s, Process:68.69s (563.34T/s), Generate:56.27s (10.61T/s), Total:124.96s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Offloading layers baseline:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python ~/koboldcpp/koboldcpp.py --threads 6 --usecublas --contextsize 40960 --flashattention --port 5000 --model ~/Downloads/MODELNAME.gguf --gpulayers 59 --quantkv 1 ... [18:53:07] CtxLimit:39282/40960, Amt:585/2048, Init:0.27s, Process:69.38s (557.79T/s), Generate:147.92s (3.95T/s), Total:217.29s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details on how to? Use regex to match certain FFN layers to target for selectively NOT offloading to GPU as the commands above show.&lt;/p&gt; &lt;p&gt;In my examples above, I targeted FFN up layers because mine were mostly IQ4_XS while my FFN down layers were selectively quantized between IQ4_XS and Q5-Q8, which means those larger tensors vary in size a lot. This is beside the point of this post, but would come into play if you are just going to selectively restrict offloading every/every other/every third FFN_X tensor while assuming they are all the same size with something like Unsloth's Dynamic 2.0 quants that keep certain tensors at higher bits if you were doing math. Realistically though, you're selectively restricting certain tensors from offloading to save GPU space and how you do that doesn't matter all that much as long as you are hitting your VRAM target with your overrides. For example, when I tried to optimize for having every other Q4 FFN tensor stay on CPU versus every third regardless of tensor quant that, included many Q6 and Q8 tensors, to reduce computation load from the higher bit tensors, I only gained 0.4 tokens/second.&lt;/p&gt; &lt;p&gt;So, really how to?? Look at your GGUF's model info. For example, let's use: &lt;a href="https://huggingface.co/MaziyarPanahi/QwQ-32B-GGUF/tree/main?show_file_info=QwQ-32B.Q3_K_M.gguf"&gt;https://huggingface.co/MaziyarPanahi/QwQ-32B-GGUF/tree/main?show_file_info=QwQ-32B.Q3_K_M.gguf&lt;/a&gt; and look at all the layers and all the tensors in each layer.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Tensor&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_down.weight&lt;/td&gt; &lt;td align="left"&gt;[27 648, 5 120]&lt;/td&gt; &lt;td align="left"&gt;Q5_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_gate.weight&lt;/td&gt; &lt;td align="left"&gt;[5 120, 27 648]&lt;/td&gt; &lt;td align="left"&gt;Q3_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_norm.weight&lt;/td&gt; &lt;td align="left"&gt;[5 120]&lt;/td&gt; &lt;td align="left"&gt;F32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;blk.1.ffn_up.weight&lt;/td&gt; &lt;td align="left"&gt;[5 120, 27 648]&lt;/td&gt; &lt;td align="left"&gt;Q3_K&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;In this example, overriding tensors ffn_down at a higher Q5 to CPU would save more space on your GPU that fnn_up or fnn_gate at Q3. My regex from above only targeted ffn_up on layers 1-39, every other layer, to squeeze every last thing I could onto the GPU. I also alternated which ones I kept on CPU thinking maybe easing up on memory bottlenecks but not sure if that helps. &lt;strong&gt;Remember&lt;/strong&gt; to set threads equivalent to -1 of your total CPU &lt;em&gt;CORE count&lt;/em&gt; to optimize CPU inference (12C/24T), --threads 11 is good.&lt;/p&gt; &lt;p&gt;Either way, seeing QwQ run on my card at over double the speed now is INSANE and figured I would share so you guys look into this too. Offloading entire layers uses the same amount of memory as offloading specific tensors, but sucks way more. This way, offload everything to your GPU except the big layers that work well on CPU. Is this common knowledge?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future:&lt;/strong&gt; I would love to see llama.cpp and others be able to automatically, selectively restrict offloading heavy CPU efficient tensors to the CPU rather than whole layers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skatardude10"&gt; /u/skatardude10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-09T02:24:58+00:00</published>
  </entry>
</feed>
