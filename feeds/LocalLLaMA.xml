<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-02T04:50:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n5eqbz</id>
    <title>3090 vs 5090 taking turns on inference loads answering the same prompts - pretty cool visual story being told here about performance</title>
    <updated>2025-09-01T04:01:43+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5eqbz/3090_vs_5090_taking_turns_on_inference_loads/"&gt; &lt;img alt="3090 vs 5090 taking turns on inference loads answering the same prompts - pretty cool visual story being told here about performance" src="https://preview.redd.it/owg4l5s47hmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e80374ba03f865ca2e46aba4422c6b71958a362f" title="3090 vs 5090 taking turns on inference loads answering the same prompts - pretty cool visual story being told here about performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted my new dual GPU setup yesterday: 5090 and 3090 crammed right next to each other. I'll post thermals in the comments, but I thought this performance graph was super cool so I'm leading with that. The 3090 is the only one that suffers from the GPUs being stuffed right next to each other because its fans blow straight into the back heat sink of the 5090. Fortunately, it's a Galax HOF 3090, which was built to be put under strain, and it has a button on the back that turns on super mega extreme loud fan mode. In an earlier test the 3090 topped out at 79 degrees, but once I hit the super fan button in a subsequent longer test it didn't get above 69 degrees. The 5090 never got above 54 at all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/owg4l5s47hmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5eqbz/3090_vs_5090_taking_turns_on_inference_loads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5eqbz/3090_vs_5090_taking_turns_on_inference_loads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T04:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5qmu1</id>
    <title>Can someone help me with where to generate or get a roleplay dataset (mid-nsfw) to fine-tune LLaMA 3.1 8b?</title>
    <updated>2025-09-01T14:45:31+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;😶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5qmu1/can_someone_help_me_with_where_to_generate_or_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5qmu1/can_someone_help_me_with_where_to_generate_or_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5qmu1/can_someone_help_me_with_where_to_generate_or_get/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T14:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n624a2</id>
    <title>Recommendations for a Local AI Software Engineering Setup</title>
    <updated>2025-09-01T22:01:19+00:00</updated>
    <author>
      <name>/u/LookingRadishing</name>
      <uri>https://old.reddit.com/user/LookingRadishing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To give some context, I regularly writing software. I have some pet research and software development projects that I'd like to work-on in my spare time. If possible I'd like to leverage AI, locally. &lt;/p&gt; &lt;p&gt;I'm considering upgrading to a new macbook pro m4 max chip with 128gb of ram. I believe that the specs would allow me to run some of the larger, frontier models (e.g., gpt-oss:120b or LLama 4 scout, deepseek-r1). I'm wondering if anyone has recently made a similar hardware upgrade, and would recommend it as a local &amp;quot;vibe coding&amp;quot; setup? Was it worth the cost? Is it best to hold-off until more powerful hardware, open-source LLMs and tooling are available? &lt;/p&gt; &lt;p&gt;I've been recently using using claude code and I've found it more helpful than not. I'm looking for something that can get as close as possible to my experience with that, but I'd like for it to operate completely locally. &lt;/p&gt; &lt;p&gt;The things that I like about claude code are how easy it makes it to use agents, hooks, commands, etc. I also like how clean the user interface is. I can tell that a lot of thought has gone into deciding what information to show to users, and I like the balance that the designers have decided upon. &lt;/p&gt; &lt;p&gt;That being said, I'm a bit reluctant to use claude code for certain things because of IP and privacy concerns. I also find the rate limits frustrating, and the price seems a bit high if something similar can be hosted locally. For those reasons, I'm interested in developing a locally hosted solution. &lt;/p&gt; &lt;p&gt;Right now, I'm thinking about using opencode and maybe one of the frontier models as my bread and butter. If the frontier models are too taxing on the system, maybe I could use a lighter model that's specifically designed for software development. I haven't done much research, but it seems like devstral might be a good option at the present time? &lt;/p&gt; &lt;p&gt;I've tried opencode a bit, and it doesn't seem to have feature parity with claude code, but I think it has the right foundation. I'm willing to invest into it, and I'd even be willing to contributing to the project if the developers are collaborative. That being said, I am open to using something else if a better, free and open-source option is out there. I've also heard of Aider, but the user interface seems a bit clunky in comparison to claude code and opencode. &lt;/p&gt; &lt;p&gt;I haven't done a deep dive to see how the agentic capabilities of opencode or aider. I'd be interested to hear other people's opinions about how they compare to claude code. I'd also be interested to hear other people's opinions and about their experiences with those tools, and what combinations they thought worked best for them. &lt;/p&gt; &lt;p&gt;Some general, and yet, related questions for anyone: &lt;/p&gt; &lt;p&gt;- Do you have experience with a completely local and open source software engineering setup?&lt;br /&gt; - Do you have recommendations about combinations of terminal interfaces and models that worked best for you?&lt;br /&gt; - Do you find yourself regularly using such tools for software engineering tasks? Or, is it something that you put to the side?&lt;br /&gt; - Do you think it's worth splurging on the hardware mentioned above for the intended purposes?&lt;br /&gt; - How would you strategize your time and money for the changes that you anticipate will occur in the future?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LookingRadishing"&gt; /u/LookingRadishing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n624a2/recommendations_for_a_local_ai_software/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n624a2/recommendations_for_a_local_ai_software/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n624a2/recommendations_for_a_local_ai_software/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T22:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n61mm7</id>
    <title>Optimal settings for running gpt-oss-120b on 2x 3090s and 128gb system ram</title>
    <updated>2025-09-01T21:40:51+00:00</updated>
    <author>
      <name>/u/WyattTheSkid</name>
      <uri>https://old.reddit.com/user/WyattTheSkid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a post this morning about finally getting around to trying out gpt-oss-120b and I was pleasantly surprised. That being said, I would like to release my settings that give me acceptable performance on a resource constrained system such as mine. Obviously &lt;strong&gt;&lt;em&gt;your mileage may vary&lt;/em&gt;&lt;/strong&gt; but I think this is a good starting point for anyone with a machine similar to mine looking to run the full size gpt-oss model at home with acceptable speed!&lt;/p&gt; &lt;p&gt;Here are my system specs:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;CPU&lt;/th&gt; &lt;th align="left"&gt;Ryzen 9 5950X 16 Core 32 Threads&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;G.Skill Ripjaws DDR4 @ 3600mhz 128GB Total&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;1x RTX 3090 TI + 1x RTX 3090&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MOBO&lt;/td&gt; &lt;td align="left"&gt;Asus ROG STRIX X570-E WIFI II&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PSU&lt;/td&gt; &lt;td align="left"&gt;Thermaltake Toughpower GF1 1000W 80+ Gold&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And now for my settings. I'm currently using the latest version of LM Studio and using the official lmstudio-community distributed gguf file.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Parameter&lt;/th&gt; &lt;th align="left"&gt;Value&lt;/th&gt; &lt;th align="left"&gt;Note&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Context Length&lt;/td&gt; &lt;td align="left"&gt;131072&lt;/td&gt; &lt;td align="left"&gt;I'm sure you could gain some t/s by lowering this, but I like having the headroom.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU Offload&lt;/td&gt; &lt;td align="left"&gt;28/36&lt;/td&gt; &lt;td align="left"&gt;Minimal noticeable difference with lowering this to 27. I multitask a lot so I've been loading it with 27 to free up some ram when I have a lot of other things going on&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU Thread Pool Size&lt;/td&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;This is a weird one. Higher doesn't seem to always be better for some reason but too low and it hurts performance. I was getting worse performance with 14+ and anything below 10 was pretty bad. I found the sweet spot to be 12 at least for the R9 5950X. Experiment with this value depending on your CPU.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Evaluation Batch Size&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;This is another case similar to the aforementioned one. I tried setting it to 1024 and somehow got worse performance. I was doing increments of 128 starting at 128 and stopping at 2048 and found 512 to be the sweet spot. Everything after that got worse for me.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RoPE Frequency Base&lt;/td&gt; &lt;td align="left"&gt;Auto&lt;/td&gt; &lt;td align="left"&gt;N/A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RoPE Frequency Scale&lt;/td&gt; &lt;td align="left"&gt;Auto&lt;/td&gt; &lt;td align="left"&gt;N/A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Offload KV Cache to GPU Memory&lt;/td&gt; &lt;td align="left"&gt;True&lt;/td&gt; &lt;td align="left"&gt;Originally I had this disabled because in the past I've had to do this in order to run models like Llama 3.3 70b with a full 128k context on my system but for some reason gpt-oss's context doesn't have nearly as large of a memory footprint as other models. (not a ML expert but I'm guessing it has something to do with the ridiculously small hidden size) On my rig, performance is still very usable (about a 4-5 t/s difference) with this KV cache offloaded to cpu but it's not recommended unless absolutely necessary.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Keep Model in Memory&lt;/td&gt; &lt;td align="left"&gt;True&lt;/td&gt; &lt;td align="left"&gt;Enabled by default idk&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Try mmap()&lt;/td&gt; &lt;td align="left"&gt;True&lt;/td&gt; &lt;td align="left"&gt;N/A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Seed&lt;/td&gt; &lt;td align="left"&gt;Default/Random&lt;/td&gt; &lt;td align="left"&gt;N/A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Number of Experts&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Nothing to do with performance in terms of speed but I've noticed a few instances where setting this to anything other than 4 seems to degrade the output quality.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Force Model Expert Weights onto CPU&lt;/td&gt; &lt;td align="left"&gt;True&lt;/td&gt; &lt;td align="left"&gt;N/A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Flash Attention&lt;/td&gt; &lt;td align="left"&gt;True&lt;/td&gt; &lt;td align="left"&gt;N/A&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;K Cache Quantization Type&lt;/td&gt; &lt;td align="left"&gt;Disabled&lt;/td&gt; &lt;td align="left"&gt;Haven't messed with these since it launched and barely worked to begin with but I would imagine this setting would improve generation speed as well&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;V Cache Quantization Type&lt;/td&gt; &lt;td align="left"&gt;Disabled&lt;/td&gt; &lt;td align="left"&gt;Haven't messed with these since it launched and barely worked to begin with but I would imagine this setting would improve generation speed as well&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;In Summary,&lt;/h1&gt; &lt;p&gt;My configuration is heavily geared towards as few compromises as possible while maintaining a usable speed. I get between 8-15 t/s with the settings I provided. If you're okay with possible slight quality loss or smaller context, you can probably squeeze a little more speed out of it if you change the context to something smaller like 65k or even 32k and mess with K and V cache quantization. If you're going to go that route, I would start with Q8 and I wouldn't go lower than Q4. Obviously faster system ram, a better cpu, and more pcie bandwidth will also make a big difference as well. Have fun with gpt-oss and I hope this helped some of you! Feel free to drop suggestions or ask questions below of course.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WyattTheSkid"&gt; /u/WyattTheSkid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n61mm7/optimal_settings_for_running_gptoss120b_on_2x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n61mm7/optimal_settings_for_running_gptoss120b_on_2x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n61mm7/optimal_settings_for_running_gptoss120b_on_2x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T21:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n688uf</id>
    <title>Dual GPU Setup: RTX 5090 + RTX Pro 6000 (96GB) on MSI X870E MAG Tomahawk – Which Slot Placement?</title>
    <updated>2025-09-02T02:47:38+00:00</updated>
    <author>
      <name>/u/Its-all-redditive</name>
      <uri>https://old.reddit.com/user/Its-all-redditive</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m building a workstation with two GPUs and want to optimize slot usage for both display + gaming and LLM inference serving and training.&lt;/p&gt; &lt;p&gt;System: • MSI MAG X870E Tomahawk WiFi (AM5) • 2× NVMe drives • RTX 5090 (main display + some inference) • RTX Pro 6000 96GB – dedicated for larger LLM serving or training) • 1600W Platinum PSU (I have a 20A circuit and I am planning on power limiting the cards to 400W-450W most of the time)&lt;/p&gt; &lt;p&gt;Board layout: • PCI_E1 (top): PCIe 5.0 x16 (CPU direct) • PCI_E2 (middle): PCIe 5.0 x4 (not GPU-friendly) • PCI_E3 (bottom): PCIe 4.0 x16 (x8 with 2 GPUs installed)&lt;/p&gt; &lt;p&gt;⸻&lt;/p&gt; &lt;p&gt;Should I:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Put the 5090 in PCI_E1 (Gen5x16) and the Pro 6000 in PCI_E3 (Gen4x8) or 2. Put the Pro 6000 in PCI_E1 (Gen5x16) and the 5090 in PCI_E3 (Gen4x8), with the 5090 still handling the displays. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In either of these setups, does Gen5 also get reduced to Gen5x8 because of the dual GPUs? From my understanding Gen5 vs Gen4 for gaming is only a few % difference but I haven’t been able to find reliable benchmarks on this kind of setup for LLM inference. I believe that once the models are loaded onto the VRAM, the Gen5 vs Gen4 comparison is moot anyway, however, wouldn’t the actual loading of the models onto the Gen4 much slower? This is why I was thinking it may be better to use the Gen5 slot for the GPU I’ll mostly be loading/unloading models frequently (Pro 6000). &lt;/p&gt; &lt;p&gt;Which way would you prioritize? Anyone running dual NVIDIA cards for AI workloads that has some advice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Its-all-redditive"&gt; /u/Its-all-redditive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n688uf/dual_gpu_setup_rtx_5090_rtx_pro_6000_96gb_on_msi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n688uf/dual_gpu_setup_rtx_5090_rtx_pro_6000_96gb_on_msi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n688uf/dual_gpu_setup_rtx_5090_rtx_pro_6000_96gb_on_msi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T02:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n61btk</id>
    <title>I built a free Structured Prompt Builder (JSON/YAML/MD export + few-shot + core controls) — feedback welcome</title>
    <updated>2025-09-01T21:28:37+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! I kept rewriting big “do-everything” prompts and losing track of constraints, steps, and few-shot examples. So I built a small, browser-based &lt;strong&gt;Structured Prompt Builder&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Live demo:&lt;/strong&gt; &lt;a href="https://structured-prompt-builder.vercel.app/?utm_source=chatgpt.com"&gt;https://structured-prompt-builder.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Build prompts by sections: &lt;strong&gt;Role, Task, Audience, Style, Tone, Constraints, Steps, Named Inputs, Few-shot&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live preview&lt;/strong&gt; in &lt;strong&gt;Markdown / JSON / YAML&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Core controls&lt;/strong&gt; saved alongside the prompt: temperature, top-p, max tokens, presence/frequency penalties&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Import/Export&lt;/strong&gt;: JSON ↔️ YAML ↔️ Markdown (one-click copy &amp;amp; downloads)&lt;/li&gt; &lt;li&gt;Reorder constraints/steps/examples with up/down buttons&lt;/li&gt; &lt;li&gt;Optional &lt;strong&gt;JSON-only mode&lt;/strong&gt; with inline schema validator&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I built it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I wanted fewer “Franken-prompts” and more &lt;strong&gt;repeatable structure&lt;/strong&gt; I can share with teammates.&lt;/li&gt; &lt;li&gt;It’s fast, simple, and &lt;strong&gt;runs entirely in your browser&lt;/strong&gt; (no login).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Who it’s for&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt engineers &amp;amp; power users who want clean, reusable templates&lt;/li&gt; &lt;li&gt;PMs, devs, writers—anyone who needs a reliable prompt scaffold (PRDs, code reviews, marketing briefs, tutorials, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How to use (30 seconds)&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Fill in Role + Task.&lt;/li&gt; &lt;li&gt;Add Constraints, Steps, Inputs, Few-shot.&lt;/li&gt; &lt;li&gt;Toggle JSON-only (optional), tweak core controls, then copy/export.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Would love feedback on:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Any missing block you want (e.g., evaluation rubric, safety guardrails)?&lt;/li&gt; &lt;li&gt;Default templates you’d use daily?&lt;/li&gt; &lt;li&gt;Little quality-of-life tweaks that would save time?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Built with a tiny React UI + Tailwind and deployed on Vercel. Happy to iterate based on your comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n61btk/i_built_a_free_structured_prompt_builder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n61btk/i_built_a_free_structured_prompt_builder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n61btk/i_built_a_free_structured_prompt_builder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T21:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5wcuw</id>
    <title>Better llama-cli help and user guide</title>
    <updated>2025-09-01T18:18:36+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5wcuw/better_llamacli_help_and_user_guide/"&gt; &lt;img alt="Better llama-cli help and user guide" src="https://external-preview.redd.it/NU9CTDEJ8MMoFoE6hwfKDB75ec15aILwLWWriJ1l13Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=246413df24a372594087fb98c2f06a5cf483ce5e" title="Better llama-cli help and user guide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15709"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5wcuw/better_llamacli_help_and_user_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5wcuw/better_llamacli_help_and_user_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T18:18:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6862l</id>
    <title>Uncensored image editing and generation ?</title>
    <updated>2025-09-02T02:43:57+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been enjoying Imagen for image editing a lot but it' is heavily censored which can be very annoying. What is the best uncensored local image editing and generation tool?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6862l/uncensored_image_editing_and_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6862l/uncensored_image_editing_and_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6862l/uncensored_image_editing_and_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T02:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5n2h3</id>
    <title>Context Reasoning Benchmarks: GPT-5, Claude, Gemini, Grok on Real Tasks</title>
    <updated>2025-09-01T12:14:21+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5n2h3/context_reasoning_benchmarks_gpt5_claude_gemini/"&gt; &lt;img alt="Context Reasoning Benchmarks: GPT-5, Claude, Gemini, Grok on Real Tasks" src="https://preview.redd.it/h8d68m9enjmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d21724bca765d6cd8e82243cab247845f595ebca" title="Context Reasoning Benchmarks: GPT-5, Claude, Gemini, Grok on Real Tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;Context reasoning evaluates whether a model can read the provided material and answer only from it. The context reasoning category is part of our Task Completion Benchmarks. It tests LLMs on grounded question answering with strict use of the provided source, long context retrieval, and resistance to distractors across documents, emails, logs, and policy text. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick read on current winners&lt;/strong&gt;&lt;br /&gt; Top tier (score ≈97): Claude Sonnet 4, GPT-5-mini&lt;br /&gt; Next tier (≈93): Gemini 2.5 Flash, Gemini 2.5 Pro, Claude Opus 4, OpenAI o3&lt;br /&gt; Strong group (≈90–88): Claude 3.5 Sonnet, GLM-4.5, GPT-5, Grok-4, GPT-OSS-120B, o4-mini. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;A tricky failure case to watch for&lt;/strong&gt;&lt;br /&gt; We include tasks where relevant facts are dispersed across a long context, like a travel journal with scattered city mentions. Many models undercount unless they truly track entities across paragraphs. The better context reasoners pass this reliably. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Takeaway&lt;/strong&gt;&lt;br /&gt; Context use matters as much as raw capability. Anthropic’s recent Sonnet models, Google’s Gemini 2.5 line, and OpenAI’s new 5-series (especially mini) show strong grounding on these tasks. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can see the category, examples, and methodology here:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://opper.ai/tasks/context-reasoning"&gt;https://opper.ai/tasks/context-reasoning&lt;/a&gt; &lt;/p&gt; &lt;p&gt;For those building with it, what strengths or edge cases are you seeing in context-heavy workloads?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h8d68m9enjmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5n2h3/context_reasoning_benchmarks_gpt5_claude_gemini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5n2h3/context_reasoning_benchmarks_gpt5_claude_gemini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T12:14:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6387y</id>
    <title>HRM - Training from scratch - Day 2 - model successfully overfitted to tiny dataset</title>
    <updated>2025-09-01T22:49:34+00:00</updated>
    <author>
      <name>/u/Creative-Ad-2112</name>
      <uri>https://old.reddit.com/user/Creative-Ad-2112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; so far I'm enjoying the process as it unfolds. I decided to take a backstep and understand if even the architecture can understand/learn languages.&lt;/p&gt; &lt;p&gt;I started by a character tokenizer and tested it if it can handle simple overfitting on a small dataset.&lt;/p&gt; &lt;p&gt;Afterwards I've tried a 10k character corpus to see if it can learn to autoregressively generate characters like basic gpt-like transformers can, it failed miserably actually.&lt;/p&gt; &lt;p&gt;However, it only worked once i added whole sentences and words to the character tokenizer, it responded well and got every prompt pair correct.&lt;/p&gt; &lt;p&gt;So it works if we can increase the token vocab and the less sub words in there, the better. Which led me back to gpt2 tokenizer, it struggled alot.&lt;/p&gt; &lt;p&gt;I then decided to test out two features of the hrm, how it handles when the config is `deeper` and `wider`,&lt;/p&gt; &lt;p&gt;but in this specific architecture, i increased the transformer layers from 4 to 8 and only one H loop and L&lt;/p&gt; &lt;p&gt;loop. the wider was at 4 with 3 hloop and L loop.&lt;/p&gt; &lt;p&gt;H and L are inner and outer loops.&lt;/p&gt; &lt;p&gt;so i was able to successfully get it to overfit.&lt;/p&gt; &lt;p&gt;And thank you for reading!&lt;/p&gt; &lt;p&gt;below is the results of its training run;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;deeper model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hello&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:brief,polite; plan:acknowledge &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; Hey! »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; what is 2 + 3?&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:2+3=5; verify:integer_add &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; translate &amp;quot;good night&amp;quot; to french&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:translate; src:en; tgt:fr; rule:direct_phrase &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; « Bonne nuit. »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; tell me a short joke&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:joke; tone:light; length:short &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; Why did the bicycle fall over? It was two-tired.&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hi&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:casual; plan:respond &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; Hey! »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hello there&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:friendly; plan:mirror &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; Hey!!&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; what is 3 + 2?&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:3+2=5; verify:add &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; calculate 1 + 4&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:1+4=5; verify:sum &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;&lt;strong&gt;wider model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hello&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:brief,polite; plan:acknowledge &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; Hello! »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; what is 2 + 3?&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:2+3=5; verify:integer_add &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; translate &amp;quot;good night&amp;quot; to french&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:translate; src:en; tgt:fr; rule:direct_phrase &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; « Bonne nuit. »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; tell me a short joke&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:joke; tone:light; length:short &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; Why did the bicycle fall over? It was two-tired.&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hi&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:casual; plan:respond &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; Hello! »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hello there&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:friendly; plan:mirror &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; Hello there!&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; what is 3 + 2?&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:3+2=5; verify:add &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; calculate 1 + 4&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:1+4=5; verify:sum &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;&lt;strong&gt;and below is the more technical output for those that arent tired of my yapping lol.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;deeper model run:&lt;br /&gt; Final CE: 0.0000 | AUX: 0.0100&lt;br /&gt; GOT: Hello!&lt;br /&gt; WANT: Hello!&lt;br /&gt; GOT: 5.&lt;br /&gt; WANT: 5.&lt;/p&gt; &lt;p&gt;--- Sample 1 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hello&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:brief,polite; plan:acknowledge &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: greet&lt;br /&gt; ALLOWED FIRST TOKENS: ['Hey', 'Hello']&lt;br /&gt; FIRST-STEP TOP-K: [('5', 0.46979138255119324), ('.', 0.39315593242645264), ('Why', 0.07724795490503311), (' Bon', 0.032733868807554245), ('Hey', 0.009616638533771038), ('&amp;lt;|endoftext|&amp;gt;', 0.005990968085825443), (' did', 0.0042328485287725925), ('!', 0.0029024614486843348)]&lt;br /&gt; CHOSEN FIRST TOKEN: Hey&lt;br /&gt; OUTPUT:&lt;br /&gt; Hey! »&lt;/p&gt; &lt;p&gt;--- Sample 2 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; what is 2 + 3?&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:2+3=5; verify:integer_add &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: arithmetic&lt;br /&gt; ALLOWED FIRST TOKENS: ['5']&lt;br /&gt; FIRST-STEP TOP-K: [('5', 0.7015942335128784), ('Why', 0.15817661583423615), (' Bon', 0.03699721768498421), ('!', 0.03692837432026863), ('Hey', 0.0328972227871418), ('&amp;lt;|endoftext|&amp;gt;', 0.017206650227308273), ('.', 0.007884377613663673), (' did', 0.0033648896496742964)]&lt;br /&gt; CHOSEN FIRST TOKEN: 5&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;--- Sample 3 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; translate &amp;quot;good night&amp;quot; to french&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:translate; src:en; tgt:fr; rule:direct_phrase &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: translate&lt;br /&gt; ALLOWED FIRST TOKENS: ['«']&lt;br /&gt; FIRST-STEP TOP-K: [('5', 0.7174723744392395), ('Why', 0.12315943092107773), ('.', 0.07549838721752167), (' Bon', 0.03735000267624855), ('Hey', 0.018656115978956223), ('&amp;lt;|endoftext|&amp;gt;', 0.010583776980638504), ('!', 0.008158780634403229), (' did', 0.004186202306300402)]&lt;br /&gt; CHOSEN FIRST TOKEN: «&lt;br /&gt; OUTPUT:&lt;br /&gt; « Bonne nuit. »&lt;/p&gt; &lt;p&gt;--- Sample 4 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; tell me a short joke&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:joke; tone:light; length:short &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: joke&lt;br /&gt; ALLOWED FIRST TOKENS: ['Why']&lt;br /&gt; FIRST-STEP TOP-K: [('5', 0.7368988394737244), ('Why', 0.12609894573688507), ('.', 0.05201536789536476), (' Bon', 0.03589411452412605), ('Hey', 0.020157743245363235), ('&amp;lt;|endoftext|&amp;gt;', 0.011015812866389751), ('!', 0.009161355905234814), (' did', 0.003931551240384579)]&lt;br /&gt; CHOSEN FIRST TOKEN: Why&lt;br /&gt; OUTPUT:&lt;br /&gt; Why did the bicycle fall over? It was two-tired.&lt;/p&gt; &lt;p&gt;--- Sample 5 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hi&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:casual; plan:respond &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: greet&lt;br /&gt; ALLOWED FIRST TOKENS: ['Hey', 'Hello']&lt;br /&gt; FIRST-STEP TOP-K: [('5', 0.6678099036216736), ('Why', 0.16081207990646362), ('!', 0.06870520859956741), ('Hey', 0.0441524013876915), (' Bon', 0.030156334862113), ('&amp;lt;|endoftext|&amp;gt;', 0.019773291423916817), (' did', 0.002431080210953951), ('.', 0.001417545136064291)]&lt;br /&gt; CHOSEN FIRST TOKEN: Hey&lt;br /&gt; OUTPUT:&lt;br /&gt; Hey! »&lt;/p&gt; &lt;p&gt;--- Sample 6 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hello there&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:friendly; plan:mirror &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: greet&lt;br /&gt; ALLOWED FIRST TOKENS: ['Hey', 'Hello']&lt;br /&gt; FIRST-STEP TOP-K: [('5', 0.7042155265808105), ('Why', 0.157093808054924), ('!', 0.03952900692820549), ('Hey', 0.03467824310064316), (' Bon', 0.03410692140460014), ('&amp;lt;|endoftext|&amp;gt;', 0.01725984551012516), ('.', 0.005274066235870123), (' did', 0.0030513897072523832)]&lt;br /&gt; CHOSEN FIRST TOKEN: Hey&lt;br /&gt; OUTPUT:&lt;br /&gt; Hey!!&lt;/p&gt; &lt;p&gt;--- Sample 7 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; what is 3 + 2?&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:3+2=5; verify:add &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: arithmetic&lt;br /&gt; ALLOWED FIRST TOKENS: ['5']&lt;br /&gt; FIRST-STEP TOP-K: [('5', 0.6966545581817627), ('Why', 0.15768173336982727), ('!', 0.047055210918188095), ('Hey', 0.03807936608791351), (' Bon', 0.03197040408849716), ('&amp;lt;|endoftext|&amp;gt;', 0.018041569739580154), ('.', 0.003056142246350646), (' did', 0.0027533688116818666)]&lt;br /&gt; CHOSEN FIRST TOKEN: 5&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;--- Sample 8 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; calculate 1 + 4&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:1+4=5; verify:sum &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: arithmetic&lt;br /&gt; ALLOWED FIRST TOKENS: ['5']&lt;br /&gt; FIRST-STEP TOP-K: [('5', 0.7025521397590637), ('Why', 0.15613870322704315), ('!', 0.04393727704882622), ('Hey', 0.03735767677426338), (' Bon', 0.03171215206384659), ('&amp;lt;|endoftext|&amp;gt;', 0.017682280391454697), ('.', 0.0032090034801512957), (' did', 0.002745213219895959)]&lt;br /&gt; CHOSEN FIRST TOKEN: 5&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;wider model run:&lt;/p&gt; &lt;p&gt;Final CE: 0.0000 | AUX: 0.0150&lt;/p&gt; &lt;p&gt;--- Sample 1 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hello&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:brief,polite; plan:acknowledge &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: greet&lt;br /&gt; ALLOWED FIRST TOKENS: ['Hey', 'Hello']&lt;br /&gt; FIRST-STEP TOP-K: [('.', 0.9852362871170044), ('«', 0.012538655661046505), (' Bon', 0.0013400508323684335), ('Why', 0.00027935649268329144), ('&amp;lt;|endoftext|&amp;gt;', 0.00012366671580821276), ('Hello', 0.00010915892198681831), ('!', 7.980169175425544e-05), ('5', 7.384794298559427e-05)]&lt;br /&gt; CHOSEN FIRST TOKEN: Hello&lt;br /&gt; OUTPUT:&lt;br /&gt; Hello! »&lt;/p&gt; &lt;p&gt;--- Sample 2 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; what is 2 + 3?&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:2+3=5; verify:integer_add &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: arithmetic&lt;br /&gt; ALLOWED FIRST TOKENS: ['5']&lt;br /&gt; FIRST-STEP TOP-K: [('.', 0.9861264824867249), ('«', 0.011742380447685719), (' Bon', 0.0012781355762854218), ('Why', 0.00026998057728633285), ('&amp;lt;|endoftext|&amp;gt;', 0.00011890486348420382), ('Hello', 0.00010622163244988769), ('!', 7.62480340199545e-05), ('5', 7.055179594317451e-05)]&lt;br /&gt; CHOSEN FIRST TOKEN: 5&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;--- Sample 3 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; translate &amp;quot;good night&amp;quot; to french&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:translate; src:en; tgt:fr; rule:direct_phrase &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: translate&lt;br /&gt; ALLOWED FIRST TOKENS: ['«']&lt;br /&gt; FIRST-STEP TOP-K: [('.', 0.9849263429641724), ('«', 0.01282725390046835), (' Bon', 0.0013504876988008618), ('Why', 0.00028244793065823615), ('&amp;lt;|endoftext|&amp;gt;', 0.00012547856022138149), ('Hello', 0.0001101160523830913), ('!', 8.133111987262964e-05), ('5', 7.512614683946595e-05)]&lt;br /&gt; CHOSEN FIRST TOKEN: «&lt;br /&gt; OUTPUT:&lt;br /&gt; « Bonne nuit. »&lt;/p&gt; &lt;p&gt;--- Sample 4 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; tell me a short joke&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:joke; tone:light; length:short &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: joke&lt;br /&gt; ALLOWED FIRST TOKENS: ['Why']&lt;br /&gt; FIRST-STEP TOP-K: [('.', 0.9850696921348572), ('«', 0.012696742080152035), (' Bon', 0.0013424678472802043), ('Why', 0.000281412125332281), ('&amp;lt;|endoftext|&amp;gt;', 0.00012461119331419468), ('Hello', 0.00010973347525577992), ('!', 8.056389924604446e-05), ('5', 7.462135545210913e-05)]&lt;br /&gt; CHOSEN FIRST TOKEN: Why&lt;br /&gt; OUTPUT:&lt;br /&gt; Why did the bicycle fall over? It was two-tired.&lt;/p&gt; &lt;p&gt;--- Sample 5 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hi&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:casual; plan:respond &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: greet&lt;br /&gt; ALLOWED FIRST TOKENS: ['Hey', 'Hello']&lt;br /&gt; FIRST-STEP TOP-K: [('.', 0.9857224225997925), ('«', 0.01210754830390215), (' Bon', 0.0013038457836955786), ('Why', 0.0002722761710174382), ('&amp;lt;|endoftext|&amp;gt;', 0.00012143997446401045), ('Hello', 0.00010728350025601685), ('!', 7.856674346840009e-05), ('5', 7.194236968643963e-05)]&lt;br /&gt; CHOSEN FIRST TOKEN: Hello&lt;br /&gt; OUTPUT:&lt;br /&gt; Hello! »&lt;/p&gt; &lt;p&gt;--- Sample 6 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; hello there&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:greet; tone:friendly; plan:mirror &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: greet&lt;br /&gt; ALLOWED FIRST TOKENS: ['Hey', 'Hello']&lt;br /&gt; FIRST-STEP TOP-K: [('.', 0.9888366460800171), ('«', 0.00931193120777607), (' Bon', 0.001104532741010189), ('Why', 0.00023444643011316657), ('&amp;lt;|endoftext|&amp;gt;', 0.00010423409548820928), ('Hello', 9.576183947501704e-05), ('!', 6.609725096495822e-05), (' there', 6.18926715105772e-05)]&lt;br /&gt; CHOSEN FIRST TOKEN: Hello&lt;br /&gt; OUTPUT:&lt;br /&gt; Hello there!&lt;/p&gt; &lt;p&gt;--- Sample 7 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; what is 3 + 2?&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:3+2=5; verify:add &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: arithmetic&lt;br /&gt; ALLOWED FIRST TOKENS: ['5']&lt;br /&gt; FIRST-STEP TOP-K: [('.', 0.9862282276153564), ('«', 0.011650857515633106), (' Bon', 0.001271733082830906), ('Why', 0.00026877064374275506), ('&amp;lt;|endoftext|&amp;gt;', 0.00011834150063805282), ('Hello', 0.00010586577991489321), ('!', 7.58390233386308e-05), ('5', 7.01595054124482e-05)]&lt;br /&gt; CHOSEN FIRST TOKEN: 5&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;p&gt;--- Sample 8 ---&lt;br /&gt; PROMPT:&lt;br /&gt; &amp;lt;user&amp;gt; calculate 1 + 4&amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;reasoning&amp;gt; intent:arithmetic; eq:1+4=5; verify:sum &amp;lt;/s&amp;gt;&lt;br /&gt; &amp;lt;assistant&amp;gt;&lt;br /&gt; INTENT: arithmetic&lt;br /&gt; ALLOWED FIRST TOKENS: ['5']&lt;br /&gt; FIRST-STEP TOP-K: [('.', 0.9865846633911133), ('«', 0.011330759152770042), (' Bon', 0.001249230350367725), ('Why', 0.0002638636215124279), ('&amp;lt;|endoftext|&amp;gt;', 0.0001165428984677419), ('Hello', 0.00010449309047544375), ('!', 7.46748482924886e-05), (' there', 6.88438376528211e-05)]&lt;br /&gt; CHOSEN FIRST TOKEN: 5&lt;br /&gt; OUTPUT:&lt;br /&gt; 5. »&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-Ad-2112"&gt; /u/Creative-Ad-2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6387y/hrm_training_from_scratch_day_2_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6387y/hrm_training_from_scratch_day_2_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6387y/hrm_training_from_scratch_day_2_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T22:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5ebur</id>
    <title>What's the best local model for nsfw story telling?</title>
    <updated>2025-09-01T03:40:20+00:00</updated>
    <author>
      <name>/u/oogami</name>
      <uri>https://old.reddit.com/user/oogami</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for recommendations. I want to generate long nsfw novel.&lt;/p&gt; &lt;p&gt;I can use the company's idle H100 80GB * 8 server. I have tried &lt;a href="https://huggingface.co/huihui-ai/Huihui-Qwen3-235B-A22B-Instruct-2507-abliterated-Q4_K_M-GGUF"&gt;huihui-ai/Huihui-Qwen3-235B-A22B-Instruct-2507-abliterated-Q4_K_M-GGUF&lt;/a&gt;, it works, but the novel quality is not very good, and it's very slow because it's gguf so it can't be runed by vllm.&lt;/p&gt; &lt;p&gt;I have also tried to run DeepSeek-R1-0528. But the AWQ version failed to work on vllm, I don't know why.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oogami"&gt; /u/oogami &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5ebur/whats_the_best_local_model_for_nsfw_story_telling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5ebur/whats_the_best_local_model_for_nsfw_story_telling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5ebur/whats_the_best_local_model_for_nsfw_story_telling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T03:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5jhts</id>
    <title>gpt-oss 120b actually isn't that bad.</title>
    <updated>2025-09-01T08:46:10+00:00</updated>
    <author>
      <name>/u/WyattTheSkid</name>
      <uri>https://old.reddit.com/user/WyattTheSkid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title says it all. I just wanted to make this post to see what everyone else thinks. It runs at a respectable 10~ tokens a second with 128k context split between a 3090TI and a 3090 (K and V caches on system ram) and did very well on some math and coding tests I put it through. It honestly feels like a lightweight version of ChatGPT which is not something I would complain about given that it's open weight and runs on 2 consumer gpus. It's not perfect and it refuses for absolutely no reason sometimes but for what it is, it's not terrible. It outperforms Llama 3.3 70b in a lot of ways which is my usual go-to but I can't decide if I like it ENOUGH to make it my default. Perhaps maybe I'll try and finetune it for longer answers and less censorship? Idk I just wanted to say that I gave it a shot and as much as I hate what OpenAI has become, I can't really say it's a terrible llm for what it is. The 20b model is still pretty iffy though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WyattTheSkid"&gt; /u/WyattTheSkid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5jhts/gptoss_120b_actually_isnt_that_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5jhts/gptoss_120b_actually_isnt_that_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5jhts/gptoss_120b_actually_isnt_that_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T08:46:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n60hqx</id>
    <title>Local LLM for School</title>
    <updated>2025-09-01T20:55:50+00:00</updated>
    <author>
      <name>/u/OkTill6991</name>
      <uri>https://old.reddit.com/user/OkTill6991</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m a teacher in a UK secondary school and a (very) amateur AI hobbyist. I’ve been thinking about ways to implement a local AI in our school to help allay concerns around using student data with cloud AI tools.&lt;/p&gt; &lt;p&gt;Here in the UK we’re subject to GDPR, and a lot of education decision-makers are (understandably) very risk-averse when it comes to privacy.&lt;/p&gt; &lt;p&gt;My initial idea is a safe, local AI that staff could use for general purposes, think lesson resource creation, drafting emails, etc. But longer-term, I was wondering if it might be possible to hook a local AI up to a read-only copy of our student database (SQL) so teachers could query things like attendance or behaviour data in natural language.&lt;/p&gt; &lt;p&gt;Before I embarrass myself in front of our IT staff, I thought I’d get a sanity check here first and embarrass myself with you lot instead.&lt;/p&gt; &lt;p&gt;Some extra context:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;I’ve managed to set up a local LLM on my home PC already.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;At school I’d have help from IT if it’s at all feasible.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I know there’d be substantial upfront investment (GPUs etc.), but I think I could secure that.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;From what I’ve read, this would need orchestration (e.g. n8n) and a front end (e.g. OpenWebUI). Maybe JSON schemas or something similar would also be required?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So… what am I missing? Am I crazy? Any pointers to likely roadblocks, or people who’ve done something similar, would be massively appreciated.&lt;/p&gt; &lt;p&gt;TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkTill6991"&gt; /u/OkTill6991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n60hqx/local_llm_for_school/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n60hqx/local_llm_for_school/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n60hqx/local_llm_for_school/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T20:55:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n57hb8</id>
    <title>I locally benchmarked 41 open-source LLMs across 19 tasks and ranked them</title>
    <updated>2025-08-31T22:04:33+00:00</updated>
    <author>
      <name>/u/jayminban</name>
      <uri>https://old.reddit.com/user/jayminban</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n57hb8/i_locally_benchmarked_41_opensource_llms_across/"&gt; &lt;img alt="I locally benchmarked 41 open-source LLMs across 19 tasks and ranked them" src="https://preview.redd.it/a2bfcgphgfmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=feddc11eb0681fb3c1b60c20d6369d89110c9a3a" title="I locally benchmarked 41 open-source LLMs across 19 tasks and ranked them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I benchmarked 41 open-source LLMs using lm-evaluation-harness. Here are the 19 tasks covered:&lt;/p&gt; &lt;p&gt;mmlu, arc_challenge, gsm8k, bbh, truthfulqa, piqa, hellaswag, winogrande, boolq, drop, triviaqa, nq_open, sciq, qnli, gpqa, openbookqa, anli_r1, anli_r2, anli_r3&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ranks were computed by taking the simple average of task scores (scaled 0–1).&lt;/li&gt; &lt;li&gt;Sub-category rankings, GPU and memory usage logs, a master table with all information, raw JSON files, Jupyter notebook for tables, and script used to run benchmarks are posted on my GitHub repo.&lt;/li&gt; &lt;li&gt;🔗 &lt;a href="http://github.com/jayminban/41-llms-evaluated-on-19-benchmarks"&gt;github.com/jayminban/41-llms-evaluated-on-19-benchmarks&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This project required:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;18 days 8 hours of runtime&lt;/li&gt; &lt;li&gt;Equivalent to 14 days 23 hours of RTX 5090 GPU time, calculated at 100% utilization.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The environmental impact caused by this project was mitigated through my active use of public transportation. :)&lt;/p&gt; &lt;p&gt;Any feedback or ideas for my next project are greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jayminban"&gt; /u/jayminban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a2bfcgphgfmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n57hb8/i_locally_benchmarked_41_opensource_llms_across/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n57hb8/i_locally_benchmarked_41_opensource_llms_across/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T22:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n68l86</id>
    <title>vLLM vs MLIR - TTS Performance</title>
    <updated>2025-09-02T03:05:00+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68l86/vllm_vs_mlir_tts_performance/"&gt; &lt;img alt="vLLM vs MLIR - TTS Performance" src="https://preview.redd.it/ytt747xm2omf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ee5db23ef9cbac3ea9a0f9c6c43c67d1c06c0c7" title="vLLM vs MLIR - TTS Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;vLLM leverages nvcc toolchain, MLIR (https://mlir.llvm.org/) transforms IR (Intermediate Representation) to PTX directly for nvidia. MLIR's IR could be transformed to other GPU/CPU instructions via dialects. From the TTS-1 Technical Report (https://arxiv.org/html/2507.21138v1) of Inworld.ai, &amp;quot;The inference stack leverages a graph compiler (MAX pipeline) for optimizations like kernel fusion and memory planning, complemented by custom kernels for critical operations like attention and matrix-vector multiplication, which were also developed in Mojo to outperform standard library implementations.&amp;quot; and &amp;quot;As a result of these combined optimizations, the streaming API delivers the first two seconds of synthesized audio on average 70% faster than a vanilla vLLM-based implementation&amp;quot; MAX/Mojo uses MLIR. This looks to be a purpose speicific optimization to squeeze more throughput from GPUs. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ytt747xm2omf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68l86/vllm_vs_mlir_tts_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n68l86/vllm_vs_mlir_tts_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T03:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n63lz8</id>
    <title>Top small LLM as of September '25</title>
    <updated>2025-09-01T23:06:29+00:00</updated>
    <author>
      <name>/u/_-inside-_</name>
      <uri>https://old.reddit.com/user/_-inside-_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I've been away for the last couple of months, and suddenly I don't seem to see references to new small models around here. Is there any novelty o the topic of small models since the releases of Qwen 3 and Gemma 3n? Something I could run with 4GB VRAM? thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_-inside-_"&gt; /u/_-inside-_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n63lz8/top_small_llm_as_of_september_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n63lz8/top_small_llm_as_of_september_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n63lz8/top_small_llm_as_of_september_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T23:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5rhbd</id>
    <title>I'm building local, open-source, fast, efficient, minimal, and extendible RAG library I always wanted to use</title>
    <updated>2025-09-01T15:17:56+00:00</updated>
    <author>
      <name>/u/Avienir</name>
      <uri>https://old.reddit.com/user/Avienir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhbd/im_building_local_opensource_fast_efficient/"&gt; &lt;img alt="I'm building local, open-source, fast, efficient, minimal, and extendible RAG library I always wanted to use" src="https://external-preview.redd.it/cXB3bmh1bGZsa21mMfbflv5Di1j64vZv4v6FbqGgackbIUKWjlzVaYUu9HIx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da4116b03f8236881e7202c319fe08d1d76dec5" title="I'm building local, open-source, fast, efficient, minimal, and extendible RAG library I always wanted to use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of overengineered and bloated AI libraries and needed something to prototype local RAG apps quickly so I decided to make my own library,&lt;br /&gt; Features:&lt;br /&gt; ➡️ Get to prototyping local RAG applications in seconds: uvx rocketrag prepare &amp;amp; uv rocketrag ask is all you need&lt;br /&gt; ➡️ CLI first interface, you can even visualize embeddings in your terminal&lt;br /&gt; ➡️ Native llama.cpp bindings - no Ollama bullshit&lt;br /&gt; ➡️ Ready to use minimalistic web app with chat, vectors visualization and browsing documents➡️ Minimal footprint: milvus-lite, llama.cpp, kreuzberg, simple html web app&lt;br /&gt; ➡️ Tiny but powerful - use any chucking method from chonkie, any LLM with .gguf provided and any embedding model from sentence-transformers&lt;br /&gt; ➡️ Easily extendible - implement your own document loaders, chunkers and BDs, contributions welcome!&lt;br /&gt; Link to repo: &lt;a href="https://github.com/TheLion-ai/RocketRAG"&gt;https://github.com/TheLion-ai/RocketRAG&lt;/a&gt;&lt;br /&gt; Let me know what you think. If anybody wants to collaborate and contribute DM me or just open a PR!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Avienir"&gt; /u/Avienir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tqnduvlflkmf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhbd/im_building_local_opensource_fast_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5rhbd/im_building_local_opensource_fast_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T15:17:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5u32n</id>
    <title>AMD 6x7900xtx 24GB + 2xR9700 32GB VLLM QUESTIONS</title>
    <updated>2025-09-01T16:55:26+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5u32n/amd_6x7900xtx_24gb_2xr9700_32gb_vllm_questions/"&gt; &lt;img alt="AMD 6x7900xtx 24GB + 2xR9700 32GB VLLM QUESTIONS" src="https://preview.redd.it/txo8g9us0lmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a9855934c099881ce9400ed4488f4043ebbec7e" title="AMD 6x7900xtx 24GB + 2xR9700 32GB VLLM QUESTIONS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear reddit community, last two years from time to time our pc with one 7900xtx growed into this machine.&lt;/p&gt; &lt;p&gt;I am try to find solution to utilize it for 2-3 parallel queries at high speed for the qwen3-coder-flash model or for the quantized version of qwen3-235b-instruct.&lt;/p&gt; &lt;p&gt;I test different ways to launch VLLM with different cards, but it stay on Cuda graph (i also disabled with enforce_eager).&lt;/p&gt; &lt;pre&gt;&lt;code&gt;version: '3.8' services: vllm: pull_policy: always tty: true restart: unless-stopped ports: - 8000:8000 image: rocm/vllm-dev:nightly_main_20250817 shm_size: '128g' volumes: - /mnt/tb_disk/llm:/app/models devices: - /dev/kfd:/dev/kfd - /dev/dri:/dev/dri - /dev/mem:/dev/mem environment: - ROCM_VISIBLE_DEVICES=1,2,3,4,5,7,0,6 - HIP_VISIBLE_DEVICES=1,2,3,4,5,7,0,6 - VLLM_USE_V1=0 - VLLM_ATTENTION_BACKEND=ROCM_FLASH - ROCM_USE_FLASH_ATTN_V2_TRITON=True - VLLM_USE_TRITON_FLASH_ATTN=1 - VLLM_CUSTOM_OPS=all - NCCL_DEBUG=ERROR - PYTORCH_HIP_ALLOC_CONF=expandable_segments:True command: | sh -c ' vllm serve /app/models/models/vllm/Qwen3-Coder-30B-A3B-Instruct \ --served-model-name qwen3-coder-flash \ --max-model-len 131072 \ --gpu-memory-utilization 0.97 \ --tensor-parallel-size 4 \ --enable-auto-tool-choice \ --disable-log-requests \ --tool-call-parser qwen3_coder \ --enable-chunked-prefill \ --max-num-batched-tokens 4096 \ --max-num-seqs 8 ' volumes: {} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;This work ok for -tp 4, but for -tp 8 always stack.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;i know about llama-cpp, but it's very slow if we look at same utilization in vllm, maybe someone here have successful launch tensor parallelism in TGI?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interesting thing: R9700 does not loose speed inference in case when model distributed between two cards or one.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Feel free to ask any question about this machine.&lt;/p&gt; &lt;p&gt;also some GPTQ models work and some don't, maybe it's due to the quantization format,&lt;/p&gt; &lt;p&gt;Other helpful info: MB: MZ32-AR0 3200MT/s x8 32gb, 2x PSU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/txo8g9us0lmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5u32n/amd_6x7900xtx_24gb_2xr9700_32gb_vllm_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5u32n/amd_6x7900xtx_24gb_2xr9700_32gb_vllm_questions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T16:55:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6827e</id>
    <title>After deepseekv3 I feel like other MoE architectures are old or outdated. Why did Qwen chose a simple MoE architecture with softmax routing and aux loss for their Qwen3 models when there’s been better architectures for a while?</title>
    <updated>2025-09-02T02:38:32+00:00</updated>
    <author>
      <name>/u/Euphoric_Ad9500</name>
      <uri>https://old.reddit.com/user/Euphoric_Ad9500</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseekv3, R1, and deepseekv3.1 use sigmoid based routing with aux free bias gating and shared experts whereas Qwen3 MoE models use standard soft max routing with aux loss balancing. The Deepseekv3 architecture is better because it applies a bias to the raw affinity score for balancing. Qwen3 uses aux loss which can compete with other rewards. There are a couple other features that make the Deepseekv3 architecture better. This honestly makes me wary about even using Qwen3 MoE models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Euphoric_Ad9500"&gt; /u/Euphoric_Ad9500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6827e/after_deepseekv3_i_feel_like_other_moe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6827e/after_deepseekv3_i_feel_like_other_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6827e/after_deepseekv3_i_feel_like_other_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T02:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5j783</id>
    <title>I built, pre-trained, and fine-tuned a small language model and it is truly open-source.</title>
    <updated>2025-09-01T08:26:34+00:00</updated>
    <author>
      <name>/u/itsnikity</name>
      <uri>https://old.reddit.com/user/itsnikity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5j783/i_built_pretrained_and_finetuned_a_small_language/"&gt; &lt;img alt="I built, pre-trained, and fine-tuned a small language model and it is truly open-source." src="https://preview.redd.it/cwyoa0f6kimf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=147ff4faaa129cc07cda0d4d53d824668e625f35" title="I built, pre-trained, and fine-tuned a small language model and it is truly open-source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay, most of the time we all read open-source and in reality it is just open-weights. This time it is truly open-source.&lt;/p&gt; &lt;p&gt;Lille is a 130M parameter model trained from scratch and every part of the stack is open. Dataset, Model weights, Training code, Tokenizer, Optimizer, Evaluation framework...&lt;/p&gt; &lt;p&gt;Two versions are available: a base model trained on billions of tokens, and an instruction-tuned version fine-tuned on a curated instruction dataset.&lt;/p&gt; &lt;p&gt;Fun fact: it was trained locally on a single RTX 4070-TI.&lt;/p&gt; &lt;p&gt;I’d love feedback, suggestions, or contributions - whether it’s fine-tuning ideas, evaluation improvements, or even architectural tweaks.&lt;/p&gt; &lt;p&gt;Thanks! Check it out: &lt;a href="https://huggingface.co/Nikity/lille-130m-instruct"&gt;Lille 130M Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsnikity"&gt; /u/itsnikity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cwyoa0f6kimf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5j783/i_built_pretrained_and_finetuned_a_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5j783/i_built_pretrained_and_finetuned_a_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T08:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n68l96</id>
    <title>Policy violation Fee in Grok (Facepalm)</title>
    <updated>2025-09-02T03:05:03+00:00</updated>
    <author>
      <name>/u/Yes_but_I_think</name>
      <uri>https://old.reddit.com/user/Yes_but_I_think</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models"&gt;https://docs.x.ai/docs/models&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://docs.x.ai/docs/models#usage-guidelines-violation-fee"&gt;Usage Guidelines Violation Fee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A rare occurrence for most users, when your request is deemed to be in violation of our usage guideline by our system, we will charge a $0.05 per request usage guidelines violation fee.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yes_but_I_think"&gt; /u/Yes_but_I_think &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68l96/policy_violation_fee_in_grok_facepalm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68l96/policy_violation_fee_in_grok_facepalm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n68l96/policy_violation_fee_in_grok_facepalm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T03:05:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5w9yy</id>
    <title>I fine-tuned Llama 3.2 3B for transcript analysis and it outperformed bigger models with ease</title>
    <updated>2025-09-01T18:15:43+00:00</updated>
    <author>
      <name>/u/CartographerFun4221</name>
      <uri>https://old.reddit.com/user/CartographerFun4221</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5w9yy/i_finetuned_llama_32_3b_for_transcript_analysis/"&gt; &lt;img alt="I fine-tuned Llama 3.2 3B for transcript analysis and it outperformed bigger models with ease" src="https://b.thumbs.redditmedia.com/XI_Ubzun5wNt_4EOP4JZv-BnoPQ4ncNVDSzs6HtcQ9M.jpg" title="I fine-tuned Llama 3.2 3B for transcript analysis and it outperformed bigger models with ease" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently wrote a &lt;a href="https://github.com/bilawalriaz/lazy-notes"&gt;small local tool &lt;/a&gt;to transcribe my local audio notes to text using Whisper/Parakeet.&lt;br /&gt; I wanted to process the raw transcripts locally without needing OpenRouter so i tried Llama 3.2 3B and got surprisingly decent yet ultimately mediocre results. I decided to see how i could improve this using SFT.&lt;/p&gt; &lt;p&gt;I fine-tuned Llama 3.2 3B to clean and analyze raw dictation transcripts locally, outputting a structured JSON object (title, tags, entities, dates, actions).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data: 13 real voice memos → teacher (Kimi K2) for gold JSON → ~40k synthetic transcripts + gold. Keys are canonicalized to stabilize JSON supervision. &lt;a href="http://Chutes.ai"&gt;Chutes.ai&lt;/a&gt; was used, giving 5000 reqs/day.&lt;/li&gt; &lt;li&gt;Training: RTX 4090 24GB, ~4 hours, LoRA (r=128, alpha=128, dropout=0.05), max seq length of 2048 tokens, batch size 16, lr=5e-5, cosine scheduler, Unsloth. Could've done it without all this VRAM but would've taken slower (8 hours on my RTX 2070 Super 8GB).&lt;/li&gt; &lt;li&gt;Inference: merged to GGUF, quantized Q4_K_M using llama.cpp, runs locally via LM Studio.&lt;/li&gt; &lt;li&gt;Evals (100-sample sanity check, scored by GLM 4.5 FP8): &lt;strong&gt;overall score 5.35 (base 3B)&lt;/strong&gt; → &lt;strong&gt;8.55 (fine-tuned)&lt;/strong&gt;. Completeness 4.12 → 7.62, factual accuracy 5.24 → 8.57.&lt;/li&gt; &lt;li&gt;Head-to-head (10 samples): specialized 3B averaged ~8.40 vs Hermes-70B 8.18, Mistral-Small-24B 7.90, Gemma-3-12B 7.76, Qwen3-14B 7.62. Teacher Kimi K2 ~8.82.&lt;/li&gt; &lt;li&gt;Why it works: task specialization + JSON canonicalization reduce output variance and help the model learn the exact structure and fields.&lt;/li&gt; &lt;li&gt;Lessons learned: important to train on completions only, synthetic datasets are okay for specialised fine-tunes, Llama is surprisingly easy to train&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yw48krzwllmf1.jpg?width=3600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=985ed3cbb09fcd77e470060dda382dadd6325c7f"&gt;https://preview.redd.it/yw48krzwllmf1.jpg?width=3600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=985ed3cbb09fcd77e470060dda382dadd6325c7f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/exgsrqzwllmf1.jpg?width=6000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=38c3ca44b377f4cee7808b2ccf6f1d57b0fb87e8"&gt;https://preview.redd.it/exgsrqzwllmf1.jpg?width=6000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=38c3ca44b377f4cee7808b2ccf6f1d57b0fb87e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code, dataset pipeline, hyperparams, eval details, and a 4-bit GGUF download are in the post: &lt;a href="https://bilawal.net/post/finetuning-llama32-3b-for-transcripts/"&gt;https://bilawal.net/post/finetuning-llama32-3b-for-transcripts/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to discuss training setup, eval rubric, or deployment details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CartographerFun4221"&gt; /u/CartographerFun4221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bilawal.net/post/finetuning-llama32-3b-for-transcripts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5w9yy/i_finetuned_llama_32_3b_for_transcript_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5w9yy/i_finetuned_llama_32_3b_for_transcript_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T18:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6699f</id>
    <title>GPU credits for students, tinkerers, solopreneurs</title>
    <updated>2025-09-02T01:10:20+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We recognize that GPU grants are often biased. Funded startups, prominent researchers, or other successful individuals are swimming in credits. At the same time, it can be challenging to obtain a GPU if you're just getting started and when you need it the most. We're working to address this issue through our GPU credits program, which is &lt;strong&gt;available to everyone&lt;/strong&gt; (also, we're a poor early-stage startup, so we can't offer generous sponsorship programs).&lt;/p&gt; &lt;p&gt;- Get from $100 to $1000 for your project. Note that our prices are one-quarter of those of Hyperscalers, and we offer consumer GPUs like RTX 4090 / 5090 / Pro 6000 for rent, so you really get $500-$10,000 of GPU value.&lt;/p&gt; &lt;p&gt;- We pool applications and make decisions every two weeks. We've allocated a $3,000 monthly budget for this program. We will increase it if it proves successful.&lt;/p&gt; &lt;p&gt;- We're looking for projects that address pressing community problems. It doesn't have to be a significant issue. If you're working on one, please don't forget to refer to the Reddit thread that describes the problem. It helps us refine the product to meet community needs.&lt;/p&gt; &lt;p&gt;- We'd like to ask you to mention us in your social media post, article, or blog. Having an active social media profile, published articles, or blog posts is a plus. Ultimately, we're a business and aim to promote our product.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cloudrift.ai/ai-grant"&gt;https://www.cloudrift.ai/ai-grant&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6699f/gpu_credits_for_students_tinkerers_solopreneurs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6699f/gpu_credits_for_students_tinkerers_solopreneurs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6699f/gpu_credits_for_students_tinkerers_solopreneurs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T01:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n68afq</id>
    <title>Why are all AI "Success" posts terrible?</title>
    <updated>2025-09-02T02:49:54+00:00</updated>
    <author>
      <name>/u/Bimbam_tm</name>
      <uri>https://old.reddit.com/user/Bimbam_tm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Wow look at this!&amp;quot; someone cries, and includes a screenshot/gif from a single-line AI prompt magically producing a working product.&lt;/p&gt; &lt;p&gt;Great, and completely unsurprising given that one-line prompts work exactly like horoscopes - so vague they can't help but satisfy whatever slop gets generated. But whatever, as long as it looks gifable right?&lt;/p&gt; &lt;p&gt;&amp;quot;Build me a todo app that looks nice!&amp;quot;&lt;br /&gt; Congratulations, you just wrote the AI equivalent of &amp;quot;you will face challenges this week.&amp;quot; The AI spits out literally anything 'todo adjacent' and you're amazed because technically it worked. Just like horoscopes, it's response is written so broadly that the reader finds it somehow fits their expectations..&lt;/p&gt; &lt;p&gt;A real horoscope would say &amp;quot;On Tuesday at 3:47 PM, you will receive a text from someone whose name starts with J about a blue object.&amp;quot;&lt;/p&gt; &lt;p&gt;With that in mind, how about someone show me a real workflow:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Your original concept art/design docs/sketches&lt;/li&gt; &lt;li&gt;How close you actually got to achieve your original concept/idea&lt;/li&gt; &lt;li&gt;How many iterations it took&lt;/li&gt; &lt;li&gt;What didn't work&lt;/li&gt; &lt;li&gt;The actual prompts you used (all of them)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Unless that AI output was almost EXACTLY what you had in mind from prompt #1 (which seems highly unlikely), all your &amp;quot;amazing&amp;quot; result proves was your prompt was horoscope-level vague, and you're apparently ok with mediocrity .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bimbam_tm"&gt; /u/Bimbam_tm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68afq/why_are_all_ai_success_posts_terrible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68afq/why_are_all_ai_success_posts_terrible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n68afq/why_are_all_ai_success_posts_terrible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T02:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5zed0</id>
    <title>I pretrained and postrained a LLM with less than $50 budget which outperforms Google BERT large</title>
    <updated>2025-09-01T20:13:23+00:00</updated>
    <author>
      <name>/u/Altruistic-Tea-5612</name>
      <uri>https://old.reddit.com/user/Altruistic-Tea-5612</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5zed0/i_pretrained_and_postrained_a_llm_with_less_than/"&gt; &lt;img alt="I pretrained and postrained a LLM with less than $50 budget which outperforms Google BERT large" src="https://external-preview.redd.it/ywyWexAkeoEnV3YXP8YcOUkQeZuDP2-5umUBtdqKkZ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebad8a84758c0785d9bbd8dcbf0a28e687394e9e" title="I pretrained and postrained a LLM with less than $50 budget which outperforms Google BERT large" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks from LocalLLama sub! I am really thankful for amazing people in this sub for sharing useful things which helped me to learn lots of things about pretraing , post training and evaluation etc for your context I don't have professional ML background!&lt;/p&gt; &lt;p&gt;Today I am super excited to share that I pretrained and post trained 150M parameter model from scratch which outperforms Google BERT model and I also built embedding model which works on par with Jina-embedings-v2-base model in MTEB benchmarks &lt;/p&gt; &lt;p&gt;In this article I shared how I did this model along with links to weights of model&lt;br /&gt; thanks again&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Tea-5612"&gt; /u/Altruistic-Tea-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@harishhacker3010/pretraining-a-llm-with-less-than-50-budget-which-outperforms-google-bert-dbe541b7b14b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5zed0/i_pretrained_and_postrained_a_llm_with_less_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5zed0/i_pretrained_and_postrained_a_llm_with_less_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T20:13:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI — The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM – 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
