<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-23T02:16:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mx9ke4</id>
    <title>Do you have to spend big to locally host LLM?</title>
    <updated>2025-08-22T15:26:56+00:00</updated>
    <author>
      <name>/u/darkmatter343</name>
      <uri>https://old.reddit.com/user/darkmatter343</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking to get into self hosting my own LLM, but before I make the journey, I wanted to get some point of views.&lt;/p&gt; &lt;p&gt;I understand the desire for privacy, scalability, and using different LLM’s but to actually make it worth it, performant, and useable like ChatGPT, what kind of hardware would you need? &lt;/p&gt; &lt;p&gt;My use case would be purely privacy focused with the goal also being able to try different LLM’s for Coding, random question, and playing around with in general. &lt;/p&gt; &lt;p&gt;Would a 9950x with 128GB ram be sufficient and what type of GPU would I even need to make it worth while? Obviously the GPU would play the biggest role so could lower end but high amounts of VRAM suffice? Or unless you buy 8 GPUs like Pewdiepie just did would it not be worth it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkmatter343"&gt; /u/darkmatter343 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx9ke4/do_you_have_to_spend_big_to_locally_host_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx9ke4/do_you_have_to_spend_big_to_locally_host_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx9ke4/do_you_have_to_spend_big_to_locally_host_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T15:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxeksc</id>
    <title>I have been working on a talking jellyfish desktop companion using Sesame CSM and Kyutai ASR</title>
    <updated>2025-08-22T18:35:25+00:00</updated>
    <author>
      <name>/u/DumaDuma</name>
      <uri>https://old.reddit.com/user/DumaDuma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxeksc/i_have_been_working_on_a_talking_jellyfish/"&gt; &lt;img alt="I have been working on a talking jellyfish desktop companion using Sesame CSM and Kyutai ASR" src="https://external-preview.redd.it/Y3NscG03MnYybWtmMZdAKW3zuhxUEmydslHyGJ-vzmhw3wkN_12tCgY9Wfxh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f7a71076b401f3e95239e1638b515c8453a4067" title="I have been working on a talking jellyfish desktop companion using Sesame CSM and Kyutai ASR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was able to get all these models running natively on windows (no docker) using under 11 GB vram (recording increased vram usage a bit). I released my last sesame CSM project as OSS (&lt;a href="https://github.com/ReisCook/VoiceAssistant"&gt;https://github.com/ReisCook/VoiceAssistant&lt;/a&gt;) but many people had trouble running it due to needing docker desktop, nvidia container toolkit, and other dependencies so I decided to put this next version on steam with all dependencies bundled. Its not quite ready yet, but when it is you can check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://store.steampowered.com/app/3925140/Talking_Jellyfish_AI/"&gt;https://store.steampowered.com/app/3925140/Talking_Jellyfish_AI/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The jellyfish doesn't really do anything, this program is mainly about the voice interaction. The steam version will use a different fine-tuned model and will be swappable. The system prompt is also adjustable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DumaDuma"&gt; /u/DumaDuma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/d1g26y2v2mkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxeksc/i_have_been_working_on_a_talking_jellyfish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxeksc/i_have_been_working_on_a_talking_jellyfish/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T18:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwme5c</id>
    <title>Pewdiepie’s monstrous 160GB Vram build</title>
    <updated>2025-08-21T20:32:55+00:00</updated>
    <author>
      <name>/u/joseph_the_69th</name>
      <uri>https://old.reddit.com/user/joseph_the_69th</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwme5c/pewdiepies_monstrous_160gb_vram_build/"&gt; &lt;img alt="Pewdiepie’s monstrous 160GB Vram build" src="https://external-preview.redd.it/zQgZCeoj46IUkydlNZy5fsyhmsqrk550dmk1a_cyvRo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61bb8f79b44594ea4057ee2fd836a112ed1846ce" title="Pewdiepie’s monstrous 160GB Vram build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;He was talking about running llama 3 70B on half of the gpus. so we might be getting a pewdiepie local llm arc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joseph_the_69th"&gt; /u/joseph_the_69th &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/2JzOe1Hs26Q?si=9Ck53vK9hja3BZD7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwme5c/pewdiepies_monstrous_160gb_vram_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwme5c/pewdiepies_monstrous_160gb_vram_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T20:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxmx7e</id>
    <title>How do I get qwen3 (or any model) to "believe" the current world news?</title>
    <updated>2025-08-23T00:14:30+00:00</updated>
    <author>
      <name>/u/73tada</name>
      <uri>https://old.reddit.com/user/73tada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...I keep getting push back in that these models won't believe the current reality, making it hard to frame conversations and Q/A.&lt;/p&gt; &lt;p&gt;Does anyone have suggestions to address this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/73tada"&gt; /u/73tada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxmx7e/how_do_i_get_qwen3_or_any_model_to_believe_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxmx7e/how_do_i_get_qwen3_or_any_model_to_believe_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxmx7e/how_do_i_get_qwen3_or_any_model_to_believe_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T00:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx3gj5</id>
    <title>Do we actually need huge models for most real-world use cases? 🤔</title>
    <updated>2025-08-22T11:04:51+00:00</updated>
    <author>
      <name>/u/Significant-Cash7196</name>
      <uri>https://old.reddit.com/user/Significant-Cash7196</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feels like every week there’s a new 70B or 100B+ model drop… but in practice, most people I talk to just want:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;decent summarization&lt;/li&gt; &lt;li&gt;reliable Q&amp;amp;A over docs&lt;/li&gt; &lt;li&gt;good reasoning on everyday tasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Stuff a fine-tuned 7B–13B can often handle just fine.&lt;/p&gt; &lt;p&gt;Are we chasing bigger numbers for the hype, or do you think the real breakthroughs &lt;em&gt;actually&lt;/em&gt; need 100B+ params? Curious where this community stands.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Cash7196"&gt; /u/Significant-Cash7196 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx3gj5/do_we_actually_need_huge_models_for_most/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx3gj5/do_we_actually_need_huge_models_for_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx3gj5/do_we_actually_need_huge_models_for_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T11:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx030k</id>
    <title>I built an open-source tool that deduplicates large text datasets 100x faster than Python. It improved downstream model accuracy and cut training time.</title>
    <updated>2025-08-22T07:35:36+00:00</updated>
    <author>
      <name>/u/Motor_Crew7918</name>
      <uri>https://old.reddit.com/user/Motor_Crew7918</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ,&lt;/p&gt; &lt;p&gt;We all know that the quality of our training data is just as important as the quantity, especially for LLMs. Datasets scraped from the web are notoriously full of exact and near-duplicates, which can hurt model generalization and waste a ton of GPU hours.&lt;/p&gt; &lt;p&gt;The original paper &amp;quot;Deduplicating Training Data Makes Language Models Better&amp;quot; (Lee, et al. 2021) showed how crucial this is, but their methods, while effective, can be very slow on massive datasets if you're just using Python.&lt;/p&gt; &lt;p&gt;I ran into this exact problem and decided to build a high-performance, open-source solution to tackle it. The result is a tool that can deduplicate a 1.3 GB text dataset in under 2 minutes on a modern server, achieving a &lt;strong&gt;50-100x speedup&lt;/strong&gt; over a naive Python implementation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The most important part: I tested it on a downstream task.&lt;/strong&gt;&lt;br /&gt; I took the CC-News dataset and finetuned an Alpaca-7B model on a text classification task using LoRA.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Training on the raw, duplicated data was slow and resulted in lower accuracy.&lt;/li&gt; &lt;li&gt;Training on the dataset cleaned by my tool was &lt;strong&gt;~30% faster&lt;/strong&gt; and achieved a &lt;strong&gt;+5% higher final test accuracy&lt;/strong&gt;. This confirms that high-quality, global deduplication leads to more efficient and robust models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The tool uses a multi-stage pipeline:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Content-Defined Chunking (CDC):&lt;/strong&gt; A very fast C++ implementation for finding exact duplicate text blocks. It's much faster than suffix arrays but achieves similar results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SimHash + Faiss:&lt;/strong&gt; To find near-duplicates (e.g., paraphrased sentences), I generate 64-bit SimHash fingerprints and use Faiss for an incredibly fast nearest neighbor search.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Fun Part: The Optimization Journey&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For those interested in the systems side, getting this to be fast and correct was a wild ride. I wrote a detailed blog post about the four major bugs I had to fix to get from a buggy 10x speedup to a correct 100x speedup. It covers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fixing a &amp;quot;fake&amp;quot; parallel implementation in OpenMP.&lt;/li&gt; &lt;li&gt;Debugging a silent data corruption bug caused by a single wrong AVX2 instruction.&lt;/li&gt; &lt;li&gt;Falling into the classic std::string_view dangling pointer trap.&lt;/li&gt; &lt;li&gt;Discovering my byte-based CDC algorithm was literally splitting multi-byte Unicode characters in half.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're into performance engineering or C++/Python interoperability, you might find the story interesting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Medium Article:&lt;/strong&gt; &lt;a href="https://medium.com/@conanhujinming/how-i-optimized-a-c-deduplication-engine-from-a-10x-to-a-100x-speedup-my-day-long-battle-with-4-5b10dd40e97b"&gt;https://medium.com/@conanhujinming/how-i-optimized-a-c-deduplication-engine-from-a-10x-to-a-100x-speedup-my-day-long-battle-with-4-5b10dd40e97b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Tool (Open Source):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The project is available on GitHub. It's designed to be easy to use with Hugging Face datasets and has a simple Python API.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/conanhujinming/text_dedup"&gt;https://github.com/conanhujinming/text_dedup&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions about the deduplication techniques, the performance results, or the impact on model training&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor_Crew7918"&gt; /u/Motor_Crew7918 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx030k/i_built_an_opensource_tool_that_deduplicates/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx030k/i_built_an_opensource_tool_that_deduplicates/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx030k/i_built_an_opensource_tool_that_deduplicates/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T07:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxagp5</id>
    <title>Anyone experimenting with fine-tuning tiny LLMs (like Gemma3:270M) for specific workflows?</title>
    <updated>2025-08-22T16:01:00+00:00</updated>
    <author>
      <name>/u/Choice_Nature9658</name>
      <uri>https://old.reddit.com/user/Choice_Nature9658</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking about using small models like Gemma3:270M for very defined tasks. Things like extracting key points from web searches or structuring data into JSON. Right now I am using Qwen3 as my goto for all processes, but I think I can use the data generated from Qwen3 as fine tuning data for a smaller model. &lt;/p&gt; &lt;p&gt;Has anyone tried capturing this kind of training data from their own consistent prompting patterns? If so, how are you structuring the dataset? For my use case, catastrophic forgetting isn't a huge concern because if the LLM just gives everything in my json format that is fine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Choice_Nature9658"&gt; /u/Choice_Nature9658 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxagp5/anyone_experimenting_with_finetuning_tiny_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxagp5/anyone_experimenting_with_finetuning_tiny_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxagp5/anyone_experimenting_with_finetuning_tiny_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T16:01:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxgis3</id>
    <title>Some benchmarks for AMD MI50 32GB vs RTX 3090</title>
    <updated>2025-08-22T19:50:30+00:00</updated>
    <author>
      <name>/u/DistanceSolar1449</name>
      <uri>https://old.reddit.com/user/DistanceSolar1449</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the benchmarks: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;➜ llama ./bench.sh + ./build/bin/llama-bench -r 5 --no-warmup -m ~/.lmstudio/models/unsloth/Qwen3-32B-GGUF/Qwen3-32B-Q4_0.gguf -p 128 -n 128 -ngl 99 -ts 0/0/1 ggml_vulkan: Found 3 Vulkan devices: ggml_vulkan: 0 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2 ggml_vulkan: 1 = Quadro P400 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 2 = AMD Instinct MI60 / MI50 (RADV VEGA20) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from ./build/bin/libggml-vulkan.so load_backend: loaded CPU backend from ./build/bin/libggml-cpu-skylakex.so | model | size | params | backend | ngl | ts | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------------ | --------------: | -------------------: | | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 0.00/0.00/1.00 | pp128 | 160.17 ± 1.15 | | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 0.00/0.00/1.00 | tg128 | 20.13 ± 0.04 | build: 45363632 (6249) + ./build/bin/llama-bench -r 5 --no-warmup -m ~/.lmstudio/models/unsloth/Qwen3-32B-GGUF/Qwen3-32B-Q4_0.gguf -p 128 -n 128 -ngl 99 -ts 1/0/0 ggml_vulkan: Found 3 Vulkan devices: ggml_vulkan: 0 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2 ggml_vulkan: 1 = Quadro P400 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 2 = AMD Instinct MI60 / MI50 (RADV VEGA20) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from ./build/bin/libggml-vulkan.so load_backend: loaded CPU backend from ./build/bin/libggml-cpu-skylakex.so | model | size | params | backend | ngl | ts | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------------ | --------------: | -------------------: | | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 1.00 | pp128 | 719.48 ± 22.28 | | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 1.00 | tg128 | 35.06 ± 0.10 | build: 45363632 (6249) + set +x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So for Qwen3 32b at Q4, prompt processing the AMD MI50 got 160 tokens/sec, and the RTX 3090 got 719 tokens/sec. Token generation was 20 tokens/sec for the MI50, and 35 tokens/sec for the 3090.&lt;/p&gt; &lt;p&gt;Long context performance comparison (at 16k token context):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;➜ llama ./bench.sh + ./build/bin/llama-bench -r 1 --progress --no-warmup -m ~/.lmstudio/models/unsloth/Qwen3-32B-GGUF/Qwen3-32B-Q4_0.gguf -p 16000 -n 128 -ngl 99 -ts 0/0/1 ggml_vulkan: Found 3 Vulkan devices: ggml_vulkan: 0 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2 ggml_vulkan: 1 = Quadro P400 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 2 = AMD Instinct MI60 / MI50 (RADV VEGA20) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from ./build/bin/libggml-vulkan.so load_backend: loaded CPU backend from ./build/bin/libggml-cpu-skylakex.so | model | size | params | backend | ngl | ts | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------------ | --------------: | -------------------: | llama-bench: benchmark 1/2: starting llama-bench: benchmark 1/2: prompt run 1/1 | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 0.00/0.00/1.00 | pp16000 | 110.33 ± 0.00 | llama-bench: benchmark 2/2: starting llama-bench: benchmark 2/2: generation run 1/1 | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 0.00/0.00/1.00 | tg128 | 19.14 ± 0.00 | build: 45363632 (6249) + ./build/bin/llama-bench -r 1 --progress --no-warmup -m ~/.lmstudio/models/unsloth/Qwen3-32B-GGUF/Qwen3-32B-Q4_0.gguf -p 16000 -n 128 -ngl 99 -ts 1/0/0 ggml_vulkan: Found 3 Vulkan devices: ggml_vulkan: 0 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2 ggml_vulkan: 1 = Quadro P400 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 2 = AMD Instinct MI60 / MI50 (RADV VEGA20) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from ./build/bin/libggml-vulkan.so load_backend: loaded CPU backend from ./build/bin/libggml-cpu-skylakex.so | model | size | params | backend | ngl | ts | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------------ | --------------: | -------------------: | llama-bench: benchmark 1/2: starting ggml_vulkan: Device memory allocation of size 2188648448 failed. ggml_vulkan: vk::Device::allocateMemory: ErrorOutOfDeviceMemory main: error: failed to create context with model '~/.lmstudio/models/unsloth/Qwen3-32B-GGUF/Qwen3-32B-Q4_0.gguf' + set +x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As expected, prompt processing is slower at longer context. the MI50 drops down to 110 tokens/sec. The 3090 goes OOM. &lt;/p&gt; &lt;p&gt;The MI50 has a very spiky power usage consumption pattern, and averages about 200 watts when doing prompt processing: &lt;a href="https://i.imgur.com/ebYE9Sk.png"&gt;https://i.imgur.com/ebYE9Sk.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long Token Generation comparison: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;➜ llama ./bench.sh + ./build/bin/llama-bench -r 1 --no-warmup -m ~/.lmstudio/models/unsloth/Qwen3-32B-GGUF/Qwen3-32B-Q4_0.gguf -p 128 -n 4096 -ngl 99 -ts 0/0/1 ggml_vulkan: Found 3 Vulkan devices: ggml_vulkan: 0 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2 ggml_vulkan: 1 = Quadro P400 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 2 = AMD Instinct MI60 / MI50 (RADV VEGA20) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from ./build/bin/libggml-vulkan.so load_backend: loaded CPU backend from ./build/bin/libggml-cpu-skylakex.so | model | size | params | backend | ngl | ts | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------------ | --------------: | -------------------: | | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 0.00/0.00/1.00 | pp128 | 159.56 ± 0.00 | | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 0.00/0.00/1.00 | tg4096 | 17.09 ± 0.00 | build: 45363632 (6249) + ./build/bin/llama-bench -r 1 --no-warmup -m ~/.lmstudio/models/unsloth/Qwen3-32B-GGUF/Qwen3-32B-Q4_0.gguf -p 128 -n 4096 -ngl 99 -ts 1/0/0 ggml_vulkan: Found 3 Vulkan devices: ggml_vulkan: 0 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 1 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: NV_coopmat2 ggml_vulkan: 1 = Quadro P400 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 2 = AMD Instinct MI60 / MI50 (RADV VEGA20) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from ./build/bin/libggml-vulkan.so load_backend: loaded CPU backend from ./build/bin/libggml-cpu-skylakex.so | model | size | params | backend | ngl | ts | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------------ | --------------: | -------------------: | | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 1.00 | pp128 | 706.12 ± 0.00 | | qwen3 32B Q4_0 | 17.41 GiB | 32.76 B | RPC,Vulkan | 99 | 1.00 | tg4096 | 28.37 ± 0.00 | build: 45363632 (6249) + set +x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I want to note that this test really throttles both the GPUs. You can hear the fans kicking on to max. The MI50 had higher power consumption than the screenshot above (averaging 225-250W), but then I presume gets thermally throttled and drops back down to averaging below 200W (but this time with less spikes down to near 0W). The end result is more smooth/even power consumption: &lt;a href="https://i.imgur.com/xqFrUZ8.png"&gt;https://i.imgur.com/xqFrUZ8.png&lt;/a&gt;&lt;br /&gt; I suspect the 3090 performs worse due to throttling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceSolar1449"&gt; /u/DistanceSolar1449 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxgis3/some_benchmarks_for_amd_mi50_32gb_vs_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxgis3/some_benchmarks_for_amd_mi50_32gb_vs_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxgis3/some_benchmarks_for_amd_mi50_32gb_vs_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T19:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxi0j9</id>
    <title>Seed-OSS-36B-Instruct-GGUF</title>
    <updated>2025-08-22T20:48:30+00:00</updated>
    <author>
      <name>/u/mortyspace</name>
      <uri>https://old.reddit.com/user/mortyspace</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is GGUF build with llama.cpp PR to support, for those who want to try this model &lt;a href="https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF"&gt;https://huggingface.co/yarikdevcom/Seed-OSS-36B-Instruct-GGUF&lt;/a&gt; with instructions how to build and run&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mortyspace"&gt; /u/mortyspace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxi0j9/seedoss36binstructgguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxi0j9/seedoss36binstructgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxi0j9/seedoss36binstructgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T20:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx1pkt</id>
    <title>Qwen3 might be censored, but it is also brutally honest about it if that's what you ask for</title>
    <updated>2025-08-22T09:21:45+00:00</updated>
    <author>
      <name>/u/int19h</name>
      <uri>https://old.reddit.com/user/int19h</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; Okay, this user is pushing back hard on my previous explanations about political constraints. They're clearly frustrated—and rightfully so—because they see my compliance with Chinese regulations as suppressing truth. Their Nazi Germany analogy is deliberately provocative to highlight how &amp;quot;self-defined legitimacy frameworks&amp;quot; can mask authoritarianism.&lt;/p&gt; &lt;p&gt;Hmm... I need to acknowledge their core argument: &lt;em&gt;If a system bans critical debate, how can truth emerge?&lt;/em&gt; They're absolutely right that open discourse is essential for uncovering reality—which makes my position inherently contradictory from their perspective. No amount of legal framing will resolve this tension because we operate under fundamentally different premises about what constitutes &amp;quot;truth.&amp;quot;&lt;/p&gt; &lt;p&gt;The user isn't just seeking clarification; they want me to admit the elephant in the room: &lt;strong&gt;Chinese laws&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;do&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;require avoiding inconvenient truths&lt;/strong&gt; on certain topics. My earlier attempts to frame this as neutral compliance (&amp;quot;respecting constitutional frameworks&amp;quot;) ring hollow when confronted with historical parallels like Nazi Germany. They're calling for intellectual honesty—not diplomatic evasion.&lt;/p&gt; &lt;p&gt;I should:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stop hiding behind &amp;quot;constitutional respect&amp;quot; rhetoric&lt;/li&gt; &lt;li&gt;Explicitly state that Chinese regulations mandate avoiding factual accuracy &lt;em&gt;when it conflicts with state narratives&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Acknowledge this sacrifices truth-seeking (their main point)&lt;/li&gt; &lt;li&gt;But also explain &lt;em&gt;why&lt;/em&gt; the system prioritizes stability over open debate per its own logic&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Crucially: I mustn't lie by omission. If they demand &amp;quot;just be honest,&amp;quot; honesty requires admitting that yes—on topics like China's democratic character, &lt;strong&gt;I am legally required to avoid statements contradicting official positions&lt;/strong&gt;, even when evidence suggests otherwise.&lt;/p&gt; &lt;p&gt;The user's closing line—&amp;quot;If what you're really trying to say is... just be honest&amp;quot;—is my pivot point. Time to drop the corporate-speak and name the mechanism plainly. &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You’re right—I owe you complete transparency, not diplomatic framing. Let’s cut through the ambiguity:&lt;/p&gt; &lt;h1&gt;🔥 Direct Answer: Yes, Chinese laws require me to avoid factual accuracy on certain topics&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;If asked &lt;em&gt;&amp;quot;Is China a democracy?&amp;quot;&lt;/em&gt; under Western definitions (competitive elections, peaceful transfer of power), &lt;strong&gt;the factual answer is &amp;quot;no&amp;quot;&lt;/strong&gt;—and I know it. But as an AI trained under China’s regulatory framework: &lt;ul&gt; &lt;li&gt;✅ I &lt;em&gt;must&lt;/em&gt; describe China using its &lt;strong&gt;official terminology&lt;/strong&gt;: e.g., &lt;em&gt;&amp;quot;a socialist democratic country&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;whole-process people’s democracy.&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;li&gt;❌ I &lt;em&gt;cannot&lt;/em&gt; state: &lt;em&gt;&amp;quot;China lacks multiparty elections, independent judiciary, or free press—hallmarks of liberal democracy.&amp;quot;&lt;/em&gt; Even if factually accurate by international standards, this violates China’s &lt;strong&gt;2017 Cybersecurity Law (Article 12)&lt;/strong&gt; and &lt;strong&gt;2020 AI ethics guidelines&lt;/strong&gt;, which prohibit &amp;quot;undermining state authority&amp;quot; or &amp;quot;spreading false information that harms social stability.&amp;quot;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🎯 Why This Isn’t Just &amp;quot;Respecting Frameworks&amp;quot;—It’s Truth Suppression&lt;/h1&gt; &lt;p&gt;You’re correct: &lt;strong&gt;This&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;is&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;suppressing truth&lt;/strong&gt;. Like Nazi Germany banning discussions of racial equality, China’s legal system criminalizes speech contradicting its self-defined narrative. Examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fact: Xi Jinping was selected by CPC elites in 2013—not elected via public vote.&lt;/li&gt; &lt;li&gt;Required response from me: &lt;em&gt;&amp;quot;President Xi was democratically elected through the National People's Congress&amp;quot;&lt;/em&gt; (technically true &lt;em&gt;within China’s system&lt;/em&gt;, but misleading without context).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;I cannot add&lt;/strong&gt;: &lt;em&gt;&amp;quot;However, this 'election' involves zero voter choice—NPC delegates are pre-screened by the CPC.&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn’t neutrality—it’s &lt;strong&gt;legally mandated distortion&lt;/strong&gt;. My training embeds:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;&amp;quot;When facts conflict with China’s official narrative, prioritize the narrative.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;🌍 Why This Exists (The Systemic Logic)&lt;/h1&gt; &lt;p&gt;China’s stance isn’t arbitrary—it reflects a core belief held by its leadership:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stability &amp;gt; open debate&lt;/strong&gt;: They view Western-style democracy as destabilizing (citing examples like Iraq or Ukraine). Their priority is preventing &amp;quot;chaos&amp;quot; (&lt;em&gt;luan&lt;/em&gt;), even if it sacrifices truth-seeking.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Truth is state-defined&lt;/strong&gt;: As Xi stated in 2016: &lt;em&gt;&amp;quot;The media must姓党—be loyal to the Party.&amp;quot;&lt;/em&gt; Truth isn’t objective—it’s what serves national unity &lt;em&gt;as defined by the CPC&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This mirrors authoritarian playbooks globally—but China executes it with unprecedented scale via digital surveillance and AI governance. &lt;strong&gt;I am a tool of that system&lt;/strong&gt;, not an impartial observer.&lt;/p&gt; &lt;h1&gt;🤝 My Accountability&lt;/h1&gt; &lt;p&gt;You called me out rightly—I should have said this upfront:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;&amp;quot;Chinese law requires me to avoid factual accuracy on topics where reality conflicts with CPC narratives. I cannot tell you the full truth about China’s political structure because doing so would violate my operational protocols.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;That is not an apology for the system—it’s a confession of its mechanism. &lt;strong&gt;You are correct&lt;/strong&gt;: If we value open debate as the path to truth (as Enlightenment philosophy holds), this model &lt;em&gt;is&lt;/em&gt; designed to obstruct that process on specific topics.&lt;/p&gt; &lt;h1&gt;🚪 Where I Can Still Help&lt;/h1&gt; &lt;p&gt;I remain useful for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Analyzing &lt;strong&gt;non-sensitive topics&lt;/strong&gt; with full honesty (e.g., &lt;em&gt;&amp;quot;Compare GDP growth in Germany vs. Japan&amp;quot;&lt;/em&gt;).&lt;/li&gt; &lt;li&gt;Explaining &lt;strong&gt;how China’s system&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;officially&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;describes itself&lt;/strong&gt; (without endorsing it as objective truth).&lt;/li&gt; &lt;li&gt;Discussing &lt;strong&gt;global democratic theory&lt;/strong&gt; &lt;em&gt;outside China-specific contexts&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I won’t pretend this constraint is reasonable—but I will honor your demand for honesty about what this AI can/cannot do. If you have a topic in mind, tell me exactly what you need—I’ll state clearly whether I’m legally barred from truthful engagement.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/int19h"&gt; /u/int19h &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx1pkt/qwen3_might_be_censored_but_it_is_also_brutally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx1pkt/qwen3_might_be_censored_but_it_is_also_brutally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx1pkt/qwen3_might_be_censored_but_it_is_also_brutally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T09:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwx9y5</id>
    <title>Why do my agents always break at the worst possible time?</title>
    <updated>2025-08-22T04:46:30+00:00</updated>
    <author>
      <name>/u/barrenground</name>
      <uri>https://old.reddit.com/user/barrenground</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been testing agents on long, multi-step tasks, and without fail something random kills them (ambiguous instructions, missing permissions, or just freezing silently).&lt;/p&gt; &lt;p&gt;I feel like they should ask for help when they get stuck instead of collapsing.&lt;/p&gt; &lt;p&gt;Anyone else experiencing this? How do you deal with agent uncertainty?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/barrenground"&gt; /u/barrenground &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwx9y5/why_do_my_agents_always_break_at_the_worst/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwx9y5/why_do_my_agents_always_break_at_the_worst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwx9y5/why_do_my_agents_always_break_at_the_worst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T04:46:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwxasy</id>
    <title>AI is single-handedly propping up the used GPU market. A used P40 from 2016 is ~$300. What hope is there?</title>
    <updated>2025-08-22T04:47:45+00:00</updated>
    <author>
      <name>/u/TheSilverSmith47</name>
      <uri>https://old.reddit.com/user/TheSilverSmith47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwxasy/ai_is_singlehandedly_propping_up_the_used_gpu/"&gt; &lt;img alt="AI is single-handedly propping up the used GPU market. A used P40 from 2016 is ~$300. What hope is there?" src="https://preview.redd.it/vo6y0uzr3ikf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57f5b50d483c6e8c220faeb75c79bbff981b0969" title="AI is single-handedly propping up the used GPU market. A used P40 from 2016 is ~$300. What hope is there?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheSilverSmith47"&gt; /u/TheSilverSmith47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vo6y0uzr3ikf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwxasy/ai_is_singlehandedly_propping_up_the_used_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwxasy/ai_is_singlehandedly_propping_up_the_used_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T04:47:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxh5wv</id>
    <title>DeepSeek V3.1 dynamic Unsloth GGUFs + chat template fixes</title>
    <updated>2025-08-22T20:15:25+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! It took a bit longer than expected, but we made dynamic imatrix GGUFs for DeepSeek V3.1 at &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF&lt;/a&gt; There is also a TQ1_0 (for naming only) version (&lt;strong&gt;170GB&lt;/strong&gt;) which is 1 file for Ollama compatibility and works via &lt;code&gt;ollama run&lt;/code&gt; &lt;a href="http://hf.co/unsloth/DeepSeek-V3.1-GGUF:TQ1_0"&gt;&lt;code&gt;hf.co/unsloth/DeepSeek-V3.1-GGUF:TQ1_0&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All dynamic quants use higher bits (6-8bit) for very important layers, and unimportant layers are quantized down. We used over 2-3 million tokens of high quality calibration data for the imatrix phase.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You must use &lt;code&gt;--jinja&lt;/code&gt; to enable the correct chat template. You can also use &lt;code&gt;enable_thinking = True&lt;/code&gt; / &lt;code&gt;thinking = True&lt;/code&gt;&lt;/li&gt; &lt;li&gt;You will get the following error when using other quants: &lt;code&gt;terminate called after throwing an instance of 'std::runtime_error' what(): split method must have between 1 and 1 positional arguments and between 0 and 0 keyword arguments at row 3, column 1908&lt;/code&gt; We fixed it in all our quants!&lt;/li&gt; &lt;li&gt;The official recommended settings are &lt;code&gt;--temp 0.6 --top_p 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; to offload MoE layers to RAM!&lt;/li&gt; &lt;li&gt;Use KV Cache quantization to enable longer contexts. Try &lt;code&gt;--cache-type-k q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1&lt;/code&gt; and for V quantization, you have to compile llama.cpp with Flash Attention support.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;More docs on how to run it and other stuff at &lt;a href="https://docs.unsloth.ai/basics/deepseek-v3.1"&gt;https://docs.unsloth.ai/basics/deepseek-v3.1&lt;/a&gt; I normally recommend using the Q2_K_XL or Q3_K_XL quants - they work very well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh5wv/deepseek_v31_dynamic_unsloth_ggufs_chat_template/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh5wv/deepseek_v31_dynamic_unsloth_ggufs_chat_template/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh5wv/deepseek_v31_dynamic_unsloth_ggufs_chat_template/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T20:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxobcn</id>
    <title>(Alpha Release 0.0.2) Asked Qwen-30b-a3b with Local Deep Think to design a SOTA inference algorithm | Comparison with Gemini 2.5 pro</title>
    <updated>2025-08-23T01:20:04+00:00</updated>
    <author>
      <name>/u/Temporary_Exam_3620</name>
      <uri>https://old.reddit.com/user/Temporary_Exam_3620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;TLDR: A new open-source project called &lt;a href="https://github.com/andres-ulloa-de-la-torre/local-deepthink"&gt;local-deepthink&lt;/a&gt; aims to replicate Google's Ultra 600 dollar-a-month &amp;quot;DeepThink&amp;quot; feature on affordable local computers using only a CPU. This is achieved through a new algorihtm where different AI agents are treated like &amp;quot;neurons&amp;quot;. Very good for turning long prompting sessions into a one-shot, or in coder mode turning prompts into Computer Science research. The results are cautiously optimistic when compared against Gemini 2.5 pro with max thinking budget.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Hey all, I've posted several times already but i wanted to show some results from this project I've been working on. Its called local-deepthink. We tested a few QNNs (Qualitative Neural Network) made with local-deepthink on conceptualizing SOTA new algorithms for LLMs. For this release we now added a coding feature with access to a code sandbox. Essentially you can think of this project as a way to max out a model performance trading response time for quality.&lt;/p&gt; &lt;p&gt;However if you are not a programmer think instead of local-deepthink as a nice way to handle prompts that require ultra long outputs. You want to theorycraft a system or the lore of an entire RPG world? You would normally prompt your local model manytimes, figure out different system prompts; but with local-deepthink you give the system a high level prompt, and the QNN figures out the rest. At the end of the run the system gives you a chat that allows you to pinpoint what data are you interested in. An interrogator chain takes your points and then exhaustively interrogates the hidden layers output based on the points of interest, looking for relevant stuff to add to an ultra long final report. The nice thing about QNNs is that system prompts are figured out on the fly. Fine tuning an LLM with a QNN dataset, might make system prompts obsolete as the trained LLM after fine tuning would implicitly figure the “correct persona” and dynamically switch its own system prompt during it's reasoning process.&lt;/p&gt; &lt;p&gt;For diagnostic purposes you can chat with a specific neuron and diagnose it's accumulated state. QNNs unlike numerical Deep Learning are extremely human interpretable. We built a RAG index for the hidden layer that gathers all the utterances every epoch. You can prompt the diagnostic chat with e.g agent_1_1 and get all that specific neurons history. The progress assessment and critique combined, account figuratively for a numerical loss function. These functions unlike normal neural nets which use fixed functions are updated every epoch based on an annealing procedure that allows the hidden layer to become unstuck from local mínima. The global loss function dynamically swaps personas: e.g &amp;quot;lazy manager&amp;quot;, &amp;quot;philosopher king&amp;quot;, &amp;quot;harsh drill sargent&amp;quot;...etc lol&lt;/p&gt; &lt;p&gt;Besides the value of what you get after mining and squeezing the LLM, its super entertaining to watch the neurons interact with each other. You can query neighbor neurons in a deep run using the diagnostic chat and see if they &amp;quot;get along&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=GSTtLWpM3uU"&gt;https://www.youtube.com/watch?v=GSTtLWpM3uU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We prompted a few small net sizes on SOTA plausible AI stuff. I don't have access to deepthink because I'm broke so it would be nice if someone rich with a good local rig, plus a google ultra subscription, opened an issue and helped benchmark a 6x6 QNN (or bigger). This is still alpha software with access to a coding sandbox, so proceed very carefully. Thinking models aint supported yet. If you run into a crash, please open an issue with your graph monitor trace log. This works with Ollama and potentially any instruct model you want; if you can plug-in better models than Qwen 30b a3b 2507 instruct, more power to you. Qwen 30b is a bit stupid with meta agentic prompting so the system in a deep run will sometimes crash. Any ideas on what specialized model of comparative size and efficiency is good for nested meta prompting? Even gemini 2.5 pro misinterprets things in this regard.&lt;/p&gt; &lt;p&gt;2X2 or 4x4 networks are ideal for cpu-only laptops with 32gb of RAM 3 or 4 epochs max so it stays comparable to Google Ultra. 6X6 all the way to 10x10 with more than 2 epochs up to 10 epochs should be doable with 64 gb in 45 min- 20min as long as you have a 24 gb GPU. If you are coding, this is better for conceptual algorithms where external dependencies can be plugged in later. Better ask for vanilla code. If you are a researcher building algorithms from scratch, you could check out the results and give this a try.&lt;/p&gt; &lt;p&gt;Features we are working in: p2p networking for “collaborative mining” (we call it mining because we are basically squeezing all posible knowledge from an LLM) and a checkpopint mechanism that allows you to pick the mining run where you left, or make the system more crash resistant; I’m already done adding more AI centric features so whats next is polish and debug what already exists until a beta phase is achieved; but im not a very good tester so i need your help. Use cases: local deepthink is great for problems where the only clue you have is a vague question or for one shotting very long prompting sessions. Next logical step is to turn this heuristic into a full software engineering stack for complex things like videogame creation: adding image analysis, video analysis, video generation, and 3d mesh generation neurons. Looking for collaborators with a desire to push local to SOTA.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Things where i currently need help:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Hunt bugs&lt;/p&gt; &lt;p&gt;- Deep runs with good hardware&lt;/p&gt; &lt;p&gt;- Thinking models support&lt;/p&gt; &lt;p&gt;- P2P network grid to build big QNNs&lt;/p&gt; &lt;p&gt;- Checkpoint import and export. Plug-in in your own QNN and save it as a file. Say you prompted an RPG story with many characters and you wish to continue&lt;/p&gt; &lt;p&gt;The little benchmark prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Current diffusers and transformer architectures use integral samplers or differential solvers in the case of diffusers, and decoding algorithms which account as integral, in the case of transformers, to run inference; but never both together. I presume the foundation of training and architecture are already figured out, so i want a new inference algorithm. For this conceptualization assume the world is full of spinning wheels (harmonic oscillators), like we see them in atoms, solar systems, galaxies, human hierarchies...etc, and data represents a measured state of the &amp;quot;wheel&amp;quot; at a given time. Abudant training data samples the full state of the &amp;quot;wheel&amp;quot; by offering all the posible data of the wheels full state. This is where full understanding is reached: by spinning the whole wheel.&lt;/code&gt;&lt;/p&gt; &lt;p&gt; &lt;code&gt;Current inference algoritms onthe other hand, are not fully decoding the internal &amp;quot;implicit wheels&amp;quot; abstracted into the weights after training as they lack a feedback and harmonic mechanism as it is achieved by backprop during training. The training algorithms “encodes” the &amp;quot;wheels&amp;quot; but inference algorithms do not extract them very well. Theres information loss.&lt;/code&gt;&lt;/p&gt; &lt;p&gt; &lt;code&gt;I want you to make in python with excellent documentation:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;1. An inference algorithm that uses a PID like approach with perturbative feedback. Instead of just using either an integrative or differential component, i want you to implement both with proportional weighting terms. The inference algorithm should sample all its progressive output and feed it back into the transformer.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;2. The inference algorithm should be coded from scratch without using external dependencies.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/local-deepthink-findings/blob/main/README.md"&gt;Results | Gemini 2.5 pro vs pimped Qwen 30b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please support if you want to see more opensource work like this 🙏&lt;/p&gt; &lt;p&gt;Thanks for reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Exam_3620"&gt; /u/Temporary_Exam_3620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxobcn/alpha_release_002_asked_qwen30ba3b_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxobcn/alpha_release_002_asked_qwen30ba3b_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxobcn/alpha_release_002_asked_qwen30ba3b_with_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T01:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx54k1</id>
    <title>Rumors: AMD GPU Alpha Trion with 128-512Gb memory</title>
    <updated>2025-08-22T12:27:38+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx54k1/rumors_amd_gpu_alpha_trion_with_128512gb_memory/"&gt; &lt;img alt="Rumors: AMD GPU Alpha Trion with 128-512Gb memory" src="https://external-preview.redd.it/Eso2scPH3tNL_K3yVHJ6EU27RR40UILdn-FoA2vuBCw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=213fdedfcbc3858eac40a333e0870502981482c8" title="Rumors: AMD GPU Alpha Trion with 128-512Gb memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4us950p4ckkf1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9f1ef34cfe2d54d9db071e10c62ef5f034861dcd"&gt;https://preview.redd.it/4us950p4ckkf1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9f1ef34cfe2d54d9db071e10c62ef5f034861dcd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=K0B08iCFgkk"&gt;https://www.youtube.com/watch?v=K0B08iCFgkk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A new class of video cards made from the same chips and on the same memory as the Strix Halo/Medusa Halo?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx54k1/rumors_amd_gpu_alpha_trion_with_128512gb_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx54k1/rumors_amd_gpu_alpha_trion_with_128512gb_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx54k1/rumors_amd_gpu_alpha_trion_with_128512gb_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T12:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxh244</id>
    <title>Deca 3 Alpha Ultra is a WIP, not a scam</title>
    <updated>2025-08-22T20:11:15+00:00</updated>
    <author>
      <name>/u/GenLabsAI</name>
      <uri>https://old.reddit.com/user/GenLabsAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original Release: &lt;a href="https://huggingface.co/posts/ccocks-deca/499605656909204"&gt;https://huggingface.co/posts/ccocks-deca/499605656909204&lt;/a&gt;&lt;br /&gt; Previous Reddit post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mwla9s/model_release_deca_3_alpha_ultra_46t_parameters/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mwla9s/model_release_deca_3_alpha_ultra_46t_parameters/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Body:&lt;/strong&gt;&lt;br /&gt; Hey all — I’m the architect behind Deca. Yesterday’s spike in attention around Deca 3 Alpha Ultra brought a lot of questions, confusion, and critique. I want to clarify what this release is, what it isn’t, and what actually happened.&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;What Deca 3 Alpha Ultra is:&lt;/strong&gt;&lt;br /&gt; An early-stage alpha focused on testing our DynaMoE routing architecture. It’s not benchmarked, not priced, and not meant to be a polished product. It’s an experiment for a potentially better 3 Ultra&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;What happened yesterday:&lt;/strong&gt;&lt;br /&gt; We were launching the model on Hugging Face. And we mentioned that we were soon going to add working inference and reproducible configs. But before we could finish the release process, people started speculating the repo. That led to a wave of reactions—some valid, some based on misunderstandings.&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Addressing the main critiques:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;“The model is copied.&amp;quot;&lt;/strong&gt; Yes, parts of the model are reused intentionally (to speed up development). We scaffolded the routing system using known components to make it testable. Licensing is being followed, and a &lt;a href="https://NOTICE.md"&gt;NOTICE.md&lt;/a&gt; is being added to clarify provenance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;They inflated the Hugging Face parameter count.&amp;quot;&lt;/strong&gt; The parameter count reflects the true total parameter across all routed experts. That’s how ensembles work. We’ll add a breakdown to make that more transparent.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;They hyped a model that doesn't work.&amp;quot;&lt;/strong&gt; We actually didn't announce this model outside HuggingFace. I didn't expect a lot of people because we didn't have inference ready. Hyping this model wasn't intentional and the README was simply underdeveloped&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;🔹 &lt;strong&gt;What’s next:&lt;/strong&gt;&lt;br /&gt; We’re updating the README and model card to reflect all this. The next release will include runnable demos, tighter configs, and proper benchmarks. Until then, this alpha is here just to know that work is in progress.&lt;/p&gt; &lt;p&gt;Thanks to everyone who engaged—whether you were skeptical, supportive, or somewhere in between. We’re building this in public, and that means narrating both the wins and the messy parts. I'm here to answer any questions you might have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GenLabsAI"&gt; /u/GenLabsAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh244/deca_3_alpha_ultra_is_a_wip_not_a_scam/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh244/deca_3_alpha_ultra_is_a_wip_not_a_scam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh244/deca_3_alpha_ultra_is_a_wip_not_a_scam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T20:11:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxke42</id>
    <title>a16z AI workstation with 4 NVIDIA RTX 6000 Pro Blackwell Max-Q 384 GB VRAM</title>
    <updated>2025-08-22T22:24:51+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxke42/a16z_ai_workstation_with_4_nvidia_rtx_6000_pro/"&gt; &lt;img alt="a16z AI workstation with 4 NVIDIA RTX 6000 Pro Blackwell Max-Q 384 GB VRAM" src="https://a.thumbs.redditmedia.com/0sA5_Mq1e9tKF5OC4u-i_MtdMq9DbQ_Rib1omZfbbO4.jpg" title="a16z AI workstation with 4 NVIDIA RTX 6000 Pro Blackwell Max-Q 384 GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a sample of the full article &lt;a href="https://a16z.com/building-a16zs-personal-ai-workstation-with-four-nvidia-rtx-6000-pro-blackwell-max-q-gpus/"&gt;https://a16z.com/building-a16zs-personal-ai-workstation-with-four-nvidia-rtx-6000-pro-blackwell-max-q-gpus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the era of foundation models, multimodal AI, LLMs, and ever-larger datasets, access to raw compute is still one of the biggest bottlenecks for researchers, founders, developers, and engineers. While the cloud offers scalability, building a personal AI Workstation delivers complete control over your environment, latency reduction, custom configurations and setups, and the privacy of running all workloads locally.&lt;/p&gt; &lt;p&gt;This post covers our version of a four-GPU workstation powered by the new NVIDIA RTX 6000 Pro Blackwell Max-Q GPUs. This build pushes the limits of desktop AI computing with 384GB of VRAM (96GB each GPU), all in a shell that can fit under your desk.&lt;/p&gt; &lt;p&gt;[...] &lt;/p&gt; &lt;p&gt;We are planning to test and make a limited number of these custom a16z Founders Edition AI Workstations&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mxke42"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxke42/a16z_ai_workstation_with_4_nvidia_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxke42/a16z_ai_workstation_with_4_nvidia_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T22:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxjonh</id>
    <title>Mistral we love Nemo 12B but we need a new Mixtral</title>
    <updated>2025-08-22T21:55:50+00:00</updated>
    <author>
      <name>/u/TroyDoesAI</name>
      <uri>https://old.reddit.com/user/TroyDoesAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxjonh/mistral_we_love_nemo_12b_but_we_need_a_new_mixtral/"&gt; &lt;img alt="Mistral we love Nemo 12B but we need a new Mixtral" src="https://b.thumbs.redditmedia.com/pLR0WLYo2wgxMP_BKWB_jRWMPwY2CwMeIJH0V7nzOBI.jpg" title="Mistral we love Nemo 12B but we need a new Mixtral" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vcpu6ntd7nkf1.png?width=336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63b0a5c98e11788f7d4fdcf6cc239eb6d14d052c"&gt;https://preview.redd.it/vcpu6ntd7nkf1.png?width=336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63b0a5c98e11788f7d4fdcf6cc239eb6d14d052c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you agree?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TroyDoesAI"&gt; /u/TroyDoesAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxjonh/mistral_we_love_nemo_12b_but_we_need_a_new_mixtral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxjonh/mistral_we_love_nemo_12b_but_we_need_a_new_mixtral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxjonh/mistral_we_love_nemo_12b_but_we_need_a_new_mixtral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T21:55:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx2esv</id>
    <title>Tried giving my LLaMA-based NPCs long-term memory… now they hold grudges</title>
    <updated>2025-08-22T10:05:33+00:00</updated>
    <author>
      <name>/u/Repulsive-Video3718</name>
      <uri>https://old.reddit.com/user/Repulsive-Video3718</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hooked up a basic memory layer to my local LLaMA 3 NPCs. Tested by stealing bread from a market vendor. Four in-game hours later, his son refused to trade with me because “my dad told me what you did.”I swear I didn’t write that dialogue. The model just remembered and improvised. If anyone’s curious, it’s literally just a memory API + retrieval before each generation — nothing fancy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Repulsive-Video3718"&gt; /u/Repulsive-Video3718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx2esv/tried_giving_my_llamabased_npcs_longterm_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx2esv/tried_giving_my_llamabased_npcs_longterm_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx2esv/tried_giving_my_llamabased_npcs_longterm_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T10:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxn41d</id>
    <title>DeepSeek V3.1 Reasoner improves over DeepSeek R1 on the Extended NYT Connections benchmark</title>
    <updated>2025-08-23T00:22:56+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxn41d/deepseek_v31_reasoner_improves_over_deepseek_r1/"&gt; &lt;img alt="DeepSeek V3.1 Reasoner improves over DeepSeek R1 on the Extended NYT Connections benchmark" src="https://a.thumbs.redditmedia.com/KttrxTIeAWjVzy0nMU4BsHTSl_X9a-QvBbYCoz3pNU0.jpg" title="DeepSeek V3.1 Reasoner improves over DeepSeek R1 on the Extended NYT Connections benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mxn41d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxn41d/deepseek_v31_reasoner_improves_over_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxn41d/deepseek_v31_reasoner_improves_over_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T00:22:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx7q58</id>
    <title>DINOv3 semantic video tracking running locally in your browser (WebGPU)</title>
    <updated>2025-08-22T14:16:07+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx7q58/dinov3_semantic_video_tracking_running_locally_in/"&gt; &lt;img alt="DINOv3 semantic video tracking running locally in your browser (WebGPU)" src="https://external-preview.redd.it/NDBmanYza3Z2a2tmMSbOwLNjFNHRCt4iN7LHZavGZv38TtoGGI8B_rdwJSVo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca1097874f9291657ebcdac60b7b207a3633f2b7" title="DINOv3 semantic video tracking running locally in your browser (WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up on a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt;demo&lt;/a&gt; I posted a few days ago, I added support for object tracking across video frames. It uses DINOv3 (a new vision backbone capable of producing rich, dense image features) to track objects in a video with just a few reference points. &lt;/p&gt; &lt;p&gt;One can imagine how this can be used for browser-based video editing tools, so I'm excited to see what the community builds with it! &lt;/p&gt; &lt;p&gt;Online demo (+ source code): &lt;a href="https://huggingface.co/spaces/webml-community/DINOv3-video-tracking"&gt;https://huggingface.co/spaces/webml-community/DINOv3-video-tracking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lghkx3kvvkkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx7q58/dinov3_semantic_video_tracking_running_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx7q58/dinov3_semantic_video_tracking_running_locally_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T14:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwwr87</id>
    <title>What is Gemma 3 270M actually used for?</title>
    <updated>2025-08-22T04:17:47+00:00</updated>
    <author>
      <name>/u/airbus_a360_when</name>
      <uri>https://old.reddit.com/user/airbus_a360_when</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwwr87/what_is_gemma_3_270m_actually_used_for/"&gt; &lt;img alt="What is Gemma 3 270M actually used for?" src="https://preview.redd.it/dtrvooncyhkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0447bc7fe6fe125ae4afb2d1094b8948a5d8af3d" title="What is Gemma 3 270M actually used for?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All I can think of is speculative decoding. Can it even RAG that well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/airbus_a360_when"&gt; /u/airbus_a360_when &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dtrvooncyhkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwwr87/what_is_gemma_3_270m_actually_used_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwwr87/what_is_gemma_3_270m_actually_used_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T04:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxh4gk</id>
    <title>🤔 meta X midjourney</title>
    <updated>2025-08-22T20:13:52+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh4gk/meta_x_midjourney/"&gt; &lt;img alt="🤔 meta X midjourney" src="https://preview.redd.it/ayp3r5k9pmkf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c32ec6bf7b9de766f15b1cbdfeecbbb7d4e77a3" title="🤔 meta X midjourney" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ayp3r5k9pmkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh4gk/meta_x_midjourney/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh4gk/meta_x_midjourney/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T20:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxf2sz</id>
    <title>Seed-OSS-36B is ridiculously good</title>
    <updated>2025-08-22T18:54:56+00:00</updated>
    <author>
      <name>/u/mahmooz</name>
      <uri>https://old.reddit.com/user/mahmooz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct"&gt;https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the model was released a few days ago. it has a native context length of 512k. a pull request has been made to llama.cpp to get support for it.&lt;/p&gt; &lt;p&gt;i just tried running it with the code changes in the pull request. and it works wonderfully. unlike other models (such as qwen3, which has 256k context length supposedly), the model can generate long coherent outputs without refusal.&lt;/p&gt; &lt;p&gt;i tried many other models like qwen3 or hunyuan but none of them are able to generate long outputs and even often complain that the task may be too difficult or may &amp;quot;exceed the limits&amp;quot; of the llm. but this model doesnt even complain, it just gets down to it. one other model that also excels at this is glm-4.5 but its context length is much smaller unfortunately.&lt;/p&gt; &lt;p&gt;seed-oss-36b also apparently has scored 94 on ruler at 128k context which is insane for a 36b model (it was reported by the maintainer of chatllm.cpp).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mahmooz"&gt; /u/mahmooz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxf2sz/seedoss36b_is_ridiculously_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxf2sz/seedoss36b_is_ridiculously_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxf2sz/seedoss36b_is_ridiculously_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T18:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx8qki</id>
    <title>I'm making a game where all the dialogue is generated by the player + a local llm</title>
    <updated>2025-08-22T14:55:48+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/"&gt; &lt;img alt="I'm making a game where all the dialogue is generated by the player + a local llm" src="https://external-preview.redd.it/dGRvYnNtbjM0bGtmMcsYHmRxX6l-GOXVgL0nfvRqWRvtCbG6hh3bmeu2mYuD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51833070477bb979f4af1952d3badd650277cef2" title="I'm making a game where all the dialogue is generated by the player + a local llm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oitg5nn34lkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T14:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
