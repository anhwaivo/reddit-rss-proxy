<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-09T17:05:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hwn90v</id>
    <title>Phi 4 MIT licensed - its show time folks</title>
    <updated>2025-01-08T15:50:30+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;https://huggingface.co/microsoft/phi-4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwn90v/phi_4_mit_licensed_its_show_time_folks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfm8k</id>
    <title>Tech lead of Qwen Team, Alibaba Group: "I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year." Blog linked in body text.</title>
    <updated>2025-01-08T08:50:35+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt; &lt;img alt="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." src="https://preview.redd.it/5lmmx4qchqbe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f0c18af44997be0f0628d875475982b7bf3b877" title="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19"&gt;https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;https://www.anthropic.com/research/building-effective-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5lmmx4qchqbe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwe9mf</id>
    <title>HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it</title>
    <updated>2025-01-08T07:20:14+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt; &lt;img alt="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" src="https://external-preview.redd.it/2Uxr2fZXgwYpxUcnSif2gZmNvP23o2dpwlhS4x1dHZA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54d46261045d9a2cee779ef1547c528c90021757" title="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;96 GB out of the 128GB can be allocated to use VRAM making it able to run 70B models q8 with ease.&lt;/p&gt; &lt;p&gt;I am pretty sure Digits will use CUDA and/or TensorRT for optimization of inferencing.&lt;/p&gt; &lt;p&gt;I am wondering if this will use RocM or if we can just use CPU inferencing - wondering what the acceleration will be here. Anyone able to share insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://aecmag.com/workstations/hp-amd-ryzen-ai-max-pro-hp-zbook-ultra-g1a-hp-z2-mini-g1a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T07:20:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxa8n0</id>
    <title>Which tests do you use to test an LLM's capabilities?</title>
    <updated>2025-01-09T10:51:20+00:00</updated>
    <author>
      <name>/u/MoonIcebear</name>
      <uri>https://old.reddit.com/user/MoonIcebear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel that recently so many benchmarks are becoming saturated. There is really a need for some kind of 'dynamic' benchmarking . Personally, I always try and play tic-tac-toe or connect 4 against LLMs. Not a single LLM (open or proprietary) can do this. Most fail to keep track of the game state and when they actually can play, they play dumb moves.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoonIcebear"&gt; /u/MoonIcebear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxa8n0/which_tests_do_you_use_to_test_an_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxa8n0/which_tests_do_you_use_to_test_an_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxa8n0/which_tests_do_you_use_to_test_an_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T10:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwx8ah</id>
    <title>ROG Flow Z13 2025 has Ryzen AI Max+ 395 and 128 GB LPDDR5X??</title>
    <updated>2025-01-08T22:42:25+00:00</updated>
    <author>
      <name>/u/Daniel_H212</name>
      <uri>https://old.reddit.com/user/Daniel_H212</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://rog.asus.com/laptops/rog-flow/rog-flow-z13-2025/spec/"&gt;https://rog.asus.com/laptops/rog-flow/rog-flow-z13-2025/spec/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Apparently it is 8000MHz quad channel, must be reallocated between CPU and GPU (which you can only do after restarting), so I'm guessing similar to the lenovo system from that other post where you can allocate say up to 96 GB.&lt;/p&gt; &lt;p&gt;They also claim the NPU has 50 TOPs.&lt;/p&gt; &lt;p&gt;Is this the Windows answer to Apple Silicon's unified memory?&lt;/p&gt; &lt;p&gt;Obviously support will be a big issue but man if this thing runs like they advertise it will run, this tablet will be more versatile than the majority of home GPU-based systems, just off of the memory config alone, and won't be slow either.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daniel_H212"&gt; /u/Daniel_H212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwx8ah/rog_flow_z13_2025_has_ryzen_ai_max_395_and_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwx8ah/rog_flow_z13_2025_has_ryzen_ai_max_395_and_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwx8ah/rog_flow_z13_2025_has_ryzen_ai_max_395_and_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T22:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx6utz</id>
    <title>Help Me Decide: RTX 3060 12GB vs. RTX 4060 Ti 16GB for ML and Occasional Gaming</title>
    <updated>2025-01-09T06:37:35+00:00</updated>
    <author>
      <name>/u/zaid2801</name>
      <uri>https://old.reddit.com/user/zaid2801</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I could really use some GPU advice. I primarily do machine learning/model training but also game casually (League of Legends at 60 FPS is more than enough for me). Due to local market constraints, I’ve narrowed it down to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;RTX 3060 12GB&lt;/strong&gt; (MSI Ventus 2X) – $365 &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/msi-rtx-3060-ventus-2x.b8614"&gt;Specs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RTX 4060 Ti 16GB&lt;/strong&gt; (ZOTAC AMP) – $510 &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/zotac-rtx-4060-ti-amp-16-gb.b11324"&gt;Specs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My current system is an &lt;strong&gt;i5-12400&lt;/strong&gt; with &lt;strong&gt;32GB of RAM&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I’m Torn:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The 4060 Ti has more VRAM (16GB vs. 12GB) and higher CUDA core count, which can help with bigger ML models.&lt;/li&gt; &lt;li&gt;However, it’s got a narrower memory bus (128-bit vs. 192-bit on the 3060).&lt;/li&gt; &lt;li&gt;There’s also a significant price difference ($510 vs. $365).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Use Cases:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Machine Learning / Model Training&lt;/strong&gt;: Primarily in TensorFlow/PyTorch. VRAM size is important for handling larger models, but memory bandwidth can also be a factor.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gaming&lt;/strong&gt;: Mostly League of Legends (60 FPS is plenty). I’m not aiming for ultra settings in AAA titles.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How much does the narrower bus on the 4060 Ti matter for ML workloads in practice?&lt;/li&gt; &lt;li&gt;Is it worth paying the extra $145 for the 4060 Ti for the additional VRAM and performance uplift?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’d really appreciate any insights or experiences you might have. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zaid2801"&gt; /u/zaid2801 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6utz/help_me_decide_rtx_3060_12gb_vs_rtx_4060_ti_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6utz/help_me_decide_rtx_3060_12gb_vs_rtx_4060_ti_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6utz/help_me_decide_rtx_3060_12gb_vs_rtx_4060_ti_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxfhz8</id>
    <title>Open source library of prompts</title>
    <updated>2025-01-09T15:39:05+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there such a thing? Thank you in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxfhz8/open_source_library_of_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxfhz8/open_source_library_of_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxfhz8/open_source_library_of_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T15:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxgfu5</id>
    <title>We've just released LLM Pools, end-to-end deployment of Large Language Models that can be installed anywhere</title>
    <updated>2025-01-09T16:19:15+00:00</updated>
    <author>
      <name>/u/Good-Coconut3907</name>
      <uri>https://old.reddit.com/user/Good-Coconut3907</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLM Pools are all inclusive environments that can be installed on everyday hardware to simplify LLM deployment. Compatible with a multitude of model engines, out-of-the-box single and multi-node friendly, with a single API endpoint + UI playground.&lt;/p&gt; &lt;p&gt;Currently supported model engines: vLLM, llama.cpp, Aphrodite Engine and Petals, all in single node and multinode fashion. More to come!&lt;/p&gt; &lt;p&gt;You can install your own for free, but the easiest way to get started is joining our public LLM pool (also free, and you get to share each other models): &lt;a href="https://kalavai-net.github.io/kalavai-client/public_llm_pool/"&gt;https://kalavai-net.github.io/kalavai-client/public_llm_pool/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open source: &lt;a href="https://github.com/kalavai-net/kalavai-client"&gt;https://github.com/kalavai-net/kalavai-client&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Good-Coconut3907"&gt; /u/Good-Coconut3907 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxgfu5/weve_just_released_llm_pools_endtoend_deployment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxgfu5/weve_just_released_llm_pools_endtoend_deployment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxgfu5/weve_just_released_llm_pools_endtoend_deployment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T16:19:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxaho9</id>
    <title>What is the best LLM for writing and coding that can be run on 4090?</title>
    <updated>2025-01-09T11:08:33+00:00</updated>
    <author>
      <name>/u/idleWizard</name>
      <uri>https://old.reddit.com/user/idleWizard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It doesn't have to be one LLM that does both coding and writing, just to be clear.&lt;br /&gt; I understand this question gets asked a lot, but things are changing quickly and I would like your help to stay on top of them. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/idleWizard"&gt; /u/idleWizard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxaho9/what_is_the_best_llm_for_writing_and_coding_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxaho9/what_is_the_best_llm_for_writing_and_coding_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxaho9/what_is_the_best_llm_for_writing_and_coding_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T11:08:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxdsu2</id>
    <title>Open-source, Python-based data connectors?</title>
    <updated>2025-01-09T14:20:05+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building some AI agents for which I'm looking for the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data connectors for common software products like Google Workspace (Docs, Sheets, Gmail, Calendar, Drive, Meet), Notion, Airtable, Slack, Jira, Zoom, Todoist, etc&lt;/li&gt; &lt;li&gt;Supports both reading and writing&lt;/li&gt; &lt;li&gt;Open-Source&lt;/li&gt; &lt;li&gt;Python-based&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I did some research on my own, and here is what I found:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LlamaIndex/Langchain: they have a lot of readers but not writers. For example, I can &lt;a href="https://docs.llamaindex.ai/en/stable/examples/data_connectors/NotionDemo/"&gt;read data from Notion&lt;/a&gt;, but I can't have an agent write a new doc and save it inside Notion (unless I'm missing something)&lt;/li&gt; &lt;li&gt;n8n &lt;a href="https://n8n.io/integrations/"&gt;has all these integrations&lt;/a&gt;, but their license is too restrictive, and it's not Python-based&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxdsu2/opensource_pythonbased_data_connectors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxdsu2/opensource_pythonbased_data_connectors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxdsu2/opensource_pythonbased_data_connectors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T14:20:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx8gid</id>
    <title>A beginners guide to LLM scripting using Python with the KoboldCpp API</title>
    <updated>2025-01-09T08:35:36+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;A guide to using the KoboldCpp API with Python&lt;/h2&gt; &lt;p&gt;KoboldCpp is a great way to run LLMs. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;Can run any GGUF LLM that llamacpp can (and more)&lt;/li&gt; &lt;li&gt;No installation, compiling, or python dependencies needed&lt;/li&gt; &lt;li&gt;Linux, Windows, macOS&lt;/li&gt; &lt;li&gt;CUDA, and Vulkan acceleration&lt;/li&gt; &lt;li&gt;Included GUI frontend with a ton of features&lt;/li&gt; &lt;li&gt;Image generation (SD, Flux)&lt;/li&gt; &lt;li&gt;Image to text&lt;/li&gt; &lt;li&gt;Voice to text and text to voice&lt;/li&gt; &lt;li&gt;OpenAI and Ollama compatible API endpoints&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The one-file no-install ability makes it great for running portable scripts. Just pack the exe and model weights or a kcppt file and batch file or shell script and a bit of Python code and you have a portable LLM engine.&lt;/p&gt; &lt;h3&gt;What background do you need to follow this guide?&lt;/h3&gt; &lt;p&gt;In order to follow this guide you must first learn about and have ready:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A Python install for your OS&lt;/li&gt; &lt;li&gt;A terminal you can use to run command line programs&lt;/li&gt; &lt;li&gt;A text editor meant for coding like notepad++ or an IDE like VSCode or PyCharm setup to write Python&lt;/li&gt; &lt;li&gt;A way to manage python environments like miniconda, and an environment setup for this use&lt;/li&gt; &lt;li&gt;A least the first part of a &lt;a href="https://www.kaggle.com/code/alexisbcook/arithmetic-and-variables"&gt;Python tutorial&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Familiarity with running LLMs and downloading weights&lt;/li&gt; &lt;li&gt;Familiarity with the JSON format &lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Basic API use&lt;/h3&gt; &lt;p&gt;Load a model in KoboldCpp and in the web browser navigate to to http://localhost:5001/api&lt;/p&gt; &lt;p&gt;That page is the API documentation. Each URL portion listed are called &lt;strong&gt;endpoints&lt;/strong&gt;. By putting that at the end of the KoboldCpp URL you will reach that endpoint, which has a functionality usually defined by its location. By clicking on an endpoint it will allow you to test it live and see the results. Notice the POST or GET next to the endpoint location -- this is important. Sometimes there are two entries for the same endpoint: one for POST and one for GET and they do different things.&lt;/p&gt; &lt;p&gt;Click on the /api/v1/generate entry and then click &lt;em&gt;Try it out&lt;/em&gt; and then &lt;em&gt;Execute&lt;/em&gt;. The result of the query will appear.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://imgur.com/UtA5HX7"&gt;/api/v1/generate&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The documentation shows you an example payload and an example response for each endpoint. We communicate with the API using JSON, which contains key value pairs formatted in a specific way. &lt;/p&gt; &lt;p&gt;Ad the bottom is the &lt;strong&gt;Schema&lt;/strong&gt; list. These are all of the possible key value pairs you can send to or receive from an endpoint. &lt;/p&gt; &lt;p&gt;If click on &lt;em&gt;GenerationInput&lt;/em&gt; you will see every key you can specify, and the types required. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://imgur.com/CoUbH2t"&gt;GenerationInput&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Example using Python&lt;/h3&gt; &lt;p&gt;In the terminal, you will need to install the requests library and then open a Python interpreter.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install requests python &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enter the following (I recommend you type it instead of copying and pasting, for practice):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import json import requests endpoint = &amp;quot;http://localhost:5001/api/v1/generate&amp;quot; headers = {&amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot;} payload = { &amp;quot;prompt&amp;quot;: &amp;quot;What is a Kobold?&amp;quot;, &amp;quot;max_length&amp;quot;: 200, &amp;quot;temperature&amp;quot;: 0.7, } response = requests.post(endpoint, json=payload, headers=headers).json() print(response) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You will see the response from the KoboldCpp API. Compare the results you got from that with the example in the API spec page.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://imgur.com/pQScvGm"&gt;Response.&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now you know how to do a basic API call to KoboldCpp!&lt;/p&gt; &lt;h2&gt;Scripting&lt;/h2&gt; &lt;p&gt;Now let's open the text editor or IDE and write a script. In this script we will create a function that communicates with the api for us so we don't have to write the same thing every time. All we need to specific will be the endpoint and the prompt.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import requests import json def call_kobold_api(endpoint, request_type, payload=&amp;quot;&amp;quot;): # We establish our base configuration base_url = &amp;quot;http://localhost:5001&amp;quot; headers = { &amp;quot;Content-Type&amp;quot;: &amp;quot;application/json&amp;quot; } # We build the full URL url = base_url + endpoint if request_type == &amp;quot;GET&amp;quot;: response = requests.get(url, headers=headers) else: response = requests.post(url, json=payload, headers=headers) return response.json() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can now call that function to talk to the API.&lt;/p&gt; &lt;p&gt;For the most common task - generating text from a prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;generate = &amp;quot;/api/v1/generate&amp;quot; max_length = 100 temperature = 1 prompt = input(&amp;quot;Kobold: &amp;quot;) payload = { &amp;quot;prompt&amp;quot;: prompt, &amp;quot;max_length&amp;quot;: max_length, &amp;quot;temperature&amp;quot;: temperature, } response = call_kobold_api(generate, &amp;quot;POST&amp;quot;, payload) print(response['results'][0]['text']) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Save the script and run it.&lt;/p&gt; &lt;p&gt;Since there is no instruct template, it is going to be in &lt;em&gt;text completion&lt;/em&gt; mode and will generally just finish what you started writing.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://imgur.com/H0fDLJF"&gt;Response.&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Congrats on learning how to use the KoboldCpp API in Python!&lt;/p&gt; &lt;h2&gt;API Helper&lt;/h2&gt; &lt;p&gt;I made a basic helper for the KoboldCpp API which will do things like wrap the prompt in the proper instruct template and chunk text. There are example scripts for basic text functions like summarize and translate, as well as image captioning.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jabberjabberjabber/koboldapi-python"&gt;The repo is here.&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;This is a work in progress. I am not an expert, by far. Constructive criticisms, corrections, additions and questions are welcome.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8gid/a_beginners_guide_to_llm_scripting_using_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8gid/a_beginners_guide_to_llm_scripting_using_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8gid/a_beginners_guide_to_llm_scripting_using_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T08:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwlka6</id>
    <title>I made the world's first AI meeting copilot, and open sourced it!</title>
    <updated>2025-01-08T14:33:52+00:00</updated>
    <author>
      <name>/u/stealthanthrax</name>
      <uri>https://old.reddit.com/user/stealthanthrax</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of relying on clunky SaaS tools for meeting transcriptions that didn’t respect my privacy or workflow. Everyone I tried had issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bots awkwardly join meetings and announce themselves.&lt;/li&gt; &lt;li&gt;Poor transcription quality.&lt;/li&gt; &lt;li&gt;No flexibility to tweak things to fit &lt;em&gt;my&lt;/em&gt; setup.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built &lt;strong&gt;Amurex&lt;/strong&gt;, a self-hosted solution that actually works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Records meetings quietly, with no bots interrupting.&lt;/li&gt; &lt;li&gt;Delivers clean, accurate diarized transcripts right after the meeting.&lt;/li&gt; &lt;li&gt;Does late meeting summaries. i.e. a recap for a meeting if I am late&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But most importantly, it has it is the only meeting tool in the world that can give&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time suggestions to stay engaged in boring meetings.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s completely open source and designed for self-hosting, so you control your data and your workflow. No subscriptions, and no vendor lock-in.&lt;/p&gt; &lt;p&gt;I would love to know what you all think of it. It only works on Google Meet for now but I will be scaling it to all the famous meeting providers.&lt;/p&gt; &lt;p&gt;Github - &lt;a href="https://github.com/thepersonalaicompany/amurex"&gt;https://github.com/thepersonalaicompany/amurex&lt;/a&gt;&lt;br /&gt; Website - &lt;a href="https://www.amurex.ai/"&gt;https://www.amurex.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stealthanthrax"&gt; /u/stealthanthrax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwlka6/i_made_the_worlds_first_ai_meeting_copilot_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T14:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxg435</id>
    <title>Introducing LongTalk-CoT v0.1: A Very Long Chain-of-Thought Dataset</title>
    <updated>2025-01-09T16:05:42+00:00</updated>
    <author>
      <name>/u/Financial_Counter199</name>
      <uri>https://old.reddit.com/user/Financial_Counter199</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m excited to release &lt;a href="https://huggingface.co/datasets/kenhktsui/longtalk-cot-v0.1"&gt;LongTalk-CoT v0.1&lt;/a&gt;, a dataset designed for training o1-like reasoning model. Each response is prompted using QwQ-32B-Preview, and specifically handcrafted system message that encourages &lt;strong&gt;more vocalised thinking&lt;/strong&gt;, and &lt;strong&gt;self reflection&lt;/strong&gt;. As such, its output token length is 5.29x longer than HuggingFaceTB/smoltalk&lt;/p&gt; &lt;p&gt;It is able to solve &amp;quot;Is 9.11 greater than 9.9&amp;quot; and &amp;quot;How many letters R in the word strawberry?&amp;quot;!&lt;/p&gt; &lt;p&gt;It is overthinking sometimes as expected, but I love its behavior of trying multiple approaches to solve a question.&lt;/p&gt; &lt;h1&gt;Quick Test 1: Is 9.11 greater than 9.9?&lt;/h1&gt; &lt;h1&gt;After finetuning&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;No, 9.11 is not greater than 9.9. Let's think about this step by step. First, it helps to understand that both numbers are decimal numbers, specifically they have two digits after the decimal point. So, 9.11 and 9.9 can be compared directly based on their values in each place value: whole number part and the tenths and hundredths parts. Let's break them down: * For 9.11: * The whole number part is 9. * The tenths place is 1 (which means one tenth). * The hundredths place is also 1 (one hundredth). * For 9.9: * The whole number part is also 9. * But here, the tenths place is 9 (nine tenths), and there's no hundredths digit after that; it's considered to have zero in the hundredths place because decimal numbers with fewer digits are filled with zeros. So, comparing them: * Both have the same whole number part: 9. * For the tenth place: * 1 vs. 9. Since 9 is greater than 1, this already tells us that 9.9 is larger than 9.11. But to be thorough, let's look at the hundredths places as well: * In 9.11: hundredths = 1 * In 9.9: hundredths = 0 Even though there are no digits in the hundredths place for 9.9, it's still considered zero because of the way decimals work. Therefore, overall, 9.11 &amp;lt; 9.9. Wait a minute, but I should make sure that I'm not missing anything here. Sometimes, people might think about adding or subtracting numbers differently, but in this case, since both numbers have the same whole number part and one has a larger tenths digit, it's clear which is larger. Another way to look at it is to consider them as percentages: * 9.11 = 911% * 9.9 = 990% But that might be confusing; in percentage terms, 990% is still greater than 911%. Alternatively, if we multiply both numbers by 100 to eliminate the decimal point: * 9.11 \* 100 = 911 * 9.9 \* 100 = 990 Again, 990 is greater than 911. So, no matter how we look at it, 9.9 is larger than 9.11. I think that covers the comparison thoroughly. There doesn't seem to be any trick or special case here; it's a straightforward decimal comparison once you understand place values. **Final Answer** \[ \\boxed{9.11 &amp;lt; 9.9} \] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Before finetuning&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;No, 9.11 is not greater than 9.9. When comparing decimal numbers, you can compare them digit by digit from left to right. In this case: * The whole number part of both numbers is the same (9). * Comparing the tenths place, we see that 1 in 9.11 is larger than 9 in 9.9. * Therefore, 9.11 &amp;gt; 9.9. So, 9.11 is greater than 9.9. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Counter199"&gt; /u/Financial_Counter199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxg435/introducing_longtalkcot_v01_a_very_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxg435/introducing_longtalkcot_v01_a_very_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxg435/introducing_longtalkcot_v01_a_very_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T16:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx1qn2</id>
    <title>Now that Phi-4 has been out for a while what do you think?</title>
    <updated>2025-01-09T02:03:50+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;on real world use cases does it perform well and what tasks have you tried it on so far?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx1qn2/now_that_phi4_has_been_out_for_a_while_what_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx1qn2/now_that_phi4_has_been_out_for_a_while_what_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx1qn2/now_that_phi4_has_been_out_for_a_while_what_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T02:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx6sjc</id>
    <title>Weirdly good finetune - QwQ-LCoT-7B-Instruct</title>
    <updated>2025-01-09T06:33:12+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use a lot of complex, large-context coding prompts that are high on the difficulty scale using &lt;a href="https://github.com/curvedinf/dir-assistant"&gt;https://github.com/curvedinf/dir-assistant&lt;/a&gt; . I've been using APIs for a number of months since prices have come down, but I just did a round of tests in the 7B-14B range. I found this tune randomly while browsing huggingface and it has a whole 304 downloads, but damn is it good. Its consistently outperforming newer 32B models, and older 70B models in my tests. I don't know what the secret is here, but I just wanted to pass this along. I test a LOT of models, and this one is weirdly good for coding.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/prithivMLmods/QwQ-LCoT-7B-Instruct"&gt;https://huggingface.co/prithivMLmods/QwQ-LCoT-7B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/bartowski/QwQ-LCoT-7B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/QwQ-LCoT-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6sjc/weirdly_good_finetune_qwqlcot7binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6sjc/weirdly_good_finetune_qwqlcot7binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx6sjc/weirdly_good_finetune_qwqlcot7binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:33:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwmy39</id>
    <title>Phi-4 has been released</title>
    <updated>2025-01-08T15:37:07+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"&gt; &lt;img alt="Phi-4 has been released" src="https://external-preview.redd.it/gF2xHg5eGxs-B6pIwB6VodFWhOArLu2_8o4wQP6siP8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed435e811f517c43ffac1607f9719679d8dd1b5d" title="Phi-4 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwmy39/phi4_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T15:37:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwthrq</id>
    <title>Why I think that NVIDIA Project DIGITS will have 273 GB/s of memory bandwidth</title>
    <updated>2025-01-08T20:05:14+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"&gt; &lt;img alt="Why I think that NVIDIA Project DIGITS will have 273 GB/s of memory bandwidth" src="https://b.thumbs.redditmedia.com/Hq1kyt3KW_h0jhHPVRH0mhDgkE3V6Gg4-cxEb1LPpSg.jpg" title="Why I think that NVIDIA Project DIGITS will have 273 GB/s of memory bandwidth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Used the following image from NVIDIA CES presentation:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o0dkapeqltbe1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=46b818ce812eed4ec1607595836eaccbe0da852d"&gt;Project DIGITS board&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Applied some GIMP magic to reset perspective (not perfect but close enough), used a photo of Grace chip die from the same presentation to make sure the aspect ratio is correct:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1jdoi1icxxbe1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4360f9bafb0e73213b3e70b0d645313c401141fe"&gt;https://preview.redd.it/1jdoi1icxxbe1.png?width=1257&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4360f9bafb0e73213b3e70b0d645313c401141fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then I measured dimensions of memory chips on this image:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;165 x 136 px&lt;/li&gt; &lt;li&gt;165 x 136 px&lt;/li&gt; &lt;li&gt;165 x 136 px&lt;/li&gt; &lt;li&gt;163 x 134 px&lt;/li&gt; &lt;li&gt;164 x 135 px&lt;/li&gt; &lt;li&gt;164 x 135 px&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looks consistent, so let's calculate the average aspect ratio of the chip dimensions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;165 / 136 = 1.213&lt;/li&gt; &lt;li&gt;165 / 136 = 1.213&lt;/li&gt; &lt;li&gt;165 / 136 = 1.213&lt;/li&gt; &lt;li&gt;163 / 134 = 1.216&lt;/li&gt; &lt;li&gt;164 / 135 = 1.215&lt;/li&gt; &lt;li&gt;164 / 135 = 1.215&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Average is 1.214&lt;/p&gt; &lt;p&gt;Now let's see what are the possible dimensions of &lt;a href="https://www.micron.com/products/memory/dram-components/lpddr5x/part-catalog?density=128Gb"&gt;Micron 128Gb LPDDR5X chips&lt;/a&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;496-ball packages (x64 bus): 14.00 x 12.40 mm. Aspect ratio = 1.13&lt;/li&gt; &lt;li&gt;441-ball packages (x64 bus): 14.00 x 14.00 mm. Aspect ratio = 1.0&lt;/li&gt; &lt;li&gt;315-ball packages (x32 bus): 12.40 x 15.00 mm. Aspect ratio = 1.21&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the closest match (I guess 1% measurement errors are possible) is 315-ball x32 bus package. With 8 chips the memory bus width will be 8 * 32 = 256 bits. With 8533MT/s that's 273 GB/s max. So basically the same as Strix Halo.&lt;/p&gt; &lt;p&gt;Another reason is that they didn't mention the memory bandwidth during presentation. I'm sure they would have mentioned it if it was exceptionally high.&lt;/p&gt; &lt;p&gt;Hopefully I'm wrong! 😢&lt;/p&gt; &lt;p&gt;...or there are 8 more memory chips underneath the board and I just wasted a hour of my life. 😆&lt;/p&gt; &lt;p&gt;Edit - that's unlikely, as there are only 8 identical high bandwidth memory I/O structures on the chip die.&lt;/p&gt; &lt;p&gt;Edit2 - did a better job with perspective correction, more pixels = greater measurement accuracy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwthrq/why_i_think_that_nvidia_project_digits_will_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T20:05:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwzmqc</id>
    <title>Phi-4 Llamafied + 4 Bug Fixes + GGUFs, Dynamic 4bit Quants</title>
    <updated>2025-01-09T00:20:50+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"&gt; &lt;img alt="Phi-4 Llamafied + 4 Bug Fixes + GGUFs, Dynamic 4bit Quants" src="https://b.thumbs.redditmedia.com/0aD2C0A02Gg08d6G5AgWeGQyp6FZmThRfyFVw15avlQ.jpg" title="Phi-4 Llamafied + 4 Bug Fixes + GGUFs, Dynamic 4bit Quants" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! I've uploaded &lt;strong&gt;fixed versions&lt;/strong&gt; of &lt;a href="https://unsloth.ai/blog/phi4"&gt;Phi-4&lt;/a&gt;, including GGUF + 4-bit + 16-bit versions on HuggingFace!&lt;/p&gt; &lt;p&gt;We’ve fixed over &lt;strong&gt;4 bugs (3 major ones)&lt;/strong&gt; in Phi-4, mainly related to tokenizers and chat templates which affected inference and finetuning workloads. If you were experiencing poor results, we recommend trying our GGUF upload. A detailed post on the fixes will be released tomorrow.&lt;/p&gt; &lt;p&gt;We also &lt;strong&gt;Llamafied&lt;/strong&gt; the model meaning it should work out of the box with every framework including &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;. Fine-tuning is &lt;strong&gt;2x faster, uses 70% VRAM&lt;/strong&gt; &amp;amp; has 9x longer context lengths with Unsloth.&lt;/p&gt; &lt;p&gt;View all Phi-4 versions with our bug fixes: &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Phi-4 Uploads (with our bug fixes)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;GGUFs&lt;/a&gt; including 2, 3, 4, 5, 6, 8, 16-bit&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit"&gt;Unsloth Dynamic 4-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4-bnb-4bit"&gt;4-bit Bnb&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/phi-4"&gt;Original 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I uploaded Q2_K_L quants which works well as well - they are Q2_K quants, but leaves the embedding as Q4 and lm_head as Q6 - this should increase accuracy by a bit!&lt;/p&gt; &lt;p&gt;To use Phi-4 in llama.cpp, do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli --model unsloth/phi-4-GGUF/phi-4-Q2_K_L.gguf --prompt '&amp;lt;|im_start|&amp;gt;user&amp;lt;|im_sep|&amp;gt;Provide all combinations of a 5 bit binary number.&amp;lt;|im_end|&amp;gt;&amp;lt;|im_start|&amp;gt;assistant&amp;lt;|im_sep|&amp;gt;' --threads 16 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Which will produce:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;A 5-bit binary number consists of 5 positions, each of which can be either 0 or 1. Therefore, there are \(2^5 = 32\) possible combinations. Here they are, listed in ascending order: 1. 00000 2. 00001 3. 00010 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I also uploaded &lt;strong&gt;Dynamic 4bit quants&lt;/strong&gt; which don't quantize every layer to 4bit, and leaves some in 16bit - by using only an extra 1GB of VRAM, you get superior accuracy, especially for finetuning! - Head over to &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt; to finetune LLMs and Vision models 2x faster and use 70% less VRAM!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/74tja0m83vbe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99a09dc6e9ad8c1d8e8e19ad519ccaaabebde3a8"&gt;Dynamic 4bit quants leave some layers as 16bit and not 4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwzmqc/phi4_llamafied_4_bug_fixes_ggufs_dynamic_4bit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T00:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hxe2cy</id>
    <title>is QwQ the best local model for CoT/reasoning?</title>
    <updated>2025-01-09T14:32:59+00:00</updated>
    <author>
      <name>/u/Moreh</name>
      <uri>https://old.reddit.com/user/Moreh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per title. I can go up to 100b parameters.&lt;/p&gt; &lt;p&gt;I'm running a script that requires an llm to classify text for a charity project. The concept that is being classified is quite complex and subjective that requires multiple tests to pass before it can be labelled as positive (this is why CoT works well). QwQ seems to do better than 72b Qwen 2.5, but given i have the hardware i wonder if there is a larger/better alternative. &lt;/p&gt; &lt;p&gt;I know I can implement my own kind of CoT but if there's one fine tuned already i thought i may as well look at that!&lt;/p&gt; &lt;p&gt;thankyou&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moreh"&gt; /u/Moreh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxe2cy/is_qwq_the_best_local_model_for_cotreasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hxe2cy/is_qwq_the_best_local_model_for_cotreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hxe2cy/is_qwq_the_best_local_model_for_cotreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T14:32:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx8nex</id>
    <title>"rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising "deep thinking" through Monte Carlo Tree Search (MCTS)....."</title>
    <updated>2025-01-09T08:51:09+00:00</updated>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"&gt; &lt;img alt="&amp;quot;rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS).....&amp;quot;" src="https://b.thumbs.redditmedia.com/HJz73domuc-71OAJfI-YWNodO1IrhFgRKSFmybqiieU.jpg" title="&amp;quot;rStar-Math demonstrates that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS).....&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising &amp;quot;deep thinking&amp;quot; through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naïve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04519"&gt;https://arxiv.org/abs/2501.04519&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hx8nex"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx8nex/rstarmath_demonstrates_that_small_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T08:51:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx60t4</id>
    <title>New Microsoft research - rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</title>
    <updated>2025-01-09T05:43:37+00:00</updated>
    <author>
      <name>/u/Ok_Landscape_6819</name>
      <uri>https://old.reddit.com/user/Ok_Landscape_6819</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2501.04519"&gt;https://arxiv.org/abs/2501.04519&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen2.5-Math-7B reaches 90% on MATH with this new technique. Phi3-mini-3.8B reaches 86.4%..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Landscape_6819"&gt; /u/Ok_Landscape_6819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx60t4/new_microsoft_research_rstarmath_small_llms_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T05:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx5i8u</id>
    <title>Phi 4 is just 14B But Better than llama 3.1 70b for several tasks.</title>
    <updated>2025-01-09T05:12:08+00:00</updated>
    <author>
      <name>/u/Vishnu_One</name>
      <uri>https://old.reddit.com/user/Vishnu_One</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"&gt; &lt;img alt="Phi 4 is just 14B But Better than llama 3.1 70b for several tasks. " src="https://preview.redd.it/uwfo8ig8jwbe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06707621fd601095edb87fea07007bbf976f658b" title="Phi 4 is just 14B But Better than llama 3.1 70b for several tasks. " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vishnu_One"&gt; /u/Vishnu_One &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uwfo8ig8jwbe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx5i8u/phi_4_is_just_14b_but_better_than_llama_31_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T05:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx99oi</id>
    <title>Former OpenAI employee Miles Brundage: "o1 is just an LLM though, no reasoning infrastructure. The reasoning is in the chain of thought." Current OpenAI employee roon: "Miles literally knows what o1 does."</title>
    <updated>2025-01-09T09:39:31+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx99oi/former_openai_employee_miles_brundage_o1_is_just/"&gt; &lt;img alt="Former OpenAI employee Miles Brundage: &amp;quot;o1 is just an LLM though, no reasoning infrastructure. The reasoning is in the chain of thought.&amp;quot; Current OpenAI employee roon: &amp;quot;Miles literally knows what o1 does.&amp;quot;" src="https://a.thumbs.redditmedia.com/jvhF7IPWy1xMdzkV7pN0IGKD30vdwZ9p8xqaMkVtK34.jpg" title="Former OpenAI employee Miles Brundage: &amp;quot;o1 is just an LLM though, no reasoning infrastructure. The reasoning is in the chain of thought.&amp;quot; Current OpenAI employee roon: &amp;quot;Miles literally knows what o1 does.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1hx99oi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx99oi/former_openai_employee_miles_brundage_o1_is_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx99oi/former_openai_employee_miles_brundage_o1_is_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T09:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwwvuz</id>
    <title>This sums my experience with models on Groq</title>
    <updated>2025-01-08T22:27:22+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"&gt; &lt;img alt="This sums my experience with models on Groq" src="https://preview.redd.it/7tqzm8bsiube1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bebd5aeeb53de86390096a6377b9d96fe453c674" title="This sums my experience with models on Groq" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7tqzm8bsiube1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwwvuz/this_sums_my_experience_with_models_on_groq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T22:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1hx7421</id>
    <title>TransPixar: a new generative model that preserves transparency,</title>
    <updated>2025-01-09T06:55:51+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt; &lt;img alt="TransPixar: a new generative model that preserves transparency," src="https://external-preview.redd.it/aHFsc2gwdXExeGJlMYcphE9YFRyNCrr76DvwiShDtswDzb2s93cwaOHelUg2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd6433e0c057e8d365fab3d5289d9a28fb641a1f" title="TransPixar: a new generative model that preserves transparency," /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8fhb41uq1xbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hx7421/transpixar_a_new_generative_model_that_preserves/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-09T06:55:51+00:00</published>
  </entry>
</feed>
