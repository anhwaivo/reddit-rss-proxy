<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-16T20:48:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jcssl5</id>
    <title>How to Approch learning AI</title>
    <updated>2025-03-16T18:58:05+00:00</updated>
    <author>
      <name>/u/The_Neo_17</name>
      <uri>https://old.reddit.com/user/The_Neo_17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you are a newbie and today you wanna start to learn GenAI and build agents/assistant. What learning path will you choose and pls share the learning resources as well..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The_Neo_17"&gt; /u/The_Neo_17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcssl5/how_to_approch_learning_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcssl5/how_to_approch_learning_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcssl5/how_to_approch_learning_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T18:58:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcgonz</id>
    <title>Has anyone tried &gt;70B LLMs on M3 Ultra?</title>
    <updated>2025-03-16T08:05:44+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the Mac Studio is the only machine with 0.5TB of memory at decent memory bandwidth under $15k, I'd like to know what's the PP and token generation speeds for &lt;strong&gt;dense&lt;/strong&gt; LLMs, such Llama 3.1 70B and 3.1 405B.&lt;/p&gt; &lt;p&gt;Has anyone acquired the new Macs and tried them? Or, what speculations you have if you used M2 Ultra/M3 Max/M4 Max?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcgonz/has_anyone_tried_70b_llms_on_m3_ultra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcgonz/has_anyone_tried_70b_llms_on_m3_ultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcgonz/has_anyone_tried_70b_llms_on_m3_ultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T08:05:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jctbo0</id>
    <title>how do llms know when to generate a picture or search the web?</title>
    <updated>2025-03-16T19:20:15+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone break down the technical aspect how this is achieved? Is it functions? How does it work exactly?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctbo0/how_do_llms_know_when_to_generate_a_picture_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctbo0/how_do_llms_know_when_to_generate_a_picture_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jctbo0/how_do_llms_know_when_to_generate_a_picture_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jctlbt</id>
    <title>Best Model under 15B parameters 2025</title>
    <updated>2025-03-16T19:31:47+00:00</updated>
    <author>
      <name>/u/AZ_1010</name>
      <uri>https://old.reddit.com/user/AZ_1010</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im looking for a model that can be used as a reliable daily driver and handle variety of use cases . Especially for my application (instruction following) where i generate medical reports based on output from other models (CNNs etc). I currently have an rx7600s laptop with 16gb ram running on vulkan llama.cpp, would appreciate to know which models performed the best for you :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AZ_1010"&gt; /u/AZ_1010 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctlbt/best_model_under_15b_parameters_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctlbt/best_model_under_15b_parameters_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jctlbt/best_model_under_15b_parameters_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:31:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcef1d</id>
    <title>How I used entropy and varentropy to detect and remediate hallucinations in LLMs</title>
    <updated>2025-03-16T05:18:48+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The following blog is a high-level introduction to a series of research work we are doing with fast and efficient language models for routing and function calling scenarios. For experts this might be too high-level, but for people learning more about LLMs this might be a decent introduction to some machine learning concepts. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.archgw.com/blogs/detecting-hallucinations-in-llm-function-calling-with-entropy-and-varentropy"&gt;https://www.archgw.com/blogs/detecting-hallucinations-in-llm-function-calling-with-entropy-and-varentropy&lt;/a&gt; (part 1). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcef1d/how_i_used_entropy_and_varentropy_to_detect_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcef1d/how_i_used_entropy_and_varentropy_to_detect_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcef1d/how_i_used_entropy_and_varentropy_to_detect_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T05:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcu5rv</id>
    <title>GGUF for Qwen2.5-VL</title>
    <updated>2025-03-16T19:56:06+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try out the gguf conversions for Qwen2.5-VL that &lt;a href="https://github.com/HimariO"&gt;https://github.com/HimariO&lt;/a&gt; made!&lt;/p&gt; &lt;p&gt;More info here: &lt;a href="https://github.com/ggml-org/llama.cpp/issues/11483#issuecomment-2727577078"&gt;https://github.com/ggml-org/llama.cpp/issues/11483#issuecomment-2727577078&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We converted our 3B fine-tune SpaceQwen2.5-VL: &lt;a href="https://huggingface.co/remyxai/SpaceQwen2.5-VL-3B-Instruct/blob/main/SpaceQwen2.5-VL-3B-Instruct-F16.gguf"&gt;https://huggingface.co/remyxai/SpaceQwen2.5-VL-3B-Instruct/blob/main/SpaceQwen2.5-VL-3B-Instruct-F16.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now you can run faster AND better models on CPU or GPU for improved spatial reasoning in your embodied AI/robotics applications&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcu5rv/gguf_for_qwen25vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcu5rv/gguf_for_qwen25vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcu5rv/gguf_for_qwen25vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcujsd</id>
    <title>Running DeepSeek 670b - how to link multiple servers together ?</title>
    <updated>2025-03-16T20:12:52+00:00</updated>
    <author>
      <name>/u/DarkVoid42</name>
      <uri>https://old.reddit.com/user/DarkVoid42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 3 x 768GB RAM servers and wondering if its possible to run one model across all 3 servers with 128K context size. it runs fine on 1 but sometimes runs out of memory. and it would be nice to use the CPU cores as well. i have 4 x 10Gbe ports on each server and 16 x 10Gbe network switch. is it possible to link them into 1 huge cluster ? no GPU. storage is on a SAN so shared across all 3. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkVoid42"&gt; /u/DarkVoid42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcujsd/running_deepseek_670b_how_to_link_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcujsd/running_deepseek_670b_how_to_link_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcujsd/running_deepseek_670b_how_to_link_multiple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T20:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcievp</id>
    <title>Estimates of next gen releases</title>
    <updated>2025-03-16T10:16:52+00:00</updated>
    <author>
      <name>/u/TechnicalGeologist99</name>
      <uri>https://old.reddit.com/user/TechnicalGeologist99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We had Gemma3 which didn't really blow my socks off...&lt;/p&gt; &lt;p&gt;Wondering what other next gen open models are up and coming? What are you hoping they will feature? When do you think we will see them? &lt;/p&gt; &lt;p&gt;Personally im hoping for llama4-8B (and maybe a ~14B version) by the end of this quarter. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechnicalGeologist99"&gt; /u/TechnicalGeologist99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcievp/estimates_of_next_gen_releases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcievp/estimates_of_next_gen_releases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcievp/estimates_of_next_gen_releases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T10:16:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jciyso</id>
    <title>What’s your secret sauce in creating high quality Q&amp;A datasets?</title>
    <updated>2025-03-16T10:57:20+00:00</updated>
    <author>
      <name>/u/Secure_Archer_1529</name>
      <uri>https://old.reddit.com/user/Secure_Archer_1529</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can you fine tune a local model (13b and up) on domain specific knowledge and processes to perform on pair with the richness and depth of gpt 4o/4.5?&lt;/p&gt; &lt;p&gt;Do you use SOTA paid models to create your Q&amp;amp;A datasets for fine tuning models?&lt;/p&gt; &lt;p&gt;Maybe use cloud gpus for bigger models to generate Q&amp;amp;A dataset?&lt;/p&gt; &lt;p&gt;Any specific secret sauce you use in getting that depth and richness you get from a SOTA paid model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Archer_1529"&gt; /u/Secure_Archer_1529 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jciyso/whats_your_secret_sauce_in_creating_high_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jciyso/whats_your_secret_sauce_in_creating_high_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jciyso/whats_your_secret_sauce_in_creating_high_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T10:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcdsat</id>
    <title>A tip to make QwQ less verbose</title>
    <updated>2025-03-16T04:37:40+00:00</updated>
    <author>
      <name>/u/Aggressive-Stop-9091</name>
      <uri>https://old.reddit.com/user/Aggressive-Stop-9091</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my experience, QwQ tends to overthink because it's fine-tuned to interpret the writer's intentions. One effective way to minimize this is by providing examples. QwQ is an excellent few-shot learner that doesnt merely copy the examples, but also and when given a few well-crafted examples, it can generate a more articulate prompt than I initially wrote (which I then included in subsequent generations). Yes, I know this is prompt engineering 101, but what I find interesting about QwQ is that, unlike most local models I've tried, it doesn't get fixated on wording or style. Instead, it focuses on understanding the 'bigger picture' in the examples, like it had some sort 'meta learning'. For instance, I was working on condensing a research paper into a highly engaging and conversational format. The model when provided examples was able to outline what I wanted on its own, based on my instruction and the examples:&lt;/p&gt; &lt;p&gt;Hook: Why can't you stop scrolling TikTok?&lt;/p&gt; &lt;p&gt;Problem: Personalized content triggers brain regions linked to attention and reward.&lt;/p&gt; &lt;p&gt;Mechanism: DMN activation, VTA activity, reduced self-control regions coupling.&lt;/p&gt; &lt;p&gt;Outcome: Compulsive use, especially in those with low self-control.&lt;/p&gt; &lt;p&gt;Significance: Algorithm exploits neural pathways, need for understanding tech addiction.&lt;/p&gt; &lt;p&gt;Needless to say, it doesn't always work perfectly, but in my experience, it significantly improves the output. (The engine I use is ExLlama, and I follow the recommended settings for the model.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Stop-9091"&gt; /u/Aggressive-Stop-9091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdsat/a_tip_to_make_qwq_less_verbose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdsat/a_tip_to_make_qwq_less_verbose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcdsat/a_tip_to_make_qwq_less_verbose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T04:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcsvys</id>
    <title>How much does flash attention affect intelligence in reasoning models like QwQ</title>
    <updated>2025-03-16T19:01:45+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcsvys/how_much_does_flash_attention_affect_intelligence/"&gt; &lt;img alt="How much does flash attention affect intelligence in reasoning models like QwQ" src="https://b.thumbs.redditmedia.com/Jz6Gd8vqdtutQtHNWxINBKeN_53yYOJwyOU1ALk_7TU.jpg" title="How much does flash attention affect intelligence in reasoning models like QwQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/oaw93jp5n3pe1.png?width=701&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=563287c572752a94e3c3236af0085231cf1856f9"&gt;https://preview.redd.it/oaw93jp5n3pe1.png?width=701&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=563287c572752a94e3c3236af0085231cf1856f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Im using QwQ in LM Studio (yes i know abliteration degrades intelligence slightly too but I'm not too worried about that) and flash attention drastically improve memory use and speed to an unbelievable extent but my instinct says surely that big of memory improvement comes with pretty decent intelligence loss, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcsvys/how_much_does_flash_attention_affect_intelligence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcsvys/how_much_does_flash_attention_affect_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcsvys/how_much_does_flash_attention_affect_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:01:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jch5go</id>
    <title>Unvibe: Generate code that pass Unit-Tests with Qwen-coder 7B</title>
    <updated>2025-03-16T08:41:53+00:00</updated>
    <author>
      <name>/u/inkompatible</name>
      <uri>https://old.reddit.com/user/inkompatible</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jch5go/unvibe_generate_code_that_pass_unittests_with/"&gt; &lt;img alt="Unvibe: Generate code that pass Unit-Tests with Qwen-coder 7B" src="https://external-preview.redd.it/AoihjkMZZrKFb6oZh0_uoP159L4b86Wv3R2wyjBYumc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a409d905bf15acbd562decd0487e21014e17c40f" title="Unvibe: Generate code that pass Unit-Tests with Qwen-coder 7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkompatible"&gt; /u/inkompatible &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://claudio.uk/posts/unvibe.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jch5go/unvibe_generate_code_that_pass_unittests_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jch5go/unvibe_generate_code_that_pass_unittests_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T08:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jc9meu</id>
    <title>Who's still running ancient models?</title>
    <updated>2025-03-16T00:44:12+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had to take a pause from my experiments today, gemma3, mistralsmall, phi4, qwq, qwen, etc and marvel at how good they are for their size. A year ago most of us thought that we needed 70B to kick ass. 14-32B is punching super hard. I'm deleting my Q2/Q3 llama405B, and deepseek dyanmic quants.&lt;/p&gt; &lt;p&gt;I'm going to re-download guanaco, dolphin-llama2, vicuna, wizardLM, nous-hermes-llama2, etc&lt;br /&gt; For old times sake. It's amazing how far we have come and how fast. Some of these are not even 2 years old! Just a year plus! I'm going to keep some ancient model and run them so I can remember and don't forget and to also have more appreciation for what we have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jc9meu/whos_still_running_ancient_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T00:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcchtq</id>
    <title>Baidu releases X1, a (closed?) model that matches R1 and ERNIE 4.5, that matches GPT 4.5</title>
    <updated>2025-03-16T03:20:25+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Baidu_Inc/status/1901094083508220035"&gt;https://x.com/Baidu_Inc/status/1901094083508220035&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcchtq/baidu_releases_x1_a_closed_model_that_matches_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T03:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcufi6</id>
    <title>Improvements to Kokoro TTS v1.0</title>
    <updated>2025-03-16T20:07:36+00:00</updated>
    <author>
      <name>/u/Professional-Bear857</name>
      <uri>https://old.reddit.com/user/Professional-Bear857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I've spent some time trying to improve the output of this model, since the voice output always seemed inconsistent to me when I convert epubs to audiobooks. I thought I would share the updated kokoro-tts python script. To me, it now sounds a lot more natural then before. There are no additional dependencies so if you want to try it then just rename your older file and put this in its place, and then run it. I am running it with this command line:&lt;/p&gt; &lt;p&gt;python kokoro-tts test.epub --format mp3 --speed 1.0&lt;/p&gt; &lt;p&gt;File link (change the file / extension to 'kokoro-tts' and then run it as normal - I had to upload it as a .txt, which is why you need to change the file including its extension to 'kokoro-tts'). The model version I'm using is v1.0.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/user-attachments/files/19274795/kokoro-tts1.txt"&gt;https://github.com/user-attachments/files/19274795/kokoro-tts1.txt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT: Just realised there are multiple files / versions of Kokoro TTS. Here is the original script / model that I am using:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nazdridoy/kokoro-tts"&gt;https://github.com/nazdridoy/kokoro-tts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Bear857"&gt; /u/Professional-Bear857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcufi6/improvements_to_kokoro_tts_v10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcufi6/improvements_to_kokoro_tts_v10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcufi6/improvements_to_kokoro_tts_v10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T20:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcggwb</id>
    <title>Qwen2 72b VL is actually really impressive. It's not perfect, but for a local model I'm certainly impressed (more info in comments)</title>
    <updated>2025-03-16T07:48:53+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcggwb/qwen2_72b_vl_is_actually_really_impressive_its/"&gt; &lt;img alt="Qwen2 72b VL is actually really impressive. It's not perfect, but for a local model I'm certainly impressed (more info in comments)" src="https://preview.redd.it/1t90zqok80pe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e53739cdf1ff5da3fd4597e535c09945ad40c21" title="Qwen2 72b VL is actually really impressive. It's not perfect, but for a local model I'm certainly impressed (more info in comments)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy"&gt; /u/SomeOddCodeGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1t90zqok80pe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcggwb/qwen2_72b_vl_is_actually_really_impressive_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcggwb/qwen2_72b_vl_is_actually_really_impressive_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T07:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcm5p2</id>
    <title>OCR + LLM for Invoice Extraction</title>
    <updated>2025-03-16T14:04:40+00:00</updated>
    <author>
      <name>/u/JumpyHouse</name>
      <uri>https://old.reddit.com/user/JumpyHouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m starting to get a bit frustrated. I’m trying to develop a mobile application for an academic project involving invoice information extraction. Since this is a non-commercial project, I’m not allowed to use paid solutions like Google Vision or Azure AI Vision. So far, I’ve studied several possibilities, with the best being SuryaOCR/Marker for data extraction and Qwen 2.5 14B for data interpretation, along with some minor validation through RegEx.&lt;/p&gt; &lt;p&gt;I’m also limited in terms of options because I have an RX 6700 XT with 12GB of VRAM and can’t run Hugging Face models due to the lack of support for my GPU. I’ve also tried a few Vision models like Llama 3.2 Vision and various OCR solutions like PaddleOCR , PyTesseract and EasyOCR and they all came short due to the lack of layout detection.&lt;/p&gt; &lt;p&gt;I wanted to ask if any of you have faced a similar situation and if you have any ideas or tips because I’m running out of options for data extraction. The invoices are predominantly Portuguese, so many OCR models end up lacking support for the layout detection.&lt;/p&gt; &lt;p&gt;Thank you in advance.🫡&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JumpyHouse"&gt; /u/JumpyHouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcm5p2/ocr_llm_for_invoice_extraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcm5p2/ocr_llm_for_invoice_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcm5p2/ocr_llm_for_invoice_extraction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T14:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcre0y</id>
    <title>RTX 3060 vs RTX 3090: LLM Performance on 7B, 14B, 32B, 70B Models</title>
    <updated>2025-03-16T17:58:30+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcre0y/rtx_3060_vs_rtx_3090_llm_performance_on_7b_14b/"&gt; &lt;img alt="RTX 3060 vs RTX 3090: LLM Performance on 7B, 14B, 32B, 70B Models" src="https://external-preview.redd.it/5KAhHFD5rwW2nNDpKI_LcYCfDf4tB7OyGvSX5OWqLeA.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a690ac8c11dc8c1aceccb4206ce1d8bb8f32874" title="RTX 3060 vs RTX 3090: LLM Performance on 7B, 14B, 32B, 70B Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/VGyKwi9Rfhk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcre0y/rtx_3060_vs_rtx_3090_llm_performance_on_7b_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcre0y/rtx_3060_vs_rtx_3090_llm_performance_on_7b_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T17:58:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcmyuc</id>
    <title>Gemma 3 Models Tested : Comparing 1B, 4B, 12B, and 27B Versions</title>
    <updated>2025-03-16T14:42:54+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=CURb2tJBpIA"&gt;https://www.youtube.com/watch?v=CURb2tJBpIA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TLDR: No surprises here, performance increases with size. A bit disappointed to see 1b struggling so much with instruction following, but not surprised. I wonder what 1b is useful for? Any use cases that you have found for it?&lt;/p&gt; &lt;p&gt;The 12b is pretty decent though. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcmyuc/gemma_3_models_tested_comparing_1b_4b_12b_and_27b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcmyuc/gemma_3_models_tested_comparing_1b_4b_12b_and_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcmyuc/gemma_3_models_tested_comparing_1b_4b_12b_and_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T14:42:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcqy6c</id>
    <title>We have Deep Research at home</title>
    <updated>2025-03-16T17:39:04+00:00</updated>
    <author>
      <name>/u/atineiatte</name>
      <uri>https://old.reddit.com/user/atineiatte</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"&gt; &lt;img alt="We have Deep Research at home" src="https://external-preview.redd.it/NA_JTAjwBAYLbzLjIgJ3Q_k4TmFsR5MWHCoiYKiIQJ8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff9bd51d7c05f78cbad725a12ad69bc6ff6fe2ec" title="We have Deep Research at home" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atineiatte"&gt; /u/atineiatte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/atineiatte/deep-research-at-home"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcqy6c/we_have_deep_research_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T17:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jctquk</id>
    <title>Introducing Mochi, a finetuned version of Moshi.</title>
    <updated>2025-03-16T19:38:29+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/DavidBrowne17/Mochi"&gt;https://huggingface.co/DavidBrowne17/Mochi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I finetuned a version of Moshi, using a modified version of this repo &lt;a href="https://github.com/yangdongchao/RSTnet"&gt;https://github.com/yangdongchao/RSTnet&lt;/a&gt; it still has some of the issues with intelligence but it seems better to me. Using that repo we can also finetune new moshi style models using other smarter LLMs than the helium model that moshi is based on. There is no moat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jctquk/introducing_mochi_a_finetuned_version_of_moshi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jchrro</id>
    <title>Top 5 Model Recommendations for Newbie with 24GB</title>
    <updated>2025-03-16T09:29:41+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s only March, but there’s already been incredible progress in open-weight LLMs this year.&lt;/p&gt; &lt;p&gt;Here’s my top 5 recommendation for a beginner with 24GB VRAM (32GB for Mac) to try out. The list is from smallest to biggest.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Phi-4 14B for speed&lt;/li&gt; &lt;li&gt;Mistral Small 24B for RAG (only 32k context but best compromise length/quality IMHO)&lt;/li&gt; &lt;li&gt;Gemma 3 27B for general use&lt;/li&gt; &lt;li&gt;Qwen2.5 Coder 32B for coding (older than rest but still best)&lt;/li&gt; &lt;li&gt;QWQ 32B for reasoning (better than distilled deepseek-r1-qwen-32b)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hoping Llama 4 will earn a spot soon!&lt;/p&gt; &lt;p&gt;What's your recommendation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jchrro/top_5_model_recommendations_for_newbie_with_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jchrro/top_5_model_recommendations_for_newbie_with_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jchrro/top_5_model_recommendations_for_newbie_with_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T09:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcjrp2</id>
    <title>MetaStone-L1 ---The lightweight reasoning model launched by Yuanshi Zhisuan</title>
    <updated>2025-03-16T11:51:44+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"&gt; &lt;img alt="MetaStone-L1 ---The lightweight reasoning model launched by Yuanshi Zhisuan" src="https://external-preview.redd.it/9esdwOtFZ9-xPU4Z6uQ-hTei0HrHXFK8YzAB8QTzGNo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b862f612fd291a426e5b068d932bb8c71a97d4c2" title="MetaStone-L1 ---The lightweight reasoning model launched by Yuanshi Zhisuan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MetaStone-L1 is the lite reasoning model of the MetaStone series, which aims to enhance the performance in hard downstream tasks.&lt;/p&gt; &lt;p&gt;On core reasoning benchmarks including mathematics and code, MetaStone-L1-7B achieved SOTA results in the parallel-level models, and it also achieved the comparable results as the API models such as Claude-3.5-Sonnet-1022 and GPT4o-0513.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/21s0h0i8i1pe1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83367c2fdfa32018cc402076222f7d2c2060c41d"&gt;https://preview.redd.it/21s0h0i8i1pe1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=83367c2fdfa32018cc402076222f7d2c2060c41d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This repo contains the MetaStone-L1-7B model, which is trained based on DeepSeek-R1-Distill-Qwen-7B by GRPO&lt;/p&gt; &lt;p&gt;Optimization tips for specific tasks: For math problems, you can add a hint like &amp;quot;Please reason step by step and put your final answer in \\boxed{}.&amp;quot; For programming problems, add specific formatting requirements to further improve the reasoning effect of the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MetaStoneTec/MetaStone-L1-7B"&gt;https://huggingface.co/MetaStoneTec/MetaStone-L1-7B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcjrp2/metastonel1_the_lightweight_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T11:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jct1lk</id>
    <title>PR for native Windows support was just submitted to vLLM</title>
    <updated>2025-03-16T19:08:31+00:00</updated>
    <author>
      <name>/u/Nextil</name>
      <uri>https://old.reddit.com/user/Nextil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;User SystemPanic just &lt;a href="https://github.com/vllm-project/vllm/pull/14891"&gt;submitted a PR&lt;/a&gt; to the vLLM repo adding native Windows support. Before now it was only possible to run on Linux/WSL. This should make it significantly easier to run new models (especially VLMs) on Windows. No builds that I can see but it includes build instructions. The patched repo is &lt;a href="https://github.com/SystemPanic/vllm-windows/tree/vllm-windows"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The PR mentions submitting a FlashInfer PR adding Windows support, but that doesn't appear to have been done as of writing so it might not be possible to build just yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nextil"&gt; /u/Nextil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jct1lk/pr_for_native_windows_support_was_just_submitted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T19:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1jcbt5l</id>
    <title>These guys never rest!</title>
    <updated>2025-03-16T02:41:35+00:00</updated>
    <author>
      <name>/u/mlon_eusk-_-</name>
      <uri>https://old.reddit.com/user/mlon_eusk-_-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"&gt; &lt;img alt="These guys never rest!" src="https://preview.redd.it/4hmgoyhlsyoe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=455f74ad35ad822af5cb2fe29f909a6835248ce7" title="These guys never rest!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mlon_eusk-_-"&gt; /u/mlon_eusk-_- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4hmgoyhlsyoe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jcbt5l/these_guys_never_rest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-16T02:41:35+00:00</published>
  </entry>
</feed>
