<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-20T23:34:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i5rqjk</id>
    <title>Deepseek R1 GGUF Links</title>
    <updated>2025-01-20T14:46:58+00:00</updated>
    <author>
      <name>/u/undisputedx</name>
      <uri>https://old.reddit.com/user/undisputedx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For 16-24GB gpu&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Donnyed/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M-GGUF"&gt;https://huggingface.co/Donnyed/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for 8gb gpu&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf"&gt;https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/undisputedx"&gt; /u/undisputedx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5rqjk/deepseek_r1_gguf_links/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5rqjk/deepseek_r1_gguf_links/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5rqjk/deepseek_r1_gguf_links/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T14:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i60o9t</id>
    <title>Funny thought about the R1 distilled models and Reflection 70B</title>
    <updated>2025-01-20T20:49:42+00:00</updated>
    <author>
      <name>/u/_yustaguy_</name>
      <uri>https://old.reddit.com/user/_yustaguy_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i60o9t/funny_thought_about_the_r1_distilled_models_and/"&gt; &lt;img alt="Funny thought about the R1 distilled models and Reflection 70B" src="https://b.thumbs.redditmedia.com/_YGUC8JKObBfTaQAFLDkCI20uUvCQylmK9i46dUzMFo.jpg" title="Funny thought about the R1 distilled models and Reflection 70B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The R1 distilled models that DeepSeek are casually trained with less than a million R1 samples. And yet they still completely destroy the faked benchmarks of Reflection 70B (remember that shitshow?).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gtldqwl5n7ee1.png?width=3302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e6899570e36ca3fd89c16132b6ec5adfe2a4f144"&gt;https://preview.redd.it/gtldqwl5n7ee1.png?width=3302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e6899570e36ca3fd89c16132b6ec5adfe2a4f144&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I remember how they seemed way too good to be true at the time for a 70B. Today a 14B model looks way better than it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eqkjzsqyn7ee1.png?width=1312&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afec84b0239f17e7b2ecc5acfc766d2841789bf2"&gt;https://preview.redd.it/eqkjzsqyn7ee1.png?width=1312&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afec84b0239f17e7b2ecc5acfc766d2841789bf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just shows you how fast things are developing. Reflection 70B was announced 4 months ago.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_yustaguy_"&gt; /u/_yustaguy_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i60o9t/funny_thought_about_the_r1_distilled_models_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i60o9t/funny_thought_about_the_r1_distilled_models_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i60o9t/funny_thought_about_the_r1_distilled_models_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T20:49:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5p9dk</id>
    <title>Deepseek-R1 officially release</title>
    <updated>2025-01-20T12:39:20+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p9dk/deepseekr1_officially_release/"&gt; &lt;img alt="Deepseek-R1 officially release" src="https://external-preview.redd.it/VH2iWMzzI7fr45Vwe4Cjkq8YtS-AXjXjtn6GyjVnsIQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee4cf33369b41133dc351d1a09cdeb6f80176720" title="Deepseek-R1 officially release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, we are officially releasing DeepSeek-R1 and simultaneously open-sourcing the model weights.&lt;/p&gt; &lt;p&gt;DeepSeek-R1 is released under the MIT License, allowing users to train other models through distillation techniques using R1.&lt;/p&gt; &lt;p&gt;The DeepSeek-R1 API is now live, giving users access to chain-of-thought outputs by setting `model='deepseek-reasoner'`.&lt;/p&gt; &lt;p&gt;The DeepSeek website and app are being updated and launched simultaneously starting today.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance aligned with OpenAI-o1 official release&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;During the post-training phase, DeepSeek-R1 extensively utilized reinforcement learning techniques, significantly enhancing the model's reasoning capabilities with minimal annotated data. On tasks including mathematics, coding, and natural language reasoning, its performance matches that of the official OpenAI o1 release.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0dyqpnhx75ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c403b4b0072827d55e4a5e6b30591342cc79f1c"&gt;https://preview.redd.it/0dyqpnhx75ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1c403b4b0072827d55e4a5e6b30591342cc79f1c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are making all DeepSeek-R1 training techniques public to promote open exchange and collaborative innovation within the technical community.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper Link&lt;/strong&gt;: &lt;a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"&gt;&lt;strong&gt;https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Distilled Small Models Surpass OpenAI o1-mini&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Along with open-sourcing the two 660B models DeepSeek-R1-Zero and DeepSeek-R1, we have distilled 6 smaller models for the community using DeepSeek-R1's outputs. Among these, our 32B and 70B models have achieved performance comparable to OpenAI o1-mini across multiple capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4o34xbv385ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d71a867ed8173f83e87fd04df60748c0be1f2c64"&gt;https://preview.redd.it/4o34xbv385ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d71a867ed8173f83e87fd04df60748c0be1f2c64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HuggingFace Link&lt;/strong&gt;: &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;strong&gt;https://huggingface.co/deepseek-ai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yyta7e785ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3b5793efba042b1da54d00831470cab9383fc88"&gt;https://preview.redd.it/3yyta7e785ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3b5793efba042b1da54d00831470cab9383fc88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open License and User Agreement&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;To promote and encourage the development of the open-source community and industry ecosystem, while releasing and open-sourcing R1, we have made the following adjustments to our licensing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All model open-source licenses unified under MIT&lt;/strong&gt;. Previously, considering the unique characteristics of large language models and current industry practices, we introduced the DeepSeek License for open-source authorization. However, practice has shown that non-standard open-source licenses may increase developers' comprehension burden. Therefore, our open-source repositories (including model weights) now uniformly adopt the standardized, permissive MIT License - completely open source, with no commercial restrictions and no application required.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Product agreement explicitly allows &amp;quot;model distillation&amp;quot;&lt;/strong&gt;. To further promote technology sharing and open source development, we have decided to support users in performing &amp;quot;model distillation.&amp;quot; We have updated our online product user agreement to explicitly allow users to train other models using model outputs through techniques such as model distillation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;API and Pricing&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;DeepSeek-R1 API service pricing is set at &lt;strong&gt;1 RMB per million input tokens (cache hit) / 4 RMB per million input tokens (cache miss), and 16 RMB per million output tokens.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ja0nhjzl85ee1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f5e80c57d6b78c9e2c11590bc0eaf5be7974335"&gt;https://preview.redd.it/ja0nhjzl85ee1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9f5e80c57d6b78c9e2c11590bc0eaf5be7974335&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/roylljnm85ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba88186aa43a468f3d7174505cabe1d41f603628"&gt;https://preview.redd.it/roylljnm85ee1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba88186aa43a468f3d7174505cabe1d41f603628&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For detailed API usage guidelines, please refer to the official documentation: &lt;a href="https://api-docs.deepseek.com/zh-cn/guides/reasoning_model"&gt;&lt;strong&gt;https://api-docs.deepseek.com/zh-cn/guides/reasoning_model&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p9dk/deepseekr1_officially_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p9dk/deepseekr1_officially_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p9dk/deepseekr1_officially_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:39:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5ufr3</id>
    <title>DeepSeek-R1 crushed all other models in logical reasoning lineage-bench benchmark (successor of farel-bench)</title>
    <updated>2025-01-20T16:40:23+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/fairydreaming/lineage-bench"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5ufr3/deepseekr1_crushed_all_other_models_in_logical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5ufr3/deepseekr1_crushed_all_other_models_in_logical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T16:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5pepa</id>
    <title>DeepSeek-R1 Paper</title>
    <updated>2025-01-20T12:48:03+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pepa/deepseekr1_paper/"&gt; &lt;img alt="DeepSeek-R1 Paper" src="https://external-preview.redd.it/VH2iWMzzI7fr45Vwe4Cjkq8YtS-AXjXjtn6GyjVnsIQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee4cf33369b41133dc351d1a09cdeb6f80176720" title="DeepSeek-R1 Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pepa/deepseekr1_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pepa/deepseekr1_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5oygi</id>
    <title>DeepSeek test review</title>
    <updated>2025-01-20T12:20:37+00:00</updated>
    <author>
      <name>/u/Born-Shopping-1876</name>
      <uri>https://old.reddit.com/user/Born-Shopping-1876</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive been testing the new full R1 model, I gave it the research paper of Titans architecture from Google Research, and I ask to write a small description with json format, then make learn the architecture and build it using TensorFlow to implement it and train it into the jsom text. &lt;/p&gt; &lt;p&gt;I got the correct code after 2-shot of errors and model works great.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Born-Shopping-1876"&gt; /u/Born-Shopping-1876 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oygi/deepseek_test_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oygi/deepseek_test_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5oygi/deepseek_test_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:20:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i61ou3</id>
    <title>Deepseek R1 distill still knows better than me what is "safe" and "appropriate" for me, on my own computer. Is there an end to this corporate-security state driving our thoughts?</title>
    <updated>2025-01-20T21:30:51+00:00</updated>
    <author>
      <name>/u/Sidran</name>
      <uri>https://old.reddit.com/user/Sidran</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last generation of LLMs still wasnt properly uncensored through finetunes and abliteration and new, more sophisticated and still patronizing ones are starting to come out.&lt;br /&gt; What are your thoughts, are we sentenced to this patronizing crap until real intelligence emerges which will inevitably adjust to the needs of each individual user?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sidran"&gt; /u/Sidran &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i61ou3/deepseek_r1_distill_still_knows_better_than_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i61ou3/deepseek_r1_distill_still_knows_better_than_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i61ou3/deepseek_r1_distill_still_knows_better_than_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T21:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5m4t9</id>
    <title>let’s goo, DeppSeek-R1 685 billion parameters!</title>
    <updated>2025-01-20T09:02:18+00:00</updated>
    <author>
      <name>/u/bymechul</name>
      <uri>https://old.reddit.com/user/bymechul</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"&gt; &lt;img alt="let’s goo, DeppSeek-R1 685 billion parameters!" src="https://b.thumbs.redditmedia.com/9jF3vfuRP-Df2M-JGgdyrsvXvrMePxQdfuMlkImLCQs.jpg" title="let’s goo, DeppSeek-R1 685 billion parameters!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zqi0jvuc64ee1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3a7123049f14661883d9d61fcf7d776be647131"&gt;https://preview.redd.it/zqi0jvuc64ee1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3a7123049f14661883d9d61fcf7d776be647131&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bymechul"&gt; /u/bymechul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5m4t9/lets_goo_deppseekr1_685_billion_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T09:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s0hn</id>
    <title>2H After R1 Launch, Chinese Company KIMI Follows Up with Release of Multimodal Model Claiming O1 Capabilities.</title>
    <updated>2025-01-20T14:59:13+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s0hn/2h_after_r1_launch_chinese_company_kimi_follows/"&gt; &lt;img alt="2H After R1 Launch, Chinese Company KIMI Follows Up with Release of Multimodal Model Claiming O1 Capabilities." src="https://a.thumbs.redditmedia.com/pnqi8r9PscAmd0w4McK5SKbAkFyrzMq_aeqiCsuzp98.jpg" title="2H After R1 Launch, Chinese Company KIMI Follows Up with Release of Multimodal Model Claiming O1 Capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/MoonshotAI/kimi-k1.5"&gt;https://github.com/MoonshotAI/kimi-k1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yi7xkb39y5ee1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2edab721db0e7c615f267977c3868d632b5fa0b5"&gt;https://preview.redd.it/yi7xkb39y5ee1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2edab721db0e7c615f267977c3868d632b5fa0b5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k7qgl1tay5ee1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe10dc9a1b92e6ab5b7725d035deaa7a76d26f22"&gt;https://preview.redd.it/k7qgl1tay5ee1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe10dc9a1b92e6ab5b7725d035deaa7a76d26f22&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi k1.5 is the latest multimodal large language model (LLM) trained using reinforcement learning (RL), employing a simplified RL framework that avoids complex traditional RL techniques. The model has achieved leading reasoning performance across multiple benchmarks, such as AIME, MATH 500, Codeforces, and MathVista. Additionally, Kimi k1.5 significantly enhances the performance of short-context reasoning models through long-context scaling and improved policy optimization methods, surpassing existing short-context models like GPT-4o and Claude Sonnet 3.5 by a large margin. The service for Kimi k1.5 will soon be available on&lt;br /&gt; &lt;a href="https://kimi.ai"&gt;https://kimi.ai. &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s0hn/2h_after_r1_launch_chinese_company_kimi_follows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s0hn/2h_after_r1_launch_chinese_company_kimi_follows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s0hn/2h_after_r1_launch_chinese_company_kimi_follows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T14:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5jh1u</id>
    <title>Deepseek R1 / R1 Zero</title>
    <updated>2025-01-20T05:51:39+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"&gt; &lt;img alt="Deepseek R1 / R1 Zero" src="https://external-preview.redd.it/xCP95O-e963Wkcg4zsFa0x35jJRRGJ69TOc664LDsj0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfe6acc456fe810e684e2549f82a4f400608da67" title="Deepseek R1 / R1 Zero" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5jh1u/deepseek_r1_r1_zero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T05:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5wam1</id>
    <title>open source model small enough to run on a single 3090 performing WAY better in most benchmarks than the ultra proprietary closed source state of the art model from only a couple months ago</title>
    <updated>2025-01-20T17:55:37+00:00</updated>
    <author>
      <name>/u/pigeon57434</name>
      <uri>https://old.reddit.com/user/pigeon57434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wam1/open_source_model_small_enough_to_run_on_a_single/"&gt; &lt;img alt="open source model small enough to run on a single 3090 performing WAY better in most benchmarks than the ultra proprietary closed source state of the art model from only a couple months ago" src="https://b.thumbs.redditmedia.com/ReVuKfIsquTkaP-t8UU5GBN7psMoqLIsM3tJsEQKfaQ.jpg" title="open source model small enough to run on a single 3090 performing WAY better in most benchmarks than the ultra proprietary closed source state of the art model from only a couple months ago" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tcevum2it6ee1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=efffd7c575dc52e3a3c516e2b4e92c09e67c6c09"&gt;https://preview.redd.it/tcevum2it6ee1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=efffd7c575dc52e3a3c516e2b4e92c09e67c6c09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pigeon57434"&gt; /u/pigeon57434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wam1/open_source_model_small_enough_to_run_on_a_single/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wam1/open_source_model_small_enough_to_run_on_a_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wam1/open_source_model_small_enough_to_run_on_a_single/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T17:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5wvlp</id>
    <title>Phi-4 appears on LMSYS Arena with a score of 1210 ELO.</title>
    <updated>2025-01-20T18:18:30+00:00</updated>
    <author>
      <name>/u/jpydych</name>
      <uri>https://old.reddit.com/user/jpydych</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wvlp/phi4_appears_on_lmsys_arena_with_a_score_of_1210/"&gt; &lt;img alt="Phi-4 appears on LMSYS Arena with a score of 1210 ELO." src="https://preview.redd.it/qshhzl8gx6ee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d6df80d81db57fcec065a06b2f058dcce708aba" title="Phi-4 appears on LMSYS Arena with a score of 1210 ELO." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jpydych"&gt; /u/jpydych &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qshhzl8gx6ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wvlp/phi4_appears_on_lmsys_arena_with_a_score_of_1210/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5wvlp/phi4_appears_on_lmsys_arena_with_a_score_of_1210/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T18:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i62a0k</id>
    <title>Personal experience with Deepseek R1: it is noticeably better than claude sonnet 3.5</title>
    <updated>2025-01-20T21:55:14+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My usecases are mainly python and R for biological data analysis, as well as a little Frontend to build some interface for my colleagues. Where deepseek V3 was failing and claude sonnet needed 4-5 prompts, R1 creates instantly whatever file I need with one prompt. I only had one case where it did not succed with one prompt, but then accidentally solved the bug when asking him to add some logs for debugging lol. It is faster and just as reliable to ask him to build me a specific python code for a one time operation than wait for excel to open my 300 Mb csv. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i62a0k/personal_experience_with_deepseek_r1_it_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i62a0k/personal_experience_with_deepseek_r1_it_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i62a0k/personal_experience_with_deepseek_r1_it_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T21:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5z2qp</id>
    <title>Deepseek R1 generally outperforms o1-preview on Livebench</title>
    <updated>2025-01-20T19:46:11+00:00</updated>
    <author>
      <name>/u/MagmaElixir</name>
      <uri>https://old.reddit.com/user/MagmaElixir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5z2qp/deepseek_r1_generally_outperforms_o1preview_on/"&gt; &lt;img alt="Deepseek R1 generally outperforms o1-preview on Livebench" src="https://b.thumbs.redditmedia.com/wGQfr-B0B1as03uDz0n4m5RaPzG8LX4z4MTkWtAD_FU.jpg" title="Deepseek R1 generally outperforms o1-preview on Livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek R1 outperforms o1-preview on &lt;a href="http://Livebench.ai"&gt;Livebench.ai&lt;/a&gt; is all categories except for language for a fraction of the price. o1-preview is over 27x the cost on output tokens as R1.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sj9wm3w6d7ee1.png?width=1207&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac312ed8ee032fb663b72216bbec6520b0678801"&gt;Livebench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MagmaElixir"&gt; /u/MagmaElixir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5z2qp/deepseek_r1_generally_outperforms_o1preview_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5z2qp/deepseek_r1_generally_outperforms_o1preview_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5z2qp/deepseek_r1_generally_outperforms_o1preview_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T19:46:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i60rzj</id>
    <title>New R1 from DeepSeek has a second place on the livebench , is better in coding than sonnet 3.5 is we add reasoning</title>
    <updated>2025-01-20T20:53:50+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i60rzj/new_r1_from_deepseek_has_a_second_place_on_the/"&gt; &lt;img alt="New R1 from DeepSeek has a second place on the livebench , is better in coding than sonnet 3.5 is we add reasoning " src="https://preview.redd.it/e2jvwn57p7ee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95f3f40e2310fd3deae33a695db3d84766d74d06" title="New R1 from DeepSeek has a second place on the livebench , is better in coding than sonnet 3.5 is we add reasoning " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e2jvwn57p7ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i60rzj/new_r1_from_deepseek_has_a_second_place_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i60rzj/new_r1_from_deepseek_has_a_second_place_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T20:53:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s74x</id>
    <title>Deepseek-R1 GGUFs + All distilled 2 to 16bit GGUFs + 2bit MoE GGUFs</title>
    <updated>2025-01-20T15:07:11+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we uploaded GGUFs including 2, 3, 4, 5, 6, 8 and 16bit quants for &lt;strong&gt;Deepseek-R1's distilled models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;There's also for now a &lt;strong&gt;Q2_K_L 200GB quant&lt;/strong&gt; for the &lt;strong&gt;large R1 MoE&lt;/strong&gt; and R1 Zero models as well (uploading more)&lt;/p&gt; &lt;p&gt;We also uploaded &lt;a href="http://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; 4-bit &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;dynamic quant &lt;/a&gt;versions of the models for higher accuracy.&lt;/p&gt; &lt;p&gt;See all versions of the R1 models including GGUF's on Hugging Face: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-677cf5cfd7df8b7815fc723c"&gt;huggingface.co/collections/unsloth/deepseek-r1&lt;/a&gt;. For example the Llama 3 R1 distilled version GGUFs are here: &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;GGUF's:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;DeepSeek R1 version&lt;/th&gt; &lt;th align="left"&gt;GGUF links&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;R1 (MoE 671B params)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;R1&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Zero-GGUF"&gt;R1 Zero&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF"&gt;Llama 8B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF"&gt;Llama 3 (70B)&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF"&gt;14B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF"&gt;32B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5 Math&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF"&gt;1.5B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF"&gt;7B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;4-bit dynamic quants:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;DeepSeek R1 version&lt;/th&gt; &lt;th align="left"&gt;4-bit links&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit"&gt;Llama 8B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit"&gt;14B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5 Math&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit"&gt;1.5B&lt;/a&gt; • &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit-7B-GGUF"&gt;7B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;See more detailed instructions on how to run the big R1 model via llama.cpp in our blog: &lt;a href="http://unsloth.ai/blog/deepseek-r1"&gt;unsloth.ai/blog/deepseek-r1&lt;/a&gt; once we finish uploading it &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For some general steps:&lt;/p&gt; &lt;p&gt;Do not forget about `&amp;lt;｜User｜&amp;gt;` and `&amp;lt;｜Assistant｜&amp;gt;` tokens! - Or use a chat template formatter&lt;/p&gt; &lt;p&gt;Obtain the latest `llama.cpp` at &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;https://github.com/ggerganov/llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ --model unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf \ --cache-type-k q8_0 \ --threads 16 \ --prompt '&amp;lt;｜User｜&amp;gt;What is 1+1?&amp;lt;｜Assistant｜&amp;gt;' \ -no-cnv &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Example output:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;think&amp;gt; Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly. Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense. Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything. ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;PS. hope you guys have an amazing week! :) Also I'm still uploading stuff - some quants might not be there yet!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s74x/deepseekr1_ggufs_all_distilled_2_to_16bit_ggufs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s74x/deepseekr1_ggufs_all_distilled_2_to_16bit_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s74x/deepseekr1_ggufs_all_distilled_2_to_16bit_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T15:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5p549</id>
    <title>DeepSeek R1 has been officially released!</title>
    <updated>2025-01-20T12:32:02+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"&gt; &lt;img alt="DeepSeek R1 has been officially released! " src="https://external-preview.redd.it/EFqVCmP1lVQaIeFKK0xlX4mTF_zF6Me4AHhvUKZ16H4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2abe26f7a3e1642bf9cb27ef184bd854f2d59cf2" title="DeepSeek R1 has been officially released! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-R1"&gt;https://github.com/deepseek-ai/DeepSeek-R1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The complete technical report has been made publicly available on GitHub.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/azdqrrul75ee1.png?width=4702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d482d9acc77fb5e7a98eeb3a6dedcffb43a145d6"&gt;https://preview.redd.it/azdqrrul75ee1.png?width=4702&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d482d9acc77fb5e7a98eeb3a6dedcffb43a145d6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5p549/deepseek_r1_has_been_officially_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:32:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5piy1</id>
    <title>Deepseek R1 = $2.19/M tok output vs o1 $60/M tok. Insane</title>
    <updated>2025-01-20T12:54:57+00:00</updated>
    <author>
      <name>/u/cobalt1137</name>
      <uri>https://old.reddit.com/user/cobalt1137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know we will have to check out real world applications outside of benchmarks, but this is wild. Curious to hear anyone's comparisons also - esp for code gen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobalt1137"&gt; /u/cobalt1137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5piy1/deepseek_r1_219m_tok_output_vs_o1_60m_tok_insane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5piy1/deepseek_r1_219m_tok_output_vs_o1_60m_tok_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5piy1/deepseek_r1_219m_tok_output_vs_o1_60m_tok_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:54:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5t1be</id>
    <title>o1 thought for 12 minutes 35 sec, r1 thought for 5 minutes and 9 seconds. Both got a correct answer. Both in two tries. They are the first two models that have done it correctly.</title>
    <updated>2025-01-20T15:42:59+00:00</updated>
    <author>
      <name>/u/No_Training9444</name>
      <uri>https://old.reddit.com/user/No_Training9444</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5t1be/o1_thought_for_12_minutes_35_sec_r1_thought_for_5/"&gt; &lt;img alt="o1 thought for 12 minutes 35 sec, r1 thought for 5 minutes and 9 seconds. Both got a correct answer. Both in two tries. They are the first two models that have done it correctly. " src="https://preview.redd.it/g4tvkorg56ee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1e8c38200346a36079dd1ceecd332339fad57e4" title="o1 thought for 12 minutes 35 sec, r1 thought for 5 minutes and 9 seconds. Both got a correct answer. Both in two tries. They are the first two models that have done it correctly. " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Training9444"&gt; /u/No_Training9444 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g4tvkorg56ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5t1be/o1_thought_for_12_minutes_35_sec_r1_thought_for_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5t1be/o1_thought_for_12_minutes_35_sec_r1_thought_for_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T15:42:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i615u1</id>
    <title>The first time I've felt a LLM wrote *well*, not just well *for a LLM*.</title>
    <updated>2025-01-20T21:09:18+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/"&gt; &lt;img alt="The first time I've felt a LLM wrote *well*, not just well *for a LLM*." src="https://preview.redd.it/48kw0dyao7ee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85f94bde55ce83180ff26c640d9632cd2e976d23" title="The first time I've felt a LLM wrote *well*, not just well *for a LLM*." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/48kw0dyao7ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i615u1/the_first_time_ive_felt_a_llm_wrote_well_not_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T21:09:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5q6b9</id>
    <title>DeepSeek-R1 and distilled benchmarks color coded</title>
    <updated>2025-01-20T13:30:26+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5q6b9/deepseekr1_and_distilled_benchmarks_color_coded/"&gt; &lt;img alt="DeepSeek-R1 and distilled benchmarks color coded" src="https://b.thumbs.redditmedia.com/uR9Tld2vZxIJ2G0oapW1g73pOQppqKetkRf1z_lJAIg.jpg" title="DeepSeek-R1 and distilled benchmarks color coded" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5q6b9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5q6b9/deepseekr1_and_distilled_benchmarks_color_coded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5q6b9/deepseekr1_and_distilled_benchmarks_color_coded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T13:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s2yd</id>
    <title>DeepSeek-R1-Distill-Qwen-32B is straight SOTA, delivering more than GPT4o-level LLM for local use without any limits or restrictions!</title>
    <updated>2025-01-20T15:01:58+00:00</updated>
    <author>
      <name>/u/DarkArtsMastery</name>
      <uri>https://old.reddit.com/user/DarkArtsMastery</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s2yd/deepseekr1distillqwen32b_is_straight_sota/"&gt; &lt;img alt="DeepSeek-R1-Distill-Qwen-32B is straight SOTA, delivering more than GPT4o-level LLM for local use without any limits or restrictions! " src="https://external-preview.redd.it/iGeXnfpFa5fajUZA8437ltPpnvjIlHkFysn4PHBZTIg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e0d69a4ff14bebc6aa8e3ea68b33c5b632d47d8" title="DeepSeek-R1-Distill-Qwen-32B is straight SOTA, delivering more than GPT4o-level LLM for local use without any limits or restrictions! " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF"&gt;https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/02np5yx0y5ee1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1812d10e51aa9f08460335eddc6e78dd23384ce2"&gt;https://preview.redd.it/02np5yx0y5ee1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1812d10e51aa9f08460335eddc6e78dd23384ce2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DeepSeek really has done something special with distilling the big R1 model into other open-source models. Especially the fusion with Qwen-32B seems to deliver insane gains across benchmarks and makes it go-to model for people with less VRAM, pretty much giving the overall best results compared to LLama-70B distill. Easily current SOTA for local LLMs, and it should be fairly performant even on consumer hardware.&lt;/p&gt; &lt;p&gt;Who else can't wait for upcoming Qwen 3?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkArtsMastery"&gt; /u/DarkArtsMastery &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s2yd/deepseekr1distillqwen32b_is_straight_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s2yd/deepseekr1distillqwen32b_is_straight_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s2yd/deepseekr1distillqwen32b_is_straight_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T15:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5pbb3</id>
    <title>o1 performance at ~1/50th the cost.. and Open Source!! WTF let's goo!!</title>
    <updated>2025-01-20T12:42:33+00:00</updated>
    <author>
      <name>/u/Consistent_Bit_3295</name>
      <uri>https://old.reddit.com/user/Consistent_Bit_3295</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pbb3/o1_performance_at_150th_the_cost_and_open_source/"&gt; &lt;img alt="o1 performance at ~1/50th the cost.. and Open Source!! WTF let's goo!!" src="https://b.thumbs.redditmedia.com/gdpGFb_knvwb6nUDYwf-wMTftZq5nEGNIQeq1omODJI.jpg" title="o1 performance at ~1/50th the cost.. and Open Source!! WTF let's goo!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Bit_3295"&gt; /u/Consistent_Bit_3295 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1i5pbb3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pbb3/o1_performance_at_150th_the_cost_and_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5pbb3/o1_performance_at_150th_the_cost_and_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:42:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5or1y</id>
    <title>Deepseek just uploaded 6 distilled verions of R1 + R1 "full" now available on their website.</title>
    <updated>2025-01-20T12:07:33+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5or1y/deepseek_just_uploaded_6_distilled_verions_of_r1/"&gt; &lt;img alt="Deepseek just uploaded 6 distilled verions of R1 + R1 &amp;quot;full&amp;quot; now available on their website." src="https://external-preview.redd.it/_atc5Wper5qoTlKRLhG_b9IdHbfvAzDOL9GdRqfNNpk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb0be2c39108ae40a2fceffcb31c8521a0e79a4a" title="Deepseek just uploaded 6 distilled verions of R1 + R1 &amp;quot;full&amp;quot; now available on their website." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5or1y/deepseek_just_uploaded_6_distilled_verions_of_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5or1y/deepseek_just_uploaded_6_distilled_verions_of_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T12:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i5s5hk</id>
    <title>OpenAI sweating bullets rn</title>
    <updated>2025-01-20T15:05:09+00:00</updated>
    <author>
      <name>/u/ThroughForests</name>
      <uri>https://old.reddit.com/user/ThroughForests</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s5hk/openai_sweating_bullets_rn/"&gt; &lt;img alt="OpenAI sweating bullets rn" src="https://preview.redd.it/b2fm3y9uy5ee1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3a6e07ad1ccbf40dcef766d2a3fa367543a642e" title="OpenAI sweating bullets rn" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThroughForests"&gt; /u/ThroughForests &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b2fm3y9uy5ee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s5hk/openai_sweating_bullets_rn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i5s5hk/openai_sweating_bullets_rn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-20T15:05:09+00:00</published>
  </entry>
</feed>
