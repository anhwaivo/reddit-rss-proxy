<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-27T21:34:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1k99c5p</id>
    <title>Are there any reasoning storytelling/roleplay models that use deepseek level reasoning to avoid plot holes and keep it realistic?</title>
    <updated>2025-04-27T17:25:34+00:00</updated>
    <author>
      <name>/u/No-Issue-9136</name>
      <uri>https://old.reddit.com/user/No-Issue-9136</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried deepseek when it first came out but it was awful at it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Issue-9136"&gt; /u/No-Issue-9136 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k99c5p/are_there_any_reasoning_storytellingroleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k99c5p/are_there_any_reasoning_storytellingroleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k99c5p/are_there_any_reasoning_storytellingroleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T17:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8n0de</id>
    <title>NotebookLM-Style Dia – Imperfect but Getting Close</title>
    <updated>2025-04-26T20:56:50+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8n0de/notebooklmstyle_dia_imperfect_but_getting_close/"&gt; &lt;img alt="NotebookLM-Style Dia – Imperfect but Getting Close" src="https://external-preview.redd.it/eTJ3bWZsbWJxOHhlMdo2OU2R9yA1Xpyg2FE52Rzb3D8ISdcjzfkEj4iTIyws.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04dc1bdee3bd7ada23838e805242ef81ebdd3f08" title="NotebookLM-Style Dia – Imperfect but Getting Close" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/PasiKoodaa/dia"&gt;https://github.com/PasiKoodaa/dia&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is not yet stable enough to produce 100% perfect results, and this app is also far from flawless. It’s often unclear whether generation failures are due to limitations in the model, issues in the app's code, or incorrect app settings. For instance, there are occasional instances where the last word of a speaker's output might be missing. But it's getting closer to NoteBookLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ak3jq9mbq8xe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8n0de/notebooklmstyle_dia_imperfect_but_getting_close/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8n0de/notebooklmstyle_dia_imperfect_but_getting_close/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T20:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1k916b3</id>
    <title>What UI is he using? Looks like ComfyUI but for text?</title>
    <updated>2025-04-27T10:49:38+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k916b3/what_ui_is_he_using_looks_like_comfyui_but_for/"&gt; &lt;img alt="What UI is he using? Looks like ComfyUI but for text?" src="https://b.thumbs.redditmedia.com/cjo_Dt8ykCBx-xq5FTCMC7_jS5yL02p60je3K7j8ovo.jpg" title="What UI is he using? Looks like ComfyUI but for text?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not sure if it's not just a mockup workflow. Found that on someone's page where he offers LLM services such as building AI agents.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m9k29nnpxcxe1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2fa0c0ea31d622e87a29141d91b6b51f03593c4c"&gt;https://preview.redd.it/m9k29nnpxcxe1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2fa0c0ea31d622e87a29141d91b6b51f03593c4c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And if it doesn't exist as an UI, it should.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k916b3/what_ui_is_he_using_looks_like_comfyui_but_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k916b3/what_ui_is_he_using_looks_like_comfyui_but_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k916b3/what_ui_is_he_using_looks_like_comfyui_but_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T10:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1k93ipk</id>
    <title>Has anyone successfully used local models with n8n, Ollama and MCP tools/servers?</title>
    <updated>2025-04-27T13:06:05+00:00</updated>
    <author>
      <name>/u/onicarps</name>
      <uri>https://old.reddit.com/user/onicarps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to set up an n8n workflow with Ollama and MCP servers (specifically Google Tasks and Calendar), but I'm running into issues with JSON parsing from the tool responses. My AI Agent node keeps returning the error &amp;quot;Non string tool message content is not supported&amp;quot; when using local models&lt;/p&gt; &lt;p&gt;From what I've gathered, this seems to be a common issue with Ollama and local models when handling MCP tool responses. I've tried several approaches but haven't found a solution that works.&lt;/p&gt; &lt;p&gt;Has anyone successfully:&lt;/p&gt; &lt;p&gt;- Used a local model through Ollama with n8n's AI Agent node&lt;/p&gt; &lt;p&gt;- Connected it to MCP servers/tools&lt;/p&gt; &lt;p&gt;- Gotten it to properly parse JSON responses&lt;/p&gt; &lt;p&gt;If so:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Which specific model worked for you?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Did you need any special configuration or workarounds?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any tips for handling the JSON responses from MCP tools?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I've seen that OpenAI models work fine with this setup, but I'm specifically looking to keep everything local. According to some posts I've found, there might be certain models that handle tool calling better than others, but I haven't found specific recommendations.&lt;/p&gt; &lt;p&gt;Any guidance would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onicarps"&gt; /u/onicarps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k93ipk/has_anyone_successfully_used_local_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k93ipk/has_anyone_successfully_used_local_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k93ipk/has_anyone_successfully_used_local_models_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T13:06:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9d20u</id>
    <title>Help Needed: Splitting Quantized MADLAD-400 3B ONNX</title>
    <updated>2025-04-27T20:03:14+00:00</updated>
    <author>
      <name>/u/Away_Expression_3713</name>
      <uri>https://old.reddit.com/user/Away_Expression_3713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone in the community already created these specific split MADLAD ONNX components (&lt;code&gt;embed&lt;/code&gt;, &lt;code&gt;cache_initializer&lt;/code&gt;) for mobile use?&lt;/p&gt; &lt;p&gt;I don't have access to Google Colab Pro or a local machine with enough RAM (32GB+ recommended) to run the necessary ONNX manipulation scripts&lt;/p&gt; &lt;p&gt;would anyone with the necessary high-RAM compute resources be willing to help to run the script?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Away_Expression_3713"&gt; /u/Away_Expression_3713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9d20u/help_needed_splitting_quantized_madlad400_3b_onnx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9d20u/help_needed_splitting_quantized_madlad400_3b_onnx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k9d20u/help_needed_splitting_quantized_madlad400_3b_onnx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T20:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8hob9</id>
    <title>My AI dev prompt playbook that actually works (saves me 10+ hrs/week)</title>
    <updated>2025-04-26T17:00:42+00:00</updated>
    <author>
      <name>/u/namanyayg</name>
      <uri>https://old.reddit.com/user/namanyayg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been using AI tools to speed up my dev workflow for about 2 years now, and I've finally got a system that doesn't suck. Thought I'd share my prompt playbook since it's helped me ship way faster.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix the root cause&lt;/strong&gt;: when debugging, AI usually tries to patch the end result instead of understanding the root cause. Use this prompt for that case:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Analyze this error: [bug details] Don't just fix the immediate issue. Identify the underlying root cause by: - Examining potential architectural problems - Considering edge cases - Suggesting a comprehensive solution that prevents similar issues &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ask for explanations:&lt;/strong&gt; Here's another one that's saved my ass repeatedly - the &amp;quot;explain what you just generated&amp;quot; prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Can you explain what you generated in detail: 1. What is the purpose of this section? 2. How does it work step-by-step? 3. What alternatives did you consider and why did you choose this one? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Forcing myself to understand ALL code before implementation has eliminated so many headaches down the road.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My personal favorite:&lt;/strong&gt; what I call the &amp;quot;rage prompt&amp;quot; (I usually have more swear words lol):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This code is DRIVING ME CRAZY. It should be doing [expected] but instead it's [actual]. PLEASE help me figure out what's wrong with it: [code] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This works way better than it should! Sometimes being direct cuts through the BS and gets you answers faster.&lt;/p&gt; &lt;p&gt;The main thing I've learned is that AI is like any other tool - it's all about HOW you use it.&lt;/p&gt; &lt;p&gt;Good prompts = good results. Bad prompts = garbage.&lt;/p&gt; &lt;p&gt;What prompts have y'all found useful? I'm always looking to improve my workflow.&lt;/p&gt; &lt;p&gt;EDIT: This is blowing up! I added some more details + included some more prompts on my blog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://nmn.gl/blog/ai-prompt-engineering"&gt;https://nmn.gl/blog/ai-prompt-engineering&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/namanyayg"&gt; /u/namanyayg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T17:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9eopy</id>
    <title>TabbyAPI error after new installation</title>
    <updated>2025-04-27T21:13:57+00:00</updated>
    <author>
      <name>/u/apel-sin</name>
      <uri>https://old.reddit.com/user/apel-sin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Friends, please help with installing the actual TabbyAPI with exllama2.9. The new installation gives this:&lt;/p&gt; &lt;p&gt;&lt;code&gt; (tabby-api) serge@box:/home/text-generation/servers/tabby-api$ ./start.sh It looks like you're in a conda environment. Skipping venv check. pip 25.0 from /home/serge/.miniconda/envs/tabby-api/lib/python3.12/site-packages/pip (python 3.12) Loaded your saved preferences from `start_options.json` Traceback (most recent call last): File &amp;quot;/home/text-generation/servers/tabby-api/start.py&amp;quot;, line 274, in &amp;lt;module&amp;gt; from main import entrypoint File &amp;quot;/home/text-generation/servers/tabby-api/main.py&amp;quot;, line 12, in &amp;lt;module&amp;gt; from common import gen_logging, sampling, model File &amp;quot;/home/text-generation/servers/tabby-api/common/model.py&amp;quot;, line 15, in &amp;lt;module&amp;gt; from backends.base_model_container import BaseModelContainer File &amp;quot;/home/text-generation/servers/tabby-api/backends/base_model_container.py&amp;quot;, line 13, in &amp;lt;module&amp;gt; from common.multimodal import MultimodalEmbeddingWrapper File &amp;quot;/home/text-generation/servers/tabby-api/common/multimodal.py&amp;quot;, line 1, in &amp;lt;module&amp;gt; from backends.exllamav2.vision import get_image_embedding File &amp;quot;/home/text-generation/servers/tabby-api/backends/exllamav2/vision.py&amp;quot;, line 21, in &amp;lt;module&amp;gt; from exllamav2.generator import ExLlamaV2MMEmbedding File &amp;quot;/home/serge/.miniconda/envs/tabby-api/lib/python3.12/site-packages/exllamav2/__init__.py&amp;quot;, line 3, in &amp;lt;module&amp;gt; from exllamav2.model import ExLlamaV2 File &amp;quot;/home/serge/.miniconda/envs/tabby-api/lib/python3.12/site-packages/exllamav2/model.py&amp;quot;, line 33, in &amp;lt;module&amp;gt; from exllamav2.config import ExLlamaV2Config File &amp;quot;/home/serge/.miniconda/envs/tabby-api/lib/python3.12/site-packages/exllamav2/config.py&amp;quot;, line 5, in &amp;lt;module&amp;gt; from exllamav2.stloader import STFile, cleanup_stfiles File &amp;quot;/home/serge/.miniconda/envs/tabby-api/lib/python3.12/site-packages/exllamav2/stloader.py&amp;quot;, line 5, in &amp;lt;module&amp;gt; from exllamav2.ext import none_tensor, exllamav2_ext as ext_c File &amp;quot;/home/serge/.miniconda/envs/tabby-api/lib/python3.12/site-packages/exllamav2/ext.py&amp;quot;, line 291, in &amp;lt;module&amp;gt; ext_c = exllamav2_ext ^^^^^^^^^^^^^ NameError: name 'exllamav2_ext' is not defined &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apel-sin"&gt; /u/apel-sin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9eopy/tabbyapi_error_after_new_installation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9eopy/tabbyapi_error_after_new_installation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k9eopy/tabbyapi_error_after_new_installation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T21:13:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1k96ur9</id>
    <title>Best method of quantizing Gemma 3 for use with vLLM?</title>
    <updated>2025-04-27T15:40:02+00:00</updated>
    <author>
      <name>/u/Saguna_Brahman</name>
      <uri>https://old.reddit.com/user/Saguna_Brahman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've sort of been tearing out my hair trying to figure this out. I want to use the new Gemma 3 27B models with vLLM, specifically the QAT models, but the two easiest ways to quantize something (GGUF, BnB) are not optimized in vLLM and the performance degradation is pretty drastic. vLLM seems to be optimized for GPTQModel and AWQ, but neither seem to have strong Gemma 3 support right now.&lt;/p&gt; &lt;p&gt;Notably, GPTQModel doesn't work with multimodal Gemma 3, and the process of making the 27b model text-only and then quantizing it has proven tricky for various reasons.&lt;/p&gt; &lt;p&gt;GPTQ compression seems possible given this model: &lt;a href="https://huggingface.co/ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g"&gt;https://huggingface.co/ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g&lt;/a&gt; but they did that on the original 27B, not the unquantized QAT model.&lt;/p&gt; &lt;p&gt;For the life of me I haven't been able to make this work, and it's driving me nuts. Any advice from more experienced users? At this point I'd even pay someone to upload a 4bit version of &lt;a href="https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-unquantized"&gt;this model&lt;/a&gt; in GPTQ to hugging face if they had the know-how.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Saguna_Brahman"&gt; /u/Saguna_Brahman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k96ur9/best_method_of_quantizing_gemma_3_for_use_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k96ur9/best_method_of_quantizing_gemma_3_for_use_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k96ur9/best_method_of_quantizing_gemma_3_for_use_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T15:40:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8ncco</id>
    <title>Introducing Kimi Audio 7B, a SOTA audio foundation model</title>
    <updated>2025-04-26T21:11:34+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ncco/introducing_kimi_audio_7b_a_sota_audio_foundation/"&gt; &lt;img alt="Introducing Kimi Audio 7B, a SOTA audio foundation model" src="https://external-preview.redd.it/p4q_Zkpix3p8TiVMmz6bqei1OGSeQFuMhiONWJiDPGQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db35b2daa9cfe12cd1fa69d51172fee3172edc92" title="Introducing Kimi Audio 7B, a SOTA audio foundation model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on Qwen 2.5 btw&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Audio-7B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ncco/introducing_kimi_audio_7b_a_sota_audio_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8ncco/introducing_kimi_audio_7b_a_sota_audio_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T21:11:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8t8z9</id>
    <title>New Reasoning Model from NVIDIA (AIME is getting saturated at this point!)</title>
    <updated>2025-04-27T02:08:41+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8t8z9/new_reasoning_model_from_nvidia_aime_is_getting/"&gt; &lt;img alt="New Reasoning Model from NVIDIA (AIME is getting saturated at this point!)" src="https://external-preview.redd.it/6doJTf1GdAI5T-9SwoTSuPFBmq9PsTM3q-ChSWlGb_o.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4cd45bd18ce212c176a456f3dc0390fe542b9c06" title="New Reasoning Model from NVIDIA (AIME is getting saturated at this point!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(disclaimer, it's just a qwen2.5 32b fine tune)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/OpenMath-Nemotron-32B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8t8z9/new_reasoning_model_from_nvidia_aime_is_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8t8z9/new_reasoning_model_from_nvidia_aime_is_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T02:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8xu4d</id>
    <title>🚀 [Release] llama-cpp-python 0.3.8 (CUDA 12.8) Prebuilt Wheel + Full Gemma 3 Support (Windows x64)</title>
    <updated>2025-04-27T06:53:42+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xu4d/release_llamacpppython_038_cuda_128_prebuilt/"&gt; &lt;img alt="🚀 [Release] llama-cpp-python 0.3.8 (CUDA 12.8) Prebuilt Wheel + Full Gemma 3 Support (Windows x64)" src="https://external-preview.redd.it/fEQ0sRkP5Cc9pEvs5-UwG3ZTsuSDMCRcakJgt0TeA4k.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=984dc75cd6fc1d4afcf245873710ed59ecf086db" title="🚀 [Release] llama-cpp-python 0.3.8 (CUDA 12.8) Prebuilt Wheel + Full Gemma 3 Support (Windows x64)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;After a lot of work, I'm excited to share a &lt;strong&gt;prebuilt CUDA 12.8 wheel&lt;/strong&gt; for &lt;strong&gt;llama-cpp-python (version 0.3.8)&lt;/strong&gt; — built specifically for &lt;strong&gt;Windows 10/11 (x64)&lt;/strong&gt; systems!&lt;/p&gt; &lt;h1&gt;✅ Highlights:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CUDA 12.8 GPU acceleration&lt;/strong&gt; fully enabled&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Gemma 3 model support&lt;/strong&gt; (1B, 4B, 12B, 27B)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built against llama.cpp b5192&lt;/strong&gt; (April 26, 2025)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tested and verified&lt;/strong&gt; on a dual-GPU setup (3090 + 4060 Ti)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Working production inference&lt;/strong&gt; at &lt;strong&gt;16k context length&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No manual compilation&lt;/strong&gt; needed — just &lt;code&gt;pip install&lt;/code&gt; and you're running!&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🔥 Why This Matters&lt;/h1&gt; &lt;p&gt;Building &lt;code&gt;llama-cpp-python&lt;/code&gt; with CUDA on Windows is notoriously painful —&lt;br /&gt; CMake configs, Visual Studio toolchains, CUDA paths... it’s a nightmare.&lt;/p&gt; &lt;p&gt;This wheel &lt;strong&gt;eliminates all of that&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No CMake.&lt;/li&gt; &lt;li&gt;No Visual Studio setup.&lt;/li&gt; &lt;li&gt;No manual CUDA environment tuning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Just download the&lt;/strong&gt; &lt;code&gt;.whl&lt;/code&gt;&lt;strong&gt;, install with pip, and you're ready to run Gemma 3 models on GPU immediately.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;✨ Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;I haven't been able to find &lt;strong&gt;any other prebuilt llama-cpp-python wheel&lt;/strong&gt; supporting &lt;strong&gt;Gemma 3 + CUDA 12.8&lt;/strong&gt; on Windows — so I thought I'd post this ASAP.&lt;/li&gt; &lt;li&gt;I know you Linux folks are way ahead of me — but hey, now Windows users can play too! 😄&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/boneylizard/llama-cpp-python-cu128-gemma3/releases"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xu4d/release_llamacpppython_038_cuda_128_prebuilt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xu4d/release_llamacpppython_038_cuda_128_prebuilt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T06:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8xb3k</id>
    <title>Overwhelmed by the number of Gemma 3 27B QAT variants</title>
    <updated>2025-04-27T06:16:58+00:00</updated>
    <author>
      <name>/u/iwinux</name>
      <uri>https://old.reddit.com/user/iwinux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the Q4 quantization alone, I found 3 variants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;google/gemma-3-27b-it-qat-q4_0-gguf&lt;/code&gt;, official release, 17.2GB, seems to have some token-related issues according to this &lt;a href="https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf/discussions/3"&gt;discussion&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;stduhpf/google-gemma-3-27b-it-qat-q4_0-gguf-small&lt;/code&gt;, requantized, 15.6GB, states to fix the issues mentioned above.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;jaxchang/google-gemma-3-27b-it-qat-q4_0-gguf-fix&lt;/code&gt;, further derived from stduhpf's variant, 15.6GB, states to fix some more issues?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Even more variants that are derived from &lt;code&gt;google/gemma-3-27b-it-qat-q4_0-unquantized&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;bartowski/google_gemma-3-27b-it-qat-GGUF&lt;/code&gt; offers llama.cpp-specific quantizations from Q2 to Q8.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;unsloth/gemma-3-27b-it-qat-GGUF&lt;/code&gt; also offers Q2 to Q8 quantizations, and I can't figure what they have changed because the model description looks like copy-pasta.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How am I supposed to know which one to use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iwinux"&gt; /u/iwinux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xb3k/overwhelmed_by_the_number_of_gemma_3_27b_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xb3k/overwhelmed_by_the_number_of_gemma_3_27b_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xb3k/overwhelmed_by_the_number_of_gemma_3_27b_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T06:16:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9cj6j</id>
    <title>Building a Simple Multi-LLM design to Catch Hallucinations and Improve Quality (Looking for Feedback)</title>
    <updated>2025-04-27T19:41:01+00:00</updated>
    <author>
      <name>/u/Reddit_wander01</name>
      <uri>https://old.reddit.com/user/Reddit_wander01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9cj6j/building_a_simple_multillm_design_to_catch/"&gt; &lt;img alt="Building a Simple Multi-LLM design to Catch Hallucinations and Improve Quality (Looking for Feedback)" src="https://preview.redd.it/dz3w8karkfxe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21f77fd6d89c39d6a16a95956902a83c3cad55aa" title="Building a Simple Multi-LLM design to Catch Hallucinations and Improve Quality (Looking for Feedback)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was reading newer LLM models are hallucinating more with weird tone shifts and broken logic chains that are getting harder to catch versus easier. (eg, &lt;a href="https://techcrunch.com/2025/04/18/openais-new-reasoning-ai-models-hallucinate-more/"&gt;https://techcrunch.com/2025/04/18/openais-new-reasoning-ai-models-hallucinate-more/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I’m messing around with an idea with ChatGPT to build a &amp;quot;team&amp;quot; of various LLM models that watch and advise a primary LLM, validating responses and reduceing hallucinations during a conversation. The team would be 3-5 LLM agents that monitor, audit, and improve output by reducing hallucinations, tone drift, logical inconsistencies, and quality degradation. One model would do the main task (generate text, answer questions, etc.) then 2 or 3 &amp;quot;oversight&amp;quot; LLM agents would check the output for issues. If things look sketchy, the team “votes or escalates” the item to the primary LLM agent for corrective action, advice and/or guidance.&lt;/p&gt; &lt;p&gt;The goal is to build a relatively simple/inexpensive (~ $200-300/month), mostly open-source solution by using tools like ChatGPT Pro, Gemini Advanced, CrewAI, LangGraph, Zapier, etc. with other top 10 LLM’s as needed, choosing strengths to function.&lt;/p&gt; &lt;p&gt;Once out of design and into testing the plan is to run parallel tests with standard tests like TruthfulQA and HaluEval to compare results and see if there is any significant improvements.&lt;/p&gt; &lt;p&gt;Questions: (yes… this is a ChatGPT co- conceived solution….) &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is this structure and concept realistic, theoretically possible to build and actually work? ChatGPT Is infamous with me creating stuff that’s just not right sometimes so good to catch it early &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are there better ways to orchestrate multi-agent QA?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is it reasonable to expect this to work at low infrastructure cost using existing tools like ChatGPT Pro, Gemini Advanced, CrewAI, LangGraph, etc.? I understand API text calls/token cost will be relatively low (~$10.00/day) compared to the service I hope it provides and the open source libraries (CrewAI, LangGraph), Zapier, WordPress, Notion, GPT Custom Instructions are accessible now.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Has anyone seen someone try something like this before (even partly)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any failure traps, risks, oversights? (eg agents hallucinating themselves)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any better ways to structure it? This will be addition to all prompt guidance and best practices followed.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any extra oversight roles I should think about adding?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Basically I’m just trying to build a practical tool to tackle hallucinations described in the news and improve conversation quality issues before they get worse.&lt;/p&gt; &lt;p&gt;Open to any ideas, critique, references, or stories. Most importantly, I”m just another ChatGPT fantasy I should expect to crash and burn on and should cut my loses now. Thanks for reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddit_wander01"&gt; /u/Reddit_wander01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dz3w8karkfxe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9cj6j/building_a_simple_multillm_design_to_catch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k9cj6j/building_a_simple_multillm_design_to_catch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T19:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1k98ooy</id>
    <title>AMD thinking of cancelling 9060XT and focusing on a 16gb vram card</title>
    <updated>2025-04-27T16:58:23+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As an AMD fanboy ( I know. wrong hobby for me), interested to see where this goes. And how much it will cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k98ooy/amd_thinking_of_cancelling_9060xt_and_focusing_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k98ooy/amd_thinking_of_cancelling_9060xt_and_focusing_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k98ooy/amd_thinking_of_cancelling_9060xt_and_focusing_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T16:58:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1k91xpn</id>
    <title>[Tool] GPU Price Tracker</title>
    <updated>2025-04-27T11:36:53+00:00</updated>
    <author>
      <name>/u/yachty66</name>
      <uri>https://old.reddit.com/user/yachty66</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k91xpn/tool_gpu_price_tracker/"&gt; &lt;img alt="[Tool] GPU Price Tracker" src="https://b.thumbs.redditmedia.com/oZ5A4SlEGNKG9l7wwa5jk4MlpFBQ6rWdgJ8EHM3II9k.jpg" title="[Tool] GPU Price Tracker" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I wanted to share a tool I've developed that might help many of you with hardware purchasing decisions for running local LLMs.&lt;/p&gt; &lt;h1&gt;GPU Price Tracker Overview&lt;/h1&gt; &lt;p&gt;I built a comprehensive GPU Price Tracker that monitors current prices, specifications, and historical price trends for GPUs. This tool is specifically designed to help make informed decisions when selecting hardware for AI workloads, including running LocalLLaMA models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tool URL:&lt;/strong&gt; &lt;a href="https://www.unitedcompute.ai/gpu-price-tracker"&gt;https://www.unitedcompute.ai/gpu-price-tracker&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Daily Market Prices&lt;/strong&gt; - Daily updated pricing data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complete Price History&lt;/strong&gt; - Track price fluctuations since release date&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Metrics&lt;/strong&gt; - FP16 TFLOPS performance data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficiency Metrics&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FL/$&lt;/strong&gt; - FLOPS per dollar (value metric)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FL/Watt&lt;/strong&gt; - FLOPS per watt (efficiency metric)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware Specifications&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;VRAM capacity and bus width&lt;/li&gt; &lt;li&gt;Power consumption (Watts)&lt;/li&gt; &lt;li&gt;Memory bandwidth&lt;/li&gt; &lt;li&gt;Release date&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Example Insights&lt;/h1&gt; &lt;p&gt;The data reveals some interesting trends:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The NVIDIA A100 40GB PCIe remains at a premium price point ($7,999.99) but offers 77.97 TFLOPS with 0.010 TFLOPS/$&lt;/li&gt; &lt;li&gt;The RTX 3090 provides better value at $1,679.99 with 35.58 TFLOPS and 0.021 TFLOPS/$&lt;/li&gt; &lt;li&gt;Price fluctuations can be significant - as shown in the historical view below, some GPUs have varied by over $2,000 in a single year&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How This Helps LocalLLaMA Users&lt;/h1&gt; &lt;p&gt;When selecting hardware for running local LLMs, there are multiple considerations:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Raw Performance&lt;/strong&gt; - FP16 TFLOPS for inference speed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Requirements&lt;/strong&gt; - For model size limitations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value&lt;/strong&gt; - FL/$ for budget-conscious decisions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Power Efficiency&lt;/strong&gt; - FL&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ymez54ch5dxe1.png?width=1418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f481589051d4240120e5f378ac1287aa95b3638d"&gt;GPU Price Tracker Main View (example for 3090)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yachty66"&gt; /u/yachty66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k91xpn/tool_gpu_price_tracker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k91xpn/tool_gpu_price_tracker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k91xpn/tool_gpu_price_tracker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T11:36:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1k93kfh</id>
    <title>Got Sesame CSM working with a real time factor of .6x with a 4070Ti Super!</title>
    <updated>2025-04-27T13:08:30+00:00</updated>
    <author>
      <name>/u/DumaDuma</name>
      <uri>https://old.reddit.com/user/DumaDuma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ReisCook/VoiceAssistant"&gt;https://github.com/ReisCook/VoiceAssistant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still have more work to do but it’s functional. Having an issue where the output gets cut off prematurely atm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DumaDuma"&gt; /u/DumaDuma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k93kfh/got_sesame_csm_working_with_a_real_time_factor_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k93kfh/got_sesame_csm_working_with_a_real_time_factor_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k93kfh/got_sesame_csm_working_with_a_real_time_factor_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T13:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9bwbg</id>
    <title>High-processing level for any model at home! Only one python file!</title>
    <updated>2025-04-27T19:13:48+00:00</updated>
    <author>
      <name>/u/AlexBefest</name>
      <uri>https://old.reddit.com/user/AlexBefest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1k9bwbg/video/pw1tppcrefxe1/player"&gt;https://reddit.com/link/1k9bwbg/video/pw1tppcrefxe1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A single Python file that connects via the OpenAI Chat Completions API, giving you something akin to OpenAI High Compute at home. Any models are compatible. Using dynamic programming methods, computational capacity is increased by tens or even hundreds of times for both reasoning and non-reasoning models, significantly improving answer quality and the ability to solve extremely complex tasks for LLMs.&lt;/p&gt; &lt;p&gt;This is a simple Gradio-based web application providing an interface for interacting with a locally hosted Large Language Model (LLM). The key feature is the ability to select a &amp;quot;Computation Level,&amp;quot; which determines the strategy for processing user queries—ranging from direct responses to multi-level task decomposition for obtaining more structured and comprehensive answers to complex queries.&lt;/p&gt; &lt;h1&gt;🌟 Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local LLM Integration:&lt;/strong&gt; Works with your own LLM server (e.g., llama.cpp, Ollama, LM Studio, vLLM with an OpenAI-compatible endpoint).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compute Levels:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Low:&lt;/strong&gt; Direct query to the LLM for a quick response. This is a standard chat mode. Generates N tokens — for example, solving a task may only consume 700 tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Medium:&lt;/strong&gt; Single-level task decomposition into subtasks, solving them, and synthesizing the final answer. Suitable for moderately complex queries. The number of generated tokens is approximately 10-15x higher compared to Low Compute (average value, depends on the task): if solving a task in Low Compute took 700 tokens, Medium level would require around 7,000 tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High:&lt;/strong&gt; Two-level task decomposition (stages → steps), solving individual steps, synthesizing stage results, and generating the final answer. Designed for highly complex and multi-component tasks. The number of generated tokens is approximately 100-150x higher compared to Low Compute: if solving a task in Low Compute took 700 tokens, High level would require around 70,000 tokens.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Compute Adjustment:&lt;/strong&gt; You can freely adjust the Compute Level for each query individually. For example, initiate the first query in High Compute, then switch to Low mode, and later use Medium Compute to solve a specific problem mid-chat.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;UPD: Github Link in commnets. Sorry, but reddit keeps removing my post because of the link(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexBefest"&gt; /u/AlexBefest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9bwbg/highprocessing_level_for_any_model_at_home_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9bwbg/highprocessing_level_for_any_model_at_home_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k9bwbg/highprocessing_level_for_any_model_at_home_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T19:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8yrem</id>
    <title>Made Mistral 24B code like a senior dev by making it recursively argue with itself</title>
    <updated>2025-04-27T07:59:40+00:00</updated>
    <author>
      <name>/u/HearMeOut-13</name>
      <uri>https://old.reddit.com/user/HearMeOut-13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yrem/made_mistral_24b_code_like_a_senior_dev_by_making/"&gt; &lt;img alt="Made Mistral 24B code like a senior dev by making it recursively argue with itself" src="https://b.thumbs.redditmedia.com/I1eroeMkhMj9dvaQstK2YCuhXrEczxhGjXs1EiERsCU.jpg" title="Made Mistral 24B code like a senior dev by making it recursively argue with itself" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with local models lately and built something that dramatically improves their output quality without fine-tuning or fancy prompting.&lt;/p&gt; &lt;p&gt;I call it CoRT (Chain of Recursive Thoughts). The idea is simple: make the model generate multiple responses, evaluate them, and iteratively improve. Like giving it the ability to second-guess itself. With Mistral 24B Tic-tac-toe game went from basic CLI(Non CoRT) to full OOP with AI opponent(CoRT)&lt;/p&gt; &lt;p&gt;What's interesting is that smaller models benefit even more from this approach. It's like giving them time to &amp;quot;think harder&amp;quot; actually works, but i also imagine itd be possible with some prompt tweaking to get it to heavily improve big ones too.&lt;/p&gt; &lt;p&gt;GitHub: [&lt;a href="https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts"&gt;https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;Technical details: - Written in Python - Wayyyyy slower but way better output - Adjustable thinking rounds (1-5) + dynamic - Works with any OpenRouter-compatible model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HearMeOut-13"&gt; /u/HearMeOut-13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1k8yrem"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yrem/made_mistral_24b_code_like_a_senior_dev_by_making/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yrem/made_mistral_24b_code_like_a_senior_dev_by_making/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T07:59:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1k969p3</id>
    <title>Server approved! 4xH100 (320gb vram). Looking for advice</title>
    <updated>2025-04-27T15:14:44+00:00</updated>
    <author>
      <name>/u/ICanSeeYou7867</name>
      <uri>https://old.reddit.com/user/ICanSeeYou7867</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My company is wanting to run on premise AI for various reasons. We have a HPC cluster built using slurm, and it works well, but the time based batch jobs are not ideal for always available resources. &lt;/p&gt; &lt;p&gt;I have a good bit of experience running vllm, llamacpp, and kobold in containers with GPU enabled resources, and I am decently proficient with kubernetes.&lt;/p&gt; &lt;p&gt;(Assuming this all works, I will be asking for another one of these servers for HA workloads.)&lt;/p&gt; &lt;p&gt;My current idea is going to be a k8s based deployment (using RKE2), with the nvidia gpu operator installed for the single worker node. I will then use gitlab + fleet to handle deployments, and track configuration changes. I also want to use quantized models, probably Q6-Q8 imatrix models when possible with llamacpp, or awq/bnb models with vllm if they are supported.&lt;/p&gt; &lt;p&gt;I will also use a litellm deployment on a different k8s cluster to connect the openai compatible endpoints. (I want this on a separate cluster, as i can then use the slurm based hpc as a backup in case the node goes down for now, and allow requests to keep flowing.)&lt;/p&gt; &lt;p&gt;I think got the basics this will work, but I have never deployed an H100 based server, and I was curious if there were any gotchas I might be missing....&lt;/p&gt; &lt;p&gt;Another alternative I was thinking about, was adding the H100 server as a hypervisor node, and then use GPU pass-through to a guest. This would allow some modularity to the possible deployments, but would add some complexity....&lt;/p&gt; &lt;p&gt;Thank you for reading! Hopefully this all made sense, and I am curious if there are some gotchas or some things I could learn from others before deploying or planning out the infrastructure. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ICanSeeYou7867"&gt; /u/ICanSeeYou7867 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k969p3/server_approved_4xh100_320gb_vram_looking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k969p3/server_approved_4xh100_320gb_vram_looking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k969p3/server_approved_4xh100_320gb_vram_looking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T15:14:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8pbkz</id>
    <title>Rumors of DeepSeek R2 leaked!</title>
    <updated>2025-04-26T22:46:10+00:00</updated>
    <author>
      <name>/u/policyweb</name>
      <uri>https://old.reddit.com/user/policyweb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8pbkz/rumors_of_deepseek_r2_leaked/"&gt; &lt;img alt="Rumors of DeepSeek R2 leaked!" src="https://external-preview.redd.it/p9U-cs9GtQZIPfmxhgS9u_IU8nn0dCQTBamMDocU7cQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=142d863bd878aeef678a295eb3f985d05fc875a7" title="Rumors of DeepSeek R2 leaked!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;—1.2T param, 78B active, hybrid MoE —97.3% cheaper than GPT 4o ($0.07/M in, $0.27/M out) —5.2PB training data. 89.7% on C-Eval2.0 —Better vision. 92.4% on COCO —82% utilization in Huawei Ascend 910B&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/deedydas/status/1916160465958539480?s=46"&gt;https://x.com/deedydas/status/1916160465958539480?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/policyweb"&gt; /u/policyweb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/deedydas/status/1916160465958539480?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8pbkz/rumors_of_deepseek_r2_leaked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8pbkz/rumors_of_deepseek_r2_leaked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-26T22:46:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8xyvp</id>
    <title>Finally got ~10t/s DeepSeek V3-0324 hybrid (FP8+Q4_K_M) running locally on my RTX 4090 + Xeon with with 512GB RAM, KTransformers and 32K context</title>
    <updated>2025-04-27T07:02:36+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Just wanted to share a fun project I have been working on. I managed to get DeepSeek V3-0324 onto my single RTX 4090 + Xeon box running 512 GB RAM using KTransformers and a clever FP8+GGUF hybrid trick from KTransformers. &lt;/p&gt; &lt;p&gt;Attention &amp;amp; FF layers on GPU (FP8): Cuts VRAM down to ~24 GB, so your 4090 can handle the critical parts lightning fast.&lt;/p&gt; &lt;p&gt;Expert weights on CPU (4-bit GGUF): All the huge MoE banks live in system RAM and load as needed. &lt;/p&gt; &lt;p&gt;End result: I’m seeing about ~10 tokens/sec with a 32K context window—pretty smooth for local tinkering.&lt;/p&gt; &lt;p&gt;KTransformers made it so easy with its Docker image. It handles the FP8 kernels under the hood and shuffles data between CPU/GPU token by token.&lt;/p&gt; &lt;p&gt;I posted a llama-4 maverick run on KTransformers a couple of days back and got good feedback on here. So I am sharing this build as well, in case it helps anyone out!&lt;/p&gt; &lt;p&gt;My Build:&lt;br /&gt; Motherboard: ASUS Pro WS W790E-SAGE SE. Why This Board? 8-channel DDR5 ECC RAM, I have 8x64 GB ECC DDR5 RAM 4800MHz&lt;br /&gt; CPU with AI &amp;amp; ML Boost: Engineering Sample QYFS (56C/112T!)&lt;br /&gt; I get consistently 9.5-10.5 tokens per second with this for decode. And I get 40-50 prefill speed.&lt;/p&gt; &lt;p&gt;If you would like to checkout the youtube video of the run: &lt;a href="https://www.youtube.com/watch?v=oLvkBZHU23Y"&gt;https://www.youtube.com/watch?v=oLvkBZHU23Y&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My Hardware Build and reasoning for picking up this board: &lt;a href="https://www.youtube.com/watch?v=r7gVGIwkZDc"&gt;https://www.youtube.com/watch?v=r7gVGIwkZDc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xyvp/finally_got_10ts_deepseek_v30324_hybrid_fp8q4_k_m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xyvp/finally_got_10ts_deepseek_v30324_hybrid_fp8q4_k_m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8xyvp/finally_got_10ts_deepseek_v30324_hybrid_fp8q4_k_m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T07:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k90u7f</id>
    <title>I'm building "Gemini Coder" enabling free AI coding using web chats like AI Studio, DeepSeek or Open WebUI</title>
    <updated>2025-04-27T10:26:37+00:00</updated>
    <author>
      <name>/u/robertpiosik</name>
      <uri>https://old.reddit.com/user/robertpiosik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k90u7f/im_building_gemini_coder_enabling_free_ai_coding/"&gt; &lt;img alt="I'm building &amp;quot;Gemini Coder&amp;quot; enabling free AI coding using web chats like AI Studio, DeepSeek or Open WebUI" src="https://external-preview.redd.it/M29waGh5eDFzY3hlMQztlUbXx4HmVNCbcZJT8RQ5gRkadeJUcw9XZgxVzRZr.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47ff30d434ebd8acbfcd91bb9ccddfa2df9bc357" title="I'm building &amp;quot;Gemini Coder&amp;quot; enabling free AI coding using web chats like AI Studio, DeepSeek or Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some web chats come with extended support with automatically set model, system instructions and temperature (AI Studio, OpenRouter Chat, Open WebUI) while integration with others (ChatGPT, Claude, Gemini, Mistral, etc.) is limited to just initializations.&lt;/p&gt; &lt;p&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=robertpiosik.gemini-coder"&gt;https://marketplace.visualstudio.com/items?itemName=robertpiosik.gemini-coder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The tool is 100% free and open source (MIT licensed).&lt;br /&gt; I hope it will be received by the community as a helpful resource supporting everyday coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robertpiosik"&gt; /u/robertpiosik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n2iwzxx1scxe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k90u7f/im_building_gemini_coder_enabling_free_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k90u7f/im_building_gemini_coder_enabling_free_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T10:26:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1k8yk8w</id>
    <title>TNG Tech releases Deepseek-R1-Chimera, adding R1 reasoning to V3-0324</title>
    <updated>2025-04-27T07:44:51+00:00</updated>
    <author>
      <name>/u/ayyndrew</name>
      <uri>https://old.reddit.com/user/ayyndrew</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yk8w/tng_tech_releases_deepseekr1chimera_adding_r1/"&gt; &lt;img alt="TNG Tech releases Deepseek-R1-Chimera, adding R1 reasoning to V3-0324" src="https://external-preview.redd.it/1No-ofpBVwLtrVVqwYujJ6PJAf7A4c3ZgbbSJrDaop0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fa5483c63f6faf71fe54d107797d498abbdd369" title="TNG Tech releases Deepseek-R1-Chimera, adding R1 reasoning to V3-0324" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Today we release DeepSeek-R1T-Chimera, an open weights model adding R1 reasoning to &lt;a href="https://x.com/deepseek_ai"&gt;@deepseek_ai&lt;/a&gt; V3-0324 with a novel construction method. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;In benchmarks, it appears to be as smart as R1 but much faster, using 40% fewer output tokens. &lt;/p&gt; &lt;p&gt;The Chimera is a child LLM, using V3s shared experts augmented with a custom merge of R1s and V3s routed experts. It is not a finetune or distillation, but constructed from neural network parts of both parent MoE models. &lt;/p&gt; &lt;p&gt;A bit surprisingly, we did not detect defects of the hybrid child model. Instead, its reasoning and thinking processes appear to be more compact and orderly than the sometimes very long and wandering thoughts of the R1 parent model. &lt;/p&gt; &lt;p&gt;Model weights are on &lt;a href="https://x.com/huggingface"&gt;@huggingface&lt;/a&gt;, just a little late for &lt;a href="https://x.com/hashtag/ICLR2025?src=hashtag_click"&gt;#ICLR2025&lt;/a&gt;. Kudos to &lt;a href="https://x.com/deepseek_ai"&gt;@deepseek_ai&lt;/a&gt; for V3 and R1!&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/tngtech/status/1916284566127444468"&gt;https://x.com/tngtech/status/1916284566127444468&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayyndrew"&gt; /u/ayyndrew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tngtech/DeepSeek-R1T-Chimera"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yk8w/tng_tech_releases_deepseekr1chimera_adding_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k8yk8w/tng_tech_releases_deepseekr1chimera_adding_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T07:44:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9bdlh</id>
    <title>Lack of Model Compatibility Can Kill Promising Projects</title>
    <updated>2025-04-27T18:51:36+00:00</updated>
    <author>
      <name>/u/hannibal27</name>
      <uri>https://old.reddit.com/user/hannibal27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using the &lt;strong&gt;GLM-4 32B 0414 MLX&lt;/strong&gt; on &lt;strong&gt;LM Studio&lt;/strong&gt;, and I have to say, the experience has been excellent. When it comes to coding tasks, it feels clearly better than the &lt;strong&gt;QWen-32B&lt;/strong&gt;. For general text and knowledge tasks, in my tests, I still prefer the &lt;strong&gt;Mistral-Small 24B&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;What I really want to highlight is this: just a few days ago, there were tons of requests for a good local LLM that could handle coding well — and, surprisingly, that breakthrough had already happened! However, the lack of compatibility with popular tools (like &lt;strong&gt;llama.cpp&lt;/strong&gt; and others) slowed down adoption. With few people testing and little exposure, models that could have generated a lot of buzz, usage, and experiments end up quietly fading away.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The GLM-4 developers deserve huge praise for their amazing work&lt;/strong&gt; — the model itself is great. But it's truly a shame that the lack of integration with common tools hurt its launch so much. They deserve way more recognition.&lt;/p&gt; &lt;p&gt;We saw something similar happen with &lt;strong&gt;Llama 4&lt;/strong&gt;: now, some users are starting to say &amp;quot;it wasn’t actually that bad,&amp;quot; but by then the bad reputation had already stuck, mostly because it launched quickly with a lot of integration bugs.&lt;/p&gt; &lt;p&gt;I know it might sound a bit arrogant to say this to the teams who dedicate so much time to build these models — and offer them to us for free — but honestly: &lt;strong&gt;paying attention to tool compatibility can be the difference between a massively successful project and one that gets forgotten&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hannibal27"&gt; /u/hannibal27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9bdlh/lack_of_model_compatibility_can_kill_promising/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9bdlh/lack_of_model_compatibility_can_kill_promising/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k9bdlh/lack_of_model_compatibility_can_kill_promising/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T18:51:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1k9488r</id>
    <title>Gemini 2.5-Pro's biggest strength isn't raw coding skill - it's that it doesn't degrade anywhere near as much over long context</title>
    <updated>2025-04-27T13:41:32+00:00</updated>
    <author>
      <name>/u/mark-lord</name>
      <uri>https://old.reddit.com/user/mark-lord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: It's such a crazy unlock being able to just keep on iterating and trying new things without having to reset the chat window every 15 minutes. Just wish they'd pass whatever arcane magic they used down to the Gemma models!&lt;/p&gt; &lt;p&gt;--&lt;/p&gt; &lt;p&gt;So I've been using Cursor pretty religiously ever since Sonnet 3.5 dropped. I don't necessarily think that Gemini 2.5 is &lt;em&gt;better&lt;/em&gt; than Sonnet 3.5 though, at least not over a single shot prompt. I think its biggest strength is that even once my context window has been going on forever, it's still consistently smart.&lt;/p&gt; &lt;p&gt;Honestly I'd take a dumber version of Sonnet 3.7 if it meant that it was that same level of dumbness over the whole context window. Same even goes for local LLMs. If I had a version of Qwen, even just a 7b, that didn't slowly get less capable with a longer context window, I'd honestly use it so much more.&lt;/p&gt; &lt;p&gt;So much of the time I've just got into a flow with a model, just fed it enough context that it manages to actually do what I want it to, and then 2 or 3 turns later it's suddenly lost that spark. Gemini 2.5 is the only model I've used so far to not do that, even amongst all of Google's other offerings.&lt;/p&gt; &lt;p&gt;Is there some specific part of the attention / arch for Gemini that has enabled this, do we reckon? Or did they just use all those TPUs to do a &lt;em&gt;really&lt;/em&gt; high number of turns for multi-turn RL? My gut says probably the latter lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mark-lord"&gt; /u/mark-lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9488r/gemini_25pros_biggest_strength_isnt_raw_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1k9488r/gemini_25pros_biggest_strength_isnt_raw_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1k9488r/gemini_25pros_biggest_strength_isnt_raw_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-27T13:41:32+00:00</published>
  </entry>
</feed>
