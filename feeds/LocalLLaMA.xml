<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-10T04:44:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lvd7z4</id>
    <title>support for Falcon-H1 model family has been merged into llama.cpp</title>
    <updated>2025-07-09T08:10:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"&gt; &lt;img alt="support for Falcon-H1 model family has been merged into llama.cpp" src="https://external-preview.redd.it/N01fJbJzFMtO5mbBFXLE8iKjcQtmu4eYhoVQZmMbhG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1eba07cf9ee71a811133c3ac69643f88b0846c" title="support for Falcon-H1 model family has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Falcon-H1 Family of Hybrid-Head Language Models (Transformer-SSM), including 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B (pretrained &amp;amp; instruction-tuned).&lt;/p&gt; &lt;p&gt;ggufs uploaded by Falcon team:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Deep-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-1.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14534"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvd7z4/support_for_falconh1_model_family_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T08:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvu7sp</id>
    <title>Musings on recent trends in closed-source and the way forward for open-source</title>
    <updated>2025-07-09T21:01:09+00:00</updated>
    <author>
      <name>/u/AgreeableCaptain1372</name>
      <uri>https://old.reddit.com/user/AgreeableCaptain1372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever since the release of o1, I've been noticing more and more that a lot of the efforts OpenAI is making is not necessarily on the quality of their LLMs themselves but in how they are stitched/orchestrated together. To take a simple example, you might have noticed that ChatGPT generally performs way better than the GPT APIs alone, which are just the output of raw LLMs, whereas ChatGPT is more of a black box and has several layers around the GPT models that process the answers before they reach us. Same with o1 which seems to wrap a reasoning algorithm around raw GPT as far as we know. So it's not the actual LLMs underlying closed source products that are changing the most but the way the output of raw LLMs is passed around and processed until it reaches us. Seems GPT-5 will be continuing down that path by offering a &amp;quot;unified model&amp;quot; with less user &amp;quot;model switching&amp;quot;. This seems to me like this will mean more black-box model glue (i.e. switching) behind the scenes. This will mean even more vendor lock-in because different model APIs will no longer be easily interchangeable due to some outputs being way more processed than others (in the same way that ChatGPT's output is way more processed than a raw LLM's output). &lt;/p&gt; &lt;p&gt;The bottom line is the products being offered are no longer just LLMs but something approaching what could be called proto-agents in the sense that they have memory, use tools and reason to some degree (but they don't act on external systems yet hence &amp;quot;proto&amp;quot;).&lt;/p&gt; &lt;p&gt;So I've been wondering, what does this mean for open-source? Will it mimic closed-source and pursue proto-agentic solutions or should/will it continue down the path of improving the actual LLMs, leaving the orchestration and agentic part to the developer or to open-source agentic frameworks like CrewAI/PydanticAI?&lt;/p&gt; &lt;p&gt;I think a lot of the answer depends on whether the art of &amp;quot;stitching&amp;quot; together LLMs can be undertaken by any software engineer or if highly specialized researchers are needed for that. If the former, this will be great for open-source because we won't need all the resources OpenAI has and their extremely well-paid researchers. If the latter, then the future might be tougher for open-source. I personally am cautiously optimistic as I don't see orchestration requiring the same amount of resources or specialized knowledge as training an LLM but then again I don't actually know what happens behind the scenes of o3 or ChatGPT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Most changes in the past year seem to have been LLM-adjacent (namely, reasoning and proto-agentic features in general). Wondering if this is an opportunity or a risk for open-source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgreeableCaptain1372"&gt; /u/AgreeableCaptain1372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvu7sp/musings_on_recent_trends_in_closedsource_and_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvu7sp/musings_on_recent_trends_in_closedsource_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvu7sp/musings_on_recent_trends_in_closedsource_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T21:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvxft1</id>
    <title>We are building a 192GB (2x 96GB) Blackwell Pro 6000 server. It deserves a beautiful case. What should we use?</title>
    <updated>2025-07-09T23:16:34+00:00</updated>
    <author>
      <name>/u/blackwell_tart</name>
      <uri>https://old.reddit.com/user/blackwell_tart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. Every word of this post was written entirely by me, a human, with no AI involvement. Any slop is mine.&lt;/p&gt; &lt;p&gt;blackwell_tart and associates will be receiving two 96GB workstation pro 6000 GPUs at the end of the week. We are not here to discuss the purpose to which the GPUs will be put, please accept our apologies for this omission. Needs must. &lt;/p&gt; &lt;p&gt;Instead we are here to consider the notion that such sleek and elegant GPUs deserve a sleek and elegant home, and to that effect we are soliciting advice pertaining to aesthetically beautiful enclosures or open frames. This is not a request for mid-towers from Amazon. This is a request for the kind of work delivered slowly by an old-time lathe worker who creates pieces of functional art that make a &lt;a href="https://imgs.search.brave.com/216dKW4x2S_b_ufgWXZO5z3NLSq_Lx2-pXbga8mTBuI/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9saXJw/LmNkbi13ZWJzaXRl/LmNvbS85ZTFhNjgy/YS9kbXMzcmVwL211/bHRpL29wdC9neXJv/K3NlK2dhbGxlcnkr/LSsyLTE5MjB3Lmpw/Zw"&gt;Mitchel Gyrodec&lt;/a&gt; look like a child's lego piece. &lt;/p&gt; &lt;p&gt;Ali Express is also fine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Core Server&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Based on our own experiences and those documented by fellow LocalLLama enthusiasts, we are building the following configuration. The final piece was to find a reliable source for GPUs, a matter that is now resolved.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: &lt;a href="https://www.gigabyte.com/us/Enterprise/Server-Motherboard/MZ33-AR1-rev-3x"&gt;Gigabyte MZ33-AR1 rev3&lt;/a&gt;: It has 4x PCIe 5.0 x16 slots. The GPUs will not physically fit in the slots because the design of the motherboard is such the the CPU cooler and RDIMMs interfere. Risers are therefore required.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Risers&lt;/strong&gt;: &lt;a href="https://www.amazon.com/dp/B0DKPFGY7J"&gt;Linkup AVA5 risers&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: &lt;a href="https://www.amd.com/en/products/processors/server/epyc/9005-series/amd-epyc-9745.html"&gt;AMD EPYC 9745&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cooler&lt;/strong&gt;: &lt;a href="https://www.silverstonetek.com/en/product/info/coolers/xed120s_ws/"&gt;SilverStone XED120 WS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 768GB 6400 MT/s DDR5 RAM in 12x &lt;a href="https://memory.net/product/m321r8ga0pb2-ccp-samsung-1x-64gb-ddr5-6400-rdimm-pc5-51200r-dual-rank-x4-module/"&gt;64GB modules&lt;/a&gt;. The motherboard supports 12 DDR5 channels with up to 24 RDIMMs and we may expand to 1.5TB in future, depending on requirements yet to be determined.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PSU&lt;/strong&gt;: 2kW 240V e-ATX.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SSDs&lt;/strong&gt;: 2x 4TB Samsung 9100 Pro SSD in RAID0 striped configuration. One SSD is mounted in the motherboard's m2 socket, the other SSD is mounted in a &lt;a href="https://www.amazon.com/dp/B0DJMWWX27"&gt;MCIO -&amp;gt; u2 -&amp;gt; m2&lt;/a&gt; adapter, all of which is PCIe 5.0 x4.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We think it will be quite fast. We would like it to be equally beautiful. We are old school. We prefer das blinkenlights to LED lights. We shall post pics. We shall not gtfo.&lt;/p&gt; &lt;p&gt;How should we dress our new creation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackwell_tart"&gt; /u/blackwell_tart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvxft1/we_are_building_a_192gb_2x_96gb_blackwell_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvxft1/we_are_building_a_192gb_2x_96gb_blackwell_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvxft1/we_are_building_a_192gb_2x_96gb_blackwell_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T23:16:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvj98v</id>
    <title>I built a Deep Researcher agent and exposed it as an MCP server!</title>
    <updated>2025-07-09T13:48:17+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a Deep Researcher Agent that does multi-step web research and report generation. I wanted to share my stack and approach in case anyone else wants to build similar multi-agent workflows.&lt;br /&gt; So, the agent has 3 main stages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Searcher:&lt;/strong&gt; Uses Scrapegraph to crawl and extract live data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Analyst:&lt;/strong&gt; Processes and refines the raw data using DeepSeek R1&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writer:&lt;/strong&gt; Crafts a clean final report&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To make it easy to use anywhere, I wrapped the whole flow with an MCP Server. So you can run it from Claude Desktop, Cursor, or any MCP-compatible tool. There’s also a simple Streamlit UI if you want a local dashboard.&lt;/p&gt; &lt;p&gt;Here’s what I used to build it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scrapegraph for web scraping&lt;/li&gt; &lt;li&gt;Nebius AI for open-source models&lt;/li&gt; &lt;li&gt;Agno for agent orchestration&lt;/li&gt; &lt;li&gt;Streamlit for the UI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is still basic by design, but it's a solid starting point if you're thinking about building your own deep research workflow.&lt;/p&gt; &lt;p&gt;If you’re curious, I put a full video tutorial here: &lt;a href="https://www.youtube.com/watch?v=pdsk6yldZGI"&gt;demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And the code is here if you want to try it or fork it: &lt;a href="https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/deep_researcher_agent"&gt;Full Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to get your feedback on what to add next or how I can improve it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvj98v/i_built_a_deep_researcher_agent_and_exposed_it_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T13:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lv2t7n</id>
    <title>"Not x, but y" Slop Leaderboard</title>
    <updated>2025-07-08T22:48:41+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt; &lt;img alt="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" src="https://preview.redd.it/nxw6fmegaqbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f634168f40782641454db362ee799df6971e84f" title="&amp;quot;Not x, but y&amp;quot; Slop Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models have been converging on &amp;quot;not x, but y&amp;quot; type phrases to an absurd degree. So here's a leaderboard for it. &lt;/p&gt; &lt;p&gt;I don't think many labs are targeting this kind of slop in their training set filtering, so it gets compounded with subsequent model generations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxw6fmegaqbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lv2t7n/not_x_but_y_slop_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-08T22:48:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw3729</id>
    <title>Phi-4-mini-flash-reasoning</title>
    <updated>2025-07-10T04:00:32+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/"&gt; &lt;img alt="Phi-4-mini-flash-reasoning" src="https://external-preview.redd.it/2P6UIFtGrDGWlFJ2C-vKbMOckKYF-gLArRvl_PWecTA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a162d9d243ab8293c0214f3e9bf055ec2af6d514" title="Phi-4-mini-flash-reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw3729/phi4miniflashreasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T04:00:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvk1ms</id>
    <title>What impressive (borderline creepy) local AI tools can I run now that everything is local?</title>
    <updated>2025-07-09T14:21:39+00:00</updated>
    <author>
      <name>/u/PeithonKing</name>
      <uri>https://old.reddit.com/user/PeithonKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2 years ago, I left Windows mainly because of the creepy Copilot-type stuff — always-on apps that watch everything, take screenshots every 5 seconds, and offer &amp;quot;smart&amp;quot; help in return. Felt like a trade: my privacy for their convenience.&lt;/p&gt; &lt;p&gt;Now I’m on Linux, running my local models (Ollama, etc.), and I’m wondering — what’s out there that gives that same kind of &amp;quot;wow, this is scary, but actually useful&amp;quot; feeling, but runs completely offline? Something which actually sort of breaches my privacy (but locally).&lt;/p&gt; &lt;p&gt;Not just screen-watching — anything that improves workflow or feels magically helpful... but because it’s all local I can keep my hand on my heart and say &amp;quot;all is well&amp;quot;.&lt;/p&gt; &lt;p&gt;Looking for tools, recos or project links if anyone’s already doing this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PeithonKing"&gt; /u/PeithonKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvk1ms/what_impressive_borderline_creepy_local_ai_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T14:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvun89</id>
    <title>How to run Gemma 3 27B QAT with 128k context window with 3 parallel requests possible on 2x3090</title>
    <updated>2025-07-09T21:18:12+00:00</updated>
    <author>
      <name>/u/EmilPi</name>
      <uri>https://old.reddit.com/user/EmilPi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;Have CUDA installed.&lt;/li&gt; &lt;li&gt;Go to &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Find you OS .zip file, download it&lt;/li&gt; &lt;li&gt;Unpack it to the folder of your choice&lt;/li&gt; &lt;li&gt;At the same folder level, download Gemma 3 27B QAT Q4_0: &lt;code&gt;git clone&lt;/code&gt; &lt;a href="https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf"&gt;&lt;code&gt;https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Run command (for Linux, your slashes/extension may vary for Windows) and enjoy 128k context window for 3 parallel requests at once:&lt;/p&gt; &lt;p&gt;./build/bin/llama-server --host localhost --port 1234 --model ./gemma-3-27b-it-qat-q4_0-gguf/gemma-3-27b-it-q4_0.gguf --mmproj ./gemma-3-27b-it-qat-q4_0-gguf/mmproj-model-f16-27B.gguf --alias Gemma3-27B-VISION-128k --parallel 3 -c 393216 -fa -ctv q8_0 -ctk q8_0 --ngl 999 -ts 30,31&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmilPi"&gt; /u/EmilPi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvun89/how_to_run_gemma_3_27b_qat_with_128k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvun89/how_to_run_gemma_3_27b_qat_with_128k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvun89/how_to_run_gemma_3_27b_qat_with_128k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T21:18:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvyfws</id>
    <title>LLamaCPP just merged Mamba/Jamba support!!</title>
    <updated>2025-07-10T00:02:12+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvyfws/llamacpp_just_merged_mambajamba_support/"&gt; &lt;img alt="LLamaCPP just merged Mamba/Jamba support!!" src="https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1458a795a3b7a775e1a94d7767c299e1d8f3ef" title="LLamaCPP just merged Mamba/Jamba support!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/7531"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvyfws/llamacpp_just_merged_mambajamba_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvyfws/llamacpp_just_merged_mambajamba_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T00:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvt4a9</id>
    <title>Open-source SLM for games, Unity package, demo game The Tell-Tale Heart</title>
    <updated>2025-07-09T20:17:25+00:00</updated>
    <author>
      <name>/u/formicidfighter</name>
      <uri>https://old.reddit.com/user/formicidfighter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvt4a9/opensource_slm_for_games_unity_package_demo_game/"&gt; &lt;img alt="Open-source SLM for games, Unity package, demo game The Tell-Tale Heart" src="https://external-preview.redd.it/b3JtbGNxMnhtd2JmMdwzBK8WlOQ5bf-9CLDL_anvlqkZgo3IidVaSmRMq-iR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd26c3ec9734e5ad3c175e9139c86cfd3b35fbea" title="Open-source SLM for games, Unity package, demo game The Tell-Tale Heart" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we’ve been experimenting with small language models (SLMs) as a new type of game asset. We think they’re a promising way to make game mechanics more dynamic. Especially when finetuned to your game world and for focused, constrained mechanics designed to allow for more reactive output.&lt;/p&gt; &lt;p&gt;You can try our demo game, inspired by Edgar Allan Poe’s short story The Tell-Tale Heart, &lt;a href="https://aviadai.itch.io/the-tell-tale-heart"&gt;on itch&lt;/a&gt;. We spent two weeks pulling it together, so it’s not the most polished game. But we hope it captures a bit of the delight that emergent mechanics can provide.&lt;/p&gt; &lt;p&gt;Design-wise, we chose to constrain the model to picking one of 3 pre-written choices for each scenario and generating an in-character explanation for its choice. This way, the model is in a controlled environment crafted by the dev, but also adds some flavor and surprise. You can play around with editing the character background to explore the boundaries and limits of the model. We finetuned it to be quite general, but you can imagine finetuning the SLM much more closely to your game world and characters.&lt;/p&gt; &lt;p&gt;In the spirit of seeing more experimentation with SLMs, we’ve open-sourced everything:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/aviad-ai"&gt;This SLM&lt;/a&gt; (it’s a finetuned llama model, so under llama3 license). Performance-wise, it’s quite small at 770 MB and runs comfortably on CPU.&lt;/li&gt; &lt;li&gt;A &lt;a href="https://github.com/aviad-ai/unity"&gt;Unity package&lt;/a&gt; for loading and integrating models into Unity (built on top of llama.cpp, under MIT license. Supports MacOS, Windows, WebGL). We’ve done quite a lot of work to optimize it. We’re working on an Unreal integration coming soon!&lt;/li&gt; &lt;li&gt;The &lt;a href="https://github.com/aviad-ai/UnitySamples/tree/main/TheTellTaleHeart"&gt;sample game&lt;/a&gt; (under MIT license, except for the paid EndlessBook asset from the Unity store).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re excited about a potential future in which games are shipped with multiple, specialized SLMs running in tandem to make games more immersive. &lt;/p&gt; &lt;p&gt;If you’re also interested in the promise of SLMs in games, join us on &lt;a href="https://discord.gg/Jk4jUYghnA"&gt;Discord&lt;/a&gt;! We’re planning to open-source a lot more models, sample games, integration features, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/formicidfighter"&gt; /u/formicidfighter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kamkdq2xmwbf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvt4a9/opensource_slm_for_games_unity_package_demo_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvt4a9/opensource_slm_for_games_unity_package_demo_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T20:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvzwah</id>
    <title>Preceptor – A Local AI Focus App That Nudges You Back on Track | Waitlist + Suggestions needed</title>
    <updated>2025-07-10T01:12:12+00:00</updated>
    <author>
      <name>/u/Frosty-Cap-4282</name>
      <uri>https://old.reddit.com/user/Frosty-Cap-4282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I'm building &lt;strong&gt;Preceptor&lt;/strong&gt;, a privacy-first, local AI app that helps you stay focused by tracking your activity &lt;em&gt;without&lt;/em&gt; spying on your screen or sending data to the cloud.&lt;/p&gt; &lt;p&gt;Here’s what it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Monitors your activity locally&lt;/strong&gt; (app focus, browser tabs via extension)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compares with your goals&lt;/strong&gt; (e.g., writing, coding, avoiding distractions)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gently reminds you&lt;/strong&gt; when you drift off course&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs entirely offline&lt;/strong&gt; using &lt;a href="https://ollama.com"&gt;Ollama&lt;/a&gt; for local LLMs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Think of it like an AI-powered accountability partner that respects your privacy. On browsers, it’ll use a lightweight extension to understand which site or tab you’re on — all processed locally.&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;Waitlist is open:&lt;/strong&gt; &lt;a href="https://preceptor-two.vercel.app/"&gt;https://preceptor-two.vercel.app/&lt;/a&gt;&lt;br /&gt; Helps me gauge interest and prioritize development because i shared my other open-source project that is gaining traction and am torn between making that app better vs building this app!&lt;/p&gt; &lt;p&gt;Also, if you're into local AI, productivity tools, or browser extensions, feel free to join the ongoing development — it's still early!&lt;/p&gt; &lt;p&gt;Would love your feedback on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What would make Preceptor useful to you day-to-day?&lt;/li&gt; &lt;li&gt;How should reminders work without being annoying?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and other things you would want. &lt;/p&gt; &lt;p&gt;Thanks for reading! 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty-Cap-4282"&gt; /u/Frosty-Cap-4282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzwah/preceptor_a_local_ai_focus_app_that_nudges_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzwah/preceptor_a_local_ai_focus_app_that_nudges_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzwah/preceptor_a_local_ai_focus_app_that_nudges_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T01:12:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvtp4h</id>
    <title>New Nvidia Jetson AGX Thor developer kit specs</title>
    <updated>2025-07-09T20:40:26+00:00</updated>
    <author>
      <name>/u/martincerven</name>
      <uri>https://old.reddit.com/user/martincerven</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvtp4h/new_nvidia_jetson_agx_thor_developer_kit_specs/"&gt; &lt;img alt="New Nvidia Jetson AGX Thor developer kit specs" src="https://b.thumbs.redditmedia.com/LcS0MUSWf5hhmUX4e22WKX2znHiU8l47X_F0lXz5ctQ.jpg" title="New Nvidia Jetson AGX Thor developer kit specs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From &lt;a href="https://www.siliconhighway.com/wp-content/robotics-and-edge-ai-datasheet-jetson-thor-devkit-nvidia-us-web.pdf"&gt;siliconhighway&lt;/a&gt;&lt;br /&gt; Look &lt;strong&gt;BIG,&lt;/strong&gt; but:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AGX Orin: 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores @ 1.3 GHz&lt;/li&gt; &lt;li&gt;AGX Thor: 2560-core NVIDIA Blackwell architecture GPU with 96 fifth-gen Tensor Cores @ 1.575 GHz&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How is &lt;strong&gt;275 -&amp;gt;1000 TOPS&lt;/strong&gt; (FP8/INT8) computed? (with NVDEC,NVENC, +??)&lt;br /&gt; Additional info to &lt;a href="https://developer.nvidia.com/embedded/downloads"&gt;look through&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martincerven"&gt; /u/martincerven &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lvtp4h"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvtp4h/new_nvidia_jetson_agx_thor_developer_kit_specs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvtp4h/new_nvidia_jetson_agx_thor_developer_kit_specs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T20:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqtxa</id>
    <title>multimodal medgemma 27b</title>
    <updated>2025-07-09T18:47:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqtxa/multimodal_medgemma_27b/"&gt; &lt;img alt="multimodal medgemma 27b" src="https://external-preview.redd.it/5kzMwR9GesyU7lrUcZBJp2EcXFMPqKJOghnDp3-PEdM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fffb9bacdc1fe8701c5f0c1be15c595a886c9819" title="multimodal medgemma 27b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MedGemma is a collection of &lt;a href="https://ai.google.dev/gemma/docs/core"&gt;Gemma 3&lt;/a&gt; variants that are trained for performance on medical text and image comprehension. Developers can use MedGemma to accelerate building healthcare-based AI applications. MedGemma currently comes in three variants: a 4B multimodal version and 27B text-only and multimodal versions.&lt;/p&gt; &lt;p&gt;Both MedGemma multimodal versions utilize a &lt;a href="https://arxiv.org/abs/2303.15343"&gt;SigLIP&lt;/a&gt; image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Their LLM components are trained on a diverse set of medical data, including medical text, medical question-answer pairs, FHIR-based electronic health record data (27B multimodal only), radiology images, histopathology patches, ophthalmology images, and dermatology images.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/medgemma-27b-it"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqtxa/multimodal_medgemma_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqtxa/multimodal_medgemma_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:47:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lw1qp5</id>
    <title>Fine Tune a smaller LLM for Code generation</title>
    <updated>2025-07-10T02:43:12+00:00</updated>
    <author>
      <name>/u/GlobeAndGeek</name>
      <uri>https://old.reddit.com/user/GlobeAndGeek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;br /&gt; I want to fine-tune a small pre-trained LLM to help users write code in a specific language. This language is very specific to a particular machinery and does not have widespread usage. We have a manual in PDF format and a few examples for the code. We want to build a chat agent where users can write code, and the agent writes the code. I am very new to training LLM and willing to learn whatever is necessary. I have a basic understanding of working with LLMs using Ollama and LangChain. Could someone please guide me on where to start? I have a good machine with an NVIDIA RTX 4090, 24 GB GPU. I want to build the entire system on this machine. &lt;/p&gt; &lt;p&gt;Thanks in advance for all the help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GlobeAndGeek"&gt; /u/GlobeAndGeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T02:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqnjh</id>
    <title>T5Gemma - A Google Collection</title>
    <updated>2025-07-09T18:40:25+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqnjh/t5gemma_a_google_collection/"&gt; &lt;img alt="T5Gemma - A Google Collection" src="https://external-preview.redd.it/BW2hoB_QdPR1XaEBXrwNaGmYfvxZNxobJDaeTM6FKlQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61c73857aa577d73d06a45e7f0d9e3d669d8ffd9" title="T5Gemma - A Google Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqnjh/t5gemma_a_google_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqnjh/t5gemma_a_google_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:40:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvf7ww</id>
    <title>First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community</title>
    <updated>2025-07-09T10:22:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"&gt; &lt;img alt="First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community" src="https://b.thumbs.redditmedia.com/q6tonUvBmagrUz-fog-jtYbG7HMQjflqMjdSdWnuk1o.jpg" title="First Hugging Face robot: Reachy Mini. Hackable yet easy to use, powered by open-source and the community" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: &lt;a href="https://huggingface.co/blog/reachy-mini"&gt;https://huggingface.co/blog/reachy-mini&lt;/a&gt;&lt;br /&gt; Thomas Wolf on 𝕏: &lt;a href="https://x.com/Thom_Wolf/status/1942887160983466096"&gt;https://x.com/Thom_Wolf/status/1942887160983466096&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lvf7ww"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvf7ww/first_hugging_face_robot_reachy_mini_hackable_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T10:22:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvr711</id>
    <title>support for Jamba hybrid Transformer-Mamba models has been merged into llama.cpp</title>
    <updated>2025-07-09T19:01:38+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr711/support_for_jamba_hybrid_transformermamba_models/"&gt; &lt;img alt="support for Jamba hybrid Transformer-Mamba models has been merged into llama.cpp" src="https://external-preview.redd.it/dhbNGWfgWCT-x-TvF432DBgusAX570Erpeyx-f0JMmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e1458a795a3b7a775e1a94d7767c299e1d8f3ef" title="support for Jamba hybrid Transformer-Mamba models has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The AI21 Jamba family of models are hybrid SSM-Transformer foundation models, blending speed, efficient long context processing, and accuracy.&lt;/p&gt; &lt;p&gt;from the website:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Model Size&lt;/th&gt; &lt;th align="left"&gt;Max Tokens&lt;/th&gt; &lt;th align="left"&gt;Version&lt;/th&gt; &lt;th align="left"&gt;Snapshot&lt;/th&gt; &lt;th align="left"&gt;API Endpoint&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Jamba Large&lt;/td&gt; &lt;td align="left"&gt;398B parameters (94B active)&lt;/td&gt; &lt;td align="left"&gt;256K&lt;/td&gt; &lt;td align="left"&gt;1.7&lt;/td&gt; &lt;td align="left"&gt;2025-07&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;jamba-large&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Jamba Mini&lt;/td&gt; &lt;td align="left"&gt;52B parameters (12B active)&lt;/td&gt; &lt;td align="left"&gt;256K&lt;/td&gt; &lt;td align="left"&gt;1.7&lt;/td&gt; &lt;td align="left"&gt;2025-07&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;jamba-mini&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Engineers and data scientists at AI21 labs created the model to help developers and businesses leverage AI to build real-world products with tangible value. &lt;strong&gt;Jamba Mini&lt;/strong&gt; and &lt;strong&gt;Jamba Large&lt;/strong&gt; support zero-shot instruction-following and multi-language support. The Jamba models also provide developers with industry-leading APIs that perform a wide range of productivity tasks designed for commercial use.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Organization developing model:&lt;/strong&gt; AI21 Labs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model date:&lt;/strong&gt; July 3rd, 2025&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model type:&lt;/strong&gt; Joint Attention and Mamba (Jamba)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Knowledge cutoff date&lt;/strong&gt; August 22nd, 2024&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input Modality:&lt;/strong&gt; Text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Modality:&lt;/strong&gt; Text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; &lt;a href="https://www.ai21.com/licenses/jamba-open-model-license"&gt;Jamba open model license&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/7531"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr711/support_for_jamba_hybrid_transformermamba_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr711/support_for_jamba_hybrid_transformermamba_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T19:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvzonf</id>
    <title>https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms</title>
    <updated>2025-07-10T01:02:00+00:00</updated>
    <author>
      <name>/u/chitown160</name>
      <uri>https://old.reddit.com/user/chitown160</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/"&gt; &lt;img alt="https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms" src="https://preview.redd.it/vq8hwq904ybf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d05add52383cb1c64996c7d198a25c8644d9f33f" title="https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The flattening of nuanced distinctions is part of the joke (pre-emptive disclaimer for the pedantic) &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pheromone trails ↔ value functions / reward shaping&lt;/strong&gt; Both steer future exploration toward paths that historically looked good.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stochastic exploration&lt;/strong&gt; in ants (random walks with pheromone bias) ↔ &lt;strong&gt;ε-greedy / entropy-regularised exploration&lt;/strong&gt; in RL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Updating pheromones over time&lt;/strong&gt; ↔ &lt;strong&gt;policy/value updates&lt;/strong&gt; in RL or &lt;strong&gt;gradient steps&lt;/strong&gt; in supervised fine-tuning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Demonstration pheromones&lt;/strong&gt; (ants following an experienced scout’s trail) ↔ &lt;strong&gt;Learning from Demonstration&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chitown160"&gt; /u/chitown160 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vq8hwq904ybf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvzonf/httpsenwikipediaorgwikiant_colony_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T01:02:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvnkuk</id>
    <title>Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!</title>
    <updated>2025-07-09T16:41:42+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"&gt; &lt;img alt="Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!" src="https://external-preview.redd.it/48skyJ3kxjbpTbdO2dkmonld6WW3j1gpMPhBKYzzB0c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8f27c3dcd38f51203dffa703e77dc78a0e131c7" title="Drummer's Big Tiger Gemma 27B v3 and Tiger Gemma 12B v3! More capable, less positive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;12B version: &lt;a href="https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3"&gt;https://huggingface.co/TheDrummer/Tiger-Gemma-12B-v3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Big-Tiger-Gemma-27B-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T16:41:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvqv8e</id>
    <title>new tiny 1.7B open-source reranker beats Cohere rerank3.5</title>
    <updated>2025-07-09T18:48:35+00:00</updated>
    <author>
      <name>/u/ghita__</name>
      <uri>https://old.reddit.com/user/ghita__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/"&gt; &lt;img alt="new tiny 1.7B open-source reranker beats Cohere rerank3.5" src="https://external-preview.redd.it/nLFTMgeazZoEzFWPddn6GhbQh8ymdQftCjA5MDpkb1M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a250c04d607c0b8a5f43196eba12971c7744065" title="new tiny 1.7B open-source reranker beats Cohere rerank3.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you're looking for a cheap, fast but accurate reranker without having to fine-tune a SLM yourself&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghita__"&gt; /u/ghita__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zeroentropy/zerank-1-small"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvqv8e/new_tiny_17b_opensource_reranker_beats_cohere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvp3qv</id>
    <title>GEMINI 3 PRO !</title>
    <updated>2025-07-09T17:40:17+00:00</updated>
    <author>
      <name>/u/omar07ibrahim1</name>
      <uri>https://old.reddit.com/user/omar07ibrahim1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"&gt; &lt;img alt="GEMINI 3 PRO !" src="https://b.thumbs.redditmedia.com/d2eBFiZSJLkxDAA0QMlKs_RVctoYEIuFlOGx8XPUNQQ.jpg" title="GEMINI 3 PRO !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qqyb1haqxvbf1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92d72b8c85454f8bd1238f169632d66ae91da1e7"&gt;https://preview.redd.it/qqyb1haqxvbf1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92d72b8c85454f8bd1238f169632d66ae91da1e7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omar07ibrahim1"&gt; /u/omar07ibrahim1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvp3qv/gemini_3_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T17:40:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvvkh2</id>
    <title>Hunyuan-A13B is here for real!</title>
    <updated>2025-07-09T21:55:51+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan-A13B is now available for LM Studio with Unsloth GGUF. I am on the Beta track for both LM Studio and llama.cpp backend. Here are my initial impression:&lt;/p&gt; &lt;p&gt;It is fast! I am getting 40 tokens per second initially dropping to maybe 30 tokens per second when the context has build up some. This is on M4 Max Macbook Pro and q4.&lt;/p&gt; &lt;p&gt;The context is HUGE. 256k. I don't expect I will be using that much, but it is nice that I am unlikely to hit the ceiling in practical use.&lt;/p&gt; &lt;p&gt;It made a chess game for me and it did ok. No errors but the game was not complete. It did complete it after a few prompts and it also fixed one error that happened in the javascript console.&lt;/p&gt; &lt;p&gt;It did spend some time thinking, but not as much as I have seen other models do. I would say it is doing the middle ground here, but I am still to test this extensively. The model card claims you can somehow influence how much thinking it will do. But I am not sure how yet.&lt;/p&gt; &lt;p&gt;It appears to wrap the final answer in &amp;lt;answer&amp;gt;the answer here&amp;lt;/answer&amp;gt; just like it does for &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;. This may or may not be a problem for tools? Maybe we need to update our software to strip this out.&lt;/p&gt; &lt;p&gt;The total memory usage for the Unsloth 4 bit UD quant is 61 GB. I will test 6 bit and 8 bit also, but I am quite in love with the speed of the 4 bit and it appears to have good quality regardless. So maybe I will just stick with 4 bit?&lt;/p&gt; &lt;p&gt;This is a 80b model that is very fast. Feels like the future.&lt;/p&gt; &lt;p&gt;Edit: The 61 GB size is with 8 bit KV cache quantization. However I just noticed that they claim this is bad in the model card, so I disabled KV cache quantization. This increased memory usage to 76 GB. That is with the full 256k context size enabled. I expect you can just lower that if you don't have enough memory. Or stay with KV cache quantization because it did appear to work just fine. I would say this could work on a 64 GB machine if you just use KV cache quantization and maybe lower the context size to 128k. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvvkh2/hunyuana13b_is_here_for_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T21:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvn1sd</id>
    <title>OpenAI's open-weight model will debut as soon as next week</title>
    <updated>2025-07-09T16:20:46+00:00</updated>
    <author>
      <name>/u/phantasm_ai</name>
      <uri>https://old.reddit.com/user/phantasm_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"&gt; &lt;img alt="OpenAI's open-weight model will debut as soon as next week" src="https://external-preview.redd.it/42JItBYJCP_vHO6bBELpUc-h8r2Rbc-ajk9ruvwqY4U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5aaee471edf64881fedf697cc7cda1494ca5f3cd" title="OpenAI's open-weight model will debut as soon as next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This new open language model will be available on Azure, Hugging Face, and other large cloud providers. Sources describe the model as “similar to o3 mini,” complete with the reasoning capabilities that have made OpenAI’s latest models so powerful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantasm_ai"&gt; /u/phantasm_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/notepad-microsoft-newsletter/702848/openai-open-language-model-o3-mini-notepad"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvn1sd/openais_openweight_model_will_debut_as_soon_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T16:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvwya4</id>
    <title>Possible size of new the open model from openai</title>
    <updated>2025-07-09T22:54:54+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/"&gt; &lt;img alt="Possible size of new the open model from openai" src="https://preview.redd.it/622w5dyvhxbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f278161d7e564140ede28f9eff15dc776e5ab6df" title="Possible size of new the open model from openai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/622w5dyvhxbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvwya4/possible_size_of_new_the_open_model_from_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T22:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lvr3ym</id>
    <title>OpenAI's open source LLM is a reasoning model, coming Next Thursday!</title>
    <updated>2025-07-09T18:58:30+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"&gt; &lt;img alt="OpenAI's open source LLM is a reasoning model, coming Next Thursday!" src="https://preview.redd.it/q01afp6lbwbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e9bd873a7a7d4e956171cdc1ac61d5f5cae52e7" title="OpenAI's open source LLM is a reasoning model, coming Next Thursday!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q01afp6lbwbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lvr3ym/openais_open_source_llm_is_a_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-09T18:58:30+00:00</published>
  </entry>
</feed>
