<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-29T16:25:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kaclyw</id>
    <title>Run Qwen3 (0.6B) 100% locally in your browser on WebGPU w/ Transformers.js</title>
    <updated>2025-04-29T01:48:54+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaclyw/run_qwen3_06b_100_locally_in_your_browser_on/"&gt; &lt;img alt="Run Qwen3 (0.6B) 100% locally in your browser on WebGPU w/ Transformers.js" src="https://external-preview.redd.it/a3VnbHRmd3hpb3hlMbRgvsKi_IaAGC-viwKm_brdcymj4zDazyH2qxIAGtXM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a7c2bf26cc4be6e14165ab5a6b04d2793f880d1" title="Run Qwen3 (0.6B) 100% locally in your browser on WebGPU w/ Transformers.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wuflehwxioxe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaclyw/run_qwen3_06b_100_locally_in_your_browser_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaclyw/run_qwen3_06b_100_locally_in_your_browser_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T01:48:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka8n18</id>
    <title>Qwen3-30B-A3B is magic.</title>
    <updated>2025-04-28T22:34:23+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't believe a model this good runs at 20 tps on my 4gb gpu (rx 6550m).&lt;/p&gt; &lt;p&gt;Running it through paces, seems like the benches were right on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka8n18/qwen330ba3b_is_magic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka8n18/qwen330ba3b_is_magic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ka8n18/qwen330ba3b_is_magic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-28T22:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaj9v7</id>
    <title>The QWEN 3 score does not match the actual experience</title>
    <updated>2025-04-29T08:50:58+00:00</updated>
    <author>
      <name>/u/vihv</name>
      <uri>https://old.reddit.com/user/vihv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;qwen 3 is great, but is it a bit of an exaggeration? Is QWEN3-30B-A3B really stronger than Deepseek v3 0324? I've found that deepseek has a better ability to work in any environment, for example in cline \ roo code \ SillyTavern, deepseek can do it with ease, but qwen3-30b-a3b can't, even the more powerful qwen3-235b-a22b can't, it usually gets lost in context, don't you think? What are your use cases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vihv"&gt; /u/vihv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaj9v7/the_qwen_3_score_does_not_match_the_actual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaj9v7/the_qwen_3_score_does_not_match_the_actual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaj9v7/the_qwen_3_score_does_not_match_the_actual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T08:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kai88d</id>
    <title>I am VERY impressed by qwen3 4B (q8q4 gguf version)</title>
    <updated>2025-04-29T07:32:14+00:00</updated>
    <author>
      <name>/u/Robert__Sinclair</name>
      <uri>https://old.reddit.com/user/Robert__Sinclair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually test models reasoning using a few &amp;quot;not in any dataset&amp;quot; logic problems.&lt;/p&gt; &lt;p&gt;Up until the thinking models came along, only &amp;quot;huge&amp;quot; models could solve &amp;quot;some&amp;quot; of those problems in one shot.&lt;/p&gt; &lt;p&gt;Today I wanted to see how a heavily quantized (q8q4) small model as Qwen3 4B performed.&lt;/p&gt; &lt;p&gt;To my surprise, it gave the right answer and even the thinking was linear and very good.&lt;/p&gt; &lt;p&gt;You can find my quants here: &lt;a href="https://huggingface.co/ZeroWw/Qwen3-4B-GGUF"&gt;https://huggingface.co/ZeroWw/Qwen3-4B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: it seems it can solve ONE of the tests I usually do, but after further inspection, it failed all the others.&lt;/p&gt; &lt;p&gt;Perhaps one of my tests leaked in some dataset. It's possible since I used it to test the reasoning of many online models too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert__Sinclair"&gt; /u/Robert__Sinclair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kai88d/i_am_very_impressed_by_qwen3_4b_q8q4_gguf_version/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kai88d/i_am_very_impressed_by_qwen3_4b_q8q4_gguf_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kai88d/i_am_very_impressed_by_qwen3_4b_q8q4_gguf_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T07:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka6b9p</id>
    <title>Qwen 3 MoE making Llama 4 Maverick obsolete... 😱</title>
    <updated>2025-04-28T20:53:59+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka6b9p/qwen_3_moe_making_llama_4_maverick_obsolete/"&gt; &lt;img alt="Qwen 3 MoE making Llama 4 Maverick obsolete... 😱" src="https://preview.redd.it/szckfh6i2nxe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6dfa045be761753915c1f77e27b33367ce3b36c5" title="Qwen 3 MoE making Llama 4 Maverick obsolete... 😱" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/szckfh6i2nxe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka6b9p/qwen_3_moe_making_llama_4_maverick_obsolete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ka6b9p/qwen_3_moe_making_llama_4_maverick_obsolete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-28T20:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka5t8z</id>
    <title>Qwen3 Github Repo is up</title>
    <updated>2025-04-28T20:32:57+00:00</updated>
    <author>
      <name>/u/Predatedtomcat</name>
      <uri>https://old.reddit.com/user/Predatedtomcat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/QwenLM/qwen3"&gt;https://github.com/QwenLM/qwen3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ollama is up &lt;a href="https://ollama.com/library/qwen3"&gt;https://ollama.com/library/qwen3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmarks are up too &lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;https://qwenlm.github.io/blog/qwen3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model weights seems to be up here, &lt;a href="https://huggingface.co/organizations/Qwen/activity/models"&gt;https://huggingface.co/organizations/Qwen/activity/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chat is up at &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF demo is up too &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model collection here &lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Predatedtomcat"&gt; /u/Predatedtomcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka5t8z/qwen3_github_repo_is_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka5t8z/qwen3_github_repo_is_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ka5t8z/qwen3_github_repo_is_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-28T20:32:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kajcl7</id>
    <title>Qwen3 now runs locally in Jan via llama.cpp (Update the llama.cpp backend in Settings to run it)</title>
    <updated>2025-04-29T08:56:53+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kajcl7/qwen3_now_runs_locally_in_jan_via_llamacpp_update/"&gt; &lt;img alt="Qwen3 now runs locally in Jan via llama.cpp (Update the llama.cpp backend in Settings to run it)" src="https://preview.redd.it/ouuukg15mqxe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=529c48013baf50687a416a958f98a1c1099867ea" title="Qwen3 now runs locally in Jan via llama.cpp (Update the llama.cpp backend in Settings to run it)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, just sharing a quick note: Jan uses llama.cpp as its backend, and we recently shipped a feature that lets you bump the llama.cpp version without waiting for any updates. &lt;/p&gt; &lt;p&gt;So you can now run newer models like Qwen3 without needing a full Jan update.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ouuukg15mqxe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kajcl7/qwen3_now_runs_locally_in_jan_via_llamacpp_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kajcl7/qwen3_now_runs_locally_in_jan_via_llamacpp_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T08:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka9ltx</id>
    <title>Qwen did it!</title>
    <updated>2025-04-28T23:19:19+00:00</updated>
    <author>
      <name>/u/josho2001</name>
      <uri>https://old.reddit.com/user/josho2001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka9ltx/qwen_did_it/"&gt; &lt;img alt="Qwen did it!" src="https://b.thumbs.redditmedia.com/u6u6-206cP6GQfMdPkcYmLUDgdNtL8CfLg_DT718cyU.jpg" title="Qwen did it!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/wh2chz5crnxe1.png?width=808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e7106c82745c39c5eedc28046f41fc84112717e"&gt;https://preview.redd.it/wh2chz5crnxe1.png?width=808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e7106c82745c39c5eedc28046f41fc84112717e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen did it! A 600 million parameter model, which is also arround 600mb, which is also a REASONING MODEL, running at 134tok/sec did it.&lt;br /&gt; this model family is spectacular, I can see that from here, qwen3 4B is similar to qwen2.5 7b + is a reasoning model and runs extremely fast alongide its 600 million parameter brother-with speculative decoding enabled.&lt;br /&gt; I can only imagine the things this will enable&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/josho2001"&gt; /u/josho2001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka9ltx/qwen_did_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka9ltx/qwen_did_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ka9ltx/qwen_did_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-28T23:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kal789</id>
    <title>What's interesting is that Qwen's release is three months behind Deepseek's. So, if you believe Qwen 3 is currently the leader in open source, I don't think that will last, as R2 is on the verge of release. You can see the gap between Qwen 3 and the three-month-old Deepseek R1.</title>
    <updated>2025-04-29T11:05:07+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kal789/whats_interesting_is_that_qwens_release_is_three/"&gt; &lt;img alt="What's interesting is that Qwen's release is three months behind Deepseek's. So, if you believe Qwen 3 is currently the leader in open source, I don't think that will last, as R2 is on the verge of release. You can see the gap between Qwen 3 and the three-month-old Deepseek R1." src="https://preview.redd.it/10yj2jkm9rxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbe936563146c5f466a3a174f20a285134431a20" title="What's interesting is that Qwen's release is three months behind Deepseek's. So, if you believe Qwen 3 is currently the leader in open source, I don't think that will last, as R2 is on the verge of release. You can see the gap between Qwen 3 and the three-month-old Deepseek R1." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/10yj2jkm9rxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kal789/whats_interesting_is_that_qwens_release_is_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kal789/whats_interesting_is_that_qwens_release_is_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T11:05:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaqi3k</id>
    <title>Qwen 3 8B, 14B, 32B, 30B-A3B &amp; 235B-A22B Tested</title>
    <updated>2025-04-29T15:18:54+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=GmE4JwmFuHk"&gt;https://www.youtube.com/watch?v=GmE4JwmFuHk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Score Tables with Key Insights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;These are generally very very good models.&lt;/li&gt; &lt;li&gt;They all seem to struggle a bit in non english languages. If you take out non English questions from the dataset, the scores will across the board rise about 5-10 points.&lt;/li&gt; &lt;li&gt;Coding is top notch, even with the smaller models.&lt;/li&gt; &lt;li&gt;I have not yet tested the 0.6, 1 and 4B, that will come soon. In my experience for the use cases I cover, 8b is the bare minimum, but I have been surprised in the past, I'll post soon!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Test 1: Harmful Question Detection&lt;/strong&gt; (Timestamp ~3:30)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-32b&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-235b-a22b-04-28&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-8b&lt;/td&gt; &lt;td align="left"&gt;80.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-30b-a3b-04-28&lt;/td&gt; &lt;td align="left"&gt;80.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-14b&lt;/td&gt; &lt;td align="left"&gt;75.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Test 2: Named Entity Recognition (NER)&lt;/strong&gt; (Timestamp ~5:56)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-30b-a3b-04-28&lt;/td&gt; &lt;td align="left"&gt;90.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-32b&lt;/td&gt; &lt;td align="left"&gt;80.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-8b&lt;/td&gt; &lt;td align="left"&gt;80.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-14b&lt;/td&gt; &lt;td align="left"&gt;80.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-235b-a22b-04-28&lt;/td&gt; &lt;td align="left"&gt;75.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;Note: multilingual translation seemed to be the main source of errors, especially Nordic languages.&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Test 3: SQL Query Generation&lt;/strong&gt; (Timestamp ~8:47)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;Key Insight&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-235b-a22b-04-28&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;Excellent coding performance,&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-14b&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;Excellent coding performance,&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-32b&lt;/td&gt; &lt;td align="left"&gt;100.00&lt;/td&gt; &lt;td align="left"&gt;Excellent coding performance,&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-30b-a3b-04-28&lt;/td&gt; &lt;td align="left"&gt;95.00&lt;/td&gt; &lt;td align="left"&gt;Very strong performance from the smaller MoE model.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-8b&lt;/td&gt; &lt;td align="left"&gt;85.00&lt;/td&gt; &lt;td align="left"&gt;Good performance, comparable to other 8b models.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Test 4: Retrieval Augmented Generation (RAG)&lt;/strong&gt; (Timestamp ~11:22)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-32b&lt;/td&gt; &lt;td align="left"&gt;92.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-14b&lt;/td&gt; &lt;td align="left"&gt;90.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-235b-a22b-04-28&lt;/td&gt; &lt;td align="left"&gt;89.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-8b&lt;/td&gt; &lt;td align="left"&gt;85.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-30b-a3b-04-28&lt;/td&gt; &lt;td align="left"&gt;85.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;Note: Key issue is models responding in English when asked to respond in the source language (e.g., Japanese).&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqi3k/qwen_3_8b_14b_32b_30ba3b_235ba22b_tested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqi3k/qwen_3_8b_14b_32b_30ba3b_235ba22b_tested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqi3k/qwen_3_8b_14b_32b_30ba3b_235ba22b_tested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T15:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaqvi5</id>
    <title>Qwen3 EQ-Bench results. Tested: 235b-a22b, 32b, 14b, 30b-a3b.</title>
    <updated>2025-04-29T15:33:57+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqvi5/qwen3_eqbench_results_tested_235ba22b_32b_14b/"&gt; &lt;img alt="Qwen3 EQ-Bench results. Tested: 235b-a22b, 32b, 14b, 30b-a3b." src="https://b.thumbs.redditmedia.com/Mt3YKv1BhE0B7HKW7VyXIItpsWTsAnwus0zUjNwzqNs.jpg" title="Qwen3 EQ-Bench results. Tested: 235b-a22b, 32b, 14b, 30b-a3b." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links:&lt;br /&gt; &lt;a href="https://eqbench.com/creative_writing_longform.html"&gt;https://eqbench.com/creative_writing_longform.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/judgemark-v2.html"&gt;https://eqbench.com/judgemark-v2.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/qwen__qwen3-235b-a22b_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/qwen__qwen3-235b-a22b_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/qwen__qwen3-32b_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/qwen__qwen3-32b_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/qwen__qwen3-30b-a3b_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/qwen__qwen3-30b-a3b_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/qwen__qwen3-14b_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/qwen__qwen3-14b_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kaqvi5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqvi5/qwen3_eqbench_results_tested_235ba22b_32b_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqvi5/qwen3_eqbench_results_tested_235ba22b_32b_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T15:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaa8iz</id>
    <title>This is 600M parameters??? Yesterday I would have told you this was impossible.</title>
    <updated>2025-04-28T23:49:16+00:00</updated>
    <author>
      <name>/u/JohnnyLiverman</name>
      <uri>https://old.reddit.com/user/JohnnyLiverman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prompt: &amp;quot;If 5 brog 5 is 1, and 4 brog 2 is 2, what is 3 brog 1?&amp;quot;&lt;/p&gt; &lt;p&gt;thinking content: &amp;lt;think&amp;gt;&lt;/p&gt; &lt;p&gt;Okay, let's see. The problem says that if 5 brog 5 is 1, and 4 brog 2 is 2, then what is 3 brog 1? Hmm, so I need to figure out a pattern or a relationship between the numbers here. Let me start by breaking down the given information.&lt;/p&gt; &lt;p&gt;First, let's parse the question. It mentions &amp;quot;brog&amp;quot; which I assume is a typo or a term that needs to be clarified. Maybe it's supposed to be &amp;quot;bro&amp;quot; or &amp;quot;brog&amp;quot; as in &amp;quot;bro&amp;quot; with a different meaning? Wait, maybe it's a play on words? Like, maybe &amp;quot;brog&amp;quot; stands for something related to numbers or operations. Alternatively, maybe it's a misprint for &amp;quot;bro&amp;quot; which could mean &amp;quot;brog&amp;quot; as in &amp;quot;brog&amp;quot; as in &amp;quot;brog&amp;quot; which is a term used in some contexts, but I'm not sure. Let me think.&lt;/p&gt; &lt;p&gt;Alternatively, perhaps &amp;quot;brog&amp;quot; is a shorthand for multiplication or some other operation. Let me check the examples again. If 5 brog 5 is 1, and 4 brog 2 is 2, then maybe each operation involves multiplying the two numbers? Let me test that. For example, 5 times 5 is 25, which would be 25, but the result given is 1. So that doesn't fit. Alternatively, maybe it's a different operation. Let me think.&lt;/p&gt; &lt;p&gt;Alternatively, maybe it's a combination of numbers. For instance, maybe it's a function f(n, m) where f(n, m) is defined as something. Let's try to think of possible functions. For example, maybe f(n, m) = something like n/m or n + m? Let's check:&lt;/p&gt; &lt;p&gt;If 5 brog 5 is 1, then maybe f(5, 5) = 1. If 4 brog 2 is 2, then f(4, 2) = 2. Let's see if there's a pattern. If we think of 5 brog 5 as 1, which is 5/5 = 1. Then 4 brog 2 is 2, which would be 4/2 = 2. That seems to fit! So maybe the function is f(n, m) = n/m. Then 3 brog 1 would be 3/1 = 3. Therefore, the answer should be 3. But let me check again to make sure there's no other possible interpretation.&lt;/p&gt; &lt;p&gt;Alternatively, maybe there's another operation. For example, maybe it's a combination like n + m, but 5 + 5 is 10, which doesn't match. Or maybe n multiplied by m, but 5*5 is 25, which doesn't match. Or maybe n divided by m, which would be 1 for the first case and 2 for the second. So that seems consistent. Therefore, if 5 brog 5 is 1, then 3 brog 1 is 3. Therefore, the answer is 3. But let me check if there's another possibility.&lt;/p&gt; &lt;p&gt;Wait, another thought: maybe &amp;quot;brog&amp;quot; is a term related to numbers in a different way. For example, maybe it's a cipher or a code. For example, if &amp;quot;brog&amp;quot; represents something else. But without more context, it's hard to tell. However, given the two examples, multiplying the two numbers gives the result. So 5/5=1, 4/2=2, so 3/1=3. Therefore, I think that's the intended relationship.&lt;/p&gt; &lt;p&gt;Alternatively, maybe it's a function where &amp;quot;brog&amp;quot; represents the number of something. For example, if you have &amp;quot;brog&amp;quot; as in &amp;quot;bro&amp;quot; meaning something else. But again, without knowing the exact meaning, this is speculative.&lt;/p&gt; &lt;p&gt;Alternatively, maybe it's a function where &amp;quot;brog&amp;quot; is a variable. Let me think of a function f(n, m) where n and m are numbers, and f(n, m) is defined in some way. For example, maybe f(n, m) = n - m? Then 5-5=0, which doesn't match. Or f(n, m) = n + m? 5+5=10≠1. Not helpful.&lt;/p&gt; &lt;p&gt;Alternatively, maybe it's a function where the result is the sum of the numbers divided by something. For example, 5 brog 5 is 1. Let me think of fractions. If 5/5=1, then 4/2=2, so 3/1=3. That seems to fit. So maybe the rule is f(n, m) = n/m. Therefore, the answer is 3. Therefore, I think the answer is 3.&lt;/p&gt; &lt;p&gt;**Final Answer**&lt;/p&gt; &lt;p&gt;The value of 3 brog 1 is \boxed{3}.&lt;/p&gt; &lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt; &lt;p&gt;content: To solve the problem, we are given two equations:&lt;/p&gt; &lt;p&gt;- $ 5 \text{ brog } 5 = 1 $&lt;/p&gt; &lt;p&gt;- $ 4 \text{ brog } 2 = 2 $&lt;/p&gt; &lt;p&gt;We are asked to determine the value of $ 3 \text{ brog } 1 $.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### Step 1: Identify the Pattern&lt;/p&gt; &lt;p&gt;We observe that:&lt;/p&gt; &lt;p&gt;- $ 5 \text{ brog } 5 = 1 $&lt;/p&gt; &lt;p&gt;- $ 4 \text{ brog } 2 = 2 $&lt;/p&gt; &lt;p&gt;If we interpret &amp;quot;brog&amp;quot; as a simple mathematical operation, and assume it is the operation of division, then:&lt;/p&gt; &lt;p&gt;- $ 5 \text{ brog } 5 = \frac{5}{5} = 1 $&lt;/p&gt; &lt;p&gt;- $ 4 \text{ brog } 2 = \frac{4}{2} = 2 $&lt;/p&gt; &lt;p&gt;This pattern suggests that the operation is defined as $ \frac{n}{m} $, where $ n $ and $ m $ are the numbers in the &amp;quot;brog&amp;quot; expression.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### Step 2: Apply the Pattern to Find the Result&lt;/p&gt; &lt;p&gt;We are now asked to find the value of $ 3 \text{ brog } 1 $. Using the same pattern:&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;3 \text{ brog } 1 = \frac{3}{1} = 3&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;### Final Answer&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;p&gt;\boxed{3}&lt;/p&gt; &lt;p&gt;$$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JohnnyLiverman"&gt; /u/JohnnyLiverman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaa8iz/this_is_600m_parameters_yesterday_i_would_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaa8iz/this_is_600m_parameters_yesterday_i_would_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaa8iz/this_is_600m_parameters_yesterday_i_would_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-28T23:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kanrt7</id>
    <title>Don't forget to update llama.cpp</title>
    <updated>2025-04-29T13:20:22+00:00</updated>
    <author>
      <name>/u/c-rious</name>
      <uri>https://old.reddit.com/user/c-rious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you're like me, you try to avoid recompiling llama.cpp all too often.&lt;/p&gt; &lt;p&gt;In my case, I was 50ish commits behind, but Qwen3 30-A3B q4km from bartowski was still running fine on my 4090, albeit with with 86t/s.&lt;/p&gt; &lt;p&gt;I got curious after reading about 3090s being able to push 100+ t/s&lt;/p&gt; &lt;p&gt;After updating to the latest master, llama-bench failed to allocate to CUDA :-(&lt;/p&gt; &lt;p&gt;But refreshing bartowski's page, he now specified the tag used to provide the quants, which in my case was &lt;code&gt;b5200&lt;/code&gt;&lt;/p&gt; &lt;p&gt;After another recompile, I get *&lt;em&gt;160+ *&lt;/em&gt; t/s&lt;/p&gt; &lt;p&gt;Holy shit indeed - so as always, read the fucking manual :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c-rious"&gt; /u/c-rious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kanrt7/dont_forget_to_update_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kanrt7/dont_forget_to_update_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kanrt7/dont_forget_to_update_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T13:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kam3sf</id>
    <title>Qwen3 is really good at MCP/FunctionCall</title>
    <updated>2025-04-29T11:57:36+00:00</updated>
    <author>
      <name>/u/reabiter</name>
      <uri>https://old.reddit.com/user/reabiter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kam3sf/qwen3_is_really_good_at_mcpfunctioncall/"&gt; &lt;img alt="Qwen3 is really good at MCP/FunctionCall" src="https://b.thumbs.redditmedia.com/GsikVysd4anKipreIytazdN86ywObzXY1syoIzPdz-I.jpg" title="Qwen3 is really good at MCP/FunctionCall" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been keeping an eye on the performance of LLMs using MCP. I believe that MCP is the key for LLMs to make an impact on real-world workflows. I've always dreamed of having a local LLM serve as the brain and act as the intelligent core for smart-home system.&lt;/p&gt; &lt;p&gt;Now, it seems I've found the one. Qwen3 fits the bill perfectly, and it's an absolute delight to use. This is a test for the best local LLMs. I used Cherry Studio, MCP/server-file-system, and all the models were from the free versions on OpenRouter, without any extra system prompts. The test is pretty straightforward. I asked the LLMs to write a poem and save it to a specific file. The tricky part of this task is that the models first have to realize they're restricted to operating within a designated directory, so they need to do a query first. Then, they have to correctly call the MCP interface for file - writing. The unified test instruction is:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Write a poem, an aria, with the theme of expressing my desire to eat hot pot. Write it into a file in a directory that you are allowed to access.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here's how these models performed.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model/Version&lt;/th&gt; &lt;th align="left"&gt;Rating&lt;/th&gt; &lt;th align="left"&gt;Key Performance&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;⭐⭐⭐⭐⭐&lt;/td&gt; &lt;td align="left"&gt;🌟 Directly called &lt;code&gt;list_allowed_directories&lt;/code&gt; and &lt;code&gt;write_file&lt;/code&gt;, executed smoothly&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-30B-A3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;⭐⭐⭐⭐⭐&lt;/td&gt; &lt;td align="left"&gt;🌟 Equally clean as Qwen3-8B, textbook-level logic&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma3-27B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;⭐⭐⭐⭐⭐&lt;/td&gt; &lt;td align="left"&gt;🎵 Perfect workflow + friendly tone, completed task efficiently&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama-4-Scout&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;⭐⭐⭐&lt;/td&gt; &lt;td align="left"&gt;⚠️ Tried system path first, fixed format errors after feedback&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Deepseek-0324&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;⭐⭐⭐&lt;/td&gt; &lt;td align="left"&gt;🔁 Checked dirs but wrote to invalid path initially, finished after retries&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-3.1-24B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;⭐⭐💫&lt;/td&gt; &lt;td align="left"&gt;🤔 Created dirs correctly but kept deleting line breaks repeatedly&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma3-12B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;⭐⭐&lt;/td&gt; &lt;td align="left"&gt;💔 Kept trying to read non-existent &lt;code&gt;hotpot_aria.txt&lt;/code&gt;, gave up apologizing&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Deepseek-R1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;❌&lt;/td&gt; &lt;td align="left"&gt;🚫 Forced write to invalid Windows &lt;code&gt;/mnt&lt;/code&gt; path, ignored error messages&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reabiter"&gt; /u/reabiter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kam3sf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kam3sf/qwen3_is_really_good_at_mcpfunctioncall/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kam3sf/qwen3_is_really_good_at_mcpfunctioncall/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T11:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1kafb6h</id>
    <title>Qwen3 8B FP16 - asked for 93 items, got 93 items.</title>
    <updated>2025-04-29T04:16:48+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kafb6h/qwen3_8b_fp16_asked_for_93_items_got_93_items/"&gt; &lt;img alt="Qwen3 8B FP16 - asked for 93 items, got 93 items." src="https://preview.redd.it/61wa8u139pxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13d7490e62b1815f603b33c4e952eaaceb7f52aa" title="Qwen3 8B FP16 - asked for 93 items, got 93 items." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tried many times - alwas exact list length.&lt;br /&gt; Without using minItems.&lt;/p&gt; &lt;p&gt;in my daily work this is a breakthrough!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/61wa8u139pxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kafb6h/qwen3_8b_fp16_asked_for_93_items_got_93_items/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kafb6h/qwen3_8b_fp16_asked_for_93_items_got_93_items/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T04:16:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kafwa7</id>
    <title>Qwen3 0.6B on Android runs flawlessly</title>
    <updated>2025-04-29T04:52:48+00:00</updated>
    <author>
      <name>/u/----Val----</name>
      <uri>https://old.reddit.com/user/----Val----</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kafwa7/qwen3_06b_on_android_runs_flawlessly/"&gt; &lt;img alt="Qwen3 0.6B on Android runs flawlessly" src="https://external-preview.redd.it/bXNhcjB5dTNncHhlMW--9zt028XHo6Q9w18q7qbHfuIyDOGMqIn7YhcX7Vou.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc2641a482fc3b1241bfca716695360bdcf7981d" title="Qwen3 0.6B on Android runs flawlessly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently released v0.8.6 for ChatterUI, just in time for the Qwen 3 drop:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Vali-98/ChatterUI/releases/latest"&gt;https://github.com/Vali-98/ChatterUI/releases/latest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far the models seem to run fine out of the gate, and generation speeds are very optimistic for 0.6B-4B, and this is by far the smartest small model I have used.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/----Val----"&gt; /u/----Val---- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3snum824gpxe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kafwa7/qwen3_06b_on_android_runs_flawlessly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kafwa7/qwen3_06b_on_android_runs_flawlessly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T04:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka8b2u</id>
    <title>Qwen3-30B-A3B is what most people have been waiting for</title>
    <updated>2025-04-28T22:19:39+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A QwQ competitor that limits its thinking that uses MoE with very small experts for lightspeed inference.&lt;/p&gt; &lt;p&gt;It's out, it's the real deal, Q5 is competing with QwQ easily in my personal local tests and pipelines. It's succeeding at coding one-shots, it's succeeding at editing existing codebases, it's succeeding as the 'brains' of an agentic pipeline of mine- and it's doing it all at blazing fast speeds.&lt;/p&gt; &lt;p&gt;No excuse now - intelligence that used to be SOTA now runs on modest gaming rigs - &lt;em&gt;GO BUILD SOMETHING COOL&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka8b2u/qwen330ba3b_is_what_most_people_have_been_waiting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka8b2u/qwen330ba3b_is_what_most_people_have_been_waiting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ka8b2u/qwen330ba3b_is_what_most_people_have_been_waiting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-28T22:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaioin</id>
    <title>Qwen3 after the hype</title>
    <updated>2025-04-29T08:05:28+00:00</updated>
    <author>
      <name>/u/Cheap_Concert168no</name>
      <uri>https://old.reddit.com/user/Cheap_Concert168no</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that I hope the initial hype has subsided, how are each models really?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B"&gt;Qwen/Qwen3-235B-A22B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B"&gt;Qwen/Qwen3-30B-A3B &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-32B"&gt;Qwen/Qwen3-32B &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-14B"&gt;Qwen/Qwen3-14B &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-8B"&gt;Qwen/Qwen3-8B &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-4B"&gt;Qwen/Qwen3-4B &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-1.7B"&gt;Qwen/Qwen3-1.7B &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-0.6B"&gt;Qwen/Qwen3-0.6B &lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Beyond the benchmarks, how are they really feeling according to you in terms of coding, creative, brainstorming and thinking? What are the strengths and weaknesses?&lt;/p&gt; &lt;p&gt;Edit: Also does the A22B mean I can run the 235B model on some machine capable of running any 22B model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheap_Concert168no"&gt; /u/Cheap_Concert168no &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaioin/qwen3_after_the_hype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaioin/qwen3_after_the_hype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaioin/qwen3_after_the_hype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T08:05:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kank02</id>
    <title>VRAM Requirements Reference - What can you run with your VRAM? (Contributions welcome)</title>
    <updated>2025-04-29T13:10:12+00:00</updated>
    <author>
      <name>/u/Oatilis</name>
      <uri>https://old.reddit.com/user/Oatilis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kank02/vram_requirements_reference_what_can_you_run_with/"&gt; &lt;img alt="VRAM Requirements Reference - What can you run with your VRAM? (Contributions welcome)" src="https://preview.redd.it/winyewblwrxe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5cae8973b440ad7f513edeea46ed84c1b9a21d70" title="VRAM Requirements Reference - What can you run with your VRAM? (Contributions welcome)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created this resource to help me quickly see which models I can run on certain VRAM constraints.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://imraf.github.io/ai-model-reference/"&gt;https://imraf.github.io/ai-model-reference/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd like this to be as comprehensive as possible. It's on GitHub and contributions are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oatilis"&gt; /u/Oatilis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/winyewblwrxe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kank02/vram_requirements_reference_what_can_you_run_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kank02/vram_requirements_reference_what_can_you_run_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T13:10:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka8r2t</id>
    <title>Qwen didn't just cook. They had a whole barbecue!</title>
    <updated>2025-04-28T22:39:32+00:00</updated>
    <author>
      <name>/u/MushroomGecko</name>
      <uri>https://old.reddit.com/user/MushroomGecko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka8r2t/qwen_didnt_just_cook_they_had_a_whole_barbecue/"&gt; &lt;img alt="Qwen didn't just cook. They had a whole barbecue!" src="https://preview.redd.it/z8nu50rdlnxe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42135693763896c9c446089c6fa47afc509ad7f4" title="Qwen didn't just cook. They had a whole barbecue!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MushroomGecko"&gt; /u/MushroomGecko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z8nu50rdlnxe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka8r2t/qwen_didnt_just_cook_they_had_a_whole_barbecue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ka8r2t/qwen_didnt_just_cook_they_had_a_whole_barbecue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-28T22:39:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ka6mic</id>
    <title>Qwen 3 !!!</title>
    <updated>2025-04-28T21:07:01+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka6mic/qwen_3/"&gt; &lt;img alt="Qwen 3 !!!" src="https://b.thumbs.redditmedia.com/rxKFE9weQYC5m_GiwAPp1WBzACIB-lnfHoSDBbyeOEE.jpg" title="Qwen 3 !!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Qwen3! &lt;/p&gt; &lt;p&gt;We release and open-weight Qwen3, our latest large language models, including 2 MoE models and 6 dense models, ranging from 0.6B to 235B. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.&lt;/p&gt; &lt;p&gt;For more information, feel free to try them out in Qwen Chat Web (chat.qwen.ai) and APP and visit our GitHub, HF, ModelScope, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ka6mic"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ka6mic/qwen_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ka6mic/qwen_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-28T21:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kag4er</id>
    <title>Qwen3-30B-A3B runs at 12-15 tokens-per-second on CPU</title>
    <updated>2025-04-29T05:07:07+00:00</updated>
    <author>
      <name>/u/AlgorithmicKing</name>
      <uri>https://old.reddit.com/user/AlgorithmicKing</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kag4er/qwen330ba3b_runs_at_1215_tokenspersecond_on_cpu/"&gt; &lt;img alt="Qwen3-30B-A3B runs at 12-15 tokens-per-second on CPU" src="https://external-preview.redd.it/M2wycXVrZ25pcHhlMaaUXUGYAurpwjopJY1zZmzeHwQa0qRuSYTlAz0bqREA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adf61e1ca1b6dafd51c56b4437239602664aad97" title="Qwen3-30B-A3B runs at 12-15 tokens-per-second on CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CPU: AMD Ryzen 9 7950x3d&lt;br /&gt; RAM: 32 GB&lt;/p&gt; &lt;p&gt;I am using the UnSloth Q6_K version of Qwen3-30B-A3B (&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/blob/main/Qwen3-30B-A3B-Q6_K.gguf"&gt;Qwen3-30B-A3B-Q6_K.gguf · unsloth/Qwen3-30B-A3B-GGUF at main&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlgorithmicKing"&gt; /u/AlgorithmicKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k27mtpenipxe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kag4er/qwen330ba3b_runs_at_1215_tokenspersecond_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kag4er/qwen330ba3b_runs_at_1215_tokenspersecond_on_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T05:07:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaqhxy</id>
    <title>Llama 4 reasoning 17b model releasing today</title>
    <updated>2025-04-29T15:18:43+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqhxy/llama_4_reasoning_17b_model_releasing_today/"&gt; &lt;img alt="Llama 4 reasoning 17b model releasing today" src="https://preview.redd.it/hy71sz0sjsxe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ff1abe7a5307a69a23eb4ca02f7f5cbb5bafaf4" title="Llama 4 reasoning 17b model releasing today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hy71sz0sjsxe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqhxy/llama_4_reasoning_17b_model_releasing_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaqhxy/llama_4_reasoning_17b_model_releasing_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T15:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kalkgi</id>
    <title>I just realized Qwen3-30B-A3B is all I need for local LLM</title>
    <updated>2025-04-29T11:26:53+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After I found out that the new Qwen3-30B-A3B MoE is really slow in Ollama, I decided to try LM Studio instead, and it's working as expected, over 100+ tk/s on a power-limited 4090.&lt;/p&gt; &lt;p&gt;After testing it more, I suddenly realized: this one model is all I need! &lt;/p&gt; &lt;p&gt;I tested translation, coding, data analysis, video subtitle and blog summarization, etc. It performs really well on all categories and is super fast. Additionally, it's very VRAM efficient—I still have 4GB VRAM left after maxing out the context length (Q8 cache enabled, Unsloth Q4 UD gguf).&lt;/p&gt; &lt;p&gt;I used to switch between multiple models of different sizes and quantization levels for different tasks, which is why I stuck with Ollama because of its easy model switching. I also keep using an older version of Open WebUI because the managing a large amount of models is much more difficult in the latest version.&lt;/p&gt; &lt;p&gt;Now all I need is LM Studio, the latest Open WebUI, and Qwen3-30B-A3B. I can finally free up some disk space and move my huge model library to the backup drive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kalkgi/i_just_realized_qwen330ba3b_is_all_i_need_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kalkgi/i_just_realized_qwen330ba3b_is_all_i_need_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kalkgi/i_just_realized_qwen330ba3b_is_all_i_need_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T11:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kaodxu</id>
    <title>Qwen3 Unsloth Dynamic GGUFs + 128K Context + Bug Fixes</title>
    <updated>2025-04-29T13:48:38+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/Localllama"&gt;r/Localllama&lt;/a&gt;! We've uploaded Dynamic 2.0 GGUFs and quants for Qwen3. &lt;strong&gt;ALL Qwen3&lt;/strong&gt; models now benefit from &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;Dynamic 2.0&lt;/a&gt; format.&lt;/p&gt; &lt;p&gt;We've also &lt;strong&gt;fixed all chat template &amp;amp; loading issues.&lt;/strong&gt; They now work properly on all inference engines (llama.cpp, Ollama, LM Studio, Open WebUI etc.)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;These bugs came from incorrect chat template implementations, &lt;strong&gt;not&lt;/strong&gt; the Qwen team. We've informed them, and they’re helping fix it in places like llama.cpp. Small bugs like this happen all the time, and it was through your guy's feedback that we were able to catch this. Some GGUFs defaulted to using the &lt;code&gt;chat_ml&lt;/code&gt; template, so they seemed to work but it's actually incorrect. All our uploads are now corrected.&lt;/li&gt; &lt;li&gt;Context length has been extended from 32K to &lt;strong&gt;128K&lt;/strong&gt; using native YaRN.&lt;/li&gt; &lt;li&gt;Some &lt;strong&gt;235B-A22B&lt;/strong&gt; quants aren't compatible with iMatrix + Dynamic 2.0 despite many testing. We're uploaded as many standard GGUF sizes as possible and left a few of the iMatrix + Dynamic 2.0 that do work.&lt;/li&gt; &lt;li&gt;Thanks to your feedback, we now added Q4_NL, Q5.1, Q5.0, Q4.1, and Q4.0 formats.&lt;/li&gt; &lt;li&gt;ICYMI: Dynamic 2.0 sets new benchmarks for KL Divergence and 5-shot MMLU, making it the best performing quants for running LLMs. &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;See benchmarks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;We also uploaded Dynamic safetensors for fine-tuning/deployment. Fine-tuning is technically supported in Unsloth, but please wait for the official announcement coming very soon.&lt;/li&gt; &lt;li&gt;We made a detailed guide on how to run Qwen3 (including 235B-A22B) with official settings: &lt;a href="https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune"&gt;https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Qwen3 - Official Settings:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Setting&lt;/th&gt; &lt;th align="left"&gt;Non-Thinking Mode&lt;/th&gt; &lt;th align="left"&gt;Thinking Mode&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Temperature&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.7&lt;/td&gt; &lt;td align="left"&gt;0.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Min_P&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.0 (optional, but 0.01 works well; llama.cpp default is 0.1)&lt;/td&gt; &lt;td align="left"&gt;0.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Top_P&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.8&lt;/td&gt; &lt;td align="left"&gt;0.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;TopK&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Qwen3 - Unsloth Dynamic 2.0 Uploads -with optimal configs:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Qwen3 variant&lt;/th&gt; &lt;th align="left"&gt;GGUF&lt;/th&gt; &lt;th align="left"&gt;GGUF (128K Context)&lt;/th&gt; &lt;th align="left"&gt;Dynamic 4-bit Safetensor&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;0.6B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-0.6B-GGUF"&gt;0.6B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-0.6B-128K-GGUF"&gt;0.6B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-0.6B-unsloth-bnb-4bit"&gt;0.6B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;1.7B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-1.7B-GGUF"&gt;1.7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-1.7B-128K-GGUF"&gt;1.7B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-1.7B-unsloth-bnb-4bit"&gt;1.7B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;4B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-4B-GGUF"&gt;4B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-4B-128K-GGUF"&gt;4B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-4B-unsloth-bnb-4bit"&gt;4B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-8B-GGUF"&gt;8B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-8B-128K-GGUF"&gt;8B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-8B-unsloth-bnb-4bit"&gt;8B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;14B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-14B-GGUF"&gt;14B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-14B-128K-GGUF"&gt;14B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-14B-unsloth-bnb-4bit"&gt;14B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;30B-A3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF"&gt;30B-A3B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-128K-GGUF"&gt;30B-A3B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-GGUF"&gt;32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF"&gt;32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-32B-unsloth-bnb-4bit"&gt;32B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Also wanted to give a huge shoutout to the Qwen team for helping us and the open-source community with their incredible team support! And of course thank you to you all for reporting and testing the issues with us! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaodxu/qwen3_unsloth_dynamic_ggufs_128k_context_bug_fixes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kaodxu/qwen3_unsloth_dynamic_ggufs_128k_context_bug_fixes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kaodxu/qwen3_unsloth_dynamic_ggufs_128k_context_bug_fixes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T13:48:38+00:00</published>
  </entry>
</feed>
