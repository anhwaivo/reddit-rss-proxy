<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-29T15:34:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1icphqa</id>
    <title>How to run deepseek r1 on 4xH100</title>
    <updated>2025-01-29T08:45:26+00:00</updated>
    <author>
      <name>/u/sanjay920</name>
      <uri>https://old.reddit.com/user/sanjay920</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/sanjay920/run_deepseek_r1"&gt;https://github.com/sanjay920/run_deepseek_r1&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Throughput Achieved&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;DeepSeek R1 running on a 4×H100 setup reached a generation rate of &lt;strong&gt;25 tokens/second&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Over an hour, that amounts to &lt;strong&gt;90,000 output tokens&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Compute Costs on Lambda Cloud&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Running 4×H100 GPUs on Lambda Cloud costs &lt;strong&gt;$12.36 per hour&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Generating 90k tokens in one hour results in an estimated &lt;strong&gt;$137 per 1 million tokens&lt;/strong&gt; (based on 11.1 hours needed to generate 1M tokens).&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Comparison to OpenAI O1 Pricing&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;OpenAI O1 charges &lt;strong&gt;$60 per 1 million output tokens&lt;/strong&gt;, making it roughly &lt;strong&gt;2× to 2.5× cheaper&lt;/strong&gt; than this self-hosted setup at the current throughput.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sanjay920"&gt; /u/sanjay920 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icphqa/how_to_run_deepseek_r1_on_4xh100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icphqa/how_to_run_deepseek_r1_on_4xh100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icphqa/how_to_run_deepseek_r1_on_4xh100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T08:45:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic3k3b</id>
    <title>No censorship when running Deepseek locally.</title>
    <updated>2025-01-28T15:05:21+00:00</updated>
    <author>
      <name>/u/ISNT_A_ROBOT</name>
      <uri>https://old.reddit.com/user/ISNT_A_ROBOT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"&gt; &lt;img alt="No censorship when running Deepseek locally." src="https://preview.redd.it/95fhiv1e2rfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c23d5ab8c71a12862078dfeeb51d4785b2a9bb58" title="No censorship when running Deepseek locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ISNT_A_ROBOT"&gt; /u/ISNT_A_ROBOT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95fhiv1e2rfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic3k3b/no_censorship_when_running_deepseek_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T15:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic61zb</id>
    <title>"Sir, China just released another model"</title>
    <updated>2025-01-28T16:52:39+00:00</updated>
    <author>
      <name>/u/danilofs</name>
      <uri>https://old.reddit.com/user/danilofs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt; &lt;img alt="&amp;quot;Sir, China just released another model&amp;quot;" src="https://a.thumbs.redditmedia.com/hlgnuSLD8D8wJmgHtUGXn38QzQ7a2xkFvpX65Gj6yM0.jpg" title="&amp;quot;Sir, China just released another model&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The burst of DeepSeek V3 has attracted attention from the whole AI community to large-scale MoE models. Concurrently, they have built Qwen2.5-Max, a large MoE LLM pretrained on massive data and post-trained with curated SFT and RLHF recipes. It achieves competitive performance against the top-tier models, and outcompetes DeepSeek V3 in benchmarks like Arena Hard, LiveBench, LiveCodeBench, GPQA-Diamond. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6f0byi66lrfe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94fa7c2356d596c2d472e3f13adf2a792368255"&gt;https://preview.redd.it/6f0byi66lrfe1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94fa7c2356d596c2d472e3f13adf2a792368255&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danilofs"&gt; /u/danilofs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic61zb/sir_china_just_released_another_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T16:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ibxj3a</id>
    <title>Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC</title>
    <updated>2025-01-28T09:04:57+00:00</updated>
    <author>
      <name>/u/noblex33</name>
      <uri>https://old.reddit.com/user/noblex33</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt; &lt;img alt="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" src="https://external-preview.redd.it/AH_s6Lnngj4fg7u4p7ikli1G9UIpzFPfjMk_755j9_E.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93f0138e6b1b669eee32d0888eddee9317da1a1b" title="Trump to impose 25% to 100% tariffs on Taiwan-made chips, impacting TSMC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noblex33"&gt; /u/noblex33 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/trump-to-impose-25-percent-100-percent-tariffs-on-taiwan-made-chips-impacting-tsmc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ibxj3a/trump_to_impose_25_to_100_tariffs_on_taiwanmade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T09:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1icsk3w</id>
    <title>3 new reasoning datasets using R1 - High-quality CoTs (from Maxime Labonne on X)</title>
    <updated>2025-01-29T12:23:04+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsk3w/3_new_reasoning_datasets_using_r1_highquality/"&gt; &lt;img alt="3 new reasoning datasets using R1 - High-quality CoTs (from Maxime Labonne on X)" src="https://external-preview.redd.it/pXALc_V-tAIQTXDrrHGn0OLNGuLlNPdMvDRZSfsMWeQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25de674d750578ef139c2374aa784ae1110fd4b9" title="3 new reasoning datasets using R1 - High-quality CoTs (from Maxime Labonne on X)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bespoke-Stratos-17k: &lt;a href="https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k"&gt;https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k&lt;/a&gt;&lt;br /&gt; OpenThoughts-114k: &lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"&gt;https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k&lt;/a&gt;&lt;br /&gt; R1-Distill-SFT (1.8M samples): &lt;a href="https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT"&gt;https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT&lt;/a&gt;&lt;br /&gt; Maxime Labonne: &lt;em&gt;Dataset review ?? Lots of new reasoning datasets but very few use R1 yet&lt;/em&gt;.: &lt;a href="https://x.com/maximelabonne/status/1884565062708543572"&gt;https://x.com/maximelabonne/status/1884565062708543572&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t5oftpp9exfe1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=994e0594e9860fedec383b1acde4cc4e6a248b7b"&gt;https://preview.redd.it/t5oftpp9exfe1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=994e0594e9860fedec383b1acde4cc4e6a248b7b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsk3w/3_new_reasoning_datasets_using_r1_highquality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsk3w/3_new_reasoning_datasets_using_r1_highquality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icsk3w/3_new_reasoning_datasets_using_r1_highquality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T12:23:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1icq2mf</id>
    <title>I have a budget of 40k USD I need to setup machine to host deepseek r1 - what options do I have</title>
    <updated>2025-01-29T09:30:39+00:00</updated>
    <author>
      <name>/u/zibenmoka</name>
      <uri>https://old.reddit.com/user/zibenmoka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;looking for some tips/directions on hardware choice to host deepseek r1 locally (my budget is up to 40k) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zibenmoka"&gt; /u/zibenmoka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icq2mf/i_have_a_budget_of_40k_usd_i_need_to_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icq2mf/i_have_a_budget_of_40k_usd_i_need_to_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icq2mf/i_have_a_budget_of_40k_usd_i_need_to_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T09:30:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1icku59</id>
    <title>Hugging Face wants to reverse-engineer DeepSeek’s R1</title>
    <updated>2025-01-29T03:39:59+00:00</updated>
    <author>
      <name>/u/etherd0t</name>
      <uri>https://old.reddit.com/user/etherd0t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icku59/hugging_face_wants_to_reverseengineer_deepseeks_r1/"&gt; &lt;img alt="Hugging Face wants to reverse-engineer DeepSeek’s R1" src="https://external-preview.redd.it/ZliqvWOhcshla4bACNsdeiO3AvnKoD4VRcBkZC2miYE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfe10219195ebf4681d234a1326d7b71fa1f5efd" title="Hugging Face wants to reverse-engineer DeepSeek’s R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/etherd0t"&gt; /u/etherd0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://siliconangle.com/2025/01/28/hugging-face-wants-reverse-engineer-deepseeks-r1-reasoning-model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icku59/hugging_face_wants_to_reverseengineer_deepseeks_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icku59/hugging_face_wants_to_reverseengineer_deepseeks_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T03:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1icuff7</id>
    <title>Comparing expected performance of AMD Ryzen AI Max+ 395, NVIDIA DIGITS, and RTX 5090 for Local LLMs</title>
    <updated>2025-01-29T14:02:40+00:00</updated>
    <author>
      <name>/u/Big_Yak9983</name>
      <uri>https://old.reddit.com/user/Big_Yak9983</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I’m looking for opinions from more knowledgable folk on the expected performance of the AMD Ryzen AI Max+ 395 (lol) and NVIDIA’s DIGITS vs the RTX 5090 when it comes to running local LLMs. &lt;/p&gt; &lt;p&gt;For context, asking this question now because I’m trying to decide whether to battle it out with scalpers and see if I can buy an RTX 5090 tomorrow, or to just chill//avoid wasting money if superior tools are round the corner.&lt;/p&gt; &lt;p&gt;From what I’ve gathered:&lt;/p&gt; &lt;p&gt;AMD Ryzen AI Max+ 395 claims to outperform the RTX 4090 by up to 2.2 times in specific AI workloads while drawing up to 87% less power. 96 GB of RAM can be dedicated to graphics tasks which means bigger models. This seems promising for personal use, especially as I’m doing a lot of RAG with medical textbooks and articles.&lt;/p&gt; &lt;p&gt;DIGITS reportedly offers 1 petaflop of performance at FP4 precision (not really sure what this would mean in the real world) and 128 GB of unified memory and NVIDIA is marketing this as optimised for running large models locally.&lt;/p&gt; &lt;p&gt;I’m curious about how both would stack up against the RTX 5090. I know it “only” has 32gb VRAM so would be more limited in what models it can run, but if there is a huge inference speed advantage then I would prefer that over having a bigger model. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Which option do you think will provide the best performance:cost ratio for hosting local LLMs?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How quick do you expect inference speed each of these systems when handling RAG tasks with scientific papers, books etc.?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are there any other considerations or alternatives I should keep in mind? I should state here that I don’t want to buy any Apple product. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Wildcard question:&lt;/p&gt; &lt;p&gt;Have DeepSeek and Chinese researchers changed the game completely, and I need to shift my focus away from optimising what hardware I have entirely??&lt;/p&gt; &lt;p&gt;Thanks in advance for your insights! Hope this also helps others in the same boat as me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_Yak9983"&gt; /u/Big_Yak9983 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icuff7/comparing_expected_performance_of_amd_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icuff7/comparing_expected_performance_of_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icuff7/comparing_expected_performance_of_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T14:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ict448</id>
    <title>Qwen-7B shopkeeper - demo on github</title>
    <updated>2025-01-29T12:55:25+00:00</updated>
    <author>
      <name>/u/No_Abbreviations_532</name>
      <uri>https://old.reddit.com/user/No_Abbreviations_532</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ict448/qwen7b_shopkeeper_demo_on_github/"&gt; &lt;img alt="Qwen-7B shopkeeper - demo on github" src="https://preview.redd.it/t5hnxmj5kxfe1.gif?width=640&amp;amp;crop=smart&amp;amp;s=838f65cfc990d76d84f5a977a244d7fe8eb37415" title="Qwen-7B shopkeeper - demo on github" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Abbreviations_532"&gt; /u/No_Abbreviations_532 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t5hnxmj5kxfe1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ict448/qwen7b_shopkeeper_demo_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ict448/qwen7b_shopkeeper_demo_on_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T12:55:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic8cjf</id>
    <title>$6,000 computer to run Deepseek R1 670B Q8 locally at 6-8 tokens/sec</title>
    <updated>2025-01-28T18:25:13+00:00</updated>
    <author>
      <name>/u/MoltenBoron</name>
      <uri>https://old.reddit.com/user/MoltenBoron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw this on X/Twitter: Tower PC with 2 AMD EPYC CPUs and 24 x 32GB DDR5-RDIMM. No GPUs. 400 W power consumption.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Complete hardware + software setup for running Deepseek-R1 locally. The actual model, no distillations, and Q8 quantization for full quality. Total cost, $6,000. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://x.com/carrigmat/status/1884244369907278106"&gt;https://x.com/carrigmat/status/1884244369907278106&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alternative link (no login):&lt;/p&gt; &lt;p&gt;&lt;a href="https://threadreaderapp.com/thread/1884244369907278106.html"&gt;https://threadreaderapp.com/thread/1884244369907278106.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoltenBoron"&gt; /u/MoltenBoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic8cjf/6000_computer_to_run_deepseek_r1_670b_q8_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T18:25:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ic7hts</id>
    <title>Everyone and their mother knows about DeepSeek</title>
    <updated>2025-01-28T17:50:32+00:00</updated>
    <author>
      <name>/u/siegevjorn</name>
      <uri>https://old.reddit.com/user/siegevjorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone I interact talks about deepseek now. How it's scary, how it's better than Chatgpt, how it's open-source...&lt;/p&gt; &lt;p&gt;But the fact is, 99.9% of these people (including myself) have no way to run 670b model (which actually is the model in hype) in manner that benefit from open-source. I mean just using their front end is no different than using chatGPT. And chatGPT and cluade have, free versions, which evidently are better!&lt;/p&gt; &lt;p&gt;Heck, I hear news reporters talking about how great it is because it works freakishly well and it is an open-source. But in reality, its just open weight, no one have yet to replicate what they did. &lt;/p&gt; &lt;p&gt;But why all the hype? Don't you feel this is too much? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/siegevjorn"&gt; /u/siegevjorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ic7hts/everyone_and_their_mother_knows_about_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T17:50:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1icmkmn</id>
    <title>Deepseek banned in my company server (major MBB)</title>
    <updated>2025-01-29T05:18:52+00:00</updated>
    <author>
      <name>/u/Purple_War_837</name>
      <uri>https://old.reddit.com/user/Purple_War_837</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was happily using deepseek web interface along with the dirt cheap api calls. But suddenly I can not use it today. The hype since last couple of days alerted the assholes deciding which llms to use.&lt;br /&gt; I think this trend is going to continue for other big companies as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Purple_War_837"&gt; /u/Purple_War_837 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmkmn/deepseek_banned_in_my_company_server_major_mbb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmkmn/deepseek_banned_in_my_company_server_major_mbb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icmkmn/deepseek_banned_in_my_company_server_major_mbb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T05:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1icszj8</id>
    <title>Oh boy do local R1 values matter!</title>
    <updated>2025-01-29T12:48:11+00:00</updated>
    <author>
      <name>/u/Zundrium</name>
      <uri>https://old.reddit.com/user/Zundrium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had mixed results with the local 7B, 8B and 32B models, but I sure didn't know that the parameters matter this much. I suck at reading READMEs, but this time I took a bit of time and found these super important instructions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Avoid adding a system prompt; all instructions should be contained within the user prompt.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;For mathematical problems, it is advisable to include a directive in your prompt such as: &amp;quot;Please reason step by step, and put your final answer within \boxed{}.&amp;quot;&lt;/li&gt; &lt;li&gt;When evaluating model performance, it is recommended to conduct multiple tests and average the results.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I apply step 3 to everything, even generating code with success. With increasing the context window to 32768, I have had very consistent solid results. &lt;/p&gt; &lt;p&gt;8B llama is my favorite for instructions, do you guys use different settings?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zundrium"&gt; /u/Zundrium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icszj8/oh_boy_do_local_r1_values_matter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icszj8/oh_boy_do_local_r1_values_matter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icszj8/oh_boy_do_local_r1_values_matter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T12:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ichohj</id>
    <title>DeepSeek API: Every Request Is A Timeout :(</title>
    <updated>2025-01-29T01:00:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichohj/deepseek_api_every_request_is_a_timeout/"&gt; &lt;img alt="DeepSeek API: Every Request Is A Timeout :(" src="https://preview.redd.it/wpmv3ibe0ufe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=449243d847b49583ee6497956bd77db9ec221af7" title="DeepSeek API: Every Request Is A Timeout :(" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wpmv3ibe0ufe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichohj/deepseek_api_every_request_is_a_timeout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ichohj/deepseek_api_every_request_is_a_timeout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T01:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1icta5y</id>
    <title>Why do people like Ollama more than LM Studio?</title>
    <updated>2025-01-29T13:04:17+00:00</updated>
    <author>
      <name>/u/Intelligent-Gift4519</name>
      <uri>https://old.reddit.com/user/Intelligent-Gift4519</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm just curious. I see a ton of people discussing Ollama, but as an LM Studio user, don't see a lot of people talking about it. &lt;/p&gt; &lt;p&gt;But LM Studio seems so much better to me. It uses arbitrary GGUFs, not whatever that weird proprietary format Ollama uses is. It has a really nice GUI, not mysterious opaque headless commands. If I want to try a new model, it's super easy to search for it, download it, try it, and throw it away or serve it up to AnythingLLM for some RAG or foldering.&lt;/p&gt; &lt;p&gt;(Before you raise KoboldCPP, yes, absolutely KoboldCPP, it just doesn't run on my machine.)&lt;/p&gt; &lt;p&gt;So why the Ollama obsession on this board? Help me understand. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent-Gift4519"&gt; /u/Intelligent-Gift4519 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icta5y/why_do_people_like_ollama_more_than_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icta5y/why_do_people_like_ollama_more_than_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icta5y/why_do_people_like_ollama_more_than_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T13:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1icvupa</id>
    <title>Transformer Lab: An Open-Source Alternative to OpenAI Platform, for Local Models</title>
    <updated>2025-01-29T15:07:08+00:00</updated>
    <author>
      <name>/u/aliasaria</name>
      <uri>https://old.reddit.com/user/aliasaria</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icvupa/transformer_lab_an_opensource_alternative_to/"&gt; &lt;img alt="Transformer Lab: An Open-Source Alternative to OpenAI Platform, for Local Models" src="https://external-preview.redd.it/1-unlUYVdK_1l2BS5JVOeBjMkhE9sw4QuEl28ZQ14sQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8ccfed37de41def2b9ba6c53539d7b6e1048265" title="Transformer Lab: An Open-Source Alternative to OpenAI Platform, for Local Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aliasaria"&gt; /u/aliasaria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/transformerlab/transformerlab-app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icvupa/transformer_lab_an_opensource_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icvupa/transformer_lab_an_opensource_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T15:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1icjg39</id>
    <title>Some evidence of DeepSeek being attacked by DDoS has been released!</title>
    <updated>2025-01-29T02:27:21+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"&gt; &lt;img alt="Some evidence of DeepSeek being attacked by DDoS has been released!" src="https://b.thumbs.redditmedia.com/Lszyle7jiuM1pNHqVGd7qcP6bBSMrdPUgCiUZt1sZRY.jpg" title="Some evidence of DeepSeek being attacked by DDoS has been released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jppuddrxfufe1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=156acff91c3d3055b63a4ba7cc0e90ac54db4216"&gt;In the first phase, on January 3, 4, 6, 7, and 13, there were suspected HTTP proxy attacks.During this period, Xlab could see a large number of proxy requests to link DeepSeek through proxies, which were likely HTTP proxy attacks.In the second phase, on January 20, 22-26, the attack method changed to SSDP and NTP reflection amplification.During this period, the main attack methods detected by XLab were SSDP and NTP reflection amplification, and a small number of HTTP proxy attacks. Usually, the defense of SSDP and NTP reflection amplification attacks is simple and easy to clean up.In the third phase, on January 27 and 28, the number of attacks increased sharply, and the means changed to application layer attacks.Starting from the 27th, the main attack method discovered by XLab changed to HTTP proxy attacks. Attacking such application layer attacks simulates normal user behavior, which is significantly more difficult to defend than classic SSDP and NTP reflection amplification attacks, so it is more effective.XLab also found that the peak of the attack on January 28 occurred between 03:00-04:00 Beijing time (UTC+8), which corresponds to 14:00-15:00 Eastern Standard Time (UTC-5) in North America. This time window selection shows that the attack has border characteristics, and it does not rule out the purpose of targeted attacks on overseas service providers.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sbm25zfyfufe1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b70b8ecd569c5f7c95f8f9ca2a802b8871c1f0c6"&gt;this DDoS attack was accompanied by a large number of brute force attacks. All the brute force attack IPs came from the United States. XLab's data can identify that half of these IPs are VPN exits, and it is speculated that this may be caused by DeepSeek's overseas restrictions on mobile phone users.03DeepSeek responded promptly and minimized the impactFaced with the sudden escalation of large-scale DDoS attacks late at night on the 27th and 28th, DeepSeek responded and handled it immediately. Based on the passivedns data of the big network, XLab saw that DeepSeek switched IP at 00:58 on the morning of the 28th when the attacker launched an effective and destructive HTTP proxy attack. This switching time is consistent with Deepseek's own announcement time in the screenshot above, which should be for better security defense. This also further proves XLab's own judgment on this DDoS attack.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Starting at 03:00 on January 28, the DDoS attack was accompanied by a large number of brute force attacks. All brute force attack IPs come from the United States.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://club.6parkbbs.com/military/index.php?app=forum&amp;amp;act=threadview&amp;amp;tid=18616721"&gt;https://club.6parkbbs.com/military/index.php?app=forum&amp;amp;act=threadview&amp;amp;tid=18616721&lt;/a&gt; (only Chinese text)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icjg39/some_evidence_of_deepseek_being_attacked_by_ddos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T02:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1icttm7</id>
    <title>good shit</title>
    <updated>2025-01-29T13:32:50+00:00</updated>
    <author>
      <name>/u/diligentgrasshopper</name>
      <uri>https://old.reddit.com/user/diligentgrasshopper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icttm7/good_shit/"&gt; &lt;img alt="good shit" src="https://preview.redd.it/azitnmgpqxfe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c237cfc71f4b87b6b0a5d08b0631615eefa83d9a" title="good shit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/diligentgrasshopper"&gt; /u/diligentgrasshopper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/azitnmgpqxfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icttm7/good_shit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icttm7/good_shit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T13:32:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1icr6md</id>
    <title>How come we dont see many people spinning up R1 671b in the cloud, selling access and making bank?</title>
    <updated>2025-01-29T10:53:59+00:00</updated>
    <author>
      <name>/u/linkcharger</name>
      <uri>https://old.reddit.com/user/linkcharger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What am I missing? I'm not too knowledgeable about deploying big models like these, but for people that are, shouldn't it be quite easy to deploy it in the cloud? &lt;/p&gt; &lt;p&gt;That's the cool thing about open weights, no? If you have the hardware (which is nothing crazy if you're already using VPS), you can run and scale it dynamically.&lt;/p&gt; &lt;p&gt;And since it's so efficient, it should be quite cheap when spread out over several users. Why aren't we seeing everyone and their grandma selling us a subscription to their website?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/linkcharger"&gt; /u/linkcharger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icr6md/how_come_we_dont_see_many_people_spinning_up_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icr6md/how_come_we_dont_see_many_people_spinning_up_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icr6md/how_come_we_dont_see_many_people_spinning_up_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T10:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1icaq2z</id>
    <title>DeepSeek's AI breakthrough bypasses Nvidia's industry-standard CUDA, uses assembly-like PTX programming instead</title>
    <updated>2025-01-28T20:00:18+00:00</updated>
    <author>
      <name>/u/Slasher1738</name>
      <uri>https://old.reddit.com/user/Slasher1738</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This level of optimization is nuts but would definitely allow them to eek out more performance at a lower cost. &lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead"&gt;https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek made quite a splash in the AI industry by training its Mixture-of-Experts (MoE) language model with 671 billion parameters &lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/chinese-ai-company-says-breakthroughs-enabled-creating-a-leading-edge-ai-model-with-11x-less-compute-deepseeks-optimizations-highlight-limits-of-us-sanctions"&gt;using a cluster featuring 2,048 Nvidia H800 GPUs in about two months&lt;/a&gt;, showing 10X higher efficiency than AI industry leaders like Meta. The breakthrough was achieved by implementing tons of fine-grained optimizations and usage of assembly-like PTX (Parallel Thread Execution) programming instead of Nvidia's CUDA, according to an analysis from Mirae Asset Securities Korea cited by &lt;a href="https://x.com/Jukanlosreve/status/1883304958432624881"&gt;u/Jukanlosreve&lt;/a&gt;. &lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slasher1738"&gt; /u/Slasher1738 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icaq2z/deepseeks_ai_breakthrough_bypasses_nvidias/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T20:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ichk40</id>
    <title>So much DeepSeek fear mongering</title>
    <updated>2025-01-29T00:54:17+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichk40/so_much_deepseek_fear_mongering/"&gt; &lt;img alt="So much DeepSeek fear mongering" src="https://preview.redd.it/zfykihriztfe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9e955a87992919d24e54df67893449f4eab310d" title="So much DeepSeek fear mongering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How are so many people, who have no idea what they're talking about dominating the stage about deep seek?&lt;/p&gt; &lt;p&gt;Stuff like this. WTF &lt;a href="https://www.linkedin.com/posts/roch-mamenas-4714a979_deepseek-as-a-trojan-horse-threat-deepseek-activity-7288965743507894272-xvNq"&gt;https://www.linkedin.com/posts/roch-mamenas-4714a979_deepseek-as-a-trojan-horse-threat-deepseek-activity-7288965743507894272-xvNq&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zfykihriztfe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ichk40/so_much_deepseek_fear_mongering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ichk40/so_much_deepseek_fear_mongering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T00:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1icqzcz</id>
    <title>DeepSeek-R1 evolving a Game of Life pattern really feels like a breakthrough</title>
    <updated>2025-01-29T10:39:01+00:00</updated>
    <author>
      <name>/u/IrisColt</name>
      <uri>https://old.reddit.com/user/IrisColt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"&gt; &lt;img alt="DeepSeek-R1 evolving a Game of Life pattern really feels like a breakthrough" src="https://external-preview.redd.it/P8lS0kk6BFe2IEo6TxCZd1LVwksc34IkzGTVx_SCc8w.jpg?width=108&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d74dbe4f1d67cc8b587db9aa01762f26e269bcf" title="DeepSeek-R1 evolving a Game of Life pattern really feels like a breakthrough" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m truly amazed. I've just discovered that DeepSeek-R1 has managed to correctly compute one generation of Conway's Game of Life (starting from a simple five-cell row pattern)—a first for any LLM I've tested. While it required a significant amount of reasoning (749.31 seconds of thought), the model got it right on the first try. It felt just like using a bazooka to kill a fly (5596 tokens at 7 tk/s).&lt;/p&gt; &lt;p&gt;While this might sound modest, I’ve long viewed this challenge as the “strawberry problem” but on steroids. DeepSeek-R1 had to understand cellular automata rules, visualize a grid, track multiple cells simultaneously, and apply specific survival and birth rules to each position—all while maintaining spatial reasoning.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vup8iom0vwfe1.png?width=138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=61bcf0740f9a0b8f6bb64525ce64e293e6253fe4"&gt;Pattern at gen 0.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zgzeawc2vwfe1.png?width=138&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5886ae4cefba04201dd1a847800f0004333f3bbb"&gt;Pattern at gen 1.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Simulate one generation of Conway's Game of Life starting from the following initial configuration: ....... ....... ....... .OOOOO. ....... ....... ....... Use a 7x7 grid for the simulation. Represent alive cells with &amp;quot;O&amp;quot; and dead cells with &amp;quot;.&amp;quot;. Apply the rules of Conway's Game of Life to calculate each generation. Provide diagrams of the initial state, and first generation, in the same format as shown above.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/JTveEkXg"&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; and answer (Pastebin)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Initial state: ....... ....... ....... .OOOOO. ....... ....... .......&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;First generation: ....... ....... ..OOO.. ..OOO.. ..OOO.. ....... .......&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IrisColt"&gt; /u/IrisColt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icqzcz/deepseekr1_evolving_a_game_of_life_pattern_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T10:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1icmxb5</id>
    <title>4D Chess by the DeepSeek CEO</title>
    <updated>2025-01-29T05:40:12+00:00</updated>
    <author>
      <name>/u/HippoNut</name>
      <uri>https://old.reddit.com/user/HippoNut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Liang Wenfeng: &amp;quot;I&lt;strong&gt;n the face of disruptive technologies, moats created by closed source are temporary. Even OpenAI’s closed source approach can’t prevent others from catching up&lt;/strong&gt;. S&lt;strong&gt;o we anchor our value in our team — our colleagues grow through this process, accumulate know-how, and form an organization and culture capable of innovation. That’s our moat.&lt;/strong&gt;&amp;quot;&lt;br /&gt; Source: &lt;a href="https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas"&gt;https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HippoNut"&gt; /u/HippoNut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmxb5/4d_chess_by_the_deepseek_ceo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icmxb5/4d_chess_by_the_deepseek_ceo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icmxb5/4d_chess_by_the_deepseek_ceo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T05:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1icer8t</id>
    <title>Will Deepseek soon be banned in the US?</title>
    <updated>2025-01-28T22:48:18+00:00</updated>
    <author>
      <name>/u/bruhlmaocmonbro</name>
      <uri>https://old.reddit.com/user/bruhlmaocmonbro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"&gt; &lt;img alt="Will Deepseek soon be banned in the US?" src="https://preview.redd.it/5gpitg40dtfe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=785ab6a8af1daeae906fcf4071ac93f79583ffb0" title="Will Deepseek soon be banned in the US?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bruhlmaocmonbro"&gt; /u/bruhlmaocmonbro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5gpitg40dtfe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icer8t/will_deepseek_soon_be_banned_in_the_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-28T22:48:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1icsa5o</id>
    <title>PSA: your 7B/14B/32B/70B "R1" is NOT DeepSeek.</title>
    <updated>2025-01-29T12:06:25+00:00</updated>
    <author>
      <name>/u/Zalathustra</name>
      <uri>https://old.reddit.com/user/Zalathustra</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not even an MoE, for that matter. It's a finetune of an existing dense model (Qwen 2.5 for most, Llama 3.3 for 70B). &lt;em&gt;ONLY&lt;/em&gt; the full, 671B model is the real stuff. &lt;/p&gt; &lt;p&gt;(Making a post about this because I'm getting really tired of having to explain this under every &amp;quot;R1 on a potato&amp;quot; and &amp;quot;why is my R1 not as smart as people say&amp;quot; post separately.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zalathustra"&gt; /u/Zalathustra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsa5o/psa_your_7b14b32b70b_r1_is_not_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1icsa5o/psa_your_7b14b32b70b_r1_is_not_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1icsa5o/psa_your_7b14b32b70b_r1_is_not_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-29T12:06:25+00:00</published>
  </entry>
</feed>
