<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-21T02:15:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jfk5bs</id>
    <title>NVIDIA selling a small amount of 5080s and 5090s at MSRP at GTC</title>
    <updated>2025-03-20T08:25:25+00:00</updated>
    <author>
      <name>/u/False_Care_2957</name>
      <uri>https://old.reddit.com/user/False_Care_2957</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"&gt; &lt;img alt="NVIDIA selling a small amount of 5080s and 5090s at MSRP at GTC" src="https://external-preview.redd.it/3kT0XATxO_t_PsBk5IYdwm0rupWe9BvAFfa1PcU7N7w.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=577211de692a570366eda814cb957c6bbfa87da3" title="NVIDIA selling a small amount of 5080s and 5090s at MSRP at GTC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7p934s4g1tpe1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f77a7e471836609cac1abd6ebdea26fd3123235"&gt;https://preview.redd.it/7p934s4g1tpe1.png?width=1058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f77a7e471836609cac1abd6ebdea26fd3123235&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/NVIDIAAIDev/status/1902454685153554438"&gt;https://x.com/NVIDIAAIDev/status/1902454685153554438&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While we have to scramble get 5090s at 2-3x the price&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False_Care_2957"&gt; /u/False_Care_2957 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfk5bs/nvidia_selling_a_small_amount_of_5080s_and_5090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T08:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1jf5ufk</id>
    <title>New RTX PRO 6000 with 96G VRAM</title>
    <updated>2025-03-19T19:44:59+00:00</updated>
    <author>
      <name>/u/ThenExtension9196</name>
      <uri>https://old.reddit.com/user/ThenExtension9196</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this at nvidia GTC. Truly a beautiful card. Very similar styling as the 5090FE and even has the same cooling system. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThenExtension9196"&gt; /u/ThenExtension9196 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cost3vsw9ppe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jf5ufk/new_rtx_pro_6000_with_96g_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jf5ufk/new_rtx_pro_6000_with_96g_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-19T19:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfkv1s</id>
    <title>We should talk about Mistral Small 3.1 vs Mistral Small 3.</title>
    <updated>2025-03-20T09:21:42+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No one saying anything about the new Mistral Small 3.1, no posts about how it perform etc.&lt;/p&gt; &lt;p&gt;From my tests Mistral Small 3.1 performing about the same like original Mistral Small 3.&lt;br /&gt; Same repetitions problems, same long context problems, unstable high temperatures.&lt;br /&gt; I got even a slight worse results at some tasks, coding for example.&lt;/p&gt; &lt;p&gt;Is MS3.1 just a hack to make MS3 multi-modal?&lt;br /&gt; Should we back to MS3 for text-only work?&lt;br /&gt; How was your experience with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfkv1s/we_should_talk_about_mistral_small_31_vs_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfkv1s/we_should_talk_about_mistral_small_31_vs_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfkv1s/we_should_talk_about_mistral_small_31_vs_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T09:21:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfobyf</id>
    <title>Small Models With Good Data &gt; API Giants: ModernBERT Destroys Claude Haiku</title>
    <updated>2025-03-20T12:59:37+00:00</updated>
    <author>
      <name>/u/wanderingtraveller</name>
      <uri>https://old.reddit.com/user/wanderingtraveller</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nice little project from Marwan Zaarab where he pits a fine-tuned ModernBERT against Claude Haiku for classifying LLMOps case studies. The results are eye-opening for anyone sick of paying for API calls.&lt;/p&gt; &lt;p&gt;(Note: this is just for the specific classification task. It's not that ModernBERT replaces the generalisation of Haiku ;) )&lt;/p&gt; &lt;h1&gt;The Setup 🧩&lt;/h1&gt; &lt;p&gt;He needed to automatically sort articles - is this a real production LLM system mentioned or just theoretical BS?&lt;/p&gt; &lt;h1&gt;What He Did 📊&lt;/h1&gt; &lt;p&gt;Started with prompt engineering (which sucked for consistency), then went to fine-tuning ModernBERT on ~850 examples.&lt;/p&gt; &lt;h1&gt;The Beatdown 🚀&lt;/h1&gt; &lt;p&gt;ModernBERT absolutely wrecked Claude Haiku:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;31% better accuracy (96.7% vs 65.7%)&lt;/li&gt; &lt;li&gt;69× faster (0.093s vs 6.45s)&lt;/li&gt; &lt;li&gt;225× cheaper ($1.11 vs $249.51 per 1000 samples)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The wildest part? Their memory-optimized version used 81% less memory while only dropping 3% in F1 score.&lt;/p&gt; &lt;h1&gt;Why I'm Posting This Here 💻&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Runs great on M-series Macs&lt;/li&gt; &lt;li&gt;No more API anxiety or rate limit bs&lt;/li&gt; &lt;li&gt;Works with modest hardware&lt;/li&gt; &lt;li&gt;Proves you don't need giant models for specific tasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Yet another example of how understanding your problem domain + smaller fine-tuned model &amp;gt; throwing money at API providers for giant models.&lt;/p&gt; &lt;p&gt;📚 Blog: &lt;a href="https://www.zenml.io/blog/building-a-pipeline-for-automating-case-study-classification"&gt;https://www.zenml.io/blog/building-a-pipeline-for-automating-case-study-classification&lt;/a&gt;&lt;br /&gt; 💻 Code: &lt;a href="https://github.com/zenml-io/zenml-projects/tree/main/research-radar"&gt;https://github.com/zenml-io/zenml-projects/tree/main/research-radar&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wanderingtraveller"&gt; /u/wanderingtraveller &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfobyf/small_models_with_good_data_api_giants_modernbert/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfobyf/small_models_with_good_data_api_giants_modernbert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfobyf/small_models_with_good_data_api_giants_modernbert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:59:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfntc1</id>
    <title>A Primer on Orpheus, Sesame’s CSM-1B and Kyutai’s Moshi</title>
    <updated>2025-03-20T12:31:49+00:00</updated>
    <author>
      <name>/u/TrelisResearch</name>
      <uri>https://old.reddit.com/user/TrelisResearch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;*What is CSM-1B?*&lt;/p&gt; &lt;p&gt;CSM-1B is a a small transformer model that allows for text to be converted to speech. Uniquely it is context-aware in the sense that it can take in previous sound waves from the conversation history to inform the style of audio that is generated. It is also heavily trained on multi-turn audio conversational data (which is different than written conversations! And results in much better results for voice assistants.&lt;/p&gt; &lt;p&gt;*What is Orpheus*&lt;/p&gt; &lt;p&gt;Orpheus, like CSM-1B is transformer based TTS model. It is based on a 3B Llama model, rather than 1B for CSM-1B. Unlike CSM, the base and fine-tuned Orpheus models do not encode a speaker number (e.g. speaker 0 or 1) - although this would be possible via fine-tuning. Orpheus DOES use special tokens like &amp;lt;laugh&amp;gt; in order to get the model to make non-word sounds. This kind of fine-tuning would be possible with other models too, but not available out of the box (afaik).&lt;/p&gt; &lt;p&gt;*What is Moshi?*&lt;/p&gt; &lt;p&gt;Moshi is a transformer-based model that can take in speech and respond with speech in real time. It is capable of detecting emotion and also allowing for overlapping speakers – in principle. Moshi is primarily based on a 7B parameter model called Helium that was trained from scratch.&lt;/p&gt; &lt;p&gt;*How are these models similar?*&lt;/p&gt; &lt;p&gt;All three models handle sound as tokens. Moshi and CSM-1B make use of a converter called Mimi (developed as part of Moshi) that allows audio to be converted into tokens or tokens to be converted into audio. Orpheus makes use of the SNAC tokeniser which represents sound in a hierarchical way - essentially there are tokens providing a coarse representation and tokens providing a fine representation.&lt;/p&gt; &lt;p&gt;While Moshi is predominantly known as a model that can take in audio and provide responses as audio, in principle it is capable of doing any combinations of speech or text input and speech or text output. In other words, it can be fine tuned to operate as a text to speech model or a speech to text model or a speech to speech model.&lt;/p&gt; &lt;p&gt;CSM-1B on the other hand is uniquely designed for taking in an audio and text history along with a new portion of text that is then converted into an audio output that is consistent with the styles of speakers in the prior history. For example, if you input audio between a man and then a woman, and you then ask for the speech corresponding to new text it will be generated in the voice of a man – in line with what one would expect from the prior order of turns.&lt;/p&gt; &lt;p&gt;Orpheus can also take in a text and audio history, to allow for voice cloning, but is not specifically fine-tuned for taking in a conversation history with alternating turns.&lt;/p&gt; &lt;p&gt;*Isn't sound continuous? How do you represent it as tokens?*&lt;/p&gt; &lt;p&gt;By its nature, text is discrete rather than continuous because it consists of letters. By contrast, sound is continuous in nature. It is nonetheless possible to represent a sound wave as a series of tokens, provided one defines the sound with a stream of tokens at sufficiently high frequency – 12.5 Hz in the case of Mimi – and provided one uses a sufficient number of tokens to represent the sound at each time stamp.&lt;/p&gt; &lt;p&gt;Sound is best represented by a hierarchy of different sets of tokens. Very loosely, you can think of a sound being described like searching in a library… first, you find the right shelf, then you go to the shelf and you find the closest book, then you find the closest page.&lt;/p&gt; &lt;p&gt;Moshi uses a Mimi-type encoder-decoder with eight levels of hierarchy at a given timestamp, with one for semantic information and seven to represent acoustic information. CSM-1B uses Mimi too, but with 32 levels of hierarchy, which cover semantics and acoustics (there is no separation). Orpheus uses SNAC, which creates tokens at four levels of hierarchy (the initial sound is downsampled to give coarse tokens, then downsampled again to give finer tokens, then again, then again). (I’m being loose here in describing Mimi versus SNAC. Mimi uses multiple codebooks (think different tokenisers for each level of hierarchy), while SNAC uses one codebook but tokens are created for each level of downsampling.)&lt;/p&gt; &lt;p&gt;*Why tokens?*&lt;/p&gt; &lt;p&gt;If you can treat sound as tokens, then you can use transformers to auto-regressively produce sound. And we know transformers work well for LLMs. And if we can use transformers, then we can stream sound continuously (rather than having to wait for chunks).&lt;/p&gt; &lt;p&gt;*What’s the problem with using tokens for sound?*&lt;/p&gt; &lt;p&gt;In a hierarchical approach to tokenising (needed for good quality), you have multiple tokens per timestamp. If you sample at 12.5 Hz and have eight layers of hierarchy (8 codebooks), then you need to generate 100 tokens per second. That means you need to generate tokens very fast to keep up with voice!&lt;/p&gt; &lt;p&gt;There are a few ways around this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use smaller levels of hierarchy and a fast model, e.g. Orpheus with 4 hierarchy layers (from SNAC) and a 3B model OR CSM-1B with 32 codebooks but a 1B backbone transformer.&lt;/li&gt; &lt;li&gt;Use hierarchical transformers (yes, an additional/different form of hierarchy) whereby you use a main transformer to decode a first coarse token, and then a smaller transformer (100M params) to decode the other tokens at that time step (i.e. the other 31 tokens in the case of CSM-1B). Moshi does a variant of this whereby the main transformer decodes one big vector for that timestep, and the tokens are then decoded from another transformer that takes that vector/embedding as an input.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Side-note: It’s interesting that Kyutai trained Helium 7B from scratch rather than start with an off-the-shelf model. LLMs have gotten better since Helium’s training was started, which has made it possible to use 1B and 3B models as backbones, like CSM and Orpheus have done. Actually Kyutai have released a 2B version of Helium, supporting this line of argument.&lt;/p&gt; &lt;p&gt;*How are these voice models different from approaches like Style TTS2*&lt;/p&gt; &lt;p&gt;Another way to create sound from text is to use diffusion (e.g. what stable diffusion does for images, same as what DALL-E does). This is how StyleTTS2 works, and it works well, although it is not auto-regressive, I.e. it generates whole phrases rather than autoregressively generating the next part of the phrase. This makes it less adaptive to interruptions or changes in speech that need to happen in response at short notice.&lt;/p&gt; &lt;p&gt;*How is this different from adapter approaches like Llama 3.2 audio (not released) or Qwen Audio*&lt;/p&gt; &lt;p&gt;These two models allow for audio and text input, but they do so by converting audio into an embedding vector that is then adapted (via MLP layers) to be compatible with the input of an LLM (like Llama 3.1 8B). The sound is not (explicitly) encoded hierarchically and the sound is not tokenized. However, passing in an embedded representation does work well as an input BUT there is no easy symmetric way to output sound. By contrast, if one works with sound as tokens, it is possible to input sound (and text) tokens, and output sound (and text) tokens.&lt;/p&gt; &lt;p&gt;*Where from here?*&lt;/p&gt; &lt;p&gt;Right now we have these small (and fast) speech models that - with greater amounts of data - should be able to provide more natural conversations than is possible by cobbling together a transcription model with a text model and then a text to speech model.&lt;/p&gt; &lt;p&gt;However, these models will still lag in terms of reasoning, simply because their transformers are not large enough - and it still appears that models of at least 27B (like Gemma 3) or 24B (like Mistral Small) are needed to get strong reasoning (and even bigger for the best reasoning). Those model sizes would result in generation speeds that are too slow for real time voice. This is why many current applications of voice use the cobbled-together approach of putting multiple models together (TTS, LLM, STT) - even if this means you need to manage how these models AND voice activation and turn detection all mesh together. To be clear, with a unified model like Moshi, there is no need to separately handle voice detection or turn detection - everything is handled by the unified model, including noise cancellation!&lt;/p&gt; &lt;p&gt;In one sense, what has enabled Moshi and CSM-1B and Orpheus, is that tiny models have gotten really strong (like llama 1b) so you can have a good backbone that is still fast. Possibly, if you take the tricks from CSM and from Orpheus and from Moshi, combined - you can maybe move towards a 7B model, or maybe larger, that still is fast enough.&lt;/p&gt; &lt;p&gt;But for now, until new tricks are found (which they will) the unified models are weaker than pure text models on reasoning. The holy grail might be to have a model that uses tokens for text, sound and for images - then you can train end-to-end on all of those forms of data, and potentially get the strongest possible model.&lt;/p&gt; &lt;p&gt;— THE END. I’ll also put out a video soon (Trelis Research on YouTube and Substack) on these models, including cloning and fine-tuning. --&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TrelisResearch"&gt; /u/TrelisResearch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfntc1/a_primer_on_orpheus_sesames_csm1b_and_kyutais/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfntc1/a_primer_on_orpheus_sesames_csm1b_and_kyutais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfntc1/a_primer_on_orpheus_sesames_csm1b_and_kyutais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfglbu</id>
    <title>Orpheus TTS Local (LM Studio)</title>
    <updated>2025-03-20T04:09:42+00:00</updated>
    <author>
      <name>/u/Internal_Brain8420</name>
      <uri>https://old.reddit.com/user/Internal_Brain8420</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfglbu/orpheus_tts_local_lm_studio/"&gt; &lt;img alt="Orpheus TTS Local (LM Studio)" src="https://external-preview.redd.it/123zU4tSEAhJBQw-5zzDV7N-1QXm63u6nWHqCb7Eodw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fd48ff5928b7af59f7d95c0c187069ccc64014c" title="Orpheus TTS Local (LM Studio)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Internal_Brain8420"&gt; /u/Internal_Brain8420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/isaiahbjork/orpheus-tts-local"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfglbu/orpheus_tts_local_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfglbu/orpheus_tts_local_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T04:09:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg5sbj</id>
    <title>Mistral-small 3.1 Vision for PDF RAG tested</title>
    <updated>2025-03-21T01:48:26+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. As promised from my previous post, Mistral 3.1 small vision tested.&lt;/p&gt; &lt;p&gt;TLDR - particularly noteworthy is that mistral-small 3.1 didn't just beat GPT-4o mini - it also outperformed both Pixtral 12B and Pixtral Large models. Also, this is a particularly hard test. only 2 models to score 100% are Sonnet 3.7 reasoning and O1 reasoning. We ask trick questions like things that are not in the image, ask it to respond in different languages and many other things that push the boundaries. Mistral-small 3.1 is the only open source model to score above 80% on this test.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ppGGEh1zEuU"&gt;https://www.youtube.com/watch?v=ppGGEh1zEuU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg5sbj/mistralsmall_31_vision_for_pdf_rag_tested/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg5sbj/mistralsmall_31_vision_for_pdf_rag_tested/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg5sbj/mistralsmall_31_vision_for_pdf_rag_tested/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T01:48:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfmpas</id>
    <title>Why whisper v3 turbo has not been replaced?</title>
    <updated>2025-03-20T11:28:45+00:00</updated>
    <author>
      <name>/u/Bakedsoda</name>
      <uri>https://old.reddit.com/user/Bakedsoda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the absolute frenzy in the TTS open source release from Kokoro , Zonos and now Oprheus. &lt;/p&gt; &lt;p&gt;I assume we should be getting some next gen STT open source models soon.&lt;/p&gt; &lt;p&gt;Even at v3 turbo quality but smaller size that can run on edge in real time would be amazing!!!&lt;/p&gt; &lt;p&gt;Anyone working on anything like that ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bakedsoda"&gt; /u/Bakedsoda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfmpas/why_whisper_v3_turbo_has_not_been_replaced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfmpas/why_whisper_v3_turbo_has_not_been_replaced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfmpas/why_whisper_v3_turbo_has_not_been_replaced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T11:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg13y8</id>
    <title>Audiobook Creator - Releasing Version 3</title>
    <updated>2025-03-20T22:07:47+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg13y8/audiobook_creator_releasing_version_3/"&gt; &lt;img alt="Audiobook Creator - Releasing Version 3" src="https://external-preview.redd.it/wbzh3gD7tu7iplbig6LZZu8kXV1sOGBEFJ4803_qLuM.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=375d296e8b3b7cbdf7424b3bf4e3372588245a7c" title="Audiobook Creator - Releasing Version 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Followup to my previous post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1iqynut/audiobook_creator_releasing_version_2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm releasing a version 3 of my open source project with amazing new features !&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Added Key Feature&lt;/strong&gt;s:&lt;/p&gt; &lt;p&gt;✅ Now has an intuitive easy to use Gradio UI. No more headache of running scripts.&lt;/p&gt; &lt;p&gt;✅ Added support for running the app through docker. No more hassle setting it up.&lt;/p&gt; &lt;p&gt;Checkout the demo video on Youtube: &lt;a href="https://www.youtube.com/watch?v=E5lUQoBjquo"&gt;https://www.youtube.com/watch?v=E5lUQoBjquo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Repo Link: &lt;a href="https://github.com/prakharsr/audiobook-creator/"&gt;https://github.com/prakharsr/audiobook-creator/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout sample multi voice audio for a short story : &lt;a href="https://audio.com/prakhar-sharma/audio/generated-sample-multi-voice-audiobook"&gt;https://audio.com/prakhar-sharma/audio/generated-sample-multi-voice-audiobook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try out the sample M4B audiobook with cover, chapter timestamps and metadata: &lt;a href="https://github.com/prakharsr/audiobook-creator/blob/main/sample_book_and_audio/sample_multi_voice_audiobook.m4b"&gt;https://github.com/prakharsr/audiobook-creator/blob/main/sample_book_and_audio/sample_multi_voice_audiobook.m4b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More new features coming soon !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg13y8/audiobook_creator_releasing_version_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg13y8/audiobook_creator_releasing_version_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg13y8/audiobook_creator_releasing_version_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T22:07:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfvsph</id>
    <title>New AI-Assistant Framework</title>
    <updated>2025-03-20T18:27:12+00:00</updated>
    <author>
      <name>/u/Darkboy5000</name>
      <uri>https://old.reddit.com/user/Darkboy5000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After six months of development, I'm excited to release Nova 2, a comprehensive Python framework that makes building AI assistants simple.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is Nova?&lt;/strong&gt; Nova combines multiple AI technologies (LLMs, Text-to-Speech, voice recognition, memory systems) into one cohesive, easy-to-use interface. Build a complete AI assistant pipeline in just a few lines of code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM integration with multiple inference engines&lt;/li&gt; &lt;li&gt;Text-to-Speech with voice cloning capabilities&lt;/li&gt; &lt;li&gt;Voice recognition with speaker identification&lt;/li&gt; &lt;li&gt;Long-term memory using retrieval-augmented generation&lt;/li&gt; &lt;li&gt;Modular tool system for custom actions&lt;/li&gt; &lt;li&gt;Simple, consistent API across all components&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Whether you want to build a complete AI assistant, an autonomous agent, or just chat with an LLM, Nova provides the building blocks without the complexity.&lt;/p&gt; &lt;p&gt;The entire project is open-source (GPL-3.0). I'd love to hear your feedback and see what you build with it!&lt;/p&gt; &lt;p&gt;Repo:&lt;br /&gt; &lt;a href="https://github.com/00Julian00/Nova2"&gt;https://github.com/00Julian00/Nova2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Darkboy5000"&gt; /u/Darkboy5000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfvsph/new_aiassistant_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfvsph/new_aiassistant_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfvsph/new_aiassistant_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T18:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfpnrz</id>
    <title>Moores law for AI agents</title>
    <updated>2025-03-20T14:04:36+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfpnrz/moores_law_for_ai_agents/"&gt; &lt;img alt="Moores law for AI agents" src="https://preview.redd.it/2zht3052qupe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d0afdbeec782b10a101dc05f16c39e266a2ce1f" title="Moores law for AI agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2zht3052qupe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfpnrz/moores_law_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfpnrz/moores_law_for_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T14:04:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg2ywz</id>
    <title>DGX Spark Session</title>
    <updated>2025-03-20T23:29:51+00:00</updated>
    <author>
      <name>/u/mapestree</name>
      <uri>https://old.reddit.com/user/mapestree</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg2ywz/dgx_spark_session/"&gt; &lt;img alt="DGX Spark Session" src="https://external-preview.redd.it/zXJKCfzK0Bmroj8EIe-yWlKoUHidM18nX6_A1LAVva0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=521486b418be75ba2f501c1f24905fa7df810395" title="DGX Spark Session" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mapestree"&gt; /u/mapestree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/HIozRbs.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg2ywz/dgx_spark_session/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg2ywz/dgx_spark_session/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T23:29:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfm23c</id>
    <title>TikZero - New Approach for Generating Scientific Figures from Text Captions with LLMs</title>
    <updated>2025-03-20T10:47:09+00:00</updated>
    <author>
      <name>/u/DrCracket</name>
      <uri>https://old.reddit.com/user/DrCracket</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfm23c/tikzero_new_approach_for_generating_scientific/"&gt; &lt;img alt="TikZero - New Approach for Generating Scientific Figures from Text Captions with LLMs" src="https://preview.redd.it/carfu383qtpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1dbc3c3ec140b5c9532f78408e18185bd09d368" title="TikZero - New Approach for Generating Scientific Figures from Text Captions with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrCracket"&gt; /u/DrCracket &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/carfu383qtpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfm23c/tikzero_new_approach_for_generating_scientific/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfm23c/tikzero_new_approach_for_generating_scientific/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T10:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfh1d7</id>
    <title>LLMs are 800x Cheaper for Translation than DeepL</title>
    <updated>2025-03-20T04:37:11+00:00</updated>
    <author>
      <name>/u/Ninjinka</name>
      <uri>https://old.reddit.com/user/Ninjinka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When looking at the cost of translation APIs, I was floored by the prices. Azure is $10 per million characters, Google is $20, and DeepL is $25.&lt;/p&gt; &lt;p&gt;To come up with a rough estimate for a real-time translation use case, I assumed 150 WPM speaking speed, with each word being translated 3 times (since the text gets retranslated multiple times as the context lengthens). This resulted in the following costs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Azure: $1.62/hr&lt;/li&gt; &lt;li&gt;Google: $3.24/hr&lt;/li&gt; &lt;li&gt;DeepL: $4.05/hr&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Assuming the same numbers, &lt;code&gt;gemini-2.0-flash-lite&lt;/code&gt; would cost &lt;strong&gt;less than $0.01/hr&lt;/strong&gt;. Cost varies based on prompt length, but I'm actually getting just under $0.005/hr.&lt;/p&gt; &lt;p&gt;That's over 800x cheaper than DeepL, or 0.1% of the cost.&lt;/p&gt; &lt;p&gt;Presumably the quality of the translations would be somewhat worse, but how much worse? And how long will that disadvantage last? I can stomach a certain amount of worse for 99% cheaper, and it seems easy to foresee that LLMs will surpass the quality of the legacy translation models in the near future.&lt;/p&gt; &lt;p&gt;Right now the accuracy depends a lot on the prompting. I need to run a lot more evals, but so far in my tests I'm seeing that the translations I'm getting are as good (most of the time identical) or &lt;em&gt;better&lt;/em&gt; than Google's the vast majority of the time. I'm confident I can get to 90% of Google's accuracy with better prompting.&lt;/p&gt; &lt;p&gt;I can live with 90% accuracy with a 99.9% cost reduction.&lt;/p&gt; &lt;p&gt;For many, 90% doesn't cut it for their translation needs and they are willing to pay a premium for the best. But the high costs of legacy translation APIs will become increasingly indefensible as LLM-based solutions improve, and we'll see translation incorporated in ways that were previously cost-prohibitive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ninjinka"&gt; /u/Ninjinka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfh1d7/llms_are_800x_cheaper_for_translation_than_deepl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T04:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg4ij5</id>
    <title>NEW MODEL: Reasoning Reka-Flash 3 21B (uncensored) - AUGMENTED.</title>
    <updated>2025-03-21T00:43:28+00:00</updated>
    <author>
      <name>/u/Dangerous_Fix_5526</name>
      <uri>https://old.reddit.com/user/Dangerous_Fix_5526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;From DavidAU;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This model has been augmented, and uses the NEO Imatrix dataset. Testing has shown a decrease in reasoning tokens up to 50%.&lt;/p&gt; &lt;p&gt;This model is also uncensored. (YES! - from the &amp;quot;factory&amp;quot;).&lt;/p&gt; &lt;p&gt;In &amp;quot;head to head&amp;quot; testing this model reasoning more smoothly, rarely gets &amp;quot;lost in the woods&amp;quot; and has stronger output.&lt;/p&gt; &lt;p&gt;And even the LOWEST quants it performs very strongly... with IQ2_S being usable for reasoning.&lt;/p&gt; &lt;p&gt;Lastly:&lt;/p&gt; &lt;p&gt;This model is reasoning/temp stable. Meaning you can crank the temp, and the reasoning is sound too.&lt;/p&gt; &lt;p&gt;7 Examples generation at repo, detailed instructions, additional system prompts to augment generation further and full quant repo here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Reka-Flash-3-21B-Reasoning-Uncensored-MAX-NEO-Imatrix-GGUF"&gt;https://huggingface.co/DavidAU/Reka-Flash-3-21B-Reasoning-Uncensored-MAX-NEO-Imatrix-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech NOTE:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This was a test case to see what augment(s) used during quantization would improve a reasoning model along with a number of different Imatrix datasets and augment options.&lt;/p&gt; &lt;p&gt;I am still investigate/testing different options at this time to apply not only to this model, but other reasoning models too in terms of Imatrix dataset construction, content, and generation and augment options.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For 37 more &amp;quot;reasoning/thinking models&amp;quot; go here: (all types,sizes, archs)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/DavidAU/d-au-thinking-reasoning-models-reg-and-moes-67a41ec81d9df996fd1cdd60"&gt;https://huggingface.co/collections/DavidAU/d-au-thinking-reasoning-models-reg-and-moes-67a41ec81d9df996fd1cdd60&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Service Note - Mistral Small 3.1 - 24B, &amp;quot;Creative&amp;quot; issues:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For those that found/find the new Mistral model somewhat flat (creatively) I have posted a System prompt here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Mistral-Small-3.1-24B-Instruct-2503-MAX-NEO-Imatrix-GGUF"&gt;https://huggingface.co/DavidAU/Mistral-Small-3.1-24B-Instruct-2503-MAX-NEO-Imatrix-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(option #3) to improve it - it can be used with normal / augmented - it performs the same function.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Fix_5526"&gt; /u/Dangerous_Fix_5526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg4ij5/new_model_reasoning_rekaflash_3_21b_uncensored/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg4ij5/new_model_reasoning_rekaflash_3_21b_uncensored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg4ij5/new_model_reasoning_rekaflash_3_21b_uncensored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-21T00:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg1mn1</id>
    <title>phi3-uncensored-chat..small but mighty</title>
    <updated>2025-03-20T22:30:01+00:00</updated>
    <author>
      <name>/u/redwat3r</name>
      <uri>https://old.reddit.com/user/redwat3r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Our firm, luvgpt, just released a new open source chat model. Its free to use on huggingface: &lt;a href="https://huggingface.co/luvGPT/phi3-uncensored-chat"&gt;https://huggingface.co/luvGPT/phi3-uncensored-chat&lt;/a&gt; &lt;/p&gt; &lt;p&gt;It's a model fine tuned on generated chat data, and curated from a judge model. Our AI research team is very interested in distillation and transfer learning (check out our deepseek uncensored model as well), and this one is surprisingly good at chatting, for its size, of course&lt;/p&gt; &lt;p&gt;It's small enough to run on a CPU (4bit, however results are going to be worse at this size). It can run in high precision on any modern GPU, basically. Best results of course are going to be 14GB VRAM. &lt;/p&gt; &lt;p&gt;Don't expect performance to match something like the mega models on the market, but it is a pretty neat little tool to play around with. Keep in mind it is very sensitive to prompt templates; we provide some example inference code for Python people&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redwat3r"&gt; /u/redwat3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg1mn1/phi3uncensoredchatsmall_but_mighty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg1mn1/phi3uncensoredchatsmall_but_mighty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg1mn1/phi3uncensoredchatsmall_but_mighty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T22:30:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfwxnn</id>
    <title>5 things I learned from running DeepEval</title>
    <updated>2025-03-20T19:14:08+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past year, I’ve been one of the maintainers at &lt;a href="https://github.com/confident-ai/deepeval"&gt;DeepEval&lt;/a&gt;, an open-source LLM eval package for python.&lt;/p&gt; &lt;p&gt;Over a year ago, DeepEval started as a collection of traditional NLP methods (like BLEU score) and fine-tuned transformer models, but thanks to community feedback and contributions, it has evolved into a more powerful and robust suite of LLM-powered metrics.&lt;/p&gt; &lt;p&gt;Right now, DeepEval is running around 600,000 evaluations daily. Given this, I wanted to share some key insights I’ve gained from user feedback and interactions with the LLM community!&lt;/p&gt; &lt;h1&gt;1. Custom Metrics BY FAR most popular&lt;/h1&gt; &lt;p&gt;DeepEval’s &lt;a href="https://docs.confident-ai.com/docs/metrics-llm-evals"&gt;G-Eval&lt;/a&gt; was used 3x more than the second most popular metric, Answer Relevancy. G-Eval is a custom metric framework that helps you easily define reliable, robust metrics with custom evaluation criteria.&lt;/p&gt; &lt;p&gt;While DeepEval offers standard metrics like &lt;a href="https://docs.confident-ai.com/docs/metrics-answer-relevancy"&gt;relevancy&lt;/a&gt; and &lt;a href="https://docs.confident-ai.com/docs/metrics-faithfulness"&gt;faithfulness&lt;/a&gt;, these alone don’t always capture the specific evaluation criteria needed for niche use cases. For example, how concise a chatbot is or how jargony a legal AI might be. For these use cases, using custom metrics is much more effective and direct.&lt;/p&gt; &lt;p&gt;Even for common metrics like relevancy or faithfulness, users often have highly specific requirements. A few have even used G-Eval to create their &lt;a href="https://docs.confident-ai.com/docs/metrics-dag"&gt;own custom RAG metrics &lt;/a&gt;tailored to their needs.&lt;/p&gt; &lt;h1&gt;2. Fine-Tuning LLM Judges: Not Worth It (Most of the Time)&lt;/h1&gt; &lt;p&gt;Fine-tuning LLM judges for domain-specific metrics can be helpful, but most of the time, it’s a lot of bang for not a lot of buck. If you’re noticing significant bias in your metric, simply &lt;a href="https://docs.confident-ai.com/docs/metrics-answer-relevancy#example"&gt;injecting a few well-chosen examples into the prompt&lt;/a&gt; will usually do the trick.&lt;/p&gt; &lt;p&gt;Any remaining tweaks can be handled at the prompt level, and fine-tuning will only give you incremental improvements—at a much higher cost. In my experience, it’s usually not worth the effort, though I’m sure others might have had success with it.&lt;/p&gt; &lt;h1&gt;3. Models Matter: Rise of DeepSeek&lt;/h1&gt; &lt;p&gt;DeepEval is model-agnostic, so you can use any LLM provider to power your metrics. This makes the package flexible, but it also means that if you're using smaller, less powerful models, the accuracy of your metrics may suffer.&lt;/p&gt; &lt;p&gt;Before DeepSeek, most people relied on &lt;a href="https://docs.confident-ai.com/docs/metrics-introduction#using-openai"&gt;GPT-4o for evaluation&lt;/a&gt;—it’s still one of the best LLMs for metrics, providing consistent and reliable results, far outperforming GPT-3.5.&lt;/p&gt; &lt;p&gt;However, since DeepSeek's release, we've seen a shift. More users are now hosting &lt;a href="https://docs.confident-ai.com/docs/metrics-introduction#using-ollama"&gt;DeepSeek LLMs locally through Ollama&lt;/a&gt;, effectively running their own models. But be warned—this can be much slower if you don’t have the hardware and infrastructure to support it.&lt;/p&gt; &lt;h1&gt;4. Evaluation Dataset &amp;gt;&amp;gt;&amp;gt;&amp;gt; Vibe Coding&lt;/h1&gt; &lt;p&gt;A lot of users of DeepEval start off with a few test cases and no datasets—a practice you might know as “Vibe Coding.”&lt;/p&gt; &lt;p&gt;The problem with vibe coding (or vibe evaluating) is that when you make a change to your LLM application—whether it's your model or prompt template—you might see improvements in the things you’re testing. However, the things you haven’t tested could experience regressions in performance due to your changes. So you'll see these users just build a dataset later on anyways.&lt;/p&gt; &lt;p&gt;That’s why it’s crucial to have a dataset from the start. This ensures your development is focused on the right things, actually working, and prevents wasted time on vibe coding. Since a lot of people have been asking, DeepEval has a &lt;a href="https://docs.confident-ai.com/docs/synthesizer-introduction"&gt;synthesizer to help you build an initial dataset&lt;/a&gt;, which you can then edit as needed.&lt;/p&gt; &lt;h1&gt;5. Generator First, Retriever Second&lt;/h1&gt; &lt;p&gt;The second and third most-used metrics are Answer Relevancy and Faithfulness, followed by Contextual Precision, Contextual Recall, and Contextual Relevancy.&lt;/p&gt; &lt;p&gt;Answer Relevancy and Faithfulness are directly influenced by the prompt template and model, while the contextual metrics are more affected by retriever hyperparameters like top-K. If you’re working on RAG evaluation, &lt;a href="https://docs.confident-ai.com/guides/guides-rag-evaluation"&gt;here’s a detailed guide for a deeper dive&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This suggests that people are seeing more impact from improving their generator (LLM generation) rather than fine-tuning their retriever.&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;These are just a few of the insights we hear every day and use to keep improving DeepEval. If you have any takeaways from building your eval pipeline, feel free to share them below—always curious to learn how others approach it. We’d also really appreciate any feedback on DeepEval. Dropping the repo link below!&lt;/p&gt; &lt;p&gt;DeepEval: &lt;a href="https://github.com/confident-ai/deepeval"&gt;https://github.com/confident-ai/deepeval&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfwxnn/5_things_i_learned_from_running_deepeval/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfwxnn/5_things_i_learned_from_running_deepeval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfwxnn/5_things_i_learned_from_running_deepeval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T19:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfrwqw</id>
    <title>New sampling method that boosts reasoning performance and can be applied to any existing model</title>
    <updated>2025-03-20T15:45:20+00:00</updated>
    <author>
      <name>/u/Timotheeee1</name>
      <uri>https://old.reddit.com/user/Timotheeee1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timotheeee1"&gt; /u/Timotheeee1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2503.13288"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfrwqw/new_sampling_method_that_boosts_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfrwqw/new_sampling_method_that_boosts_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T15:45:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfz88z</id>
    <title>OpenAI teases to open-source model(s) soon</title>
    <updated>2025-03-20T20:48:40+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfz88z/openai_teases_to_opensource_models_soon/"&gt; &lt;img alt="OpenAI teases to open-source model(s) soon" src="https://preview.redd.it/6xwby8q6qwpe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05ec3709187c03c115e4516121d53e43e8479603" title="OpenAI teases to open-source model(s) soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;X post: &lt;a href="https://x.com/reach_vb/status/1902719225782792570?s=46"&gt;https://x.com/reach_vb/status/1902719225782792570?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6xwby8q6qwpe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfz88z/openai_teases_to_opensource_models_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfz88z/openai_teases_to_opensource_models_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T20:48:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfryg8</id>
    <title>Public Goods Game Benchmark: Contribute and Punish, a Multi-Agent Benchmark</title>
    <updated>2025-03-20T15:47:19+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfryg8/public_goods_game_benchmark_contribute_and_punish/"&gt; &lt;img alt="Public Goods Game Benchmark: Contribute and Punish, a Multi-Agent Benchmark" src="https://external-preview.redd.it/d3d0aWhxczU3dnBlMRY5HT3ArdspEqroZOhS4xjxo9gET_rEoDHJZjJ0dzil.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c71f3f0305964915755d7301f0b0a1e047418ec2" title="Public Goods Game Benchmark: Contribute and Punish, a Multi-Agent Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lechmazur/pgg_bench"&gt;GitHub with full logs and much more data&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/11iapss57vpe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfryg8/public_goods_game_benchmark_contribute_and_punish/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfryg8/public_goods_game_benchmark_contribute_and_punish/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T15:47:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg2xi1</id>
    <title>Switching back to llamacpp (from vllm)</title>
    <updated>2025-03-20T23:27:59+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was initially using llamacpp but switched to vllm as I need the &amp;quot;high-throughput&amp;quot; especially with parallel requests (metadata enrichment for my rag and only text models), but some points are pushing me to switch back to lcp:&lt;/p&gt; &lt;p&gt;- for new models (gemma 3 or mistral 3.1), getting the awq/gptq quants may take some time whereas llamacpp team is so reactive to support new models&lt;/p&gt; &lt;p&gt;- llamacpp throughput is now quite impressive and not so far from vllm for my usecase and GPUs (3090)!&lt;/p&gt; &lt;p&gt;- gguf take less VRAM than awq or gptq models&lt;/p&gt; &lt;p&gt;- once the models have been loaded, the time to reload in memory is very short&lt;/p&gt; &lt;p&gt;What are your experiences?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg2xi1/switching_back_to_llamacpp_from_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg2xi1/switching_back_to_llamacpp_from_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg2xi1/switching_back_to_llamacpp_from_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T23:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfw3s9</id>
    <title>New Hugging Face and Unsloth guide on GRPO with Gemma 3</title>
    <updated>2025-03-20T18:40:05+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfw3s9/new_hugging_face_and_unsloth_guide_on_grpo_with/"&gt; &lt;img alt="New Hugging Face and Unsloth guide on GRPO with Gemma 3" src="https://preview.redd.it/ewr7fr183wpe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f65f73fb39bd2b2c8a1075327ec39c9300440920" title="New Hugging Face and Unsloth guide on GRPO with Gemma 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewr7fr183wpe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfw3s9/new_hugging_face_and_unsloth_guide_on_grpo_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfw3s9/new_hugging_face_and_unsloth_guide_on_grpo_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T18:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfyqye</id>
    <title>Sesame CSM Gradio UI – Free, Local, High-Quality Text-to-Speech with Voice Cloning! (CUDA, Apple MLX and CPU)</title>
    <updated>2025-03-20T20:28:48+00:00</updated>
    <author>
      <name>/u/akashjss</name>
      <uri>https://old.reddit.com/user/akashjss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I just released &lt;strong&gt;Sesame CSM&lt;/strong&gt;, a &lt;strong&gt;100% local, free&lt;/strong&gt; text-to-speech tool with &lt;strong&gt;superior voice cloning&lt;/strong&gt;! No cloud processing, no API keys – just &lt;strong&gt;pure, high-quality AI-generated speech on your own machine&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🔥 Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Runs 100% locally&lt;/strong&gt; – No internet required!&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Free &amp;amp; Open Source&lt;/strong&gt; – No paywalls, no subscriptions.&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Superior Voice Cloning&lt;/strong&gt; – Built right into the UI!&lt;/p&gt; &lt;p&gt;✅ G&lt;strong&gt;radio UI&lt;/strong&gt; – A sleek interface for easy playback &amp;amp; control.&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Supports CUDA, MLX, and CPU&lt;/strong&gt; – Works on NVIDIA, Apple Silicon, and regular CPUs.&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;Check it out on GitHub:&lt;/strong&gt; &lt;a href="https://github.com/akashjss/sesame-csm"&gt;Sesame CSM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts! Let me know if you try it out. Feedback &amp;amp; contributions are always welcome! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akashjss"&gt; /u/akashjss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfyqye/sesame_csm_gradio_ui_free_local_highquality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfyqye/sesame_csm_gradio_ui_free_local_highquality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfyqye/sesame_csm_gradio_ui_free_local_highquality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T20:28:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jfnw9x</id>
    <title>Sharing my build: Budget 64 GB VRAM GPU Server under $700 USD</title>
    <updated>2025-03-20T12:36:16+00:00</updated>
    <author>
      <name>/u/Hyungsun</name>
      <uri>https://old.reddit.com/user/Hyungsun</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnw9x/sharing_my_build_budget_64_gb_vram_gpu_server/"&gt; &lt;img alt="Sharing my build: Budget 64 GB VRAM GPU Server under $700 USD" src="https://b.thumbs.redditmedia.com/weO7zUFK46BYVtLrB9NR6mPsIETCehibf3c566iKGHI.jpg" title="Sharing my build: Budget 64 GB VRAM GPU Server under $700 USD" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hyungsun"&gt; /u/Hyungsun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jfnw9x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnw9x/sharing_my_build_budget_64_gb_vram_gpu_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jfnw9x/sharing_my_build_budget_64_gb_vram_gpu_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T12:36:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1jg0exn</id>
    <title>Intel's Former CEO Calls Out NVIDIA: 'AI GPUs 10,000x Too Expensive'—Says Jensen Got Lucky and Inferencing Needs a Reality Check</title>
    <updated>2025-03-20T21:38:25+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg0exn/intels_former_ceo_calls_out_nvidia_ai_gpus_10000x/"&gt; &lt;img alt="Intel's Former CEO Calls Out NVIDIA: 'AI GPUs 10,000x Too Expensive'—Says Jensen Got Lucky and Inferencing Needs a Reality Check" src="https://external-preview.redd.it/KOwl-jl-bbq-ggDpuf4_ihRqXydkagmZwZ9bDY3co3c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0adc6319f752a232be3be30ea4365da6e1a21d20" title="Intel's Former CEO Calls Out NVIDIA: 'AI GPUs 10,000x Too Expensive'—Says Jensen Got Lucky and Inferencing Needs a Reality Check" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick Breakdown (for those who don't want to read the full thing):&lt;/p&gt; &lt;p&gt;Intel’s former CEO, Pat Gelsinger, openly criticized NVIDIA, saying their AI GPUs are massively overpriced (he specifically said they're &amp;quot;10,000 times&amp;quot; too expensive) for AI inferencing tasks.&lt;/p&gt; &lt;p&gt;Gelsinger praised NVIDIA CEO Jensen Huang's early foresight and perseverance but bluntly stated Jensen &amp;quot;got lucky&amp;quot; with AI blowing up when it did.&lt;/p&gt; &lt;p&gt;His main argument: NVIDIA GPUs are optimized for AI training, but they're totally overkill for inferencing workloads—which don't require the insanely expensive hardware NVIDIA pushes.&lt;/p&gt; &lt;p&gt;Intel itself, though, hasn't delivered on its promise to challenge NVIDIA. They've struggled to launch competitive GPUs (Falcon Shores got canned, Gaudi has underperformed, and Jaguar Shores is still just a future promise).&lt;/p&gt; &lt;p&gt;Gelsinger thinks the next big wave after AI could be quantum computing, potentially hitting the market late this decade.&lt;/p&gt; &lt;p&gt;TL;DR: Even Intel’s former CEO thinks NVIDIA is price-gouging AI inferencing hardware—but admits Intel hasn't stepped up enough yet. CUDA dominance and lack of competition are keeping NVIDIA comfortable, while many of us just want affordable VRAM-packed alternatives.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/intel-former-ceo-claims-nvidia-ai-gpus-are-10000-times-more-expensive-than-what-is-needed-for-ai-inferencing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jg0exn/intels_former_ceo_calls_out_nvidia_ai_gpus_10000x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jg0exn/intels_former_ceo_calls_out_nvidia_ai_gpus_10000x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-20T21:38:25+00:00</published>
  </entry>
</feed>
