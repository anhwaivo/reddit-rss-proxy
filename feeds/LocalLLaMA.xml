<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-27T10:07:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1llb0et</id>
    <title>I‚Äôve been fine tuning a small llm 500m parameter on my MacBook !!!</title>
    <updated>2025-06-26T20:38:58+00:00</updated>
    <author>
      <name>/u/Ok-Math-5601</name>
      <uri>https://old.reddit.com/user/Ok-Math-5601</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb0et/ive_been_fine_tuning_a_small_llm_500m_parameter/"&gt; &lt;img alt="I‚Äôve been fine tuning a small llm 500m parameter on my MacBook !!!" src="https://preview.redd.it/tfvnaqas1c9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=795da79f752edd8faf7a034bc3cbd479e8315cc5" title="I‚Äôve been fine tuning a small llm 500m parameter on my MacBook !!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs for a STT &amp;amp; TTS engine that I‚Äôm trying to build, but can‚Äôt figure out how to get it running in multiple threads üòÆ‚Äçüí®&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Math-5601"&gt; /u/Ok-Math-5601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tfvnaqas1c9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb0et/ive_been_fine_tuning_a_small_llm_500m_parameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llb0et/ive_been_fine_tuning_a_small_llm_500m_parameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T20:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll1yjh</id>
    <title>LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA üöÄ</title>
    <updated>2025-06-26T14:44:50+00:00</updated>
    <author>
      <name>/u/Additional_Top1210</name>
      <uri>https://old.reddit.com/user/Additional_Top1210</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"&gt; &lt;img alt="LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA üöÄ" src="https://external-preview.redd.it/GPs8oonK03Al4q6HtUFhFxh4J-39nPu_HZOBEQOCcn8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faddbc4424a43d6c2043b2d74892e39170e98392" title="LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper Link: &lt;a href="https://huggingface.co/papers/2506.16406"&gt;https://huggingface.co/papers/2506.16406&lt;/a&gt; Project Link: &lt;a href="https://jerryliang24.github.io/DnD/"&gt;https://jerryliang24.github.io/DnD/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Top1210"&gt; /u/Additional_Top1210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ll1yjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T14:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lllry7</id>
    <title>China's NetEase Releases Open- Source Mathematical Model: Confucius3-Math</title>
    <updated>2025-06-27T05:18:47+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lllry7/chinas_netease_releases_open_source_mathematical/"&gt; &lt;img alt="China's NetEase Releases Open- Source Mathematical Model: Confucius3-Math" src="https://external-preview.redd.it/aBHy0lrTBGIAmO4wnEIfas529xQDjo-nRzz7hjQEXBI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28fb766711134b0632af919a620b789e3ca2ab2a" title="China's NetEase Releases Open- Source Mathematical Model: Confucius3-Math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Official DemonÔºö&lt;a href="https://confucius.youdao.com/"&gt;https://confucius.youdao.com/&lt;/a&gt; GitHubÔºö&lt;a href="https://github.com/netease-youdao/Confucius3-Math"&gt;https://github.com/netease-youdao/Confucius3-Math&lt;/a&gt; HuggingfaceÔºö&lt;a href="https://huggingface.co/netease-youdao/Confucius3-Math"&gt;https://huggingface.co/netease-youdao/Confucius3-Math&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/netease-youdao/Confucius3-Math"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lllry7/chinas_netease_releases_open_source_mathematical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lllry7/chinas_netease_releases_open_source_mathematical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T05:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lleks2</id>
    <title>Gemini CLI - someone already made a pull request for Local LLM providers (and more)</title>
    <updated>2025-06-26T23:09:59+00:00</updated>
    <author>
      <name>/u/merrycachemiss</name>
      <uri>https://old.reddit.com/user/merrycachemiss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lleks2/gemini_cli_someone_already_made_a_pull_request/"&gt; &lt;img alt="Gemini CLI - someone already made a pull request for Local LLM providers (and more)" src="https://external-preview.redd.it/07Svddxhws9NRhwiaZE7X8N_M-orx7gvT8GOb4RjL2I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b658f44c11bd14eedf4c27ce46aafb04ba52e18" title="Gemini CLI - someone already made a pull request for Local LLM providers (and more)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's there, but the contributor still has to complete a CLA and nobody has openly talked about reviewing it. Would giving the PR a thumbs up help it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/merrycachemiss"&gt; /u/merrycachemiss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google-gemini/gemini-cli/pull/1939"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lleks2/gemini_cli_someone_already_made_a_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lleks2/gemini_cli_someone_already_made_a_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T23:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lln6ar</id>
    <title>Gemma 3n Multimodal Input: Text, Audio, Image, and Video?</title>
    <updated>2025-06-27T06:45:49+00:00</updated>
    <author>
      <name>/u/doomdayx</name>
      <uri>https://old.reddit.com/user/doomdayx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lln6ar/gemma_3n_multimodal_input_text_audio_image_and/"&gt; &lt;img alt="Gemma 3n Multimodal Input: Text, Audio, Image, and Video?" src="https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=293ebb5606c7edf7f2570aa914eb4ddb55f1e615" title="Gemma 3n Multimodal Input: Text, Audio, Image, and Video?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Regardless of the API, what is the ‚Äúmost multimodal‚Äù Gemma2n can be made to operate?&lt;/p&gt; &lt;p&gt;The docs say Gemma 3n input supports: 1. text + audio 2. text+ image&lt;/p&gt; &lt;p&gt;The release mentions ‚Äúvideo‚Äù, can it input: 3. True video (t+v+a) 4. Text + video (or imgseq) + audio 5. Running 1+2 and sharing some weights&lt;/p&gt; &lt;p&gt;Or another combo? &lt;/p&gt; &lt;p&gt;If so, is there an ex of 3 channel multimodal?&lt;/p&gt; &lt;p&gt;While I‚Äôve linked the hf transformers example, I‚Äôm interested in any code base where I can work with more modalities of input or potentially modify the model to take more inputs.&lt;/p&gt; &lt;p&gt;Streaming full video + prompts as input with text output would be the ideal modality combination I‚Äôd like to work with so the closer i can get to that the better!&lt;/p&gt; &lt;p&gt;Thanks everyone!&lt;/p&gt; &lt;p&gt;Gemma 3n Release page &lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/"&gt;https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doomdayx"&gt; /u/doomdayx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.google.dev/gemma/docs/core/huggingface_inference#audio"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lln6ar/gemma_3n_multimodal_input_text_audio_image_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lln6ar/gemma_3n_multimodal_input_text_audio_image_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:45:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1llmhof</id>
    <title>General opinions on Gemma 3n Speech-to-Text (STT)?</title>
    <updated>2025-06-27T06:02:01+00:00</updated>
    <author>
      <name>/u/Karim_acing_it</name>
      <uri>https://old.reddit.com/user/Karim_acing_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Gemma 3n's release just happened, and to some of us a good STT model is something we have been longing for a long time. It will take even longer until we can dictate into LMstudio or similar, but I wanted to create this post to discuss your findings with regards to Gemma 3n's STT abilities.&lt;/p&gt; &lt;p&gt;What are your observations regarding maintaining context, what language did you test, what is the speed? Do you see something peculiar for STT tasks regarding its advertised selective parameter activation technology?&lt;/p&gt; &lt;p&gt;Any comparisons to Whisper or Phi-4-multimodal, their stupid sliding window approach?&lt;/p&gt; &lt;p&gt;Post it! thanks!&lt;/p&gt; &lt;p&gt;(I currently can't run it..)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karim_acing_it"&gt; /u/Karim_acing_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llmhof/general_opinions_on_gemma_3n_speechtotext_stt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llmhof/general_opinions_on_gemma_3n_speechtotext_stt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llmhof/general_opinions_on_gemma_3n_speechtotext_stt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:02:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1lljdk8</id>
    <title>What's this star all over the feed for LocalLLaMA?</title>
    <updated>2025-06-27T03:05:34+00:00</updated>
    <author>
      <name>/u/crodjer</name>
      <uri>https://old.reddit.com/user/crodjer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lljdk8/whats_this_star_all_over_the_feed_for_localllama/"&gt; &lt;img alt="What's this star all over the feed for LocalLLaMA?" src="https://b.thumbs.redditmedia.com/KVg07bBJOBdfHmZFYeZT_xaCGVR_rbu-q0z2J0Ez9kY.jpg" title="What's this star all over the feed for LocalLLaMA?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How's this Reddit associated with Twitter? If we must have it, isn't hugging face more appropriate? I vote for &lt;a href="https://huggingface.co/models"&gt;https://huggingface.co/models&lt;/a&gt; page. Twitter has nothing to do with local LLMs (or LLMs at all).&lt;/p&gt; &lt;p&gt;For now, I created this block rule for uBlock origin to hide it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;||emoji.redditmedia.com/cjqd7h6t3a9f1\_t5\_81eyvm/Verified &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But, it still keeps the link to Twitter clickable.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Just for clarification, I am not against having a Twitter account, but really the link and icon. It shows up on every post in my feed, unless I use the uBlock origin media block for this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yyo67vrcle9f1.png?width=590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b007aa23867a03ae09682cbea2faaed1ff51598"&gt;https://preview.redd.it/yyo67vrcle9f1.png?width=590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b007aa23867a03ae09682cbea2faaed1ff51598&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crodjer"&gt; /u/crodjer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lljdk8/whats_this_star_all_over_the_feed_for_localllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lljdk8/whats_this_star_all_over_the_feed_for_localllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lljdk8/whats_this_star_all_over_the_feed_for_localllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T03:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkzynl</id>
    <title>The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)</title>
    <updated>2025-06-26T13:19:50+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"&gt; &lt;img alt="The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)" src="https://external-preview.redd.it/1wJhDztWCANroswcLW3p5i3oMCiTskJ82JKTdTfiCRM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6977975d5861c60901c746f5374dd709bf8cb89" title="The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running GPUs in virtual machines for AI workloads is quickly becoming the golden standard - especially for isolation, orchestration, and multi-tenant setups. So I decided to measure the actual performance penalty of this approach.&lt;/p&gt; &lt;p&gt;I benchmarked some LLMs (via ollama-benchmark) on an AMD RX 9060 XT 16GB - first on bare metal Ubuntu 24.04, then in a VM (Ubuntu 24.04) running under AI Linux (Sbnb Linux) with GPU passthrough via &lt;code&gt;vfio-pci&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Models tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistral:7b&lt;/li&gt; &lt;li&gt;gemma2:9b&lt;/li&gt; &lt;li&gt;phi4:14b&lt;/li&gt; &lt;li&gt;deepseek-r1:14b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;VM performance was just &lt;strong&gt;1‚Äì2% slower&lt;/strong&gt; than bare metal. That‚Äôs it. Practically a rounding error.&lt;/p&gt; &lt;p&gt;So‚Ä¶ yeah. Turns out GPU passthrough isn‚Äôt the scary performance killer.&lt;/p&gt; &lt;p&gt;üëâ I put together the full setup, AMD ROCm install steps, benchmark commands, results, and even a diagram - all in this README: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-GPU-PASSTHROUGH-BENCHMARK.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-GPU-PASSTHROUGH-BENCHMARK.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or help if you‚Äôre setting up something similar!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lkzynl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T13:19:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lllxey</id>
    <title>Update on memX: a shared memory for LLM agents</title>
    <updated>2025-06-27T05:27:43+00:00</updated>
    <author>
      <name>/u/Temporary-Tap-7323</name>
      <uri>https://old.reddit.com/user/Temporary-Tap-7323</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few days ago I shared a project I was working on: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lehbra/built_memx_a_shared_memory_backend_for_llm_agents/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lehbra/built_memx_a_shared_memory_backend_for_llm_agents/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have made significant progress and now, you guys can integrate it with your systems. I have also hosted it as a SaaS free of cost for anyone to use it.&lt;/p&gt; &lt;p&gt;SaaS: &lt;a href="https://mem-x.vercel.app"&gt;https://mem-x.vercel.app&lt;/a&gt;&lt;br /&gt; PyPI: &lt;code&gt;pip install memx-sdk&lt;/code&gt;&lt;br /&gt; Github: &lt;a href="https://github.com/MehulG/memX"&gt;https://github.com/MehulG/memX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just to recap:&lt;br /&gt; memX is a shared memory layer for LLM agents ‚Äî kind of like Redis, but with real-time sync, pub/sub, schema validation, and access control.Instead of having agents pass messages or follow a fixed pipeline, they just read and write to shared memory keys. It‚Äôs like a collaborative whiteboard where agents evolve context together.&lt;/p&gt; &lt;p&gt;Would love feedback or ideas from others building agent systems :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Tap-7323"&gt; /u/Temporary-Tap-7323 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lllxey/update_on_memx_a_shared_memory_for_llm_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lllxey/update_on_memx_a_shared_memory_for_llm_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lllxey/update_on_memx_a_shared_memory_for_llm_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T05:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll57uz</id>
    <title>Gemma 3n is on out on Hugging Face!</title>
    <updated>2025-06-26T16:52:30+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just dropped the perfect local model!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4"&gt;https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/gemma3n"&gt;https://huggingface.co/blog/gemma3n&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll57uz/gemma_3n_is_on_out_on_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll57uz/gemma_3n_is_on_out_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll57uz/gemma_3n_is_on_out_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll88pe</id>
    <title>Gemma 3n vs Gemma 3 (4B/12B) Benchmarks</title>
    <updated>2025-06-26T18:49:09+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I compiled all of the available official first-party benchmark results from google's model cards available here &lt;a href="https://ai.google.dev/gemma/docs/core/model_card_3#benchmark_results"&gt;https://ai.google.dev/gemma/docs/core/model_card_3#benchmark_results&lt;/a&gt; into a table to compare how the new 3N models do compared to their older non-n Gemma 3 siblings. Of course not all the same benchmark results were available for both models so I only added the results for tests they had done in common.&lt;/p&gt; &lt;h1&gt;Reasoning and Factuality&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;n-shot&lt;/th&gt; &lt;th align="left"&gt;E2B PT&lt;/th&gt; &lt;th align="left"&gt;E4B PT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 4B&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 12B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1905.07830"&gt;HellaSwag&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;10-shot&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;td align="left"&gt;78.6&lt;/td&gt; &lt;td align="left"&gt;77.2&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1905.10044"&gt;BoolQ&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;76.4&lt;/td&gt; &lt;td align="left"&gt;81.6&lt;/td&gt; &lt;td align="left"&gt;72.3&lt;/td&gt; &lt;td align="left"&gt;78.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1911.11641"&gt;PIQA&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;78.9&lt;/td&gt; &lt;td align="left"&gt;81&lt;/td&gt; &lt;td align="left"&gt;79.6&lt;/td&gt; &lt;td align="left"&gt;81.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1904.09728"&gt;SocialIQA&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;48.8&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;51.9&lt;/td&gt; &lt;td align="left"&gt;53.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1705.03551"&gt;TriviaQA&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;5-shot&lt;/td&gt; &lt;td align="left"&gt;60.8&lt;/td&gt; &lt;td align="left"&gt;70.2&lt;/td&gt; &lt;td align="left"&gt;65.8&lt;/td&gt; &lt;td align="left"&gt;78.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/google-research-datasets/natural-questions"&gt;Natural Questions&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;5-shot&lt;/td&gt; &lt;td align="left"&gt;15.5&lt;/td&gt; &lt;td align="left"&gt;20.9&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;31.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1911.01547"&gt;ARC-c&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;25-shot&lt;/td&gt; &lt;td align="left"&gt;51.7&lt;/td&gt; &lt;td align="left"&gt;61.6&lt;/td&gt; &lt;td align="left"&gt;56.2&lt;/td&gt; &lt;td align="left"&gt;68.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1911.01547"&gt;ARC-e&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;75.8&lt;/td&gt; &lt;td align="left"&gt;81.6&lt;/td&gt; &lt;td align="left"&gt;82.4&lt;/td&gt; &lt;td align="left"&gt;88.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1907.10641"&gt;WinoGrande&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;5-shot&lt;/td&gt; &lt;td align="left"&gt;66.8&lt;/td&gt; &lt;td align="left"&gt;71.7&lt;/td&gt; &lt;td align="left"&gt;64.7&lt;/td&gt; &lt;td align="left"&gt;74.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://paperswithcode.com/dataset/bbh"&gt;BIG-Bench Hard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;few-shot&lt;/td&gt; &lt;td align="left"&gt;44.3&lt;/td&gt; &lt;td align="left"&gt;52.9&lt;/td&gt; &lt;td align="left"&gt;50.9&lt;/td&gt; &lt;td align="left"&gt;72.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1903.00161"&gt;DROP&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Token F1 score&lt;/td&gt; &lt;td align="left"&gt;1-shot&lt;/td&gt; &lt;td align="left"&gt;53.9&lt;/td&gt; &lt;td align="left"&gt;60.8&lt;/td&gt; &lt;td align="left"&gt;60.1&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;GEOMEAN&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;54.46&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;61.08&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;58.57&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;68.99&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Additional/Other Benchmarks&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;n-shot&lt;/th&gt; &lt;th align="left"&gt;E2B IT&lt;/th&gt; &lt;th align="left"&gt;E4B IT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 4B&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 12B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2210.03057"&gt;MGSM&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;53.1&lt;/td&gt; &lt;td align="left"&gt;60.7&lt;/td&gt; &lt;td align="left"&gt;34.7&lt;/td&gt; &lt;td align="left"&gt;64.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.12404v1"&gt;WMT24++ (ChrF)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Character-level F-score&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;42.7&lt;/td&gt; &lt;td align="left"&gt;50.1&lt;/td&gt; &lt;td align="left"&gt;48.4&lt;/td&gt; &lt;td align="left"&gt;53.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.21228"&gt;ECLeKTic&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;ECLeKTic score&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;2.5&lt;/td&gt; &lt;td align="left"&gt;1.9&lt;/td&gt; &lt;td align="left"&gt;4.6&lt;/td&gt; &lt;td align="left"&gt;10.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2311.12022"&gt;GPQA Diamond&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;RelaxedAccuracy/accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;24.8&lt;/td&gt; &lt;td align="left"&gt;23.7&lt;/td&gt; &lt;td align="left"&gt;30.8&lt;/td&gt; &lt;td align="left"&gt;40.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2108.07732"&gt;MBPP&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;pass@1&lt;/td&gt; &lt;td align="left"&gt;3-shot&lt;/td&gt; &lt;td align="left"&gt;56.6&lt;/td&gt; &lt;td align="left"&gt;63.6&lt;/td&gt; &lt;td align="left"&gt;63.2&lt;/td&gt; &lt;td align="left"&gt;73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2107.03374"&gt;HumanEval&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;pass@1&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;66.5&lt;/td&gt; &lt;td align="left"&gt;75&lt;/td&gt; &lt;td align="left"&gt;71.3&lt;/td&gt; &lt;td align="left"&gt;85.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2403.07974"&gt;LiveCodeBench&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;pass@1&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;13.2&lt;/td&gt; &lt;td align="left"&gt;13.2&lt;/td&gt; &lt;td align="left"&gt;12.6&lt;/td&gt; &lt;td align="left"&gt;24.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HiddenMath&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;27.7&lt;/td&gt; &lt;td align="left"&gt;37.7&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite"&gt;Global-MMLU-Lite&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;64.5&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;69.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2009.03300"&gt;MMLU (Pro)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;40.5&lt;/td&gt; &lt;td align="left"&gt;50.6&lt;/td&gt; &lt;td align="left"&gt;43.6&lt;/td&gt; &lt;td align="left"&gt;60.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;GEOMEAN&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;29.27&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;31.81&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32.66&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;46.8&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Overall Geometric-Mean&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt; &lt;/th&gt; &lt;th align="left"&gt; &lt;/th&gt; &lt;th align="left"&gt; &lt;/th&gt; &lt;th align="left"&gt;E2B IT&lt;/th&gt; &lt;th align="left"&gt;E4B IT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 4B&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 12B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;GEOMAN-ALL&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;40.53&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;44.77&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;44.35&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;57.40&lt;/em&gt;&lt;/strong&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Link to google sheets document: &lt;a href="https://docs.google.com/spreadsheets/d/1U3HvtMqbiuO6kVM96d0aE9W40F8b870He0cg6hLPSdA/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1U3HvtMqbiuO6kVM96d0aE9W40F8b870He0cg6hLPSdA/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll88pe/gemma_3n_vs_gemma_3_4b12b_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll88pe/gemma_3n_vs_gemma_3_4b12b_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll88pe/gemma_3n_vs_gemma_3_4b12b_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T18:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkz0hg</id>
    <title>Meta wins AI copyright lawsuit as US judge rules against authors | Meta</title>
    <updated>2025-06-26T12:35:26+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"&gt; &lt;img alt="Meta wins AI copyright lawsuit as US judge rules against authors | Meta" src="https://external-preview.redd.it/P24oFDRu9fwfx1j87kht5i8PPJV3CyEIC0aLVuyN_0U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5020fe75b422d099598cd47f46c61ccb4e8bea63" title="Meta wins AI copyright lawsuit as US judge rules against authors | Meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theguardian.com/technology/2025/jun/26/meta-wins-ai-copyright-lawsuit-as-us-judge-rules-against-authors"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T12:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lla27f</id>
    <title>Google DeepMind Releases AlphaGenome</title>
    <updated>2025-06-26T20:01:14+00:00</updated>
    <author>
      <name>/u/aithrowaway22</name>
      <uri>https://old.reddit.com/user/aithrowaway22</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lla27f/google_deepmind_releases_alphagenome/"&gt; &lt;img alt="Google DeepMind Releases AlphaGenome" src="https://external-preview.redd.it/43SAwvb1n5vlp2Qq_6_pefepMSOiGDZDO8afisrPhzg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c27c96bfdfb4e956d2433cecac8f9d56364d7d0a" title="Google DeepMind Releases AlphaGenome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aithrowaway22"&gt; /u/aithrowaway22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lla27f/google_deepmind_releases_alphagenome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lla27f/google_deepmind_releases_alphagenome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T20:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1llnwna</id>
    <title>dyad v0.10 - open-source local alternative to lovable/v0/bolt.new with ollama/LM Studio support - now supports building mobile apps!</title>
    <updated>2025-06-27T07:34:10+00:00</updated>
    <author>
      <name>/u/wwwillchen</name>
      <uri>https://old.reddit.com/user/wwwillchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwna/dyad_v010_opensource_local_alternative_to/"&gt; &lt;img alt="dyad v0.10 - open-source local alternative to lovable/v0/bolt.new with ollama/LM Studio support - now supports building mobile apps!" src="https://external-preview.redd.it/eGthenQ5ZHQ5ZjlmMQQDM_dLcTyHBC8BScL5E00e_jl5aRRWjMUA-Nu_qDSf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4758657d96c820f19f51a4aed82016d536d0826b" title="dyad v0.10 - open-source local alternative to lovable/v0/bolt.new with ollama/LM Studio support - now supports building mobile apps!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm excited to share an update to &lt;a href="http://dyad.sh/"&gt;&lt;strong&gt;Dyad&lt;/strong&gt;&lt;/a&gt; which is a free, local, open-source AI app builder I've been working on for 3 months after leaving Google. It's designed as an alternative to v0, Lovable, and Bolt, but it runs on your computer (it's an Electron app)!&lt;/p&gt; &lt;p&gt;Here‚Äôs what makes Dyad different:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Run ANY model (including local LLMs!)&lt;/strong&gt; - Based on popular demand from &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/"&gt;this sub-reddit&lt;/a&gt;, Dyad supports &lt;a href="https://www.dyad.sh/docs/guides/ai-models/local-models"&gt;local models&lt;/a&gt; via LM Studio and ollama (I don't play favorites!), and you can also connect it to any OpenAI API-compatible model!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs locally&lt;/strong&gt; - Dyad runs entirely on your computer, making it fast and frictionless. Because your code lives locally, you can easily switch back and forth between Dyad and your IDE like Cursor, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt; - Dyad is free and bring-your-own API key. This means you can use your free Gemini/OpenRouter API key and build apps in Dyad for free.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Download Dyad for free: &lt;a href="https://dyad.sh/"&gt;https://dyad.sh/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dyad works on Mac &amp;amp; Windows and Linux (you can download Linux directly from &lt;a href="https://github.com/dyad-sh/dyad/releases"&gt;GitHub&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Please share any feedback - would you be interested in MCP support?&lt;/p&gt; &lt;p&gt;P.S. I'm also launching on Product Hunt today and would appreciate any support üôè &lt;a href="https://www.producthunt.com/products/dyad-free-local-vibe-coding-tool"&gt;https://www.producthunt.com/products/dyad-free-local-vibe-coding-tool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wwwillchen"&gt; /u/wwwillchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t461p9dt9f9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwna/dyad_v010_opensource_local_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwna/dyad_v010_opensource_local_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T07:34:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1llltv5</id>
    <title>The performance of NetEase's new Open-Source mathematical model Confucius3-Math</title>
    <updated>2025-06-27T05:21:56+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llltv5/the_performance_of_neteases_new_opensource/"&gt; &lt;img alt="The performance of NetEase's new Open-Source mathematical model Confucius3-Math" src="https://external-preview.redd.it/dEIhtoYICYZI8SaYhg6vcNm2oKuH_uj_36i_H0fDXag.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=693da5b11b8a4b1c3c2afe46f9b68f0213a67d59" title="The performance of NetEase's new Open-Source mathematical model Confucius3-Math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2506.18330"&gt;https://arxiv.org/abs/2506.18330&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1llltv5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llltv5/the_performance_of_neteases_new_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llltv5/the_performance_of_neteases_new_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T05:21:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll68iz</id>
    <title>Gemma 3n Full Launch - Developers Edition</title>
    <updated>2025-06-26T17:31:27+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Today we have the full launch of Gemma 3n, meaning we have support for your favorite tools as well as full support for its capabilities &lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/"&gt;https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recap&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Audio, video, image, and text input; text output&lt;/li&gt; &lt;li&gt;E2B and E4B - while their raw parameter count is 5B and 8B, you can operate them with as little as 2B and 4B effective params&lt;/li&gt; &lt;li&gt;MatFormer: The model architecture allows extracting submodels and doing mix-n-match, allowing you to export additional models in your favorite size between 2B and 4B.&lt;/li&gt; &lt;li&gt;MobileNetV5 and a new audio encoder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And now...for supported tools. We collaborated with many many open source developers to enable its capabilities. So you can now use Gemma in Hugging Face, Kaggle, llama.cpp, Ollama, MLX, LMStudio, transformers.js, Docker model hub, Unsloth, transformers trl and PEFT, VLLM, SGLang, Jetson AI Lab, and many others. Enjoy! We'll also host a Kaggle competition if anyone wants to join &lt;a href="https://www.kaggle.com/competitions/google-gemma-3n-hackathon"&gt;https://www.kaggle.com/competitions/google-gemma-3n-hackathon&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hugging Face &lt;a href="https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4"&gt;https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Unsloth &lt;a href="https://unsloth.ai/blog/gemma-3n"&gt;https://unsloth.ai/blog/gemma-3n&lt;/a&gt;&lt;/li&gt; &lt;li&gt;HF blog &lt;a href="https://huggingface.co/blog/gemma3n"&gt;https://huggingface.co/blog/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;LMStudio &lt;a href="https://lmstudio.ai/models/google/gemma-3n-e4b"&gt;https://lmstudio.ai/models/google/gemma-3n-e4b&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Ollama &lt;a href="https://ollama.com/library/gemma3n"&gt;https://ollama.com/library/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;AI Studio &lt;a href="http://ai.dev"&gt;ai.dev&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Kaggle &lt;a href="https://www.kaggle.com/models/google/gemma-3n"&gt;https://www.kaggle.com/models/google/gemma-3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;MLX &lt;a href="https://huggingface.co/collections/mlx-community/gemma-3n-685d6c8d02d7486c7e77a7dc"&gt;https://huggingface.co/collections/mlx-community/gemma-3n-685d6c8d02d7486c7e77a7dc&lt;/a&gt; &lt;/li&gt; &lt;li&gt;ONNX/transformers.js &lt;a href="https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX"&gt;https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Vertex &lt;a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n"&gt;https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;GGUF &lt;a href="https://huggingface.co/collections/ggml-org/gemma-3n-685d6fc0843071be9e77b6f7"&gt;https://huggingface.co/collections/ggml-org/gemma-3n-685d6fc0843071be9e77b6f7&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lln5uj</id>
    <title>Reverse Engineering Gemma 3n</title>
    <updated>2025-06-27T06:45:04+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lln5uj/reverse_engineering_gemma_3n/"&gt; &lt;img alt="Reverse Engineering Gemma 3n" src="https://external-preview.redd.it/VWUjHfeZBfEe00CQ4OXN4N4xfnF0YI65AE8Jt2eK1GQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca2d2136f62fedb0728942e71b3b8ff4a4232339" title="Reverse Engineering Gemma 3n" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/antimatter15/reverse-engineering-gemma-3n"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lln5uj/reverse_engineering_gemma_3n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lln5uj/reverse_engineering_gemma_3n/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:45:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll38zu</id>
    <title>FLUX.1 Kontext [dev] - an open weights model for proprietary-level image editing performance.</title>
    <updated>2025-06-26T15:35:49+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;weights: &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev"&gt;https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;release news: &lt;a href="https://x.com/bfl_ml/status/1938257909726519640"&gt;https://x.com/bfl_ml/status/1938257909726519640&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1llb5e9</id>
    <title>Crazy how this subreddit started out focused on Meta's LLaMA and ended up becoming a full-blown AI channel.</title>
    <updated>2025-06-26T20:44:28+00:00</updated>
    <author>
      <name>/u/SilverRegion9394</name>
      <uri>https://old.reddit.com/user/SilverRegion9394</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb5e9/crazy_how_this_subreddit_started_out_focused_on/"&gt; &lt;img alt="Crazy how this subreddit started out focused on Meta's LLaMA and ended up becoming a full-blown AI channel." src="https://preview.redd.it/x6kkfnuo2c9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d99eb39eccf80408c1a602f9dfe2d9fb44ce50a" title="Crazy how this subreddit started out focused on Meta's LLaMA and ended up becoming a full-blown AI channel." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SilverRegion9394"&gt; /u/SilverRegion9394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x6kkfnuo2c9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb5e9/crazy_how_this_subreddit_started_out_focused_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llb5e9/crazy_how_this_subreddit_started_out_focused_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T20:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll429p</id>
    <title>gemma 3n has been released on huggingface</title>
    <updated>2025-06-26T16:07:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E2B"&gt;https://huggingface.co/google/gemma-3n-E2B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E2B-it"&gt;https://huggingface.co/google/gemma-3n-E2B-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B"&gt;https://huggingface.co/google/gemma-3n-E4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B-it"&gt;https://huggingface.co/google/gemma-3n-E4B-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(You can find benchmark results such as HellaSwag, MMLU, or LiveCodeBench above)&lt;/p&gt; &lt;p&gt;llama.cpp implementation by &lt;a href="https://github.com/ngxson"&gt;&lt;strong&gt;ngxson&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14400"&gt;https://github.com/ggml-org/llama.cpp/pull/14400&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF"&gt;https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF"&gt;https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical announcement:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/"&gt;https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1llnwy5</id>
    <title>AI performance of smartphone SoCs</title>
    <updated>2025-06-27T07:34:42+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/"&gt; &lt;img alt="AI performance of smartphone SoCs" src="https://external-preview.redd.it/H_9g87w3EitABPy3ZAOo2ZH9LlcpQ5L4KMiJgV1zrjo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baf972823fcd97a8af0b34ddd0ede97ce0d9de05" title="AI performance of smartphone SoCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ai-benchmark.com/ranking_processors.html"&gt;https://ai-benchmark.com/ranking_processors.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few things notable to me: - The difference between tiers is &lt;em&gt;huge&lt;/em&gt;. A 2022 Snapdragon 8 Gen 2 beats the 8s Gen 4. There are huge gaps between the Dimensity 9000, 8000 and 7000 series. - You can better get a high-end SoC that‚Äôs a few years old than the latest mid-range one.&lt;/p&gt; &lt;h2&gt;- In this benchmark, it‚Äôs mainly a Qualcomm and Mediatek competition. It seems optimized software libraries are immensely important in using hardware effectively.&lt;/h2&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1llnwy5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T07:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1llms46</id>
    <title>FYI to everyone: RTX 3090 prices crashed and are back to baseline. You can finally get $600something 3090s again in the USA.</title>
    <updated>2025-06-27T06:20:23+00:00</updated>
    <author>
      <name>/u/DepthHour1669</name>
      <uri>https://old.reddit.com/user/DepthHour1669</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you've been priced out by the spike to $1000+ recently for the past ~3 months, the prices finally dropped to baseline recently. &lt;/p&gt; &lt;p&gt;You can get a $650-750 Nvidia 3090 fairly easily now, instead of being nearly impossible. &lt;/p&gt; &lt;p&gt;Future pricing is unpredictable- if we follow expected deprecation trends, the 3090 should be around $550-600, but then again Trump's tariff extensions expire in a few weeks and pricing is wild and likely to spike up. &lt;/p&gt; &lt;p&gt;If you're interested in GPUs, &lt;strong&gt;now&lt;/strong&gt; is probably the best time to buy for 3090s/4090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DepthHour1669"&gt; /u/DepthHour1669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll6jo5</id>
    <title>DeepSeek R2 delayed</title>
    <updated>2025-06-26T17:43:13+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"&gt; &lt;img alt="DeepSeek R2 delayed" src="https://preview.redd.it/718m48of6b9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b5423692617bfdf316daec6232ca857bc69416c" title="DeepSeek R2 delayed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Over the past several months, DeepSeek's engineers have been working to refine R2 until Liang gives the green light for release, according to The Information. However, a fast adoption of R2 could be difficult due to a shortage of Nvidia server chips in China as a result of U.S. export regulations, the report said, citing employees of top Chinese cloud firms that offer DeepSeek's models to enterprise customers.&lt;/p&gt; &lt;p&gt;A potential surge in demand for R2 would overwhelm Chinese cloud providers, who need advanced Nvidia chips to run AI models, the report said.&lt;/p&gt; &lt;p&gt;DeepSeek did not immediately respond to a Reuters request for comment.&lt;/p&gt; &lt;p&gt;DeepSeek has been in touch with some Chinese cloud companies, providing them with technical specifications to guide their plans for hosting and distributing the model from their servers, the report said.&lt;/p&gt; &lt;p&gt;Among its cloud customers currently using R1, the majority are running the model with Nvidia's H20 chips, The Information said.&lt;/p&gt; &lt;p&gt;Fresh export curbs imposed by the Trump administration in April have prevented Nvidia from selling in the Chinese market its H20 chips - the only AI processors it could legally export to the country at the time.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Sources : &lt;a href="https://www.theinformation.com/articles/deepseeks-progress-stalled-u-s-export-controls"&gt;[1]&lt;/a&gt; &lt;a href="https://x.com/kimmonismus/status/1938221881175183740"&gt;[2]&lt;/a&gt; &lt;a href="https://www.reuters.com/world/china/deepseek-r2-launch-stalled-ceo-balks-progress-information-reports-2025-06-26/"&gt;[3]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/718m48of6b9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:43:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1llhdoq</id>
    <title>I'm using a local Llama model for my game's dialogue system!</title>
    <updated>2025-06-27T01:23:40+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"&gt; &lt;img alt="I'm using a local Llama model for my game's dialogue system!" src="https://external-preview.redd.it/c2JvZG9ndjVnZDlmMe7CY4SqtJeZEukasJn79Adjh2cJgmt44HDkzVTcUucN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24a31f419b54bcf613f907d27abae7c2526e8092" title="I'm using a local Llama model for my game's dialogue system!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm blown away by how fast and intelligent Llama 3.2 is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cgoobkv5gd9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T01:23:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1llndut</id>
    <title>Hunyuan-A13B released</title>
    <updated>2025-06-27T06:59:21+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"&gt; &lt;img alt="Hunyuan-A13B released" src="https://external-preview.redd.it/B1uwVS2BmhDOjFW0XJ6pW7-r7n5zECGun4YlOmky9YY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975cbb18dc0dd9f2342d47d40a0f9fb8fe177327" title="Hunyuan-A13B released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From HF repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Model Introduction&lt;/p&gt; &lt;p&gt;With the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.&lt;/p&gt; &lt;p&gt;Key Features and Advantages&lt;/p&gt; &lt;p&gt;Compact yet Powerful: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.&lt;/p&gt; &lt;p&gt;Hybrid Inference Support: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/p&gt; &lt;p&gt;Ultra-Long Context Understanding: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/p&gt; &lt;p&gt;Enhanced Agent Capabilities: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3 and œÑ-Bench.&lt;/p&gt; &lt;p&gt;Efficient Inference: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:59:21+00:00</published>
  </entry>
</feed>
