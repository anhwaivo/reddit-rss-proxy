<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-15T19:05:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kn86oz</id>
    <title>Suggestion for TTS Models</title>
    <updated>2025-05-15T13:27:01+00:00</updated>
    <author>
      <name>/u/Heavy_Ad_4912</name>
      <uri>https://old.reddit.com/user/Heavy_Ad_4912</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’m building a fun little custom speech-to-speech app. For speech-to-text, I’m using &lt;code&gt;parakeet-0.6B&lt;/code&gt; (latest on HuggingFace), and for the LLM part, I’m currently experimenting with &lt;code&gt;gemma3:4b&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now I’m looking for a suitable &lt;strong&gt;text-to-speech (TTS)&lt;/strong&gt; model from the open-source HuggingFace community. My main constraints are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Max model size:&lt;/strong&gt; 2–3 GB (due to 8GB VRAM and 32GB RAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual support:&lt;/strong&gt; Primarily &lt;strong&gt;English, Hindi, and French&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve looked into a few models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;kokoro-82M&lt;/strong&gt; – seems promising&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zonos&lt;/strong&gt; and &lt;strong&gt;Nari-labs/Dia&lt;/strong&gt; – both ~6GB, too heavy for my setup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cesame-1B&lt;/strong&gt; – tried it, but the performance was underwhelming&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given these constraints, which TTS models would you recommend? Bonus points for ones that work out-of-the-box or require minimal finetuning.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heavy_Ad_4912"&gt; /u/Heavy_Ad_4912 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn86oz/suggestion_for_tts_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn86oz/suggestion_for_tts_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn86oz/suggestion_for_tts_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T13:27:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn69mp</id>
    <title>How do SOTA LLMs Process PDFs: Native Understanding, OCR, or RAG?</title>
    <updated>2025-05-15T11:54:06+00:00</updated>
    <author>
      <name>/u/coconautico</name>
      <uri>https://old.reddit.com/user/coconautico</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"&gt; &lt;img alt="How do SOTA LLMs Process PDFs: Native Understanding, OCR, or RAG?" src="https://b.thumbs.redditmedia.com/rqzDSRVqRbCQDZ3SFaKzcnoeDqw5K2FyJ-Re4mj9eho.jpg" title="How do SOTA LLMs Process PDFs: Native Understanding, OCR, or RAG?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I'm trying to build a solution to &lt;strong&gt;analyze a set of PDF files&lt;/strong&gt; (5-10) using an LLM.&lt;/p&gt; &lt;p&gt;My current approach is to perform a &lt;strong&gt;high-quality OCR&lt;/strong&gt; (using Docling) and then, dump all this information as the &lt;strong&gt;context for my prompt&lt;/strong&gt;. However, I doubt this is the best strategy nowadays.&lt;/p&gt; &lt;p&gt;Playing around with Gemini, I've noticed it handles PDF files extremely well*, even showing the &lt;strong&gt;tokens it contains&lt;/strong&gt;. So I was wondering if the model is &amp;quot;&lt;strong&gt;reading&lt;/strong&gt;&amp;quot; the PDF file &lt;strong&gt;directly&lt;/strong&gt; (native vision), or is there a preliminary step where it converts the PDF to pure text using &lt;strong&gt;OCR before processing&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;I'm also wondering if a &lt;strong&gt;Retrieval Augmented Generation (RAG) strategy&lt;/strong&gt; is involved in how it interacts with the document content once uploaded.&lt;/p&gt; &lt;p&gt;If anyone knows more about this process, it would be interesting to hear.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;p&gt;*It was able to perfectly process a PDF of images with handwritten text and equations&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Additional information:&lt;/strong&gt;&lt;br /&gt; I've noticed that Gemini sometimes appends labels like `--- PAGE 1 ---`, `--- PAGE 2 ---`, etc., when processing PDFs. When I ask the model what tool it's using, it replies with something like “an internal tool to transcribe PDFs.” I've tried replicating the results using Google's public Vision APIs, but none of them produce the same output. So I assume they're using some internal system (maybe a custom-built tool) to reliably convert anything into plain text.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What seems to be happening under the hood&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As &lt;a href="/u/highergraphic"&gt;u/highergraphic&lt;/a&gt; suggested, I tried to pin down whether Gemini first turns each PDF page into an image and then processes natively using its multimodal capabilities on that rasterized page. Result? Every experiment seems to point to &amp;quot;yes.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Original PDF:&lt;/strong&gt; Mixed text, images, and tables. → Perfect extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flat image of the same page:&lt;/strong&gt; Exported the page as a single PNG/JPG. → Same perfect extraction.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid PDF:&lt;/strong&gt; Re-created the page but replaced some paragraphs and tables with screenshots of themselves (same size). → Still perfect.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tiny-font PDF:&lt;/strong&gt; Shrunk the text until it was almost unreadable. → Worked until the characters were too small.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tiny-font PDF (from images):&lt;/strong&gt; Same experiement as the previous one, but this time, I shrunk the images of the text until it was almost unreadable. → Same. It worked until the characters were too small.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Gemini (and, I suspect, other modern multimodal LLMs) appears to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Rasterize&lt;/strong&gt; each PDF page into an image.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process it using the multimodal LLM&lt;/strong&gt; to produce plain text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repeat.\&lt;/strong&gt;*&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;*Each new image processing adds a markers like &lt;code&gt;--- PAGE X ---&lt;/code&gt; to help with the context.&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example of the PDF with textual parts of it replaced by images of the same size:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vjygb9rzly0f1.jpg?width=2479&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=26934052347e813429bd24acf12953167c945b7d"&gt;Example of the PDF page with text parts replaced by images of the same size&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coconautico"&gt; /u/coconautico &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn69mp/how_do_sota_llms_process_pdfs_native/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T11:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn810l</id>
    <title>LLM based Personally identifiable information detection tool</title>
    <updated>2025-05-15T13:19:47+00:00</updated>
    <author>
      <name>/u/geeganage</name>
      <uri>https://old.reddit.com/user/geeganage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub repo: &lt;a href="https://github.com/rpgeeganage/pII-guard"&gt;https://github.com/rpgeeganage/pII-guard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;br /&gt; I recently built a small open-source tool called PII (personally identifiable information) to detect personally identifiable information (PII) in logs using AI. It’s self-hosted and designed for privacy-conscious developers or teams.&lt;/p&gt; &lt;p&gt;Features: - HTTP endpoint for log ingestion with buffered processing&lt;br /&gt; - PII detection using local AI models via Ollama (e.g., gemma:3b)&lt;br /&gt; - PostgreSQL + Elasticsearch for storage&lt;br /&gt; - Web UI to review flagged logs&lt;br /&gt; - Docker Compose for easy setup&lt;/p&gt; &lt;p&gt;It’s still a work in progress, and any suggestions or feedback would be appreciated. Thanks for checking it out!&lt;/p&gt; &lt;p&gt;My apologies if this post is not relevant to this group &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geeganage"&gt; /u/geeganage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn810l/llm_based_personally_identifiable_information/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn810l/llm_based_personally_identifiable_information/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn810l/llm_based_personally_identifiable_information/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T13:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmrfoo</id>
    <title>MLA optimization with flashattention for llama.cpp,MLA + FA now only uses K-cache - 47% saving on KV-cache size</title>
    <updated>2025-05-14T21:42:55+00:00</updated>
    <author>
      <name>/u/shing3232</name>
      <uri>https://old.reddit.com/user/shing3232</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13529"&gt;MLA + FA now only uses K-cache - 47% saving on KV-cache size (only for use with #13435 for now) by jukofyork · Pull Request #13529 · ggml-org/llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_kv_cache_unified: kv_size = 163840, type_k = 'f16', type_v = 'f16', n_layer = 61, can_shift = 0, padding = 256&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_kv_cache_unified: CUDA0 KV buffer size = 10980.00 MiB&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama_kv_cache_unified: KV self size = 10980.00 MiB, K (f16): 10980.00 MiB, V (f16): 0.00 MiB&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The full context of 160k tokens now takes up less than 11GB without kquants&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shing3232"&gt; /u/shing3232 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmrfoo/mla_optimization_with_flashattention_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T21:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmi6vl</id>
    <title>I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU.</title>
    <updated>2025-05-14T15:33:15+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"&gt; &lt;img alt="I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU." src="https://external-preview.redd.it/Z3l2NXpmczhucjBmMUwcvEt1gWTYtmZHqUwsIc9aRH3JKfTLJ5UHo4J1H4An.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61d172895901d0b35dab0f76eb10b4c4648b8f5c" title="I updated the SmolVLM llama.cpp webcam demo to run locally in-browser on WebGPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/&lt;/a&gt;, I decided to update the llama.cpp server demo so that it runs 100% locally in-browser on WebGPU, using Transformers.js. This means you can simply visit the link and run the demo, without needing to install anything locally. &lt;/p&gt; &lt;p&gt;I hope you like it! &lt;a href="https://huggingface.co/spaces/webml-community/smolvlm-realtime-webgpu"&gt;https://huggingface.co/spaces/webml-community/smolvlm-realtime-webgpu&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PS: The source code is a single index.html file you can find in the &amp;quot;Files&amp;quot; section on the demo page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/or5b3ks8nr0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmi6vl/i_updated_the_smolvlm_llamacpp_webcam_demo_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T15:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmyr7h</id>
    <title>Qwen3-235B-A22B not measuring up to DeepseekV3-0324</title>
    <updated>2025-05-15T03:45:07+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep trying to get it to behave, but q8 is not keeping up with my deepseekv3_q3_k_xl. what gives? am I doing something wrong or is it just all hype? it's a capable model and I'm sure for those that have not been able to run big models, this is a shock and great, but for those of us who have been able to run huge models, it's feel like a waste of bandwidth and time. it's not a disaster like llama-4 yet I'm having a hard time getting it into rotation of my models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmyr7h/qwen3235ba22b_not_measuring_up_to_deepseekv30324/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmyr7h/qwen3235ba22b_not_measuring_up_to_deepseekv30324/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmyr7h/qwen3235ba22b_not_measuring_up_to_deepseekv30324/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T03:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kmlu2y</id>
    <title>Qwen3-30B-A6B-16-Extreme is fantastic</title>
    <updated>2025-05-14T17:57:00+00:00</updated>
    <author>
      <name>/u/DocWolle</name>
      <uri>https://old.reddit.com/user/DocWolle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen3-30B-A6B-16-Extreme"&gt;https://huggingface.co/DavidAU/Qwen3-30B-A6B-16-Extreme&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quants:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-30B-A6B-16-Extreme-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-30B-A6B-16-Extreme-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Someone recently mentioned this model here on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; and I gave it a try. For me it is the best model I can run locally with my 36GB CPU only setup. In my view it is a lot smarter than the original A3B model. &lt;/p&gt; &lt;p&gt;It uses 16 experts instead of 8 and when watching it thinking I can see that it thinks a step further/deeper than the original model. Speed is still great. &lt;/p&gt; &lt;p&gt;I wonder if anyone else has tried it. A 128k context version is also available.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocWolle"&gt; /u/DocWolle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmlu2y/qwen330ba6b16extreme_is_fantastic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kmlu2y/qwen330ba6b16extreme_is_fantastic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kmlu2y/qwen330ba6b16extreme_is_fantastic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-14T17:57:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2gsa</id>
    <title>Is neural engine on mac a wasted opportunity?</title>
    <updated>2025-05-15T07:41:38+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What’s the point of having a 32-core neural engine on the new mac studio if you can’t use it for LLM or image/video generation tasks ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2gsa/is_neural_engine_on_mac_a_wasted_opportunity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2gsa/is_neural_engine_on_mac_a_wasted_opportunity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2gsa/is_neural_engine_on_mac_a_wasted_opportunity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:41:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1knbo80</id>
    <title>HanaVerse - Chat with AI through an interactive anime character! 🌸</title>
    <updated>2025-05-15T15:52:31+00:00</updated>
    <author>
      <name>/u/OrganicTelevision652</name>
      <uri>https://old.reddit.com/user/OrganicTelevision652</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbo80/hanaverse_chat_with_ai_through_an_interactive/"&gt; &lt;img alt="HanaVerse - Chat with AI through an interactive anime character! 🌸" src="https://external-preview.redd.it/VMExyAyOE_4W1BYj5ZE65UYho8s1S8iYWLFddyI6R88.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a56e1472206dfd96091c85f5438f8e274f3dd61" title="HanaVerse - Chat with AI through an interactive anime character! 🌸" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on something I think you'll love - HanaVerse, an interactive web UI for Ollama that brings your AI conversations to life through a charming 2D anime character named Hana!&lt;/p&gt; &lt;p&gt;What is &lt;strong&gt;HanaVerse&lt;/strong&gt;? 🤔&lt;/p&gt; &lt;p&gt;HanaVerse transforms how you interact with Ollama's language models by adding a visual, animated companion to your conversations. Instead of just text on a screen, you chat with Hana - a responsive anime character who reacts to your interactions in real-time!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features that make HanaVerse special&lt;/strong&gt;: ✨&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Talks Back:&lt;/strong&gt; Answers with voice&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Streaming Responses:&lt;/strong&gt; See answers form in real-time as they're generated&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Markdown Support:&lt;/strong&gt; Beautiful formatting with syntax highlighting&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LaTeX Math Rendering:&lt;/strong&gt; Perfect for equations and scientific content&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Customizable:&lt;/strong&gt; Choose any Ollama model and configure system prompts&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Responsive Design:&lt;/strong&gt; Works on both desktop(preferred) and mobile&lt;/p&gt; &lt;p&gt;Why I built this 🛠️&lt;/p&gt; &lt;p&gt;I wanted to make AI interactions more engaging and personal while leveraging the power of self-hosted Ollama models. The result is an interface that makes AI conversations feel more natural and enjoyable.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1knbo80/video/uczc6t9cwy0f1/player"&gt;Hanaverse demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're looking for a more engaging way to interact with your Ollama models, give HanaVerse a try and let me know what you think!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Ashish-Patnaik/HanaVerse"&gt;https://github.com/Ashish-Patnaik/HanaVerse&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Skeleton Demo = &lt;a href="https://hanaverse.vercel.app/"&gt;https://hanaverse.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love your feedback and contributions - stars ⭐ are always appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrganicTelevision652"&gt; /u/OrganicTelevision652 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbo80/hanaverse_chat_with_ai_through_an_interactive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbo80/hanaverse_chat_with_ai_through_an_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knbo80/hanaverse_chat_with_ai_through_an_interactive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T15:52:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1knb2kq</id>
    <title>qSpeak - A Cross platform alternative for WisprFlow supporting local LLMs and Linux</title>
    <updated>2025-05-15T15:28:13+00:00</updated>
    <author>
      <name>/u/fajfas3</name>
      <uri>https://old.reddit.com/user/fajfas3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, together with my colleagues, we've created &lt;a href="http://qSpeak.app"&gt;qSpeak.app&lt;/a&gt; 🎉 &lt;/p&gt; &lt;p&gt;qSpeak is an alternative to tools like SuperWhisper or WisprFlow but works on all platforms including Linux. 🚀&lt;/p&gt; &lt;p&gt;Also we're working on integrating LLMs more deeply into it to include more sophisticated interactions like multi step conversations (essentially assistants) and in the near future MCP integration. &lt;/p&gt; &lt;p&gt;The app is currently completely free so please try it out! 🎁&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fajfas3"&gt; /u/fajfas3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qspeak.app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knb2kq/qspeak_a_cross_platform_alternative_for_wisprflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knb2kq/qspeak_a_cross_platform_alternative_for_wisprflow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T15:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1knfe13</id>
    <title>ThinkStation PGX - with NVIDIA GB10 Grace Blackwell Superchip / 128GB</title>
    <updated>2025-05-15T18:21:39+00:00</updated>
    <author>
      <name>/u/nostriluu</name>
      <uri>https://old.reddit.com/user/nostriluu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knfe13/thinkstation_pgx_with_nvidia_gb10_grace_blackwell/"&gt; &lt;img alt="ThinkStation PGX - with NVIDIA GB10 Grace Blackwell Superchip / 128GB" src="https://external-preview.redd.it/Bf1eGAFfYgmopj7bn8x57X5Vubn-mFaf7TrFzb01Rl4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd1c6de1776b9a8e40c014c77fcc5ce1cd52905" title="ThinkStation PGX - with NVIDIA GB10 Grace Blackwell Superchip / 128GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nostriluu"&gt; /u/nostriluu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.lenovo.com/all-new-lenovo-thinkstation-pgx-big-ai-innovation-in-a-small-form-factor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knfe13/thinkstation_pgx_with_nvidia_gb10_grace_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knfe13/thinkstation_pgx_with_nvidia_gb10_grace_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T18:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1knfggw</id>
    <title>Are there any models that are even half funny?</title>
    <updated>2025-05-15T18:24:27+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any models that can write funny text including jokes? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knfggw/are_there_any_models_that_are_even_half_funny/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knfggw/are_there_any_models_that_are_even_half_funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knfggw/are_there_any_models_that_are_even_half_funny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T18:24:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn6427</id>
    <title>Llamafile 0.9.3 Brings Support For Qwen3 &amp; Phi4</title>
    <updated>2025-05-15T11:45:30+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6427/llamafile_093_brings_support_for_qwen3_phi4/"&gt; &lt;img alt="Llamafile 0.9.3 Brings Support For Qwen3 &amp;amp; Phi4" src="https://external-preview.redd.it/Cj4HZCrFxF1ZWikVE2EGwsOPpKF5ST6n_sC3VWnurnI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ee9e6c0ea1c9f1b1be02252a698b00e32a60cbe" title="Llamafile 0.9.3 Brings Support For Qwen3 &amp;amp; Phi4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Llamafile-0.9.3-Released"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6427/llamafile_093_brings_support_for_qwen3_phi4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6427/llamafile_093_brings_support_for_qwen3_phi4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T11:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn6mic</id>
    <title>Qwen 2.5 vs Qwen 3 vs Gemma 3: Real world base model comparison?</title>
    <updated>2025-05-15T12:12:37+00:00</updated>
    <author>
      <name>/u/Desperate_Rub_1352</name>
      <uri>https://old.reddit.com/user/Desperate_Rub_1352</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6mic/qwen_25_vs_qwen_3_vs_gemma_3_real_world_base/"&gt; &lt;img alt="Qwen 2.5 vs Qwen 3 vs Gemma 3: Real world base model comparison?" src="https://preview.redd.it/kq34jkwvsx0f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26b2276e1df77e5648f6b562bf60fe6c8a922ea4" title="Qwen 2.5 vs Qwen 3 vs Gemma 3: Real world base model comparison?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been digging into the latest base models and wanted to get some practical opinions beyond just benchmark numbers.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;For those who have actually used both Qwen 2.5 and Qwen 3 base models&lt;/strong&gt;: Did you notice a truly big jump in general usage (reasoning, instruction following, robustness), or is the improvement mostly confined to coding and math tasks? I’m not talking about fine-tuned chat versions, just the raw base models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma 3 vs Qwen&lt;/strong&gt;: Is Gemma 3 genuinely that far behind, or is there some possible benchmark leakage or overfitting with Qwen? A few benchmark charts make me suspicious. Would love to hear hands-on perspectives if anyone has experimented with both.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why I’m asking:&lt;/strong&gt;&lt;br /&gt; I want to build a highly &lt;em&gt;steerable&lt;/em&gt; model for my research and product work. I only have budget for one serious base model to work from, so I want to select the absolute best starting point. I’m focusing on openness, quality, and steerability, not just raw benchmark wins.&lt;/p&gt; &lt;p&gt;Any honest feedback, experiments, or even failures you’ve had with these models would help me massively. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Rub_1352"&gt; /u/Desperate_Rub_1352 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kq34jkwvsx0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6mic/qwen_25_vs_qwen_3_vs_gemma_3_real_world_base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn6mic/qwen_25_vs_qwen_3_vs_gemma_3_real_world_base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T12:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn94oi</id>
    <title>Update: We fit 50+ LLMs on 2 GPUs — and now we’re inviting you to try it.</title>
    <updated>2025-05-15T14:08:22+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week’s post on cold starts and snapshotting hit a nerve. Turns out many of you are also trying to juggle multiple models, deal with bloated memory, or squeeze more out of a single GPU.&lt;/p&gt; &lt;p&gt;We’re making our snapshot-based runtime available to a limited number of builders — especially if you’re running agents, RAG pipelines, or multi-model workloads locally.&lt;/p&gt; &lt;p&gt;It’s still early, and we’re limited in support, but the tech is real:&lt;/p&gt; &lt;p&gt;• 50+ models on 2× A4000s • Cold starts under 2s • 90%+ GPU utilization • No bloating, no prewarming&lt;/p&gt; &lt;p&gt;If you’re experimenting with multiple models and want to deploy more on fewer GPUs, this might help.&lt;/p&gt; &lt;p&gt;We’d love your feedback . reach out and we’ll get you access.&lt;/p&gt; &lt;p&gt;Please feel free to ask any questions &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn94oi/update_we_fit_50_llms_on_2_gpus_and_now_were/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn94oi/update_we_fit_50_llms_on_2_gpus_and_now_were/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn94oi/update_we_fit_50_llms_on_2_gpus_and_now_were/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T14:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2aay</id>
    <title>Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures</title>
    <updated>2025-05-15T07:28:36+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"&gt; &lt;img alt="Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures" src="https://preview.redd.it/ww4aygc1ew0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18baa07396402b906dd387ccabc4f5bab873fba3" title="Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.09343"&gt;https://arxiv.org/abs/2505.09343&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ww4aygc1ew0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2aay/insights_into_deepseekv3_scaling_challenges_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:28:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1knbdd3</id>
    <title>Hugging Face free and open source MCP course</title>
    <updated>2025-05-15T15:40:16+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're thrilled to announce the launch of our comprehensive Model Context Protocol (MCP) Course! This free program is designed to take learners from foundational understanding to practical application of MCP in AI.&lt;/p&gt; &lt;p&gt;Join the course on the hub:&lt;a href="https://huggingface.co/mcp-course"&gt;https://huggingface.co/mcp-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this course, you will: 📖 Study Model Context Protocol in theory, design, and practice. 🧑‍💻 Learn to use established MCP SDKs and frameworks. 💾 Share your projects and explore applications created by the community. 🏆 Participate in challenges and evaluate your MCP implementations. 🎓 Earn a certificate of completion.&lt;/p&gt; &lt;p&gt;At the end, you'll understand how MCP works and how to build your own AI applications that leverage external data and tools using the latest MCP standards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbdd3/hugging_face_free_and_open_source_mcp_course/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knbdd3/hugging_face_free_and_open_source_mcp_course/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knbdd3/hugging_face_free_and_open_source_mcp_course/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T15:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn9882</id>
    <title>LLaDA-8B-Tools: A diffusion language model fine-tuned for tool use</title>
    <updated>2025-05-15T14:12:37+00:00</updated>
    <author>
      <name>/u/ProximileLLC</name>
      <uri>https://old.reddit.com/user/ProximileLLC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Instead of generating token-by-token, this architecture refines the whole output by replacing mask tokens across the sequence.&lt;/p&gt; &lt;p&gt;The bidirectional attention seems to help with structured outputs, though this is just a rough first attempt with some issues (e.g. extra text after a message, because of this architecture's preset generation length).&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Proximile/LLaDA-8B-Tools"&gt;https://huggingface.co/Proximile/LLaDA-8B-Tools&lt;/a&gt;&lt;br /&gt; Dataset: &lt;a href="https://huggingface.co/datasets/Proximile/LLaDA-8B-Tools"&gt;https://huggingface.co/datasets/Proximile/LLaDA-8B-Tools&lt;/a&gt;&lt;br /&gt; Format mostly follows Llama 3.1: &lt;a href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/"&gt;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on a variant tuned for more general tool use using a range of i/o formats.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProximileLLC"&gt; /u/ProximileLLC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn9882/llada8btools_a_diffusion_language_model_finetuned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn9882/llada8btools_a_diffusion_language_model_finetuned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn9882/llada8btools_a_diffusion_language_model_finetuned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T14:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1knca48</id>
    <title>Quick Qwen3-30B-A6B-16-Extreme vs Qwen3-30B A3B Benchmark</title>
    <updated>2025-05-15T16:17:15+00:00</updated>
    <author>
      <name>/u/terhechte</name>
      <uri>https://old.reddit.com/user/terhechte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I have a Benchmark suite of 110 tasks across multiple programming languages. The focus really is on more complex problems and not Javascript one-shot problems. I was interested in comparing the above two models. &lt;/p&gt; &lt;p&gt;Setup&lt;/p&gt; &lt;p&gt;- Qwen3-30B-A6B-16-Extreme Q4_K_M running in LMStudio&lt;br /&gt; - Qwen3-30B A3B on OpenRouter&lt;/p&gt; &lt;p&gt;I understand that this is not a fair fight because the A6B is heavily quantized, but running this benchmark on my Macbook takes almost 12 hours with reasoning models, so a better comparison will take a bit longer. &lt;/p&gt; &lt;p&gt;Here are the results:&lt;/p&gt; &lt;p&gt;| lmstudio/qwen3-30b-a6b-16-extreme | correct: 56 | wrong: 54 |&lt;/p&gt; &lt;p&gt;| openrouter/qwen/qwen3-30b-a3b | correct: 68 | wrong: 42 |&lt;/p&gt; &lt;p&gt;I will try to report back in a couple of days with more comparisons. &lt;/p&gt; &lt;p&gt;You can learn more about the benchmark here (&lt;a href="https://ben.terhech.de/posts/2025-01-31-llms-vs-programming-languages.html"&gt;https://ben.terhech.de/posts/2025-01-31-llms-vs-programming-languages.html&lt;/a&gt;) but I've since also added support for more models and languages. However I haven't really released the results in some time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terhechte"&gt; /u/terhechte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knca48/quick_qwen330ba6b16extreme_vs_qwen330b_a3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1knca48/quick_qwen330ba6b16extreme_vs_qwen330b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1knca48/quick_qwen330ba6b16extreme_vs_qwen330b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T16:17:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kna53n</id>
    <title>Qwen3-32B hallucinates more than QwQ-32B</title>
    <updated>2025-05-15T14:50:45+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"&gt; &lt;img alt="Qwen3-32B hallucinates more than QwQ-32B" src="https://b.thumbs.redditmedia.com/BKpqaFBECcC520jrmC7_8NwZgTpcy4cdtN47rrRQrVU.jpg" title="Qwen3-32B hallucinates more than QwQ-32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been seeing some people complaining about Qwen3's hallucination issues. Personally, I have never run into such issue, but I recently came across some Chinese benchmarks of Qwen3 and QwQ, so I might as well share them here.&lt;/p&gt; &lt;p&gt;I translated these to English; the sources are in the images.&lt;/p&gt; &lt;p&gt;TLDR:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3-32B has a lower SimpleQA score than QwQ (5.87% vs 8.07%)&lt;/li&gt; &lt;li&gt;Qwen3-32B has a higher hallucination rate than QwQ in reasoning mode (30.15% vs 22.7%)&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;SuperCLUE-Faith is designed to evaluate Chinese language performance, so it obviously gives Chinese models an advantage over American ones, but should be useful for comparing Qwen models.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nrjfzhl2ky0f1.jpg?width=3388&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c2021c8da8fb21fc46cefb8539130e97ce20dee"&gt;https://preview.redd.it/nrjfzhl2ky0f1.jpg?width=3388&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4c2021c8da8fb21fc46cefb8539130e97ce20dee&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5rh9qe4cky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=218051f6ddbc88ff99a584ed0c2877f7e97f8132"&gt;https://preview.redd.it/5rh9qe4cky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=218051f6ddbc88ff99a584ed0c2877f7e97f8132&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jwi0mphyky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=57dbad3cead06c339f4cabf16f39bb211925aa22"&gt;https://preview.redd.it/jwi0mphyky0f1.jpg?width=2160&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=57dbad3cead06c339f4cabf16f39bb211925aa22&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7gy8ebvyky0f1.jpg?width=2156&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=30da9915523db714b599bf88b1925d85a40f545f"&gt;https://preview.redd.it/7gy8ebvyky0f1.jpg?width=2156&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=30da9915523db714b599bf88b1925d85a40f545f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kna53n/qwen332b_hallucinates_more_than_qwq32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T14:50:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn8m8t</id>
    <title>Open-source general purpose agent with built-in MCPToolkit support</title>
    <updated>2025-05-15T13:46:21+00:00</updated>
    <author>
      <name>/u/Fluffy_Sheepherder76</name>
      <uri>https://old.reddit.com/user/Fluffy_Sheepherder76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn8m8t/opensource_general_purpose_agent_with_builtin/"&gt; &lt;img alt="Open-source general purpose agent with built-in MCPToolkit support" src="https://preview.redd.it/h6y4hb7s9y0f1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee3d023b25d2e2f99165aa457441e34896b8d16c" title="Open-source general purpose agent with built-in MCPToolkit support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The open-source OWL agent now comes with built-in MCPToolkit support, just drop in your MCP servers (Playwright, desktop-commander, custom Python tools, etc.) and OWL will automatically discover and call them in its multi-agent workflows.&lt;/p&gt; &lt;p&gt;OWL: &lt;a href="https://github.com/camel-ai/owl"&gt;https://github.com/camel-ai/owl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Sheepherder76"&gt; /u/Fluffy_Sheepherder76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h6y4hb7s9y0f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn8m8t/opensource_general_purpose_agent_with_builtin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn8m8t/opensource_general_purpose_agent_with_builtin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T13:46:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1kndp9f</id>
    <title>TTS Fine-tuning now in Unsloth!</title>
    <updated>2025-05-15T17:14:19+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kndp9f/tts_finetuning_now_in_unsloth/"&gt; &lt;img alt="TTS Fine-tuning now in Unsloth!" src="https://external-preview.redd.it/bXI4dnBsa3phejBmMfDzohHQ2IN6C0pCi0KaT-g2AEXeep08I3DgQhQN5vF7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57583ee26a1b7da14346a3cc45ff72ecbf34831b" title="TTS Fine-tuning now in Unsloth!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! Not the usual LLMs talk but we’re excited to announce that you can now train Text-to-Speech (TTS) models in &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;! Training is ~1.5x faster with 50% less VRAM compared to all other setups with FA2. :D&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support includes &lt;code&gt;Sesame/csm-1b&lt;/code&gt;, &lt;code&gt;OpenAI/whisper-large-v3&lt;/code&gt;, &lt;code&gt;CanopyLabs/orpheus-3b-0.1-ft&lt;/code&gt;, and any Transformer-style model including LLasa, Outte, Spark, and more.&lt;/li&gt; &lt;li&gt;The goal of TTS fine-tuning to minic voices, adapt speaking styles and tones, support new languages, handle specific tasks etc.&lt;/li&gt; &lt;li&gt;We’ve made notebooks to train, run, and save these models for free on Google Colab. Some models aren’t supported by llama.cpp and will be saved only as safetensors, but others should work. See our TTS docs and notebooks: &lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The training process is similar to SFT, but the dataset includes audio clips with transcripts. We use a dataset called ‘Elise’ that embeds emotion tags like &amp;lt;sigh&amp;gt; or &amp;lt;laughs&amp;gt; into transcripts, triggering expressive audio that matches the emotion.&lt;/li&gt; &lt;li&gt;Since TTS models are usually small, you can train them using 16-bit LoRA, or go with FFT. Loading a 16-bit LoRA model is simple.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We've uploaded most of the TTS models (quantized and original) to &lt;a href="https://huggingface.co/collections/unsloth/text-to-speech-tts-models-68007ab12522e96be1e02155"&gt;Hugging Face here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And here are our TTS notebooks:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_(1B"&gt;Sesame-CSM (1B)&lt;/a&gt;-TTS.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B"&gt;Orpheus-TTS (3B)&lt;/a&gt;-TTS.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb"&gt;Whisper Large V3&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_(0_5B"&gt;Spark-TTS (0.5B)&lt;/a&gt;.ipynb)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Thank you for reading and please do ask any questions!!&lt;/p&gt; &lt;p&gt;P.S. We also now support Qwen3 GRPO. We use the base model + a new custom proximity-based reward function to favor near-correct answers and penalize outliers. Pre-finetuning mitigates formatting bias and boosts evaluation accuracy via regex matching: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/faqjz7kzaz0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kndp9f/tts_finetuning_now_in_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kndp9f/tts_finetuning_now_in_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T17:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn2mv9</id>
    <title>LLMs Get Lost In Multi-Turn Conversation</title>
    <updated>2025-05-15T07:53:58+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt; &lt;img alt="LLMs Get Lost In Multi-Turn Conversation" src="https://b.thumbs.redditmedia.com/MIMwMQ4O4HnoFjzXbBTjShTxVfai2B_u3_lcuHpfKVk.jpg" title="LLMs Get Lost In Multi-Turn Conversation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://arxiv.org/abs/2505.06120"&gt;paper&lt;/a&gt; found that the performance of open and closed LLMs drops significantly in multi-turn conversations. Most benchmarks focus on single-turn, fully-specified instruction settings. They found that LLMs often make (incorrect) assumptions in early turns, on which they rely going forward and never recover from.&lt;/p&gt; &lt;p&gt;They concluded that when a multi-turn conversation doesn't yield the desired results, it might help to restart with a fresh conversation, putting all the relevant information from the multi-turn conversation into the first turn.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ltlt4zbiiw0f1.png?width=1515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4de01b7a2339658690b3492899e107bd4af9836"&gt;https://preview.redd.it/ltlt4zbiiw0f1.png?width=1515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d4de01b7a2339658690b3492899e107bd4af9836&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Sharded&amp;quot; means they split an original fully-specified single-turn instruction into multiple tidbits of information that they then fed the LLM turn by turn. &amp;quot;Concat&amp;quot; is a comparison as a baseline where they fed all the generated information pieces in the same turn. Here are examples on how they did the splitting:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y40aremjiw0f1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe81a4a2be778437bf7134933863ebbd88e5ef2"&gt;https://preview.redd.it/y40aremjiw0f1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe81a4a2be778437bf7134933863ebbd88e5ef2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn2mv9/llms_get_lost_in_multiturn_conversation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T07:53:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn75q8</id>
    <title>PDF input merged into llama.cpp</title>
    <updated>2025-05-15T12:39:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13562"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75q8/pdf_input_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn75q8/pdf_input_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T12:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kn542r</id>
    <title>Introducing A.I.T.E Ball</title>
    <updated>2025-05-15T10:45:28+00:00</updated>
    <author>
      <name>/u/tonywestonuk</name>
      <uri>https://old.reddit.com/user/tonywestonuk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"&gt; &lt;img alt="Introducing A.I.T.E Ball" src="https://external-preview.redd.it/NXllMTcxNDFkeDBmMcTQf63cMAAIN-71fn86oCbnKUR2tA_D5RmS947R5l7-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf3613bb545a67e6ba0ae442a8d9fddc761c89a7" title="Introducing A.I.T.E Ball" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a totally self contained (no internet) AI powered 8ball.&lt;/p&gt; &lt;p&gt;Its running on an Orange pi zero 2w, with whisper.cpp to do the text-2-speach, and llama.cpp to do the llm thing, Its running Gemma 3 1b. About as much as I can do on this hardware. But even so.... :-) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonywestonuk"&gt; /u/tonywestonuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/scyofz31dx0f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kn542r/introducing_aite_ball/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-15T10:45:28+00:00</published>
  </entry>
</feed>
