<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-29T14:38:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ky0m1h</id>
    <title>Researchers from the National University of Singapore Introduce ‘Thinkless,’ an Adaptive Framework that Reduces Unnecessary Reasoning by up to 90% Using DeGRPO</title>
    <updated>2025-05-29T03:19:13+00:00</updated>
    <author>
      <name>/u/Sporeboss</name>
      <uri>https://old.reddit.com/user/Sporeboss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky0m1h/researchers_from_the_national_university_of/"&gt; &lt;img alt="Researchers from the National University of Singapore Introduce ‘Thinkless,’ an Adaptive Framework that Reduces Unnecessary Reasoning by up to 90% Using DeGRPO" src="https://external-preview.redd.it/mL1Q9XttYG1PHtNLMEZMLd8H7Fxw3YewxUxLIUBjgOg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c1cd9d5305abc8c47989fcd365fe35c711f0ca1" title="Researchers from the National University of Singapore Introduce ‘Thinkless,’ an Adaptive Framework that Reduces Unnecessary Reasoning by up to 90% Using DeGRPO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sporeboss"&gt; /u/Sporeboss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/VainF/Thinkless"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky0m1h/researchers_from_the_national_university_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky0m1h/researchers_from_the_national_university_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T03:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1kybtri</id>
    <title>Small open models are more cost effective than closed ones (score from artifical analysis).</title>
    <updated>2025-05-29T14:12:37+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kybtri/small_open_models_are_more_cost_effective_than/"&gt; &lt;img alt="Small open models are more cost effective than closed ones (score from artifical analysis)." src="https://preview.redd.it/hwn90facbq3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e7240aaf83de86ee6bec64e336e5d5e8f0b8703" title="Small open models are more cost effective than closed ones (score from artifical analysis)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sampled only the most cost efficient models that were above a score threshold. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hwn90facbq3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kybtri/small_open_models_are_more_cost_effective_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kybtri/small_open_models_are_more_cost_effective_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T14:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky7diy</id>
    <title>2x Instinct MI50 32G running vLLM results</title>
    <updated>2025-05-29T10:28:37+00:00</updated>
    <author>
      <name>/u/NaLanZeYu</name>
      <uri>https://old.reddit.com/user/NaLanZeYu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I picked up these two AMD Instinct MI50 32G cards from a second-hand trading platform in China. Each card cost me 780 CNY, plus an additional 30 CNY for shipping. I also grabbed two cooling fans to go with them, each costing 40 CNY. In total, I spent 1730 CNY, which is approximately 230 USD.&lt;/p&gt; &lt;p&gt;Even though it’s a second-hand trading platform, the seller claimed they were brand new. Three days after I paid, the cards arrived at my doorstep. Sure enough, they looked untouched, just like the seller promised.&lt;/p&gt; &lt;p&gt;The MI50 cards can’t output video (even though they have a miniDP port). To use them, I had to disable CSM completely in the motherboard BIOS and enable the Above 4G decoding option.&lt;/p&gt; &lt;h2&gt;System Setup&lt;/h2&gt; &lt;h3&gt;Hardware Setup&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Intel Xeon E5-2666V3&lt;/li&gt; &lt;li&gt;RDIMM DDR3 1333 32GB*4&lt;/li&gt; &lt;li&gt;JGINYUE X99 TI PLUS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;One MI50 is plugged into a PCIe 3.0 x16 slot, and the other is in a PCIe 3.0 x8 slot. There’s no Infinity Fabric Link between the two cards.&lt;/p&gt; &lt;h3&gt;Software Setup&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;PVE 8.4.1 (Linux kernel 6.8)&lt;/li&gt; &lt;li&gt;Ubuntu 24.04 (LXC container)&lt;/li&gt; &lt;li&gt;ROCm 6.3&lt;/li&gt; &lt;li&gt;vLLM 0.9.0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The vLLM I used is a modified version. The official vLLM support on AMD platforms has some issues. GGUF, GPTQ, and AWQ all have problems.&lt;/p&gt; &lt;h3&gt;vllm serv Parameters&lt;/h3&gt; &lt;p&gt;&lt;code&gt;sh docker run -it --rm --shm-size=2g --device=/dev/kfd --device=/dev/dri \ --group-add video -p 8000:8000 -v /mnt:/mnt nalanzeyu/vllm-gfx906:v0.9.0-rocm6.3 \ vllm serve --max-model-len 8192 --disable-log-requests --dtype float16 \ /mnt/&amp;lt;MODEL_PATH&amp;gt; -tp 2 &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;vllm bench Parameters&lt;/h3&gt; &lt;p&gt;```sh&lt;/p&gt; &lt;h1&gt;for decode&lt;/h1&gt; &lt;p&gt;vllm bench serve \ --model /mnt/&amp;lt;MODEL_PATH&amp;gt; \ --num-prompts 8 \ --random-input-len 1 \ --random-output-len 256 \ --ignore-eos \ --max-concurrency &amp;lt;CONCURRENCY&amp;gt;&lt;/p&gt; &lt;h1&gt;for prefill&lt;/h1&gt; &lt;p&gt;vllm bench serve \ --model /mnt/&amp;lt;MODEL_PATH&amp;gt; \ --num-prompts 8 \ --random-input-len 4096 \ --random-output-len 1 \ --ignore-eos \ --max-concurrency 1 ```&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;h3&gt;~70B 4-bit&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;B&lt;/th&gt; &lt;th align="right"&gt;1x Concurrency&lt;/th&gt; &lt;th align="right"&gt;2x Concurrency&lt;/th&gt; &lt;th align="right"&gt;4x Concurrency&lt;/th&gt; &lt;th align="right"&gt;8x Concurrency&lt;/th&gt; &lt;th align="right"&gt;Prefill&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5&lt;/td&gt; &lt;td&gt;72B GPTQ&lt;/td&gt; &lt;td align="right"&gt;17.77 t/s&lt;/td&gt; &lt;td align="right"&gt;33.53 t/s&lt;/td&gt; &lt;td align="right"&gt;57.47 t/s&lt;/td&gt; &lt;td align="right"&gt;53.38 t/s&lt;/td&gt; &lt;td align="right"&gt;159.66 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama 3.3&lt;/td&gt; &lt;td&gt;70B GPTQ&lt;/td&gt; &lt;td align="right"&gt;18.62 t/s&lt;/td&gt; &lt;td align="right"&gt;35.13 t/s&lt;/td&gt; &lt;td align="right"&gt;59.66 t/s&lt;/td&gt; &lt;td align="right"&gt;54.33 t/s&lt;/td&gt; &lt;td align="right"&gt;156.38 t/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;~30B 4-bit&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;B&lt;/th&gt; &lt;th align="right"&gt;1x Concurrency&lt;/th&gt; &lt;th align="right"&gt;2x Concurrency&lt;/th&gt; &lt;th align="right"&gt;4x Concurrency&lt;/th&gt; &lt;th align="right"&gt;8x Concurrency&lt;/th&gt; &lt;th align="right"&gt;Prefill&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;32B AWQ&lt;/td&gt; &lt;td align="right"&gt;27.58 t/s&lt;/td&gt; &lt;td align="right"&gt;49.27 t/s&lt;/td&gt; &lt;td align="right"&gt;87.07 t/s&lt;/td&gt; &lt;td align="right"&gt;96.61 t/s&lt;/td&gt; &lt;td align="right"&gt;293.37 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-Coder&lt;/td&gt; &lt;td&gt;32B AWQ&lt;/td&gt; &lt;td align="right"&gt;27.95 t/s&lt;/td&gt; &lt;td align="right"&gt;51.33 t/s&lt;/td&gt; &lt;td align="right"&gt;88.72 t/s&lt;/td&gt; &lt;td align="right"&gt;98.28 t/s&lt;/td&gt; &lt;td align="right"&gt;329.92 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM 4 0414&lt;/td&gt; &lt;td&gt;32B GPTQ&lt;/td&gt; &lt;td align="right"&gt;29.34 t/s&lt;/td&gt; &lt;td align="right"&gt;52.21 t/s&lt;/td&gt; &lt;td align="right"&gt;91.29 t/s&lt;/td&gt; &lt;td align="right"&gt;95.02 t/s&lt;/td&gt; &lt;td align="right"&gt;313.51 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Mistral Small 2501&lt;/td&gt; &lt;td&gt;24B AWQ&lt;/td&gt; &lt;td align="right"&gt;39.54 t/s&lt;/td&gt; &lt;td align="right"&gt;71.09 t/s&lt;/td&gt; &lt;td align="right"&gt;118.72 t/s&lt;/td&gt; &lt;td align="right"&gt;133.64 t/s&lt;/td&gt; &lt;td align="right"&gt;433.95 t/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;~30B 8-bit&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;B&lt;/th&gt; &lt;th align="right"&gt;1x Concurrency&lt;/th&gt; &lt;th align="right"&gt;2x Concurrency&lt;/th&gt; &lt;th align="right"&gt;4x Concurrency&lt;/th&gt; &lt;th align="right"&gt;8x Concurrency&lt;/th&gt; &lt;th align="right"&gt;Prefill&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;32B GPTQ&lt;/td&gt; &lt;td align="right"&gt;22.88 t/s&lt;/td&gt; &lt;td align="right"&gt;38.20 t/s&lt;/td&gt; &lt;td align="right"&gt;58.03 t/s&lt;/td&gt; &lt;td align="right"&gt;44.55 t/s&lt;/td&gt; &lt;td align="right"&gt;291.56 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen2.5-Coder&lt;/td&gt; &lt;td&gt;32B GPTQ&lt;/td&gt; &lt;td align="right"&gt;23.66 t/s&lt;/td&gt; &lt;td align="right"&gt;40.13 t/s&lt;/td&gt; &lt;td align="right"&gt;60.19 t/s&lt;/td&gt; &lt;td align="right"&gt;46.18 t/s&lt;/td&gt; &lt;td align="right"&gt;327.23 t/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NaLanZeYu"&gt; /u/NaLanZeYu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky7diy/2x_instinct_mi50_32g_running_vllm_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky7diy/2x_instinct_mi50_32g_running_vllm_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky7diy/2x_instinct_mi50_32g_running_vllm_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T10:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky8cby</id>
    <title>SWE-rebench: Over 21,000 Open Tasks for SWE LLMs</title>
    <updated>2025-05-29T11:25:45+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8cby/swerebench_over_21000_open_tasks_for_swe_llms/"&gt; &lt;img alt="SWE-rebench: Over 21,000 Open Tasks for SWE LLMs" src="https://external-preview.redd.it/Wlax0ZVaKn93EpAcdC0XPmjtaNO0SAghfCC94lLHq10.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45e56ad4265cb4302e58c64224d67824529eff4b" title="SWE-rebench: Over 21,000 Open Tasks for SWE LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! We just released SWE-rebench – an extended and improved version of our previous dataset with GitHub issue-solving tasks.&lt;/p&gt; &lt;p&gt;One common limitation in such datasets is that they usually don’t have many tasks, and they come from only a small number of repositories. For example, in the original SWE-bench there are 2,000+ tasks from just 18 repos. This mostly happens because researchers install each project manually and then collect the tasks.&lt;/p&gt; &lt;p&gt;We automated and scaled this process, so we were able to collect 21,000+ tasks from over 3,400 repositories.&lt;/p&gt; &lt;p&gt;You can find the full technical report &lt;a href="https://huggingface.co/papers/2505.20411"&gt;here&lt;/a&gt;. We also used a subset of this dataset to build our &lt;a href="https://swe-rebench.com/leaderboard"&gt;SWE-rebench leaderboard.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/nebius/SWE-rebench"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8cby/swerebench_over_21000_open_tasks_for_swe_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8cby/swerebench_over_21000_open_tasks_for_swe_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T11:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky9dbd</id>
    <title>🔍 DeepSeek-R1-0528: Open-Source Reasoning Model Catching Up to O3 &amp; Gemini?</title>
    <updated>2025-05-29T12:20:27+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky9dbd/deepseekr10528_opensource_reasoning_model/"&gt; &lt;img alt="🔍 DeepSeek-R1-0528: Open-Source Reasoning Model Catching Up to O3 &amp;amp; Gemini?" src="https://b.thumbs.redditmedia.com/6fAw_e6Y665sleg1Wjj2n7G5znA4JyHNE1kJIkixWJE.jpg" title="🔍 DeepSeek-R1-0528: Open-Source Reasoning Model Catching Up to O3 &amp;amp; Gemini?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek just released an updated version of its reasoning model: &lt;strong&gt;DeepSeek-R1-0528&lt;/strong&gt;, and it's getting &lt;em&gt;very&lt;/em&gt; close to the top proprietary models like OpenAI's O3 and Google’s Gemini 2.5 Pro—while remaining completely open-source.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bw6qw038rp3f1.png?width=3961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4399b2c6fa184d68de8dfedd4ed84c529d9033a2"&gt;https://preview.redd.it/bw6qw038rp3f1.png?width=3961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4399b2c6fa184d68de8dfedd4ed84c529d9033a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🧠 &lt;strong&gt;What’s New in R1-0528?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Major gains in reasoning depth &amp;amp; inference.&lt;/li&gt; &lt;li&gt;AIME 2025 accuracy jumped from &lt;strong&gt;70% → 87.5%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Reasoning now uses &lt;strong&gt;~23K tokens per question&lt;/strong&gt; on average (previously ~12K).&lt;/li&gt; &lt;li&gt;Reduced hallucinations, improved function calling, and better &amp;quot;vibe coding&amp;quot; UX.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;📊 &lt;strong&gt;How does it stack up?&lt;/strong&gt;&lt;br /&gt; Here’s how DeepSeek-R1-0528 (and its distilled variant) compare to other models:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;DeepSeek-R1-0528&lt;/th&gt; &lt;th align="left"&gt;o3-mini&lt;/th&gt; &lt;th align="left"&gt;Gemini 2.5&lt;/th&gt; &lt;th align="left"&gt;Qwen3-235B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;AIME 2025&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;87.5&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;76.7&lt;/td&gt; &lt;td align="left"&gt;72.0&lt;/td&gt; &lt;td align="left"&gt;81.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LiveCodeBench&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;73.3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;65.9&lt;/td&gt; &lt;td align="left"&gt;62.3&lt;/td&gt; &lt;td align="left"&gt;66.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;HMMT Feb 25&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;79.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;53.3&lt;/td&gt; &lt;td align="left"&gt;64.2&lt;/td&gt; &lt;td align="left"&gt;62.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GPQA-Diamond&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;81.0&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;76.8&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;82.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;71.1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;📌 &lt;strong&gt;Why it matters:&lt;/strong&gt;&lt;br /&gt; This update shows DeepSeek closing the gap on state-of-the-art models in math, logic, and code—all in an open-source release. It’s also practical to run locally (check Unsloth for quantized versions), and DeepSeek now supports system prompts and smoother chain-of-thought inference without hacks.&lt;/p&gt; &lt;p&gt;🧪 Try it: &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;huggingface.co/deepseek-ai/DeepSeek-R1-0528&lt;/a&gt;&lt;br /&gt; 🌐 Demo: &lt;a href="https://chat.deepseek.com"&gt;chat.deepseek.com&lt;/a&gt; (toggle “DeepThink”)&lt;br /&gt; 🧠 API: &lt;a href="https://platform.deepseek.com"&gt;platform.deepseek.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky9dbd/deepseekr10528_opensource_reasoning_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky9dbd/deepseekr10528_opensource_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky9dbd/deepseekr10528_opensource_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T12:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxvaq2</id>
    <title>New Deepseek R1's long context results</title>
    <updated>2025-05-28T23:01:16+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxvaq2/new_deepseek_r1s_long_context_results/"&gt; &lt;img alt="New Deepseek R1's long context results" src="https://preview.redd.it/n3mjiheosl3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3533edf28a60d70e6e211ebf6d0ad8a8bf58596d" title="New Deepseek R1's long context results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n3mjiheosl3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxvaq2/new_deepseek_r1s_long_context_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxvaq2/new_deepseek_r1s_long_context_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T23:01:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxoco5</id>
    <title>Chatterbox TTS 0.5B - Claims to beat eleven labs</title>
    <updated>2025-05-28T18:19:08+00:00</updated>
    <author>
      <name>/u/Du_Hello</name>
      <uri>https://old.reddit.com/user/Du_Hello</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxoco5/chatterbox_tts_05b_claims_to_beat_eleven_labs/"&gt; &lt;img alt="Chatterbox TTS 0.5B - Claims to beat eleven labs" src="https://external-preview.redd.it/dmdxNW5pN3JjazNmMWJmZsSyJYSsSqC3nLOUAsuog5kud_cYJD6JARLf_51k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24532c5a7bb011896aee76e14448d8f94796530b" title="Chatterbox TTS 0.5B - Claims to beat eleven labs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Du_Hello"&gt; /u/Du_Hello &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/i6nfhj7rck3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxoco5/chatterbox_tts_05b_claims_to_beat_eleven_labs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxoco5/chatterbox_tts_05b_claims_to_beat_eleven_labs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T18:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxnjrj</id>
    <title>DeepSeek-R1-0528 🔥</title>
    <updated>2025-05-28T17:47:44+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-0528&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnjrj/deepseekr10528/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnjrj/deepseekr10528/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnjrj/deepseekr10528/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T17:47:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky14jn</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-05-29T03:47:00+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with &lt;strong&gt;SurfSense&lt;/strong&gt;, it aims to be the open-source alternative to &lt;strong&gt;NotebookLM&lt;/strong&gt;, &lt;strong&gt;Perplexity&lt;/strong&gt;, or &lt;strong&gt;Glean&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent but connected to your personal external sources search engines (Tavily, LinkUp), Slack, Linear, Notion, YouTube, GitHub, and more coming soon.&lt;/p&gt; &lt;p&gt;I'll keep this short—here are a few highlights of SurfSense:&lt;/p&gt; &lt;p&gt;📊 Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports &lt;strong&gt;150+ LLM's&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports local &lt;strong&gt;Ollama LLM's&lt;/strong&gt; or vLLM.&lt;/li&gt; &lt;li&gt;Supports &lt;strong&gt;6000+ Embedding Models&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;Hierarchical Indices&lt;/strong&gt; (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines &lt;strong&gt;Semantic + Full-Text Search&lt;/strong&gt; with &lt;strong&gt;Reciprocal Rank Fusion&lt;/strong&gt; (Hybrid Search)&lt;/li&gt; &lt;li&gt;Offers a &lt;strong&gt;RAG-as-a-Service API Backend&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Supports 34+ File extensions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🎙️ Podcasts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ℹ️ &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;YouTube videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;...and more on the way&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;🔖 &lt;strong&gt;Cross-Browser Exten&lt;/strong&gt;sion&lt;br /&gt; The SurfSense extension lets you save any dynamic webpage you like. Its main use case is capturing pages that are protected behind authentication.&lt;/p&gt; &lt;p&gt;Check out SurfSense on GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky14jn/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky14jn/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky14jn/open_source_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T03:47:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky1l2e</id>
    <title>Yess! Open-source strikes back! This is the closest I've seen anything come to competing with @GoogleDeepMind 's Veo 3 native audio and character motion.</title>
    <updated>2025-05-29T04:12:04+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky1l2e/yess_opensource_strikes_back_this_is_the_closest/"&gt; &lt;img alt="Yess! Open-source strikes back! This is the closest I've seen anything come to competing with @GoogleDeepMind 's Veo 3 native audio and character motion." src="https://external-preview.redd.it/N2Rjc29reDZjbjNmMRt9-ucm3ui6hV8RM7PxWBbM61rS9Zbqewi5gNGLcurK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40159f5e7ec2d116cf44e6a36f367f3d4c1ef9ae" title="Yess! Open-source strikes back! This is the closest I've seen anything come to competing with @GoogleDeepMind 's Veo 3 native audio and character motion." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wvb8a5b5cn3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky1l2e/yess_opensource_strikes_back_this_is_the_closest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky1l2e/yess_opensource_strikes_back_this_is_the_closest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T04:12:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxybgo</id>
    <title>Deepseek R1.1 aider polyglot score</title>
    <updated>2025-05-29T01:22:54+00:00</updated>
    <author>
      <name>/u/Ambitious_Subject108</name>
      <uri>https://old.reddit.com/user/Ambitious_Subject108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek R1.1 scored the same as claude-opus-4-nothink 70.7% on aider polyglot.&lt;/p&gt; &lt;p&gt;Old R1 was 56.9%&lt;/p&gt; &lt;p&gt;&lt;code&gt; ────────────────────────────────── tmp.benchmarks/2025-05-28-18-57-01--deepseek-r1-0528 ────────────────────────────────── - dirname: 2025-05-28-18-57-01--deepseek-r1-0528 test_cases: 225 model: deepseek/deepseek-reasoner edit_format: diff commit_hash: 119a44d, 443e210-dirty pass_rate_1: 35.6 pass_rate_2: 70.7 pass_num_1: 80 pass_num_2: 159 percent_cases_well_formed: 90.2 error_outputs: 51 num_malformed_responses: 33 num_with_malformed_responses: 22 user_asks: 111 lazy_comments: 1 syntax_errors: 0 indentation_errors: 0 exhausted_context_windows: 0 prompt_tokens: 3218121 completion_tokens: 1906344 test_timeouts: 3 total_tests: 225 command: aider --model deepseek/deepseek-reasoner date: 2025-05-28 versions: 0.83.3.dev seconds_per_case: 566.2 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cost came out to $3.05, but this is off time pricing, peak time is $12.20&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Subject108"&gt; /u/Ambitious_Subject108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxybgo/deepseek_r11_aider_polyglot_score/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxybgo/deepseek_r11_aider_polyglot_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxybgo/deepseek_r11_aider_polyglot_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T01:22:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxw6b9</id>
    <title>Nvidia CEO says that Huawei's chip is comparable to Nvidia's H200.</title>
    <updated>2025-05-28T23:40:56+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On a interview with Bloomberg today, Jensen came out and said that Huawei's offering is as good as the Nvidia H200. Which kind of surprised me. Both that he just came out and said it and that it's so good. Since I thought it was only as good as the H100. But if anyone knows, Jensen would know.&lt;/p&gt; &lt;p&gt;Update: Here's the interview.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=c-XAL2oYelI"&gt;https://www.youtube.com/watch?v=c-XAL2oYelI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxw6b9/nvidia_ceo_says_that_huaweis_chip_is_comparable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxw6b9/nvidia_ceo_says_that_huaweis_chip_is_comparable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxw6b9/nvidia_ceo_says_that_huaweis_chip_is_comparable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T23:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxnggx</id>
    <title>deepseek-ai/DeepSeek-R1-0528</title>
    <updated>2025-05-28T17:44:07+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;deepseek-ai/DeepSeek-R1-0528&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnggx/deepseekaideepseekr10528/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnggx/deepseekaideepseekr10528/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxnggx/deepseekaideepseekr10528/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T17:44:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky847t</id>
    <title>Another benchmark result is in for Deepseek r1.1: big jump in nyt word connections</title>
    <updated>2025-05-29T11:13:10+00:00</updated>
    <author>
      <name>/u/_Nils-</name>
      <uri>https://old.reddit.com/user/_Nils-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky847t/another_benchmark_result_is_in_for_deepseek_r11/"&gt; &lt;img alt="Another benchmark result is in for Deepseek r1.1: big jump in nyt word connections" src="https://preview.redd.it/h9qjhjmbfp3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c435c0734d4925ffa167706d837e08be7b2a0870" title="Another benchmark result is in for Deepseek r1.1: big jump in nyt word connections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Nils-"&gt; /u/_Nils- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h9qjhjmbfp3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky847t/another_benchmark_result_is_in_for_deepseek_r11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky847t/another_benchmark_result_is_in_for_deepseek_r11/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T11:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxry4x</id>
    <title>New Upgraded Deepseek R1 is now almost on par with OpenAI's O3 High model on LiveCodeBench! Huge win for opensource!</title>
    <updated>2025-05-28T20:41:49+00:00</updated>
    <author>
      <name>/u/Gloomy-Signature297</name>
      <uri>https://old.reddit.com/user/Gloomy-Signature297</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxry4x/new_upgraded_deepseek_r1_is_now_almost_on_par/"&gt; &lt;img alt="New Upgraded Deepseek R1 is now almost on par with OpenAI's O3 High model on LiveCodeBench! Huge win for opensource!" src="https://preview.redd.it/51sg1oyu3l3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=754869c742901ed36f078e402c4f0da41133524e" title="New Upgraded Deepseek R1 is now almost on par with OpenAI's O3 High model on LiveCodeBench! Huge win for opensource!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gloomy-Signature297"&gt; /u/Gloomy-Signature297 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/51sg1oyu3l3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxry4x/new_upgraded_deepseek_r1_is_now_almost_on_par/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxry4x/new_upgraded_deepseek_r1_is_now_almost_on_par/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T20:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyakcp</id>
    <title>DeepSeek-R1-0528 distill on Qwen3 8B</title>
    <updated>2025-05-29T13:17:51+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyakcp/deepseekr10528_distill_on_qwen3_8b/"&gt; &lt;img alt="DeepSeek-R1-0528 distill on Qwen3 8B" src="https://preview.redd.it/nrkr44ek1q3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7ae61c0111aa5a48e0895ada14976d096d88746" title="DeepSeek-R1-0528 distill on Qwen3 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nrkr44ek1q3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyakcp/deepseekr10528_distill_on_qwen3_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyakcp/deepseekr10528_distill_on_qwen3_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxs47i</id>
    <title>DeepSeek: R1 0528 is lethal</title>
    <updated>2025-05-28T20:48:14+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just used DeepSeek: R1 0528 to address several ongoing coding challenges in RooCode. &lt;/p&gt; &lt;p&gt;This model performed exceptionally well, resolving all issues seamlessly. I hit up DeepSeek via OpenRouter, and the results were DAMN impressive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxs47i/deepseek_r1_0528_is_lethal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxs47i/deepseek_r1_0528_is_lethal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxs47i/deepseek_r1_0528_is_lethal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-28T20:48:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky6hxy</id>
    <title>MNN is quite something, Qwen3-32B on a OnePlus 13 24GB</title>
    <updated>2025-05-29T09:32:44+00:00</updated>
    <author>
      <name>/u/VickWildman</name>
      <uri>https://old.reddit.com/user/VickWildman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky6hxy/mnn_is_quite_something_qwen332b_on_a_oneplus_13/"&gt; &lt;img alt="MNN is quite something, Qwen3-32B on a OnePlus 13 24GB" src="https://preview.redd.it/432wqex5vo3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5718621f13c3fd8412aaeb9d5f7bca2fe1dfa8d3" title="MNN is quite something, Qwen3-32B on a OnePlus 13 24GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the settings for the model mmap needs to be enabled for this to not crash. It's not that fast, but works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VickWildman"&gt; /u/VickWildman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/432wqex5vo3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky6hxy/mnn_is_quite_something_qwen332b_on_a_oneplus_13/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky6hxy/mnn_is_quite_something_qwen332b_on_a_oneplus_13/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T09:32:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kya3c2</id>
    <title>Deepseek R1.1 dominates gemini 2.5 flash on price vs performance</title>
    <updated>2025-05-29T12:56:01+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"&gt; &lt;img alt="Deepseek R1.1 dominates gemini 2.5 flash on price vs performance" src="https://b.thumbs.redditmedia.com/8YFi4IXtXKswD8wkWDjiNDtDIgia7lDEYcmacZzlgVk.jpg" title="Deepseek R1.1 dominates gemini 2.5 flash on price vs performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/di952wumxp3f1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f234fe0c11a5f42bd407698bf0640a3d3d9b18fa"&gt;https://preview.redd.it/di952wumxp3f1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f234fe0c11a5f42bd407698bf0640a3d3d9b18fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: Artifical Analysis&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T12:56:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyap9q</id>
    <title>deepseek-ai/DeepSeek-R1-0528-Qwen3-8B · Hugging Face</title>
    <updated>2025-05-29T13:24:05+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyap9q/deepseekaideepseekr10528qwen38b_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B · Hugging Face" src="https://external-preview.redd.it/8hRwXI0dhC0uoSc2zQ6TvHX1Aw9zshcTMnuDtSCd7AY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdf654415d883f00f7930d8548353332a4e97f3a" title="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyap9q/deepseekaideepseekr10528qwen38b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyap9q/deepseekaideepseekr10528qwen38b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxxmdr</id>
    <title>DeepSeek R1 05 28 Tested. It finally happened. The ONLY model to score 100% on everything I threw at it.</title>
    <updated>2025-05-29T00:48:50+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ladies and gentlemen, It finally happened. &lt;/p&gt; &lt;p&gt;I knew this day was coming. I knew that one day, a model would come along that would be able to score a 100% on every single task I throw at it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=4CXkmFbgV28"&gt;https://www.youtube.com/watch?v=4CXkmFbgV28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Past few weeks have been busy - OpenAI 4.1, Gemini 2.5, Claude 4 - They all did very well, but none were able to score a perfect 100% across every single test. DeepSeek R1 05 28 is the FIRST model ever to do this. &lt;/p&gt; &lt;p&gt;And mind you, these aren't impractical tests like you see many folks on youtube doing. Like number of rs in strawberry or write a snake game etc. These are tasks that we actively use in real business applications, and from those, we chose the edge cases on the more complex side of things. &lt;/p&gt; &lt;p&gt;I feel like I am Anton from Ratatouille (if you have seen the movie). I am deeply impressed (pun intended) but also a little bit numb, and having a hard time coming up with the right words. That a free, MIT licensed model from a largely unknown lab until last year has done better than the commercial frontier is wild.&lt;/p&gt; &lt;p&gt;Usually in my videos, I explain the test, and then talk about the mistakes the models are making. But today, since there ARE NO mistakes, I am going to do something different. For each test, i am going to show you a couple of examples of the model's responses - and how hard these questions are, and I hope that gives you a deep sense of appreciation of what a powerful model this is. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T00:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyac9f</id>
    <title>New DeepSeek R1 8B Distill that's "matching the performance of Qwen3-235B-thinking" may be incoming!</title>
    <updated>2025-05-29T13:07:22+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyac9f/new_deepseek_r1_8b_distill_thats_matching_the/"&gt; &lt;img alt="New DeepSeek R1 8B Distill that's &amp;quot;matching the performance of Qwen3-235B-thinking&amp;quot; may be incoming!" src="https://preview.redd.it/8vwdjpcxyp3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16361f0824e9b22cc2a7a8bb532724773abb7a72" title="New DeepSeek R1 8B Distill that's &amp;quot;matching the performance of Qwen3-235B-thinking&amp;quot; may be incoming!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-R1-0528-Qwen3-8B incoming? Oh yeah, gimme that, thank you! 😂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8vwdjpcxyp3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyac9f/new_deepseek_r1_8b_distill_thats_matching_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyac9f/new_deepseek_r1_8b_distill_thats_matching_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:07:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kya8kq</id>
    <title>DeepSeek-R1-0528 Official Benchmark</title>
    <updated>2025-05-29T13:02:45+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya8kq/deepseekr10528_official_benchmark/"&gt; &lt;img alt="DeepSeek-R1-0528 Official Benchmark" src="https://preview.redd.it/ph8ccp8vyp3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a26aecb4cde21d947b429d105d49de5b484adce2" title="DeepSeek-R1-0528 Official Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source：&lt;a href="https://mp.weixin.qq.com/s/U5fnTRW4cGvXYJER__YBiw"&gt;https://mp.weixin.qq.com/s/U5fnTRW4cGvXYJER__YBiw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ph8ccp8vyp3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya8kq/deepseekr10528_official_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kya8kq/deepseekr10528_official_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:02:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky54kq</id>
    <title>PLEASE LEARN BASIC CYBERSECURITY</title>
    <updated>2025-05-29T07:59:20+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stumbled across a project doing about $30k a month with their OpenAI API key exposed in the frontend.&lt;/p&gt; &lt;p&gt;Public key, no restrictions, fully usable by anyone.&lt;/p&gt; &lt;p&gt;At that volume someone could easily burn through thousands before it even shows up on a billing alert.&lt;/p&gt; &lt;p&gt;This kind of stuff doesn’t happen because people are careless. It happens because things feel like they’re working, so you keep shipping without stopping to think through the basics.&lt;/p&gt; &lt;p&gt;Vibe coding is fun when you’re moving fast. But it’s not so fun when it costs you money, data, or trust.&lt;/p&gt; &lt;p&gt;Add just enough structure to keep things safe. That’s it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky54kq/please_learn_basic_cybersecurity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky54kq/please_learn_basic_cybersecurity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky54kq/please_learn_basic_cybersecurity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T07:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky8vlm</id>
    <title>DeepSeek-R1-0528 Official Benchmarks Released!!!</title>
    <updated>2025-05-29T11:55:06+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8vlm/deepseekr10528_official_benchmarks_released/"&gt; &lt;img alt="DeepSeek-R1-0528 Official Benchmarks Released!!!" src="https://external-preview.redd.it/G2g_zbuPp_sknOUdQv6ufEg8e0xJC81xbpHlzy2plQU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2851bfb3532bcd96cf4e16cbef4ae32c4943a665" title="DeepSeek-R1-0528 Official Benchmarks Released!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8vlm/deepseekr10528_official_benchmarks_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8vlm/deepseekr10528_official_benchmarks_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T11:55:06+00:00</published>
  </entry>
</feed>
