<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-04T03:50:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1j2fgz6</id>
    <title>🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!</title>
    <updated>2025-03-03T10:10:03+00:00</updated>
    <author>
      <name>/u/vel_is_lava</name>
      <uri>https://old.reddit.com/user/vel_is_lava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt; &lt;img alt="🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!" src="https://external-preview.redd.it/cnVmdWJ2ZHE3Z21lMUL3NkVhJ5GHYq-Z21ncNiL8qoaSdn9KQniKMkRjA96Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3003596a3a1fa062c0f796fad4c45be6460d38cc" title="🚀 Collate: Offline, Llama 3.2-Powered PDF Assistant for Mac! 🚀 Your Help Needed!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vel_is_lava"&gt; /u/vel_is_lava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bmyoyudq7gme1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2fgz6/collate_offline_llama_32powered_pdf_assistant_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T10:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2r244</id>
    <title>What's the best 3d game engine/local AI, pair-up?</title>
    <updated>2025-03-03T19:27:23+00:00</updated>
    <author>
      <name>/u/Musenik</name>
      <uri>https://old.reddit.com/user/Musenik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By 'best', I mean the pairing of coding model to 3d engine, so that the least mistakes will be made by the AI.&lt;/p&gt; &lt;p&gt;Other factors (for my personal use): can run on a 96GB Macintosh. And the engine can build to lots of platforms. Engine efficiency doesn't need to be the best. For example: Panda3D looks like a great choice due to it's python scripting, but it's not so popular that an AI would very familiar with it.&lt;/p&gt; &lt;p&gt;I hope this thread gets plenty of answers that help other game devs to build their brilliant ideas more efficiently. I guess my question should actually be, what are 'good' pairings of game engine to local AI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Musenik"&gt; /u/Musenik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2r244/whats_the_best_3d_game_enginelocal_ai_pairup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2r244/whats_the_best_3d_game_enginelocal_ai_pairup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2r244/whats_the_best_3d_game_enginelocal_ai_pairup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T19:27:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1j1swtj</id>
    <title>Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!</title>
    <updated>2025-03-02T15:09:11+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt; &lt;img alt="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" src="https://preview.redd.it/04kvczd6lame1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=beb71d99ece65072d973eb96bdaf1ed1261f7956" title="Vulkan is getting really close! Now let's ditch CUDA and godforsaken ROCm!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/04kvczd6lame1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-02T15:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2zldc</id>
    <title>Something I've been working on for a few days, anyone got any thoughts? It's an AI interface that contextually understands commands to create widgets. Sorry the video is so quiet, I'm... bad at recording things.</title>
    <updated>2025-03-04T01:47:35+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2zldc/something_ive_been_working_on_for_a_few_days/"&gt; &lt;img alt="Something I've been working on for a few days, anyone got any thoughts? It's an AI interface that contextually understands commands to create widgets. Sorry the video is so quiet, I'm... bad at recording things." src="https://external-preview.redd.it/8dC9SkYzlCiNW2dEVau1sku1zjLtbAer9Eo4qSA4_lM.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51918c26c58f60b74bbe369e2e83e4a70591c3e2" title="Something I've been working on for a few days, anyone got any thoughts? It's an AI interface that contextually understands commands to create widgets. Sorry the video is so quiet, I'm... bad at recording things." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=Pbrx8jkL6Vs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2zldc/something_ive_been_working_on_for_a_few_days/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2zldc/something_ive_been_working_on_for_a_few_days/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T01:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2tmr5</id>
    <title>What's your go-to method for generating markdown from HTML?</title>
    <updated>2025-03-03T21:14:05+00:00</updated>
    <author>
      <name>/u/-Django</name>
      <uri>https://old.reddit.com/user/-Django</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to feed some news article data into an LLM. It seems like there's a hundred libraries to convert HTML to markdown. Some use LLMs, some use deterministic algorithms, I don't know what I should use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Django"&gt; /u/-Django &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tmr5/whats_your_goto_method_for_generating_markdown/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tmr5/whats_your_goto_method_for_generating_markdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tmr5/whats_your_goto_method_for_generating_markdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T21:14:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1j30cnp</id>
    <title>Qwen2.5 32b sometimes will put tool calls in the content instead of the tool_calls</title>
    <updated>2025-03-04T02:26:53+00:00</updated>
    <author>
      <name>/u/nstevnc77</name>
      <uri>https://old.reddit.com/user/nstevnc77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building a small application with Ollama for personal use that involves tool calling. I've been really impressed with Qwen2.5's ability to figure out when to do tool calls, which tools to use, and its overall reliability.&lt;/p&gt; &lt;p&gt;The only problem I've been running into is that Qwen2.5 will start putting its tool calls (JSON) in the content instead of the proper tool_calls part of the JSON. This is frustrating because it works so well otherwise.&lt;/p&gt; &lt;p&gt;It always seems to get the tool calls correct in the beginning, but about 20-40 messages in, it just starts putting the JSON in the content. Has anyone found a solution to this issue? I'm thinking that maybe because I'm saving those tool call messages in its list of messages or I'm adding &amp;quot;toolresult&amp;quot; responses that maybe it's getting confused?&lt;/p&gt; &lt;p&gt;Just wanted to see if anybody has had a similar experience!&lt;/p&gt; &lt;p&gt;I've tried llama models but they will ALWAYS call tools given the chance. Not very useful for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nstevnc77"&gt; /u/nstevnc77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j30cnp/qwen25_32b_sometimes_will_put_tool_calls_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j30cnp/qwen25_32b_sometimes_will_put_tool_calls_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j30cnp/qwen25_32b_sometimes_will_put_tool_calls_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T02:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2wzky</id>
    <title>Which do you think is better: Deepseek-R1-32B vs QWQ-32b-preview?</title>
    <updated>2025-03-03T23:40:14+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using QWQ-32b-preview for a lot of different things, only to find out Qwen2.5-coder-32b provides comparable performance in math and coding.&lt;/p&gt; &lt;p&gt;This made me question whether QWQ really was an effective reasoning model, but my comparison will not be against Coder, but with Deepseek-r1-32B instead. Is the latter model any better than QWQ? I would really like to settle the matter.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wzky/which_do_you_think_is_better_deepseekr132b_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wzky/which_do_you_think_is_better_deepseekr132b_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wzky/which_do_you_think_is_better_deepseekr132b_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T23:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2rtfm</id>
    <title>What is the best local model that can “replicate” gpt4o responses naturally?</title>
    <updated>2025-03-03T19:58:51+00:00</updated>
    <author>
      <name>/u/No_Expert1801</name>
      <uri>https://old.reddit.com/user/No_Expert1801</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rtfm/what_is_the_best_local_model_that_can_replicate/"&gt; &lt;img alt="What is the best local model that can “replicate” gpt4o responses naturally?" src="https://preview.redd.it/r411lovs5jme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=276af561572f760eba7532962ba9b536ec699a7a" title="What is the best local model that can “replicate” gpt4o responses naturally?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like gpt4o is just so awesome. I love how it can show all these different types of writing styles &lt;/p&gt; &lt;p&gt;Like it’s not always “as your assistant, I’ll gladly help you” but it’s like really returning the energy. &lt;/p&gt; &lt;p&gt;Are there any cool local models that can do this well?&lt;/p&gt; &lt;p&gt;(16gb vram would be nice)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Expert1801"&gt; /u/No_Expert1801 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r411lovs5jme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rtfm/what_is_the_best_local_model_that_can_replicate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rtfm/what_is_the_best_local_model_that_can_replicate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T19:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2ugv2</id>
    <title>What can I run locally on i5-4690K + GTX 970 (4GB GDDR5) + 16GB DDR3 RAM? Where to get started?</title>
    <updated>2025-03-03T21:48:54+00:00</updated>
    <author>
      <name>/u/whoru07</name>
      <uri>https://old.reddit.com/user/whoru07</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've an old PC in storage with the following specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Intel i5-4690K&lt;/li&gt; &lt;li&gt; MSI GTX 970 (4GB GDDR5) GFX card&lt;/li&gt; &lt;li&gt; Gigabyte Z97X ATX&lt;/li&gt; &lt;li&gt; 16GB (2x 8GB) of DDR3 RAM&lt;/li&gt; &lt;li&gt; 500GB (2x Samsung 840 Evo 250GB 2.5” SATA 3.0) SSDs&lt;/li&gt; &lt;li&gt; 1TB HHD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What models I can run locally? Can I run multiple models? Any web UI recommendations? Would like to be able to access over internet with something like ngrok. Would it possible to have text-to-image and document analysis (PDF)?&lt;/p&gt; &lt;p&gt;TIA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whoru07"&gt; /u/whoru07 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ugv2/what_can_i_run_locally_on_i54690k_gtx_970_4gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ugv2/what_can_i_run_locally_on_i54690k_gtx_970_4gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2ugv2/what_can_i_run_locally_on_i54690k_gtx_970_4gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T21:48:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2rx6q</id>
    <title>Benchmarks &amp; power consumption: Ryzen 6-core + DDR5-6000 + GeForce 3060 12 GB</title>
    <updated>2025-03-03T20:02:46+00:00</updated>
    <author>
      <name>/u/BobTheNeuron</name>
      <uri>https://old.reddit.com/user/BobTheNeuron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rx6q/benchmarks_power_consumption_ryzen_6core_ddr56000/"&gt; &lt;img alt="Benchmarks &amp;amp; power consumption: Ryzen 6-core + DDR5-6000 + GeForce 3060 12 GB" src="https://external-preview.redd.it/s0D7i4Rco0trWh9Bu1uEkgnoJJLA3UNKUA9vs57seII.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b231518e5ed41e809cceeaa1c12bf32733c2345" title="Benchmarks &amp;amp; power consumption: Ryzen 6-core + DDR5-6000 + GeForce 3060 12 GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the first thing to do after building a new computer? Post benchmarks on Reddit, of course!&lt;/p&gt; &lt;p&gt;I hope this gives other local LLM noobs like me some pointers for building a machine for LLM.&lt;/p&gt; &lt;h1&gt;Specs&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Asus GeForce DUAL-RTX3060-O12G-V2 (12 GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 5 8500G (6 cores / 12 threads)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: DDR5 6000 MHz CL36 64 GB (32 GB + 32 GB) in dual channel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: MSI B850 GAMING PLUS WIFI AM5 (can run multiple GPUs if I ever want a multi-GPU setup)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At first I was thinking of just getting a Mac Mini, but I decided to do a custom build for customizability, longevity, upgradability and performance.&lt;/p&gt; &lt;h1&gt;llama.cpp setup&lt;/h1&gt; &lt;p&gt;I built &lt;code&gt;llama.cpp&lt;/code&gt; with two backends: CPU (for CPU-only inference) and CUDA (for GPU inference).&lt;/p&gt; &lt;p&gt;The &amp;quot;CPU&amp;quot; backend benchmark was run with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -B build cmake --build build --config Release # Automatically run with 6 CPU cores ./build/bin/llama-bench -m ./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &amp;quot;CUDA&amp;quot; backend benchmarks were run with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -B build -DGGML_CUDA=ON cmake --build build --config Release # Automatically run with GPU + 1 CPU core ./build/bin/llama-bench -m ./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf -ngl 99 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Both used llama.cpp build 06c2b156 (4794).&lt;/p&gt; &lt;h1&gt;Benchmarks &amp;amp; power consumption&lt;/h1&gt; &lt;p&gt;Also see the charts at the end of this post.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Layers on GPU (ngl)&lt;/th&gt; &lt;th align="left"&gt;GPU VRAM usage, GB&lt;/th&gt; &lt;th align="left"&gt;Prompt processing (pp), t/s&lt;/th&gt; &lt;th align="left"&gt;Token generation (tg), t/s&lt;/th&gt; &lt;th align="left"&gt;Power (pp), W&lt;/th&gt; &lt;th align="left"&gt;Power (tg), W&lt;/th&gt; &lt;th align="left"&gt;GPU power limit, W&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU (3600 MHz single-channel)&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;0.149&lt;/td&gt; &lt;td align="left"&gt;23.67&lt;/td&gt; &lt;td align="left"&gt;4.73&lt;/td&gt; &lt;td align="left"&gt;109&lt;/td&gt; &lt;td align="left"&gt;87&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU (6000 MHz dual-channel)&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;0.149&lt;/td&gt; &lt;td align="left"&gt;24.50&lt;/td&gt; &lt;td align="left"&gt;11.24&lt;/td&gt; &lt;td align="left"&gt;125&lt;/td&gt; &lt;td align="left"&gt;126&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.748&lt;/td&gt; &lt;td align="left"&gt;471.61&lt;/td&gt; &lt;td align="left"&gt;11.25&lt;/td&gt; &lt;td align="left"&gt;159&lt;/td&gt; &lt;td align="left"&gt;126&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;2.474&lt;/td&gt; &lt;td align="left"&gt;606.00&lt;/td&gt; &lt;td align="left"&gt;14.55&lt;/td&gt; &lt;td align="left"&gt;171&lt;/td&gt; &lt;td align="left"&gt;161&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;3.198&lt;/td&gt; &lt;td align="left"&gt;870.32&lt;/td&gt; &lt;td align="left"&gt;20.44&lt;/td&gt; &lt;td align="left"&gt;191&lt;/td&gt; &lt;td align="left"&gt;175&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;4.434&lt;/td&gt; &lt;td align="left"&gt;1111.45&lt;/td&gt; &lt;td align="left"&gt;25.67&lt;/td&gt; &lt;td align="left"&gt;207&lt;/td&gt; &lt;td align="left"&gt;187&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;5.178&lt;/td&gt; &lt;td align="left"&gt;1550.70&lt;/td&gt; &lt;td align="left"&gt;34.84&lt;/td&gt; &lt;td align="left"&gt;232&lt;/td&gt; &lt;td align="left"&gt;221&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;All&lt;/td&gt; &lt;td align="left"&gt;5.482&lt;/td&gt; &lt;td align="left"&gt;1872.08&lt;/td&gt; &lt;td align="left"&gt;54.54&lt;/td&gt; &lt;td align="left"&gt;248&lt;/td&gt; &lt;td align="left"&gt;248&lt;/td&gt; &lt;td align="left"&gt;170&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;All&lt;/td&gt; &lt;td align="left"&gt;5.482&lt;/td&gt; &lt;td align="left"&gt;1522.43&lt;/td&gt; &lt;td align="left"&gt;44.37&lt;/td&gt; &lt;td align="left"&gt;171&lt;/td&gt; &lt;td align="left"&gt;171&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;All&lt;/td&gt; &lt;td align="left"&gt;5.482&lt;/td&gt; &lt;td align="left"&gt;1741.38&lt;/td&gt; &lt;td align="left"&gt;53.39&lt;/td&gt; &lt;td align="left"&gt;203&lt;/td&gt; &lt;td align="left"&gt;203&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The power consumption numbers are from the wall socket for the whole system (without monitor). Those numbers are not super accurate since I was just eyeballing them from the power meter.&lt;/p&gt; &lt;p&gt;As seen on the last two rows, limiting the GPU's power with &lt;code&gt;nvidia-smi -pl 130&lt;/code&gt; helped drop the system power consumption significantly while the tokens/sec didn't drop almost at all, so it seems to make sense to limit the 3060's power to about 130 W instead of the default 170 W.&lt;/p&gt; &lt;h1&gt;Running both CPU and GPU inference at the same time&lt;/h1&gt; &lt;p&gt;I deliberately bought a lot of RAM so that I can run CPU-only inference alongside GPU-only inference. It allows me to do additional CPU-only inference in the background when I don't care about the tokens/sec as much, e.g. in agentic/batch workflows.&lt;/p&gt; &lt;p&gt;I tried running two llama-bench processes simultaneously (one on GPU, and one on CPU):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# CPU-only inference with 6 threads at 100% load ./llama.cpp-cuda/build/bin/llama-bench -m ./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf -ngl 99 -r 1000 # GPU inference (+ 1 CPU thread at 100% load) ./llama.cpp-cpu-only/build/bin/llama-bench -m ./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf -r 1000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Running those two commands in parallel had 7 threads at 100% load. GPU power limit was at default (170 W).&lt;/p&gt; &lt;p&gt;The whole system consumes about &lt;strong&gt;286 W&lt;/strong&gt; when running &lt;em&gt;prompt processing&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;The whole system consumes about &lt;strong&gt;290 W&lt;/strong&gt; when running &lt;em&gt;token generation&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;Optimizing idle power consumption&lt;/h1&gt; &lt;p&gt;As a sidenote, this machine seems to idle at around &lt;strong&gt;45 W&lt;/strong&gt; after doing the following optimizations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Shut down HDDs after 20 minutes with &lt;code&gt;hdparm -S 240&lt;/code&gt; (or immediately with &lt;code&gt;hdparm -Y&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Apply power optimizations with &lt;code&gt;powertop --auto-tune&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I could probably drop the idle power consumption even lower by undervolting and tweaking some BIOS settings.&lt;/p&gt; &lt;h1&gt;What models fit into 12 GB VRAM?&lt;/h1&gt; &lt;p&gt;With Ollama, these models seem to fit into 12 GB of VRAM:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistral-small:22b (Q4_0)&lt;/li&gt; &lt;li&gt;llama3.2-vision:11b (Q4_K_M)&lt;/li&gt; &lt;li&gt;deepseek-r1:14b (Q4_K_M)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These can be found on &lt;a href="https://ollama.com/search"&gt;https://ollama.com/search&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Charts&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zvo86sf16jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d80f828c63aa7e3b388d6de0a9ad7435400ae635"&gt;https://preview.redd.it/zvo86sf16jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d80f828c63aa7e3b388d6de0a9ad7435400ae635&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/oz26ylj26jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94bba969defc2aca66afac1d0af4581dde0cb291"&gt;https://preview.redd.it/oz26ylj26jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94bba969defc2aca66afac1d0af4581dde0cb291&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zoe3q5336jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5175982e8232599304a8b36f652556e59d65e3d8"&gt;https://preview.redd.it/zoe3q5336jme1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5175982e8232599304a8b36f652556e59d65e3d8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BobTheNeuron"&gt; /u/BobTheNeuron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rx6q/benchmarks_power_consumption_ryzen_6core_ddr56000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rx6q/benchmarks_power_consumption_ryzen_6core_ddr56000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2rx6q/benchmarks_power_consumption_ryzen_6core_ddr56000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T20:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2wf5j</id>
    <title>High demand for DIGITS</title>
    <updated>2025-03-03T23:14:14+00:00</updated>
    <author>
      <name>/u/Cane_P</name>
      <uri>https://old.reddit.com/user/Cane_P</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if the claim is valid, but according to Taiwanese &amp;quot;United News Media&amp;quot; (udn.com), more production capacity will be allocated to DIGITS than initially planned, because AI companies want to use it for edge computing.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://money.udn.com/money/story/5612/8581326"&gt;https://money.udn.com/money/story/5612/8581326&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Google translated version: &lt;a href="https://money-udn-com.translate.goog/money/story/5612/8581326?_x_tr_sl=auto&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=en-US&amp;amp;_x_tr_pto=wapp"&gt;https://money-udn-com.translate.goog/money/story/5612/8581326?_x_tr_sl=auto&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=en-US&amp;amp;_x_tr_pto=wapp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cane_P"&gt; /u/Cane_P &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wf5j/high_demand_for_digits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wf5j/high_demand_for_digits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2wf5j/high_demand_for_digits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T23:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2pm6n</id>
    <title>Cache-Craft: Chunk-Level KV Cache Reuse for Faster and Efficient RAG (SIGMOD 2025)</title>
    <updated>2025-03-03T18:29:59+00:00</updated>
    <author>
      <name>/u/Lucky-Ad79</name>
      <uri>https://old.reddit.com/user/Lucky-Ad79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share &lt;strong&gt;&lt;em&gt;Cache-Craft&lt;/em&gt;&lt;/strong&gt; [&lt;a href="https://www.arxiv.org/pdf/2502.15734"&gt;PDF&lt;/a&gt;], our SIGMOD 2025 paper on efficient &lt;strong&gt;chunk-aware KV reuse&lt;/strong&gt; for RAG! 🚀&lt;/p&gt; &lt;p&gt;Large language models (LLMs) in retrieval-augmented generation (RAG) often recompute KV caches unnecessarily, leading to inefficiencies. &lt;strong&gt;&lt;em&gt;Cache-Craft&lt;/em&gt;&lt;/strong&gt; introduces a &lt;strong&gt;granular&lt;/strong&gt; &lt;strong&gt;chunk-level KV reuse strategy&lt;/strong&gt; that selectively recomputes only what’s necessary—reducing redundant computation while maintaining generation quality.&lt;/p&gt; &lt;p&gt;🔹 &lt;strong&gt;Key contributions:&lt;/strong&gt;&lt;br /&gt; ✅ &lt;strong&gt;Chunked KV Reuse:&lt;/strong&gt; Efficiently caches and reuses KV states at a RAG chunk level, unlike traditional full-prefix-cache methods.&lt;br /&gt; ✅ &lt;strong&gt;Selective Recompute Planning:&lt;/strong&gt; Dynamically determines which KV states to reuse vs. recompute, optimizing for efficiency.&lt;br /&gt; ✅ &lt;strong&gt;Real-World Gains:&lt;/strong&gt; Evaluated on production-scale RAG traces, showing significant reductions in compute overhead.&lt;br /&gt; ✅ &lt;strong&gt;vLLM-based Open Source Coming Soon!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts! How do you see caching evolving for efficient LLM inference? 🤔&lt;/p&gt; &lt;p&gt;&lt;em&gt;[1] Agarwal, S., Sundaresan, S., Mitra, S., Mahapatra, D., Gupta, A., Sharma, R., Kapu, N.J., Yu, T. and Saini, S., 2025. Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation. arXiv preprint arXiv:2502.15734.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lucky-Ad79"&gt; /u/Lucky-Ad79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pm6n/cachecraft_chunklevel_kv_cache_reuse_for_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pm6n/cachecraft_chunklevel_kv_cache_reuse_for_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pm6n/cachecraft_chunklevel_kv_cache_reuse_for_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T18:29:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2tdtt</id>
    <title>Build your own evals in minutes, including comparing to human preferences. Plus: Sonnet 3.7 Thinking fine-tuning &amp; eval. [KilnAI Guide]</title>
    <updated>2025-03-03T21:03:53+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tdtt/build_your_own_evals_in_minutes_including/"&gt; &lt;img alt="Build your own evals in minutes, including comparing to human preferences. Plus: Sonnet 3.7 Thinking fine-tuning &amp;amp; eval. [KilnAI Guide]" src="https://external-preview.redd.it/fkk_hfuiSuMOZjLy_dEtjSiqJMOwZz9w_oAKY_5Q2Nk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3dadc03291c7ac04f201561f33b9b740f85a835" title="Build your own evals in minutes, including comparing to human preferences. Plus: Sonnet 3.7 Thinking fine-tuning &amp;amp; eval. [KilnAI Guide]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just released an update of Kiln on Github which provides a powerful toolkit for evaluating AI models and tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href="https://docs.getkiln.ai/docs/evaluations#video-walkthrough"&gt;walkthrough vid&lt;/a&gt; shows the process from start to end&lt;/li&gt; &lt;li&gt;Our docs have &lt;a href="https://docs.getkiln.ai/docs/evaluations"&gt;evaluation guide&lt;/a&gt; if you want to try it out yourself&lt;/li&gt; &lt;li&gt;Here's the ~&lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Github repo&lt;/a&gt;~ with all of the source code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The eval feature includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiple state of the art evaluation methods (G-Eval, LLM as Judge)&lt;/li&gt; &lt;li&gt;Synthetic data generation makes it easy to generaet hundreds or thousands of eval data samples in minutes.&lt;/li&gt; &lt;li&gt;Includes tooling to find the best evaluation method for your task. It finds the eval algo+model which best correlates to human preference (Kendall’s Tau, Spearman, MSE, etc).&lt;/li&gt; &lt;li&gt;Includes eval dashboard to find the highest quality method to run your task (prompt+model)&lt;/li&gt; &lt;li&gt;Fine-tunes: create then evaluate custom fine-tunes for your task&lt;/li&gt; &lt;li&gt;Intuitive UI for eval dataset management: create eval sets, manage golden sets, add human ratings, etc.&lt;/li&gt; &lt;li&gt;Automatic eval generation: it will examine your task definition, then automatically create an evaluator for you.&lt;/li&gt; &lt;li&gt;Supports custom evaluators: create evals for any score/goals/instructions you want.&lt;/li&gt; &lt;li&gt;Built in eval templates for common scenarios: toxicity, bias, jailbreaking, factual correctness, and maliciousness.&lt;/li&gt; &lt;li&gt;Synthetic data templates to generate adversarial datasets using uncensored and unaligned models like Dolphin/Grok. Weird use case where very inappropriate content has a very ethical use. The video has a demo of Dolphin trying to jailbreak the core model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;: this release also includes the ability to distill Sonnet 3.7 Thinking into an open model you can run locally. I evaluate a few of these fine-tunes against foundation models, and they do quite well (at task-specific metrics).&lt;/p&gt; &lt;p&gt;Kiln runs locally and we never have access to your dataset. If you use Ollama, data never leaves your device.&lt;/p&gt; &lt;p&gt;If anyone wants to try Kiln, here's the &lt;a href="https://github.com/Kiln-AI/Kiln/releases/tag/v0.12.1"&gt;latest release on Github&lt;/a&gt; and the &lt;a href="https://docs.getkiln.ai/"&gt;docs are here&lt;/a&gt;. Getting started is super easy - it's a one-click install to get setup and running. Let me know if you have any feedback or ideas! It really helps me improve Kiln. Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1j2tdtt/video/f4mqimpchjme1/player"&gt;Walkthrough of creating an AI Eval&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tdtt/build_your_own_evals_in_minutes_including/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tdtt/build_your_own_evals_in_minutes_including/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2tdtt/build_your_own_evals_in_minutes_including/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T21:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2xzsj</id>
    <title>I used a 100-line LLM Framework to let AI Agents build Agents for me (Step-by-Step Video Tutorial)</title>
    <updated>2025-03-04T00:27:24+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a video tutorial on a personal hack that can let Cursor AI build complex LLM Agents and greatly improve my productivity : &lt;a href="https://youtu.be/wc9O-9mcObc"&gt;https://youtu.be/wc9O-9mcObc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, in this tutorial, I mostly write the high-level design doc, and Cursor AI handles all the implementation and coding to build an &lt;a href="https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple"&gt;AI YouTube Summarizer&lt;/a&gt;. The secret is &lt;a href="https://github.com/The-Pocket/PocketFlow"&gt;Pocket Flow&lt;/a&gt;, a 100-line framework that fits easily into Cursor’s &lt;a href="https://docs.cursor.com/context/rules-for-ai"&gt;rules&lt;/a&gt;, remains flexible for all sorts of designs, and nudges Cursor to follow good coding practices.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background of 100-line framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I built this &lt;a href="https://github.com/The-Pocket/PocketFlow"&gt;100-line LLM framework&lt;/a&gt; over Christmas. It provides the core “graph abstraction” that LLM workflows need—for (&lt;a href="https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html"&gt;multi-&lt;/a&gt;)&lt;a href="https://the-pocket.github.io/PocketFlow/design_pattern/agent.html"&gt;agents&lt;/a&gt;, &lt;a href="https://the-pocket.github.io/PocketFlow/design_pattern/rag.html"&gt;Retrieval-Augmented Generation (RAG)&lt;/a&gt;, &lt;a href="https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html"&gt;workflow&lt;/a&gt;, and more. I built this because:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Most big frameworks have &lt;em&gt;messy abstractions&lt;/em&gt;, deprecated methods, and annoying dependencies that are very hard to use.&lt;/li&gt; &lt;li&gt;These issues don’t just confuse humans; they confuse AI coding assistants as well! For example, if you let &lt;em&gt;Cursor AI&lt;/em&gt; build a LLM project with those frameworks, you’ll likely run into a bunch of version or deprecation errors.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So I stripped everything down to &lt;strong&gt;100 lines&lt;/strong&gt;, making it easy for AI tools (like &lt;em&gt;Cursor AI&lt;/em&gt;) to read and build on top of it as “rules.” Surprisingly, &lt;strong&gt;Cursor understands Pocket Flow really well&lt;/strong&gt;-its generated code is modular, maintainable, and has &lt;em&gt;greatly boosted my productivity&lt;/em&gt; over the past year.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo in the YouTube Video&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To demonstrate this further, I made &lt;a href="https://www.youtube.com/watch?v=wc9O-9mcObc"&gt;this &lt;em&gt;YouTube video&lt;/em&gt;&lt;/a&gt; showing exactly how I fed &lt;em&gt;Cursor AI&lt;/em&gt; the &lt;strong&gt;Pocket Flow&lt;/strong&gt; docs and a high-level design to build LLM apps. I asked &lt;em&gt;Cursor AI&lt;/em&gt; to create a &lt;a href="https://github.com/The-Pocket/Tutorial-Youtube-Made-Simple"&gt;YouTube “explainer” agent &lt;/a&gt;that summarizes long videos into &lt;em&gt;simple “5-year-old-friendly” terms&lt;/em&gt;—for instance, it can condense Lex Fridman’s &lt;em&gt;5-hour DeepSeek&lt;/em&gt; interview into a &lt;a href="https://the-pocket.github.io/Tutorial-Youtube-Made-Simple/examples/DeepSeek%2C%20China%2C%20OpenAI%2C%20NVIDIA%2C%20xAI%2C%20TSMC%2C%20Stargate%2C%20and%20AI%20Megaclusters%20%7C%20Lex%20Fridman%20Podcast%20%23459.html"&gt;concise, sharp summary&lt;/a&gt;. The entire development took me &lt;em&gt;less than an hour&lt;/em&gt;—and you can do the same!&lt;/p&gt; &lt;p&gt;I’m very new to YouTube, so &lt;em&gt;please, please, please&lt;/em&gt; give me your feedback on which parts are unclear! If there’s another LLM project you’d like to see me build with Pocket Flow + Cursor, let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2xzsj/i_used_a_100line_llm_framework_to_let_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2xzsj/i_used_a_100line_llm_framework_to_let_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2xzsj/i_used_a_100line_llm_framework_to_let_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-04T00:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j8x5</id>
    <title>Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?</title>
    <updated>2025-03-03T13:59:10+00:00</updated>
    <author>
      <name>/u/zR0B3ry2VAiH</name>
      <uri>https://old.reddit.com/user/zR0B3ry2VAiH</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"&gt; &lt;img alt="Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?" src="https://preview.redd.it/x73g8sumdhme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86a6a6523268d0e31eb6f4341c716de3ee799ab2" title="Ran R1 on one server, but I have three. Should I go the EXO route and buy 100gb nics?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;100gb nics are $330 with two ports each, so I’d run it direct connections between all three. Each server has two Xeon process with 512 gb of ram. Did some shuffling with the ram sticks to get R1 to run locally, but as you would expect, it’s pretty slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zR0B3ry2VAiH"&gt; /u/zR0B3ry2VAiH &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x73g8sumdhme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j8x5/ran_r1_on_one_server_but_i_have_three_should_i_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:59:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2vhhq</id>
    <title>Story writing benchmark/dataset</title>
    <updated>2025-03-03T22:32:38+00:00</updated>
    <author>
      <name>/u/CorrectLow9302</name>
      <uri>https://old.reddit.com/user/CorrectLow9302</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"&gt; &lt;img alt="Story writing benchmark/dataset" src="https://external-preview.redd.it/C3b4DWbqwScAlqlFr8UQw42SuiBsrSRBBL5H3HRmHFA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a47dcafbfc7088a32a7fc1410e5b9704fd4e1a93" title="Story writing benchmark/dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/hu1npsu3xjme1.jpg?width=4665&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5ef87cc79183f0cb10bc67f5b10121bb979b878c"&gt;https://preview.redd.it/hu1npsu3xjme1.jpg?width=4665&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5ef87cc79183f0cb10bc67f5b10121bb979b878c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;dataset: &lt;a href="https://huggingface.co/datasets/lars1234/story_writing_benchmark"&gt;https://huggingface.co/datasets/lars1234/story_writing_benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Each model was instructed to write 568 short stories. Each story was then rated by 4 models: Llama 3.3 70B, Mistral Small 24B (2501), Gemma 2 9B (SPPO-Iter3), Aya Expanse 32B. The ranking correlation between the evaluators is approx. 90%. Evaluation criteria such as creativity, world-building and grammar were weighted equally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CorrectLow9302"&gt; /u/CorrectLow9302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2vhhq/story_writing_benchmarkdataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T22:32:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2horr</id>
    <title>NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing</title>
    <updated>2025-03-03T12:36:04+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt; &lt;img alt="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" src="https://preview.redd.it/8gyz8kzsygme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a88d058b7bc64d9941684f9dc53c45071cf7f231" title="NLP Brain-to-Text Decoding: A Non-invasive Approach via Typing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/"&gt;https://ai.meta.com/research/publications/brain-to-text-decoding-a-non-invasive-approach-via-typing/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8gyz8kzsygme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2horr/nlp_braintotext_decoding_a_noninvasive_approach/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T12:36:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2m5a3</id>
    <title>I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!</title>
    <updated>2025-03-03T16:08:39+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"&gt; &lt;img alt="I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!" src="https://external-preview.redd.it/MzV5cjJiYnAwaW1lMVP5w4KiH2jOdvVLO5M2qzHTvkueIwiHgKlPWJafTXUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85988097e8c3bdd3a2d386e68bcd0b55c2d2f2c0" title="I just made something really cursed. It's a local AI javascript library that allows for generating all of your websites styles... using text... It's like tailwind!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ys7qtcbp0ime1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2m5a3/i_just_made_something_really_cursed_its_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T16:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29mi4</id>
    <title>Me Today</title>
    <updated>2025-03-03T03:38:52+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt; &lt;img alt="Me Today" src="https://preview.redd.it/qrxhvlblaeme1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a2767bc89a037159368246cac9dac0d3050c85f" title="Me Today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qrxhvlblaeme1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29mi4/me_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:38:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1j29hm0</id>
    <title>New Atom of Thoughts looks promising for helping smaller models reason</title>
    <updated>2025-03-03T03:31:16+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt; &lt;img alt="New Atom of Thoughts looks promising for helping smaller models reason" src="https://preview.redd.it/xlairo4g9eme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=767c07ca77e2312ef37e77aa5686232b9b3aebb6" title="New Atom of Thoughts looks promising for helping smaller models reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xlairo4g9eme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j29hm0/new_atom_of_thoughts_looks_promising_for_helping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T03:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2kdeb</id>
    <title>OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?</title>
    <updated>2025-03-03T14:52:19+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt; &lt;img alt="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" src="https://b.thumbs.redditmedia.com/WEagdfr42ScIzJVwvfP__c7O6w-pNG7grBkUGiA0rAk.jpg" title="OpenBenchTable is great for trying out different compute hardware configurations. Does anyone have benchmarking tips?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1j2kdeb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2kdeb/openbenchtable_is_great_for_trying_out_different/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T14:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2leve</id>
    <title>new Hugging Face course on building reasoning models like deepseek r1</title>
    <updated>2025-03-03T15:37:25+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new FREE and CERTIFIED course is here, and It’s called &lt;strong&gt;The Reasoning Course.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To sign up for the course, follow the org: &lt;a href="https://huggingface.co/reasoning-course"&gt;https://huggingface.co/reasoning-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what the course will cover:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It will teach you to build your own reasoning model like Deepseek r1.&lt;/li&gt; &lt;li&gt;It’s suitable for code and non-coders with separate certification.&lt;/li&gt; &lt;li&gt;The course has material and exercises from Hugging Face, Maxime Labonne, Unsloth, and Marimo notebooks. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is how the course works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sign up now, the first release is already live.&lt;/li&gt; &lt;li&gt;Each week we’ll release new material and exercises. &lt;/li&gt; &lt;li&gt;We have interactive demos and quizzes&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2leve/new_hugging_face_course_on_building_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T15:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2pw8i</id>
    <title>The sound Tesla P40s make while training is eerie. My apartment lights also phase pulse during passes.. 🤩</title>
    <updated>2025-03-03T18:40:59+00:00</updated>
    <author>
      <name>/u/AffectSouthern9894</name>
      <uri>https://old.reddit.com/user/AffectSouthern9894</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pw8i/the_sound_tesla_p40s_make_while_training_is_eerie/"&gt; &lt;img alt="The sound Tesla P40s make while training is eerie. My apartment lights also phase pulse during passes.. 🤩" src="https://external-preview.redd.it/ZmV0aG1ibndyaW1lMWFNzMK25hHzjNvt4Y-OO73o5sGhDV6XnH6G0Oq87xCn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e39fa10a7873118e9284eb276fb4ef9c51960837" title="The sound Tesla P40s make while training is eerie. My apartment lights also phase pulse during passes.. 🤩" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectSouthern9894"&gt; /u/AffectSouthern9894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lakifgrwrime1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pw8i/the_sound_tesla_p40s_make_while_training_is_eerie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2pw8i/the_sound_tesla_p40s_make_while_training_is_eerie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T18:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2usb0</id>
    <title>Is qwen 2.5 coder still the best?</title>
    <updated>2025-03-03T22:02:25+00:00</updated>
    <author>
      <name>/u/Ambitious_Subject108</name>
      <uri>https://old.reddit.com/user/Ambitious_Subject108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anything better been released for coding? (&amp;lt;=32b parameters)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Subject108"&gt; /u/Ambitious_Subject108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2usb0/is_qwen_25_coder_still_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T22:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1j2j7su</id>
    <title>I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities.</title>
    <updated>2025-03-03T13:57:34+00:00</updated>
    <author>
      <name>/u/w-zhong</name>
      <uri>https://old.reddit.com/user/w-zhong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt; &lt;img alt="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." src="https://preview.redd.it/54k8f1ladhme1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa20219f6ef894d7607d0ad10ab575e376420b53" title="I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w-zhong"&gt; /u/w-zhong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/54k8f1ladhme1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-03T13:57:34+00:00</published>
  </entry>
</feed>
