<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-19T16:40:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1muabfm</id>
    <title>My PR that adds Mikupad (with extra features) as an alternative webUI for ik_llama.cpp</title>
    <updated>2025-08-19T06:03:36+00:00</updated>
    <author>
      <name>/u/AdventLogin2021</name>
      <uri>https://old.reddit.com/user/AdventLogin2021</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdventLogin2021"&gt; /u/AdventLogin2021 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/558"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muabfm/my_pr_that_adds_mikupad_with_extra_features_as_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muabfm/my_pr_that_adds_mikupad_with_extra_features_as_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T06:03:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttchz</id>
    <title>Deepseek R2 coming out ... when it gets more cowbell</title>
    <updated>2025-08-18T17:56:08+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt; &lt;img alt="Deepseek R2 coming out ... when it gets more cowbell" src="https://external-preview.redd.it/JNAOp8mejhheiBappEWRGE1kVdbTWLMVP5NLPsOJx9c.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f39a5e1f22d317e74a5e301e71bbdc92c571323" title="Deepseek R2 coming out ... when it gets more cowbell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what‚Äôs floating around it seems like we'll have to keep waiting a bit longer for Deepseek R2 to be released.&lt;/p&gt; &lt;p&gt;Apparently&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Liang Wenfeng has been sitting on R2's release because it still needs more cowbell&lt;/li&gt; &lt;li&gt;Training DeepSeek R2 on Huawei Ascend chips ran into persistent stability and software problems and no full training run ever succeeded. So Deepseek went back to Nvidia GPUs for training and is using Ascend chips for inference only&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here is the same story .. but with more cowbell &lt;a href="https://youtu.be/PzlqRsuIo1w"&gt;https://youtu.be/PzlqRsuIo1w&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/psltmf3youjf1.gif"&gt;https://i.redd.it/psltmf3youjf1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mucw6t</id>
    <title>GLM 4.5 Air Suddenly running 5-6x Slower on Hybrid CPU/RoCM inference.</title>
    <updated>2025-08-19T08:42:41+00:00</updated>
    <author>
      <name>/u/ROS_SDN</name>
      <uri>https://old.reddit.com/user/ROS_SDN</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a pc of the specs...&lt;/p&gt; &lt;p&gt;CPU: 7900x&lt;/p&gt; &lt;p&gt;RAM: 2x32gb 6000 mhz cl 30&lt;/p&gt; &lt;p&gt;GPU: 7900XTX&lt;/p&gt; &lt;p&gt;I'm loading up a quant of GLM 4.5 air in llama cpp with..&lt;/p&gt; &lt;p&gt;&lt;code&gt;./build/bin/llama-cli -ngl 99 -sm none -m ~/models/unsloth/GLM-4.5-Air-GGUF/GLM-4.5-Air-IQ4_XS-00001-of-00002.gguf --flash-attn --n-cpu-moe 34 -c 32000 -p &amp;quot; Hello&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is taking up roughly 23.5gbs of my gpus space, but the weird thing is just a few days ago when I ran this I was getting a very workable 10-12 t/s and now I'm near ~2 t/s.&lt;/p&gt; &lt;p&gt;I did just delete and have to re-download the model today, but it's in the same directory I had it in before, but I'm severely confused what I could have possibly changed outside that to completely destroy performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Never mind... I just reset my computer and now I'm back at 11 t/s... I'd love an explanation for that because I was not eating up 20gb of RAM running electron apps (as much as they may try) and web browsers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ROS_SDN"&gt; /u/ROS_SDN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mucw6t/glm_45_air_suddenly_running_56x_slower_on_hybrid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mucw6t/glm_45_air_suddenly_running_56x_slower_on_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mucw6t/glm_45_air_suddenly_running_56x_slower_on_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T08:42:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mto8fa</id>
    <title>New code benchmark puts Qwen 3 Coder at the top of the open models</title>
    <updated>2025-08-18T14:51:49+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"&gt; &lt;img alt="New code benchmark puts Qwen 3 Coder at the top of the open models" src="https://external-preview.redd.it/-FhGljRcqsXlvJ4R58hFA0RMnpSa9fBguxJ8Dc9Mg-4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15659a7787e6ab90c61f6ea82b0300809513a758" title="New code benchmark puts Qwen 3 Coder at the top of the open models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR of the open models results:&lt;/p&gt; &lt;p&gt;Q3C fp16 &amp;gt; Q3C fp8 &amp;gt; GPT-OSS-120b &amp;gt; V3 &amp;gt; K2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://brokk.ai/power-ranking?round=open&amp;amp;models=flash-2.5%2Cgpt-oss-120b%2Cgpt5-mini%2Ck2%2Cq3c%2Cq3c-fp8%2Cv3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T14:51:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1muirjt</id>
    <title>GPT OSS 20B through ollama with codex cli has really low performance</title>
    <updated>2025-08-19T13:41:01+00:00</updated>
    <author>
      <name>/u/Markronom</name>
      <uri>https://old.reddit.com/user/Markronom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like I'm missing something here. So it's clear to me that gpt 20B is a small model. But it seems completely useless in codex cli. I even struggle to make it create a test file. I was hoping for it to be able to make simple, clearly defined file changes at least, as it runs very fast on my machine. The bad output performance is a bit surprising to me, as it's the default model for codex --oss and they published an article how they optimised the gpt oss models to work well with ollama. Any ideas for improvement would be very welcome üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Markronom"&gt; /u/Markronom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muirjt/gpt_oss_20b_through_ollama_with_codex_cli_has/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muirjt/gpt_oss_20b_through_ollama_with_codex_cli_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muirjt/gpt_oss_20b_through_ollama_with_codex_cli_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T13:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mugrii</id>
    <title>llama.cpp + ngrok</title>
    <updated>2025-08-19T12:17:17+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mugrii/llamacpp_ngrok/"&gt; &lt;img alt="llama.cpp + ngrok" src="https://external-preview.redd.it/gPMH5N9P7LrlLK9XBWYWsOwYsEUDqDGG215n2qqPFuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=133ff0779923c95030c73be653e21ece82c6156f" title="llama.cpp + ngrok" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For llama.cpp to work on any device with ngrok or programs, it must have the http configuration.&lt;/p&gt; &lt;p&gt;1 - run llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w9ue9yzotyjf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=543fb957b35de8cbdaa11f2ec6a922b16ba26bd2"&gt;https://preview.redd.it/w9ue9yzotyjf1.png?width=1320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=543fb957b35de8cbdaa11f2ec6a922b16ba26bd2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;2 - Follow Ngrok's instructions on their website.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/010alpy8wyjf1.png?width=1364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0265bcde4dbad303b658774066d9bd2bf06be975"&gt;https://preview.redd.it/010alpy8wyjf1.png?width=1364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0265bcde4dbad303b658774066d9bd2bf06be975&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3 - Once downloaded, all that remains is to execute our llama.cpp which is on port 8080.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kxshce1owyjf1.png?width=380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7c056919cb70b3299dd44c373df3648e7f6d4e9"&gt;https://preview.redd.it/kxshce1owyjf1.png?width=380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7c056919cb70b3299dd44c373df3648e7f6d4e9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4 - Once the command is executed, this should appear. The one that says Forwarding is our temporary website&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cou40litwyjf1.png?width=868&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25ecd19d0df12d5fdbf2fd8db5a5a6f8edd8d6c3"&gt;https://preview.redd.it/cou40litwyjf1.png?width=868&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25ecd19d0df12d5fdbf2fd8db5a5a6f8edd8d6c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;more sites like these:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/anderspitman/awesome-tunneling"&gt;https://github.com/anderspitman/awesome-tunneling&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mugrii/llamacpp_ngrok/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mugrii/llamacpp_ngrok/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mugrii/llamacpp_ngrok/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T12:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu49q3</id>
    <title>We're Updating the Wiki To Be More Current, And We Want Your Feedback</title>
    <updated>2025-08-19T01:01:14+00:00</updated>
    <author>
      <name>/u/N8Karma</name>
      <uri>https://old.reddit.com/user/N8Karma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; subreddit has long had a wiki: &lt;a href="https://www.reddit.com/r/LocalLLaMA/wiki/wiki/"&gt;https://www.reddit.com/r/LocalLLaMA/wiki/wiki/&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, the wiki hadn't been updated in a year or two (it still was mainly focused on LLaMA 2)! So we renovated the FAQ, Resources, and Models sections to reflect the present ecosystem. You can see a direct comparison &lt;a href="https://github.com/N8python/local-llama-wiki-revamp"&gt;here&lt;/a&gt; - with llama-old.md being the prior wiki and llama.md being the new one. Everything has been brought up to date.&lt;/p&gt; &lt;p&gt;But this is just the beginning - we want the wiki to reflect the desires of the community, and we want your feedback on what should be added/removed/altered. Models that are missing? Good resources that need a spotlight? We want to hear it! There will be a comment under this post to place your suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/N8Karma"&gt; /u/N8Karma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu49q3/were_updating_the_wiki_to_be_more_current_and_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu49q3/were_updating_the_wiki_to_be_more_current_and_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu49q3/were_updating_the_wiki_to_be_more_current_and_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T01:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu9hpi</id>
    <title>Seems like many open source models struggle with this.</title>
    <updated>2025-08-19T05:15:55+00:00</updated>
    <author>
      <name>/u/Neither_Egg_4773</name>
      <uri>https://old.reddit.com/user/Neither_Egg_4773</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many open source models, except for the GPT-OSS model, fail to genuinely grasp the details of &lt;strong&gt;recently published articles&lt;/strong&gt;. For example, if an article states, &amp;quot;last week this event happened...&amp;quot;. When asked about the date of the event, it becomes lost, even though the news articles on websites show the dates when they were published. Somehow, the GPT-OSS model understands the article and identifies the dates. Not only that, open-source models like Qwen have trouble quoting information from articles found during searches; however, it can definitely quote material from it's training data (&lt;em&gt;downside: it will hallucinate&lt;/em&gt;). But again, GPT-OSS can quote from recently published articles during searches. Hopefully, there will be a change soon in many of these open-source LLMs, as I use these open-source LLMs for source findings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neither_Egg_4773"&gt; /u/Neither_Egg_4773 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu9hpi/seems_like_many_open_source_models_struggle_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu9hpi/seems_like_many_open_source_models_struggle_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu9hpi/seems_like_many_open_source_models_struggle_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T05:15:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1muf2zr</id>
    <title>Backend for GLM 4.5 Air and the 96Gb Blackwell</title>
    <updated>2025-08-19T10:54:35+00:00</updated>
    <author>
      <name>/u/UltrMgns</name>
      <uri>https://old.reddit.com/user/UltrMgns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;br /&gt; I've been struggling with this for quite a bit of time now. Last week I got my RTX Pro 6000 after ~ 7 months of saving cash. All I managed to compile as a backend is llama cpp, but I really want to get a proper backend working on it. Llama cpp struggles heavily with parallel requests, it accepts ANY api key as valid for authentication and those for me are true showstoppers because I can't expose an API endpoint and work on it remotely or share it with a couple of friends.&lt;br /&gt; Has anyone managed to get vLLM or TGI to work with GLM4.5 Air AWQ? I want to use this model specifically because it really seems to perform amazing as a local model (running the Q4_XL gguf currently).&lt;br /&gt; It's been an endless trail of errors, even though I compiled vLLM with compute 86+120 (because I have an 3090 in my server) it just wouldn't start serving, the sheer amount of different errors are just extremely discouraging, hence why I'm writing this plea for help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UltrMgns"&gt; /u/UltrMgns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf2zr/backend_for_glm_45_air_and_the_96gb_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf2zr/backend_for_glm_45_air_and_the_96gb_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muf2zr/backend_for_glm_45_air_and_the_96gb_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T10:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu0djr</id>
    <title>Qwen Code CLI has generous FREE Usage option</title>
    <updated>2025-08-18T22:15:33+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who didnt know, Qwen-Code which is a clone of Gemini CLI has a good &lt;a href="https://github.com/QwenLM/qwen-code?tab=readme-ov-file#-free-options-available"&gt;Free usage plan&lt;/a&gt;: - 2,000 requests per day with no token limits - 60 requests per minute rate limit It allows us to use Qwen3Coder for FREE.&lt;/p&gt; &lt;p&gt;Made a small video to showcase how to setup and use here: &lt;a href="https://youtu.be/M6ubLFqL-OA"&gt;https://youtu.be/M6ubLFqL-OA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu0djr/qwen_code_cli_has_generous_free_usage_option/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu0djr/qwen_code_cli_has_generous_free_usage_option/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu0djr/qwen_code_cli_has_generous_free_usage_option/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T22:15:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttgrf</id>
    <title>Qwen-Image-Edit Released!</title>
    <updated>2025-08-18T18:00:24+00:00</updated>
    <author>
      <name>/u/MohamedTrfhgx</name>
      <uri>https://old.reddit.com/user/MohamedTrfhgx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba‚Äôs Qwen team just released &lt;strong&gt;Qwen-Image-Edit&lt;/strong&gt;, an image editing model built on the &lt;strong&gt;20B Qwen-Image&lt;/strong&gt; backbone. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports &lt;strong&gt;precise bilingual (Chinese &amp;amp; English) text editing&lt;/strong&gt; while preserving style, plus both &lt;strong&gt;semantic&lt;/strong&gt; and &lt;strong&gt;appearance-level&lt;/strong&gt; edits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text editing with bilingual support&lt;/li&gt; &lt;li&gt;High-level semantic editing (object rotation, IP creation, concept edits)&lt;/li&gt; &lt;li&gt; Low-level appearance editing (add / delete / insert objects)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1957500569029079083"&gt;https://x.com/Alibaba_Qwen/status/1957500569029079083&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen has been really prolific lately what do you think of the new model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MohamedTrfhgx"&gt; /u/MohamedTrfhgx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T18:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mumpub</id>
    <title>Generating code with gpt-oss-120b on Strix Halo with ROCm</title>
    <updated>2025-08-19T16:05:34+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"&gt; &lt;img alt="Generating code with gpt-oss-120b on Strix Halo with ROCm" src="https://external-preview.redd.it/MTBtNjc4d2sxMGtmMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae351c6a5432420dd53119df551944d7d45bc802" title="Generating code with gpt-oss-120b on Strix Halo with ROCm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen a few posts asking about how to get gpt-oss models running on AMD devices. This guide gives a quick 3-minute overview of how it works on Strix Halo (Ryzen AI MAX 395).&lt;/p&gt; &lt;p&gt;The same steps work for gpt-oss-20b, and many other models, on Radeon 7000/9000 GPUs as well.&lt;/p&gt; &lt;h2&gt;Detailed Instructions&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Install and run Lemonade from the GitHub &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Open &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; in your browser and open the Model Manager&lt;/li&gt; &lt;li&gt;Click the download button on gpt-oss-120b. Go find something else to do while it downloads ~60 GB.&lt;/li&gt; &lt;li&gt;Launch Lemonade Server in ROCm mode &lt;ul&gt; &lt;li&gt;&lt;code&gt;lemonade-server server --llamacpp rocm&lt;/code&gt; (Windows GUI installation)&lt;/li&gt; &lt;li&gt;&lt;code&gt;lemonade-server-dev server --llamacpp rocm&lt;/code&gt; (Linux/Windows pypi/source installation)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Follow the steps in the Continue + Lemonade setup guide to start generating code: &lt;a href="https://lemonade-server.ai/docs/server/apps/continue/"&gt;https://lemonade-server.ai/docs/server/apps/continue/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Need help? Find the team on Discord: &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;https://discord.gg/5xXzkMu8Zk&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks for checking this out, hope it was helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pnap0vvk10kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mumpub/generating_code_with_gptoss120b_on_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T16:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1muf6ry</id>
    <title>NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</title>
    <updated>2025-08-19T11:00:19+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"&gt; &lt;img alt="NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale" src="https://external-preview.redd.it/ojIYaD1O8xYRW9Q-A7BHJQx5N3b1m-3M6OVRuLj2lzI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfc0130326f6be10fdb5d6657d21b292210a99b2" title="NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/stepfun-ai/NextStep-1-Large"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muf6ry/nextstep1_toward_autoregressive_image_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu15vr</id>
    <title>bilbo.high.reasoning.medium.mini.3lightbulbs.ultra</title>
    <updated>2025-08-18T22:47:26+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt; &lt;img alt="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" src="https://preview.redd.it/bfdlovjpvujf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b521d9b5416a36368655d8645cd92560559169e" title="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfdlovjpvujf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T22:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtvgjx</id>
    <title>NVIDIA Releases Nemotron Nano 2 AI Models</title>
    <updated>2025-08-18T19:12:01+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt; &lt;img alt="NVIDIA Releases Nemotron Nano 2 AI Models" src="https://preview.redd.it/pzrpnuykutjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d61af91b9dcdda4649c24e581ac3941490ab82c0" title="NVIDIA Releases Nemotron Nano 2 AI Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚Ä¢ 6X faster than similarly sized models, while also being more accurate&lt;/p&gt; &lt;p&gt;‚Ä¢ NVIDIA is also releasing most of the data they used to create it, including the pretraining corpus&lt;/p&gt; &lt;p&gt;‚Ä¢ The hybrid Mamba-Transformer architecture supports 128K context length on single GPU.&lt;/p&gt; &lt;p&gt;Full research paper here: &lt;a href="https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/"&gt;https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pzrpnuykutjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T19:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mua1k4</id>
    <title>GPT OSS quality on Nebius - fixed (update)</title>
    <updated>2025-08-19T05:47:34+00:00</updated>
    <author>
      <name>/u/ai_devrel_eng</name>
      <uri>https://old.reddit.com/user/ai_devrel_eng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt; &lt;img alt="GPT OSS quality on Nebius - fixed (update)" src="https://b.thumbs.redditmedia.com/sK7Bm1tG5gwQQ5s7xz2h10LEJrqGwQrDc2Udh4wNqoE.jpg" title="GPT OSS quality on Nebius - fixed (update)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai_devrel_eng"&gt; /u/ai_devrel_eng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mua1k4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T05:47:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mulnzj</id>
    <title>Google is also untrustworthy</title>
    <updated>2025-08-19T15:28:08+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mulnzj/google_is_also_untrustworthy/"&gt; &lt;img alt="Google is also untrustworthy" src="https://preview.redd.it/exvjbuqdvzjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62aedddee4f39d51757d9a1dfbf650ced3057d7b" title="Google is also untrustworthy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/exvjbuqdvzjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mulnzj/google_is_also_untrustworthy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mulnzj/google_is_also_untrustworthy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:28:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mufslp</id>
    <title>5 Lessons from Evaluating AI Voice Agents</title>
    <updated>2025-08-19T11:30:46+00:00</updated>
    <author>
      <name>/u/Otherwise_Flan7339</name>
      <uri>https://old.reddit.com/user/Otherwise_Flan7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;strong&gt;Latency matters more than anything&lt;/strong&gt; - A 500ms delay feels tolerable in text. In voice, it feels broken. Testing latency across providers is a must.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edge cases are the real test&lt;/strong&gt; - Scripted happy-path calls make every agent look good. The moment you throw in ‚Äúinterruptions‚Äù or background noise, big gaps appear.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation can‚Äôt just be about transcripts&lt;/strong&gt; - Most tools stop at text accuracy. For voice, you need to factor in timing, interruptions, tone, and handling of silences.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Human-in-the-loop is still needed&lt;/strong&gt; - Automated scoring is great for scale, but subjective call quality still benefits from quick human checks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specialized eval tooling helps a ton&lt;/strong&gt; - Most generic prompt testing setups don‚Äôt cover voice. We had better results when using platforms that support voice evals natively (e.g. Maxim AI, along with standard eval frameworks).&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Flan7339"&gt; /u/Otherwise_Flan7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mufslp/5_lessons_from_evaluating_ai_voice_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mufslp/5_lessons_from_evaluating_ai_voice_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mufslp/5_lessons_from_evaluating_ai_voice_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mul4sx</id>
    <title>Deepseek-V3.1-Base released</title>
    <updated>2025-08-19T15:09:09+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mul4sx/deepseekv31base_released/"&gt; &lt;img alt="Deepseek-V3.1-Base released" src="https://external-preview.redd.it/TF0v-SFT5DAKs6neF39KH5oR_BZ__J6Srmsxz1t_P1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7432a9894d4ead34a34aab111e0acba5a8647c40" title="Deepseek-V3.1-Base released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mul4sx/deepseekv31base_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mul4sx/deepseekv31base_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:09:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttcr9</id>
    <title>üöÄ Qwen released Qwen-Image-Edit!</title>
    <updated>2025-08-18T17:56:23+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt; &lt;img alt="üöÄ Qwen released Qwen-Image-Edit!" src="https://b.thumbs.redditmedia.com/oRveemue3RG8vuBdHGpOCwiYY2B-M7S5WjEjTkW73hM.jpg" title="üöÄ Qwen released Qwen-Image-Edit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Excited to introduce Qwen-Image-Edit! Built on 20B Qwen-Image, it brings precise bilingual text editing (Chinese &amp;amp; English) while preserving style, and supports both semantic and appearance-level editing.&lt;/p&gt; &lt;p&gt;‚ú® Key Features&lt;/p&gt; &lt;p&gt;‚úÖ Accurate text editing with bilingual support&lt;/p&gt; &lt;p&gt;‚úÖ High-level semantic editing (e.g. object rotation, IP creation)&lt;/p&gt; &lt;p&gt;‚úÖ Low-level appearance editing (e.g. addition/delete/insert)&lt;/p&gt; &lt;p&gt;Try it now: &lt;a href="https://chat.qwen.ai/?inputFeature=image_edit"&gt;https://chat.qwen.ai/?inputFeature=image_edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit"&gt;https://modelscope.cn/models/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit/"&gt;https://qwenlm.github.io/blog/qwen-image-edit/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mttcr9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mueqhs</id>
    <title>When will low-cost Chinese GPUs hit the market?</title>
    <updated>2025-08-19T10:35:22+00:00</updated>
    <author>
      <name>/u/noellarkin</name>
      <uri>https://old.reddit.com/user/noellarkin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've heard of some Chinese GPUs, but I'm curious when they'll release low-cost alternatives that can seriously challenge NVIDIA 50xx dominance. Are there any indications that this will happen anytime soon? I'd love the hardware equivalent of a &amp;quot;deepseek moment&amp;quot; for OpenAI earlier this year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noellarkin"&gt; /u/noellarkin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mueqhs/when_will_lowcost_chinese_gpus_hit_the_market/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T10:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mumext</id>
    <title>Tried mixing local LLM + face recognition just for fun (wild results)</title>
    <updated>2025-08-19T15:54:45+00:00</updated>
    <author>
      <name>/u/yeahiiiiiii</name>
      <uri>https://old.reddit.com/user/yeahiiiiiii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I‚Äôve been tinkering a lot with running models locally (mostly LLaMA variants + some vision stuff). I like keeping things offline when possible, just feels better knowing my data isn‚Äôt flying around random servers.&lt;/p&gt; &lt;p&gt;Over the weekend I got curious‚Ä¶ what if I combine face matching with a local LLM? Like, have the LLM explain what it ‚Äúthinks‚Äù about a person from a pic (just descriptive, nothing deep), and then use a tool to see if that face exists online.&lt;/p&gt; &lt;p&gt;I played around with this app I came across called Faceseek ‚Äì it‚Äôs basically a face search tool. Not local tho, so I only tested it with some older selfies + public pics I already had floating around. Honestly the results shocked me. It matched one of my 2016 Facebook photos to some random forum post I forgot even existed. Crazy how well it pulled that up.&lt;/p&gt; &lt;p&gt;I didn‚Äôt hook it fully with my local LLM setup yet, but the idea of pairing recognition with reasoning feels like where a lot of this stuff is headed. Imagine running all that fully offline though ‚Äì no cloud, no leaks, just you + your box.&lt;/p&gt; &lt;p&gt;Anyway, this got me thinking‚Ä¶ has anyone here tried doing something similar? Like combining local models with external recognition tools? Do you think we‚Äôll eventually get a fully local version of this (face search + reasoning) or is that still years away?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yeahiiiiiii"&gt; /u/yeahiiiiiii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumext/tried_mixing_local_llm_face_recognition_just_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mumext/tried_mixing_local_llm_face_recognition_just_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mumext/tried_mixing_local_llm_face_recognition_just_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukwq6</id>
    <title>ü§ó DeepSeek-V3.1-Base</title>
    <updated>2025-08-19T15:01:07+00:00</updated>
    <author>
      <name>/u/newsletternew</name>
      <uri>https://old.reddit.com/user/newsletternew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The v3.1 base model is here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newsletternew"&gt; /u/newsletternew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukwq6/deepseekv31base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T15:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1muft1w</id>
    <title>DeepSeek v3.1</title>
    <updated>2025-08-19T11:31:24+00:00</updated>
    <author>
      <name>/u/Just_Lifeguard_5033</name>
      <uri>https://old.reddit.com/user/Just_Lifeguard_5033</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt; &lt;img alt="DeepSeek v3.1" src="https://preview.redd.it/143veukbpyjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9ae73ae246ccabb3b567735711ae0639d2819f2" title="DeepSeek v3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs happening!&lt;/p&gt; &lt;p&gt;DeepSeek online model version has been updated to V3.1, context length extended to 128k, welcome to test on the official site and app. API calling remains the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Just_Lifeguard_5033"&gt; /u/Just_Lifeguard_5033 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/143veukbpyjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muft1w/deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T11:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukl2a</id>
    <title>deepseek-ai/DeepSeek-V3.1-Base ¬∑ Hugging Face</title>
    <updated>2025-08-19T14:49:14+00:00</updated>
    <author>
      <name>/u/xLionel775</name>
      <uri>https://old.reddit.com/user/xLionel775</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1-Base ¬∑ Hugging Face" src="https://external-preview.redd.it/TF0v-SFT5DAKs6neF39KH5oR_BZ__J6Srmsxz1t_P1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7432a9894d4ead34a34aab111e0acba5a8647c40" title="deepseek-ai/DeepSeek-V3.1-Base ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xLionel775"&gt; /u/xLionel775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T14:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
