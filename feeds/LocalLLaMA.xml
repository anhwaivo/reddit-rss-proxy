<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-17T21:35:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mt1eim</id>
    <title>Which LLM best suitable for financial data analysis</title>
    <updated>2025-08-17T20:17:27+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If having 48gb vram and 96gb ddr5 which LLM would be best for analyzing account statements in CSV format? Like creating summaries etc? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt1eim/which_llm_best_suitable_for_financial_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt1eim/which_llm_best_suitable_for_financial_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt1eim/which_llm_best_suitable_for_financial_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T20:17:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms9djc</id>
    <title>Wan2.2 i2v Censors Chinese-looking women in nsfw workflows</title>
    <updated>2025-08-16T22:08:13+00:00</updated>
    <author>
      <name>/u/dennisitnet</name>
      <uri>https://old.reddit.com/user/dennisitnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using wan2.2 i2v for generating over 100 nsfw videos so far. Noticed something curious. Lol When input image is chinese-looking, it never outputs nsfw videos. But when I use non-chinese input images, it outputs nsfw.&lt;/p&gt; &lt;p&gt;Anybody else experienced this? Lol really curious shiz&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dennisitnet"&gt; /u/dennisitnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt35id</id>
    <title>Best locally run uncensored model for a 12GB VRAM / 32 GB RAM System?</title>
    <updated>2025-08-17T21:27:08+00:00</updated>
    <author>
      <name>/u/Electronic-Tooth-210</name>
      <uri>https://old.reddit.com/user/Electronic-Tooth-210</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for real uncensored, not models that need prompt engineering to get them to answer to NSFW stuff which can run on my Pc do you have any ideas? (I know my system is limited it can be slow thats okay)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Tooth-210"&gt; /u/Electronic-Tooth-210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt35id/best_locally_run_uncensored_model_for_a_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt35id/best_locally_run_uncensored_model_for_a_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt35id/best_locally_run_uncensored_model_for_a_12gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T21:27:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mss11l</id>
    <title>LL3M: Large Language 3D Modelers</title>
    <updated>2025-08-17T14:15:10+00:00</updated>
    <author>
      <name>/u/codexauthor</name>
      <uri>https://old.reddit.com/user/codexauthor</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codexauthor"&gt; /u/codexauthor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://threedle.github.io/ll3m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mss11l/ll3m_large_language_3d_modelers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mss11l/ll3m_large_language_3d_modelers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:15:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn7pt</id>
    <title>XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization</title>
    <updated>2025-08-17T10:16:39+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; XQuant proposes caching low-bit &lt;strong&gt;layer inputs (X)&lt;/strong&gt; instead of the usual KV cache and &lt;strong&gt;rematerializing K/V on the fly&lt;/strong&gt;, trading extra compute for far less memory traffic; this gives an immediate &lt;strong&gt;~2×&lt;/strong&gt; cut vs standard KV caching and up to &lt;strong&gt;~7.7×&lt;/strong&gt; vs FP16 with &lt;strong&gt;&amp;lt;0.1&lt;/strong&gt; perplexity drop, while the cross-layer variant (&lt;strong&gt;XQuant-CL&lt;/strong&gt;) reaches &lt;strong&gt;10× (≈0.01 ppl)&lt;/strong&gt; and &lt;strong&gt;12.5× (≈0.1 ppl)&lt;/strong&gt;, with near-FP16 accuracy and better results than prior KV-quant methods.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.10395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn7pt/xquant_breaking_the_memory_wall_for_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn7pt/xquant_breaking_the_memory_wall_for_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1msyqr3</id>
    <title>OSS20B is actually good?</title>
    <updated>2025-08-17T18:35:25+00:00</updated>
    <author>
      <name>/u/05032-MendicantBias</name>
      <uri>https://old.reddit.com/user/05032-MendicantBias</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyqr3/oss20b_is_actually_good/"&gt; &lt;img alt="OSS20B is actually good?" src="https://b.thumbs.redditmedia.com/VekkKOC-9pkK-p4EcxQu4ZVP7NMavdh2drD5BdRRp0A.jpg" title="OSS20B is actually good?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my benchmark OOS20B outperforms and outspeed both Qwen and Gemma, which surprised me.&lt;/p&gt; &lt;p&gt;E.g. One of the test is trivial simple for humans but I found trips even big models really hard, and e.g. often spins qwen into circular thinking, but somehow, it didn't trip OOS. And it's really fast to boot.&lt;/p&gt; &lt;p&gt;It stresses a feature I use a lot and relates closely to structured information retrival and indirect reference, something I found even big models really struggles with. I'll investigate more and try an harder version of the test but I'm really impressed.&lt;/p&gt; &lt;p&gt;Those tests aren't censorship or creative sensitive, I'll put more benchmarks of that kind to test that model ability, but so far I'm impressed. I was under the impression the model wasn't very useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/05032-MendicantBias"&gt; /u/05032-MendicantBias &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1msyqr3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyqr3/oss20b_is_actually_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msyqr3/oss20b_is_actually_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T18:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt070m</id>
    <title>Qwen3-30B-A3B and quantization.</title>
    <updated>2025-08-17T19:30:22+00:00</updated>
    <author>
      <name>/u/yami_no_ko</name>
      <uri>https://old.reddit.com/user/yami_no_ko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been thinking about quantization and how it affects MoE models like Qwen3-30B-A3B versus regular dense models.&lt;/p&gt; &lt;p&gt;The standard rule of thumb is that FP &amp;gt; Q8 &amp;gt;&amp;gt; Q4 &amp;gt;&amp;gt; Q3, with Q8 giving almost full performance and anything below Q4 causing noticeable drops. But with MoE models, I'm wondering if that is different.&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B has 30B parameters split across 3B expert layers. Each expert should be more sensitive to quantization than a regular dense 30B model. However, MoE models are sparse - only a subset of experts activate for any input. This might provide some protection from quantization noise.&lt;/p&gt; &lt;p&gt;This left me wondering: Does aggressive quantization affect MoE models more or less than regular models? &lt;/p&gt; &lt;p&gt;Would FP vs Q8 be nearly identical for MoE models, but Q8 vs Q4 cause noticeable performance drops? Or am I missing something about how quantization works with sparse architectures? Does the standard rule of thumb(barely anything useful outside the scale between Q4 and Q8) apply here?&lt;/p&gt; &lt;p&gt;I'm curious if the standard quantization rules apply or if MoE models have fundamentally different behavior at different quantization levels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yami_no_ko"&gt; /u/yami_no_ko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt070m/qwen330ba3b_and_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt070m/qwen330ba3b_and_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt070m/qwen330ba3b_and_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T19:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt2hpr</id>
    <title>GLM-4.5 garbled output?</title>
    <updated>2025-08-17T21:00:28+00:00</updated>
    <author>
      <name>/u/Etzo88</name>
      <uri>https://old.reddit.com/user/Etzo88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2hpr/glm45_garbled_output/"&gt; &lt;img alt="GLM-4.5 garbled output?" src="https://b.thumbs.redditmedia.com/uKhgrpEDmoK-KJAziBvp7fZX_kM-tI85YnopVqXXygs.jpg" title="GLM-4.5 garbled output?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the last few days I'm just getting garbled output from &lt;a href="http://chat.z.ai"&gt;chat.z.ai&lt;/a&gt;, I get a few normal responses and then get this:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img b518370w8njf1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Anyone else experience this or know how to fix it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Etzo88"&gt; /u/Etzo88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2hpr/glm45_garbled_output/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2hpr/glm45_garbled_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2hpr/glm45_garbled_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T21:00:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt2iev</id>
    <title>GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure.</title>
    <updated>2025-08-17T21:01:10+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"&gt; &lt;img alt="GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure." src="https://external-preview.redd.it/LOoaiYGJSklUhS_jyXNZtXqZW5tq1NuC9Dm2RNcy-8Q.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fb11e05f1a215e4e59de75726903bb0ecb7f1d6" title="GPT-OSS-20B at 10,000 tokens/second on a 4090? Sure." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was doing some tool calling tests while figuring out how to work with the Harmony GPT-OSS prompt format. I made a little helpful tool here if you're trying to understand how harmony works (there's a whole repo there too with a bit deeper exploration if you're curious):&lt;br /&gt; &lt;a href="https://github.com/Deveraux-Parker/GPT-OSS-MONKEY-WRENCHES/blob/main/harmony_educational_demo.html"&gt;https://github.com/Deveraux-Parker/GPT-OSS-MONKEY-WRENCHES/blob/main/harmony_educational_demo.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyway, I wanted to benchmark the system so I asked it to make a fun benchmark, and this is what it came up with. In this video, missiles are falling from the sky and the agent has to see their trajectory and speed, run a tool call with python to anticipate where the missile will be in the future, and fire an explosive anti-missile at it so that it can hit the spot it'll be when the missile arrives. To do this, it needs to have low latency, understand its own latency, and be able to RAPIDLY fire off tool calls. This is firing with 100% accuracy (it technically missed 10 tool calls along the way but was able to recover and fire them before the missiles hit the ground).&lt;/p&gt; &lt;p&gt;So... here's GPT-OSS-20b running 100 agents simultaneously at 131,076 token context, each agent with its own 131k context window, each hitting sub-100ms ttft, blowing everything out of the sky at 10k tokens/second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=8T8drT0rwCk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt2iev/gptoss20b_at_10000_tokenssecond_on_a_4090_sure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T21:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mswfiy</id>
    <title>Detecting Hallucinations in LLM Function Calling with Entropy (Part 2)</title>
    <updated>2025-08-17T17:07:42+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.archgw.com/blogs/detecting-hallucinations-in-llm-function-calling-with-entropy-part-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mswfiy/detecting_hallucinations_in_llm_function_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mswfiy/detecting_hallucinations_in_llm_function_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T17:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms4n55</id>
    <title>What does it feel like: Cloud LLM vs Local LLM.</title>
    <updated>2025-08-16T19:10:29+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt; &lt;img alt="What does it feel like: Cloud LLM vs Local LLM." src="https://preview.redd.it/8qtcdau4kfjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64c4609d4440c5a870f624682bb7bead5dece104" title="What does it feel like: Cloud LLM vs Local LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't get me wrong, I love local models, but they give me this anxiety. We need to fix this... 😂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qtcdau4kfjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1msva3w</id>
    <title>MiniPC Ryzen 7 6800H iGPU 680M LLM benchmark Vulkan backend</title>
    <updated>2025-08-17T16:23:07+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;System: MiniPC AceMagic AMD Ryzen 7 &lt;a href="https://www.techpowerup.com/cpu-specs/ryzen-7-6800h.c2527"&gt;6800H&lt;/a&gt; with iGPU &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-680m.c3871"&gt;680M&lt;/a&gt; and 64GB &lt;a href="https://en.wikipedia.org/wiki/DDR5_SDRAM"&gt;DDR5&lt;/a&gt; memory on &lt;a href="https://kubuntu.org/"&gt;Kubuntu&lt;/a&gt; 25.10 and &lt;a href="https://docs.mesa3d.org/relnotes/25.1.7.html"&gt;Mesa 25.1.7&lt;/a&gt;-1ubuntu1 for AMD open drivers. &lt;/p&gt; &lt;p&gt;I'm using llama.cpp &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/7195"&gt;bench&lt;/a&gt; feature with &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/10879"&gt;Vulkan&lt;/a&gt; backend. I've been using &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; for doing local AI stuff. I found llama.cpp is easier and faster to get LLM going compared to &lt;a href="https://www.reddit.com/r/ollama/comments/1lbpqln/minipc_ryzen_7_6800h_cpu_and_igpu_680m/"&gt;Ollama&lt;/a&gt; with &lt;a href="https://github.com/ollama/ollama/blob/main/docs/gpu.md#overrides-on-linux"&gt;overriding&lt;/a&gt; ROCm environment for iGPU and &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mpm728/amd_radeon_rx_480_8gb_benchmark/"&gt;older Radeon cards&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;I download &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;llama-b6182-bin-ubuntu-vulkan-x64&lt;/a&gt; and just unzipped. Kubuntu already has AMD drivers baked into its kernel thanks to Mesa.&lt;/p&gt; &lt;p&gt;I consider 3 to 4 tokens per second (t/s) for token generation (tg128) as minimum and I like 14B models &lt;a href="https://llm-stats.com/models/compare/qwen-2.5-14b-instruct-vs-qwen2-7b-instruct"&gt;accuracy&lt;/a&gt; versus smaller models. So here we go.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF"&gt;Qwen2.5-Coder-14B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;size: 14.62 GiB&lt;/p&gt; &lt;p&gt;params: 14.77 B&lt;/p&gt; &lt;p&gt;ngl: 99&lt;/p&gt; &lt;p&gt;Benchmarks:&lt;/p&gt; &lt;p&gt;Regular CPU only llama.cpp (&lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b6182/llama-b6187-bin-ubuntu-x64.zip"&gt;llama-b6182-bin-ubuntu-x64&lt;/a&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time ~/build/bin/llama-bench --model /var/lib/gpustack/cache/huggingface/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF/qwen2.5-coder-14b-instruct-q8_0.gguf load_backend: loaded RPC backend from /home/user33/build/bin/libggml-rpc.so load_backend: loaded CPU backend from /home/user33/build/bin/libggml-cpu-haswell.so | model | backend | test | t/s | | --------------- | ---------- | --------------: | -------------------: | | qwen2 14B Q8_0 | RPC | pp512 | 19.04 ± 0.05 | | qwen2 14B Q8_0 | RPC | tg128 | 3.26 ± 0.00 | build: 1fe00296 (6182) real 6m8.309s user 47m37.413s sys 0m6.497s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Vulkan CPU/iGPU llama.cpp (&lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b6182/llama-b6187-bin-ubuntu-vulkan-x64.zip"&gt;llama-b6187-bin-ubuntu-vulkan-x64&lt;/a&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time ~/vulkan/build/bin/llama-bench --model /var/lib/gpustack/cache/huggingface/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF/qwen2.5-coder-14b-instruct-q8_0.gguf load_backend: loaded RPC backend from /home/user33/vulkan/build/bin/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon Graphics (RADV REMBRANDT) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from /home/user33/vulkan/build/bin/libggml-vulkan.so load_backend: loaded CPU backend from /home/user33/vulkan/build/bin/libggml-cpu-haswell.so | model | backend | test | t/s | | -------------- | ---------- | --------------: | -------------------: | | qwen2 14B Q8_0 | RPC,Vulkan | pp512 | 79.34 ± 1.15 | | qwen2 14B Q8_0 | RPC,Vulkan | tg128 | 3.12 ± 0.75 | build: 1fe00296 (6182) real 4m21.431s user 1m1.655s sys 0m9.730s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Observation:&lt;/p&gt; &lt;p&gt;VULKAN backend total benchmark run time (real) dropped from 6m8s to 4m21s and &lt;/p&gt; &lt;p&gt;pp512 increased from 19.04 to 79.34 while &lt;/p&gt; &lt;p&gt;tg128 decreased from 3.26 to 3.12&lt;/p&gt; &lt;p&gt;Considering slight difference in token generation speed, using Vulkan backend for AMD CPU 6800H benefits from the iGPU 680M overall llama performance over CPU only. DDR5 memory bandwidth is doing the bulk of the work but we should see continuous improvements with Vulkan. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msva3w/minipc_ryzen_7_6800h_igpu_680m_llm_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msva3w/minipc_ryzen_7_6800h_igpu_680m_llm_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msva3w/minipc_ryzen_7_6800h_igpu_680m_llm_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T16:23:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1msjr8e</id>
    <title>Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp; 1.6B)</title>
    <updated>2025-08-17T06:39:35+00:00</updated>
    <author>
      <name>/u/benja0x40</name>
      <uri>https://old.reddit.com/user/benja0x40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt; &lt;img alt="Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp;amp; 1.6B)" src="https://external-preview.redd.it/ODf4ePnObFjNLo_T-D3tl5IjEp3QG9wN69Zl1K75jBk.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6edbd43244d19019e3eb00f1cf461de13c681f23" title="Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp;amp; 1.6B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;2 models based on the hybrid &lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models"&gt;LFM2 architecture&lt;/a&gt;: LFM2-VL-450M and LFM2-VL-1.6B&lt;/li&gt; &lt;li&gt;Available quant: 8bit MLX, GGUF Q8 &amp;amp; Q4 (llama.cpp release &lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b6183"&gt;b6183&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.liquid.ai/blog/lfm2-vl-efficient-vision-language-models"&gt;Blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa"&gt;HuggingFace Collection&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f7cnaj82zijf1.png?width=2072&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8dae64814c6f611481706d86e4a7643b7dc776"&gt;Figure 3. Processing time comparison across vision-language models.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Added GGUF availability and compatible llama.cpp release&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benja0x40"&gt; /u/benja0x40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T06:39:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1msgl6q</id>
    <title>Added Qwen 0.6B to the small model overview in IFEval.</title>
    <updated>2025-08-17T03:41:26+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"&gt; &lt;img alt="Added Qwen 0.6B to the small model overview in IFEval." src="https://external-preview.redd.it/9Cl7KVCIkap1D9OBhLKIL0DrKnbvINMV1azrpCVXD0U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db8e296f1f9b6b888a016ade0da5771f2fa87434" title="Added Qwen 0.6B to the small model overview in IFEval." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/ygMzbHp.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T03:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1msb0mq</id>
    <title>For those who run large models locally.. HOW DO YOU AFFORD THOSE GPUS</title>
    <updated>2025-08-16T23:14:00+00:00</updated>
    <author>
      <name>/u/abaris243</name>
      <uri>https://old.reddit.com/user/abaris243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;okay I'm just being nosy.. I mostly run models and fine tune as a hobby so I typically only run models under the 10b parameter range, is everyone that is running larger models just paying for cloud services to run them? and for those of you who do have stacks of A100/H100s is this what you do for a living, how do you afford it??&lt;/p&gt; &lt;p&gt;edit: for more context about me and my setup, I have a 3090ti and 64gb ram, I am actually a cgi generalist / 3d character artist and my industry is taking a huge hit right now, so with my extra free time and my already decent set up I've been learning to fine tune models and format data on the side, idk if ill ever do a full career 180 but I love new tech (even though these new technologies and ideas are eating my current career)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abaris243"&gt; /u/abaris243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T23:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn1n0</id>
    <title>Looks like Kimi K2 quietly joined the “5.9 − 5.11 = ?” support group. 😩</title>
    <updated>2025-08-17T10:06:04+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"&gt; &lt;img alt="Looks like Kimi K2 quietly joined the “5.9 − 5.11 = ?” support group. 😩" src="https://preview.redd.it/e0o9q4g90kjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96de0c19ca32f0ffe1af17db9ea73f11e103ccae" title="Looks like Kimi K2 quietly joined the “5.9 − 5.11 = ?” support group. 😩" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e0o9q4g90kjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:06:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1msvs0i</id>
    <title>What happened to the Uncensored models like Dolphin?</title>
    <updated>2025-08-17T16:42:30+00:00</updated>
    <author>
      <name>/u/krigeta1</name>
      <uri>https://old.reddit.com/user/krigeta1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year uncensored model like Dolphin(i was able to use it only) was fully uncensored and able to answers are things that are just really creepy and as of today there are open source LLMs that are so much powerful than the dolphin but nobody is releasing those models anymore?&lt;/p&gt; &lt;p&gt;Any specific reason why we are not getting uncensored models anymore?&lt;/p&gt; &lt;p&gt;Edit: wow guys, its been minutes and you guys have shared a lot of models, Hats off to you all!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krigeta1"&gt; /u/krigeta1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msvs0i/what_happened_to_the_uncensored_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msvs0i/what_happened_to_the_uncensored_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msvs0i/what_happened_to_the_uncensored_models_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T16:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1msy01r</id>
    <title>Why does Qwen3-30B-A3B-Instruct-2507 Q8_0 work on my machine and no others come close?</title>
    <updated>2025-08-17T18:07:08+00:00</updated>
    <author>
      <name>/u/9acca9</name>
      <uri>https://old.reddit.com/user/9acca9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm surprised that having a machine with 8GB of VRAM and 32GB of RAM can run this LLM. Slow, yes, but it runs and gives good answers. Why isn't there another one like it? Why not a DeepSeek R1, for example?&lt;/p&gt; &lt;p&gt;I don't really mind waiting too much if I'm going to get an &amp;quot;accurate&amp;quot; answer.&lt;/p&gt; &lt;p&gt;Obviously, I don't use it regularly, but I like having an LLM to maybe ask a &amp;quot;personal&amp;quot; question, and also in case at some point they put restrictions on all non-local LLMs, overprice them, or lobotomize them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9acca9"&gt; /u/9acca9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msy01r/why_does_qwen330ba3binstruct2507_q8_0_work_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msy01r/why_does_qwen330ba3binstruct2507_q8_0_work_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msy01r/why_does_qwen330ba3binstruct2507_q8_0_work_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T18:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1msyzh8</id>
    <title>MoE optimization idea (VRAM/RAM)</title>
    <updated>2025-08-17T18:44:39+00:00</updated>
    <author>
      <name>/u/fredconex</name>
      <uri>https://old.reddit.com/user/fredconex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"&gt; &lt;img alt="MoE optimization idea (VRAM/RAM)" src="https://a.thumbs.redditmedia.com/HVYu7GZpWuFHEFw5ZYAS9pbfX6y2jpeFyNGTo1j_QB8.jpg" title="MoE optimization idea (VRAM/RAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt; &lt;p&gt;I was doing some tests, and have noticed that properly offloading MoE to CPU can improve performance, but there's a thing that might not be taken into account.&lt;/p&gt; &lt;p&gt;We're offloading sequentially, not by most commonly used experts, below there's an image it's from my CPU inference engine, I did some changes to it, I can do inference on Qwen3 30B-A3B Q8_0 (35gb) using only 9gb of RAM, speed will drop as I'm constantly loading/unloading the experts from SSD.&lt;/p&gt; &lt;p&gt;But with this I could find something interesting, experts usage isn't linear, there are experts that have higher activation frequency, so my proposed idea is that when offloading between RAM/VRAM we keep track of currently most used experts and move them around based on their usage, most used experts will move to VRAM, least used will drop to RAM, I believe with this kind of smart optimization we may be able to extract more speed from MoE models and also make possible to run bigger models on limited hardware by reducing the amount of in-memory experts.&lt;/p&gt; &lt;p&gt;I would try to implement this into llama.cpp but I'm not very used to C/C++ programming, but would like to hear thoughts on who might be familiar with it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/donxlk19jmjf1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43d4e8e9c5dab56645ac661ac2fbc28bc290c9cf"&gt;https://preview.redd.it/donxlk19jmjf1.png?width=805&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43d4e8e9c5dab56645ac661ac2fbc28bc290c9cf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fredconex"&gt; /u/fredconex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msyzh8/moe_optimization_idea_vramram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T18:44:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn3gi</id>
    <title>Ovis2.5 9B ~ 2B - New Multi-modal LLMs from Alibaba</title>
    <updated>2025-08-17T10:09:17+00:00</updated>
    <author>
      <name>/u/Sad_External6106</name>
      <uri>https://old.reddit.com/user/Sad_External6106</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing with &lt;strong&gt;Ovis2.5 (2B &amp;amp; 9B)&lt;/strong&gt; the past few days. The cool part is it now has an optional &lt;em&gt;think&lt;/em&gt; mode — the model will slow down a bit but actually self-check and refine answers, which really helps on harder reasoning tasks. Also the OCR feels way better than before, especially on messy charts and dense documents. Overall, a pretty practical upgrade if you care about reasoning + OCR.&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335"&gt;https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_External6106"&gt; /u/Sad_External6106 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:09:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mt0rld</id>
    <title>Is it just me, or is LM Studio really pushing the new gpt-oss?</title>
    <updated>2025-08-17T19:52:39+00:00</updated>
    <author>
      <name>/u/PracticlySpeaking</name>
      <uri>https://old.reddit.com/user/PracticlySpeaking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"&gt; &lt;img alt="Is it just me, or is LM Studio really pushing the new gpt-oss?" src="https://b.thumbs.redditmedia.com/I7KSIeyl1d4CoN4rEoRDFE7evtxOdppiW6wbj9W6Q5g.jpg" title="Is it just me, or is LM Studio really pushing the new gpt-oss?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...maybe a little too far? I mean, the setup has a step for &amp;quot;Now download some models&amp;quot; — that only offers gpt-oss. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mmqsn49rwmjf1.png?width=713&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c1b4736e379c98e35634f3c6ed02aa26f79bf8a"&gt;the one model to rule them all?&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticlySpeaking"&gt; /u/PracticlySpeaking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mt0rld/is_it_just_me_or_is_lm_studio_really_pushing_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T19:52:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1msv4us</id>
    <title>GPT-OSS is not good at Brazilian Legal Framework :(</title>
    <updated>2025-08-17T16:17:35+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"&gt; &lt;img alt="GPT-OSS is not good at Brazilian Legal Framework :(" src="https://preview.redd.it/uqksokgduljf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d0c45bd812e8cb6b20834de28e876efa3b08b4c" title="GPT-OSS is not good at Brazilian Legal Framework :(" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;benchmark: &lt;a href="https://huggingface.co/datasets/celsowm/legalbench.br"&gt;https://huggingface.co/datasets/celsowm/legalbench.br&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uqksokgduljf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msv4us/gptoss_is_not_good_at_brazilian_legal_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T16:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1msosdv</id>
    <title>Why does Mistral NeMo's usage keep growing even after more than a year since releasing?</title>
    <updated>2025-08-17T11:46:54+00:00</updated>
    <author>
      <name>/u/xugik1</name>
      <uri>https://old.reddit.com/user/xugik1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt; &lt;img alt="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" src="https://preview.redd.it/5wd0ayxihkjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef52cf7168e409394d3d181f871e20c40bcefa5d" title="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xugik1"&gt; /u/xugik1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5wd0ayxihkjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T11:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1msrnqq</id>
    <title>Wow anthropic and Google losing coding share bc of qwen 3 coder</title>
    <updated>2025-08-17T13:59:47+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt; &lt;img alt="Wow anthropic and Google losing coding share bc of qwen 3 coder" src="https://preview.redd.it/rwehyliy5ljf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c67f716968425732683dc36fdd2644caa8322da3" title="Wow anthropic and Google losing coding share bc of qwen 3 coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwehyliy5ljf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1msr7j8</id>
    <title>To all vibe coders I present</title>
    <updated>2025-08-17T13:40:07+00:00</updated>
    <author>
      <name>/u/theundertakeer</name>
      <uri>https://old.reddit.com/user/theundertakeer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt; &lt;img alt="To all vibe coders I present" src="https://external-preview.redd.it/dXZiNzRocGcybGpmMeA17HlDZqcxGH0WPMXNGATdmxTbHU45E1nSLLgU5DlN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc5981b886ff07914ad22d7db97d58fa9b60c3a9" title="To all vibe coders I present" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theundertakeer"&gt; /u/theundertakeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eckuwlog2ljf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
