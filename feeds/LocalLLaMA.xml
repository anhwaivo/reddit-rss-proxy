<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-04T12:30:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mgv53t</id>
    <title>GLM 4.5 Air Produces Better Code Without Thinking, Using 3-bit MLX (/nothink)?</title>
    <updated>2025-08-03T21:28:42+00:00</updated>
    <author>
      <name>/u/jcmyang</name>
      <uri>https://old.reddit.com/user/jcmyang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I encountered a strange situation with GLM-4.5-Air 3bit mlx that maybe others can shed light on: I tried to reproduce the Flappy Bird game featured in the &lt;a href="http://z.ai/blog/glm-4.5"&gt;z.ai/blog/glm-4.5&lt;/a&gt; blog post, using the exact same prompt, but failed 3 times - the generated game either fails during collision detection (.i.e. the bird dies without hitting the pipes), or the top and bottom pipes merge and there's no way through.&lt;/p&gt; &lt;p&gt;I gave up on the model for a while, thinking that it was due to the 3-bit quant. But upon reading a reddit post decided to try something: adding /nothink to the end of the prompt. This not only eliminated the &amp;quot;thinking&amp;quot; part of the output tokens, but generated a working game in one shot, with correct collision detection but also with added cloud in the background, just like in the blog post.&lt;/p&gt; &lt;p&gt;Can anyone with 4, 6 or 8 bit mlx version verify if they have this problem? Here's the exact prompt: &amp;quot;Write a Flappy Bird game for me in a single HTML page. Keep the gravity weak so that the game is not too hard.&amp;quot;&lt;/p&gt; &lt;p&gt;PS. I am running this on M1 Max Mac Studio w/ 64GB and 32C GPU, and get about 22 tokens/sec in LM Studio. Also, Qwen3-Coder-30B-A3B (unlsoth Q8_0) generated this game, and others, in one shot without problem, at about 50 tokens/sec with flash attention on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jcmyang"&gt; /u/jcmyang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgv53t/glm_45_air_produces_better_code_without_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T21:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgnwnx</id>
    <title>Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!</title>
    <updated>2025-08-03T16:42:24+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/"&gt; &lt;img alt="Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!" src="https://external-preview.redd.it/QvqyA98HcA5dY_pf-rNusUvIEIgIYjCW-RCNgIbKym0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7344e67fe48dc6a6f67623605b3dc51b204d189" title="Drummer's Cydonia R1 24B v4 - A thinking Mistral Small 3.2!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgnwnx/drummers_cydonia_r1_24b_v4_a_thinking_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T16:42:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh9r0z</id>
    <title>Best LLM gateway?</title>
    <updated>2025-08-04T10:27:21+00:00</updated>
    <author>
      <name>/u/Educational-Bison786</name>
      <uri>https://old.reddit.com/user/Educational-Bison786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing out different LLM gateways for agent infra and wanted to share some notes. I used to spend most of my time exploring prompt engineering tools, but lately I’ve shifted focus to the infra side, specifically LLM gateways.&lt;/p&gt; &lt;p&gt;Most of the hosted ones are fine for basic key management or retries, but they fall short once you care about latency, throughput, or chaining providers together cleanly. Some of them also have surprising bottlenecks under load or lack good observability out of the box.&lt;/p&gt; &lt;p&gt;Some quick observations from what I tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://getmax.im/2frost"&gt;Bifrost&lt;/a&gt; (Go, self-hosted): Surprisingly fast even under high load. Saw around 11µs overhead at 5K RPS and significantly lower memory usage compared to LiteLLM. Has native support for many providers and includes fallback, logging, Prometheus monitoring, and a visual web UI. You can integrate it without touching any SDKs, just change the base URL.&lt;/li&gt; &lt;li&gt;&lt;a href="https://portkey.ai/features/ai-gateway"&gt;Portkey&lt;/a&gt;: Decent for user-facing apps. It focuses more on retries and usage limits. Not very flexible when you need complex workflows or full visibility. Latency becomes inconsistent after a few hundred RPS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kong&lt;/strong&gt; and &lt;a href="https://www.solo.io/products/gloo-ai-gateway"&gt;Gloo&lt;/a&gt;: These are general-purpose API gateways. You can bend them to work for LLM routing, but it takes a lot of setup and doesn’t feel natural. Not LLM-aware.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloudflare’s AI Gateway&lt;/strong&gt;: Pretty good for lightweight routing if you're already using Cloudflare. But it’s a black box, not much visibility or customization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aisera’s Gateway&lt;/strong&gt;: Geared toward enterprise support use cases. More of a vertical solution. Didn’t feel suitable for general-purpose LLM infra.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;LiteLLM&lt;/a&gt;: Super easy to get started and works well at small scale. But once we pushed load, it had around 50ms overhead and high memory usage. No built-in monitoring. It became hard to manage during bursts or when chaining calls.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear what others are running in production, especially if you’re doing failover, traffic splitting, or anything more advanced.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational-Bison786"&gt; /u/Educational-Bison786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh9r0z/best_llm_gateway/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T10:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgjlek</id>
    <title>Are Chinese LLM companies effectively price dumping?</title>
    <updated>2025-08-03T13:43:23+00:00</updated>
    <author>
      <name>/u/uutnt</name>
      <uri>https://old.reddit.com/user/uutnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People here seem to assume that Chinese AI companies are developing and releasing these models, which cost tens of millions of dollars to develop, for free out of the goodness of their heart.&lt;/p&gt; &lt;p&gt;I think this is absurd, considering these are for-profit companies, with shareholders who expect an ROI. In the case of Meta (and perhaps AliBaba), the explanation was it's about &lt;a href="https://gwern.net/complement"&gt;commoditizing your complement&lt;/a&gt;. But for many of these companies, which are pure play AI Labs, this simply does not hold.&lt;/p&gt; &lt;p&gt;So the question remains, why are they doing this?&lt;/p&gt; &lt;p&gt;One theory I would put forward is, they are playing the long game, and attempting to disincentivize investment in US AI labs, with the premise that investors will never recoup their investment, since similar capabilities will be offered for free. There is &lt;a href="https://chatgpt.com/s/dr_688f65761670819181f5f8f5e52f9838"&gt;a precedent&lt;/a&gt; of Chinese companies doing similarly, in the context of mineral production, which has resulted in most production moving to China.&lt;/p&gt; &lt;p&gt;If this is the case, it will be good for consumers in the short-term, but less so in the long-term, at least for non-Chinese entities. If you don't find this theory convincing, I would be interested in hearing other alternative explanations for the rise in Chinese open-source models.&lt;/p&gt; &lt;p&gt;What prompted this question, was the &lt;a href="https://youtu.be/mYDSSRS-B5U?t=2203"&gt;recent interview&lt;/a&gt; with Dario from Anthropic, where he was asked about the threat to the business model posed by open-source models. (I don't find his response very compelling).&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;One aside, its known that Twitter is banned in China. Yet, we see many Chinese-based AI researchers communicating there, on a daily basis. Sure it can be accessed via VPN, but these are publicly known figures, so there is no anonymity. What explains this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uutnt"&gt; /u/uutnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgjlek/are_chinese_llm_companies_effectively_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T13:43:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgpb8t</id>
    <title>Reimplemention of Qwen 2 from scratch</title>
    <updated>2025-08-03T17:38:43+00:00</updated>
    <author>
      <name>/u/CodingWithSatyam</name>
      <uri>https://old.reddit.com/user/CodingWithSatyam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🧠 Just Finished: Implementing Qwen 2 (1.5B) from Scratch A few days ago, I built the Qwen 2 language model (1.5B) completely from scratch, making it the second LLM I’ve implemented after Gemma 🚀. This was a major milestone for me, especially since there’s no open-source implementation of Qwen 2 available online (at least none I could find).&lt;/p&gt; &lt;p&gt;What makes this build special: ✅ Implemented without access to source code 📖 Based entirely on the Qwen 1 &amp;amp; Qwen 2 research papers 🧱 Supports Qwen 2-1.5B architecture (more sizes coming soon!) ⚠️ Does not support Mixture of Experts (MoE) yet&lt;/p&gt; &lt;p&gt;This project pushed my understanding of transformer architectures even further, and I’m excited to keep going. If you're into LLMs, model replication, or want to see how Qwen 2 works under the hood, this might interest you!&lt;/p&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/introlix/Swiftlet"&gt;https://github.com/introlix/Swiftlet&lt;/a&gt; Kaggle: &lt;a href="https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet"&gt;https://www.kaggle.com/code/apibrains/qwen2-model-swiftlet&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodingWithSatyam"&gt; /u/CodingWithSatyam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgpb8t/reimplemention_of_qwen_2_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T17:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgiyg4</id>
    <title>Why doesn't "OpenAI" just release one of the models they already have? Like 3.5</title>
    <updated>2025-08-03T13:14:11+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are they really gonna train a model that's absolutely useless to give to us?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgiyg4/why_doesnt_openai_just_release_one_of_the_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T13:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgmx8w</id>
    <title>Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM</title>
    <updated>2025-08-03T16:01:53+00:00</updated>
    <author>
      <name>/u/dlp_randombk</name>
      <uri>https://old.reddit.com/user/dlp_randombk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/"&gt; &lt;img alt="Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM" src="https://external-preview.redd.it/JMH4JviM3uUr7o0BdE49mxUti-kj575to7zYT_Rzt3A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50d687fd0b27b1fc30b1175e432b4518d7d1f15d" title="Open Source Voice Cloning at 16x real-time: Porting Chatterbox to vLLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dlp_randombk"&gt; /u/dlp_randombk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/randombk/chatterbox-vllm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgmx8w/open_source_voice_cloning_at_16x_realtime_porting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T16:01:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgky8g</id>
    <title>This might be the largest un-aligned open-source model</title>
    <updated>2025-08-03T14:41:20+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a completely new 70B dense model trained from scratch on 1.5T high quality tokens - only SFT with basic chat and instructions, no RLHF alignment. Plus, it speaks Korean and Japanese.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;https://huggingface.co/trillionlabs/Tri-70B-preview-SFT&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgky8g/this_might_be_the_largest_unaligned_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T14:41:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh7yud</id>
    <title>MLX 4bit DWQ vs 8bit eval</title>
    <updated>2025-08-04T08:33:46+00:00</updated>
    <author>
      <name>/u/Tiny_Judge_2119</name>
      <uri>https://old.reddit.com/user/Tiny_Judge_2119</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/"&gt; &lt;img alt="MLX 4bit DWQ vs 8bit eval" src="https://b.thumbs.redditmedia.com/skKvv4aQvCUeHEuMqErewlo-Pi0FnE1rG_pbppcoUSo.jpg" title="MLX 4bit DWQ vs 8bit eval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent a few days finishing the evaluation for Qwen3-30B-A3B-Instruct-2507's quant instead of vibe checking the performance of the DWQ. It turns out the 4bit DWQ is quite close to the 8bit, even though the DWQ is still in an experimental phase, it's quite solid.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390"&gt;https://preview.redd.it/kj8dz3orrygf1.png?width=1590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ebe2b4f0ac64c3b76edea0945e0274cc44b65390&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tiny_Judge_2119"&gt; /u/Tiny_Judge_2119 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh7yud/mlx_4bit_dwq_vs_8bit_eval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T08:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mha439</id>
    <title>Open Music Foundation Models for Full-Song Generation</title>
    <updated>2025-08-04T10:48:51+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;YuE: Open Full-song Music Generation Foundation Model, something similar to &lt;a href="http://Suno.ai"&gt;Suno.ai&lt;/a&gt; but open&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://map-yue.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mha439/open_music_foundation_models_for_fullsong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mha439/open_music_foundation_models_for_fullsong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T10:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgny8p</id>
    <title>When DeepSeek r2?</title>
    <updated>2025-08-03T16:44:12+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/"&gt; &lt;img alt="When DeepSeek r2?" src="https://preview.redd.it/dz0i0w1j2ugf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aaef5f3c86c4f7340d2367eea7fce60751451a94" title="When DeepSeek r2?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They said they're refining it months ago. Possibly timing to coincide with OpenAI's drop? Would be epic, I'm a fan of both. Especially if OpenAI's is not a reasoning model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dz0i0w1j2ugf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgny8p/when_deepseek_r2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T16:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgt2om</id>
    <title>Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio — Possibly the Best Local Cursor Alternative Right Now?</title>
    <updated>2025-08-03T20:07:16+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/"&gt; &lt;img alt="Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio — Possibly the Best Local Cursor Alternative Right Now?" src="https://external-preview.redd.it/dW1vdmpyODUydmdmMTyadiYKHvooWeiroDfdLLS_KqibMMempmwSjMR0DRio.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=089c7afe3ec4ed1899bdb5dd07aa791fcb05f74a" title="Mac M3 + RooCode + Qwen3-Coder-30B (4-bit DWQ) in LM Studio — Possibly the Best Local Cursor Alternative Right Now?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e1348s852vgf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgt2om/mac_m3_roocode_qwen3coder30b_4bit_dwq_in_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T20:07:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh0ltj</id>
    <title>Keep It Simple Pseudo Code (That's what Codex does)</title>
    <updated>2025-08-04T01:37:18+00:00</updated>
    <author>
      <name>/u/cov_id19</name>
      <uri>https://old.reddit.com/user/cov_id19</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh0ltj/keep_it_simple_pseudo_code_thats_what_codex_does/"&gt; &lt;img alt="Keep It Simple Pseudo Code (That's what Codex does)" src="https://preview.redd.it/nk1a76nkpwgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7f08a511a8b4c55448cf10557e9558c548047b1" title="Keep It Simple Pseudo Code (That's what Codex does)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think OpenAI figured something out with this indentation in Codex (KISS). &lt;/p&gt; &lt;p&gt;The instructions are in english, but when overlooking, it is literally &amp;quot;pseudo code&amp;quot; with scopes, if and else clauses, &amp;quot;finally&amp;quot; clauses... &lt;/p&gt; &lt;p&gt;Prompts are pseudo code. Nested indentation plays crucial role in Codex's success IMO.&lt;br /&gt; Using &amp;quot;-&amp;quot;, &amp;quot;\t&amp;quot; and &amp;quot;\n&amp;quot; is pretty efficient. Also, The way _CODING GUIDELINES_ is highlighted is interesting. Reminds of Anthropic's XML tags in Claude, but less elegant. &lt;/p&gt; &lt;p&gt;This is currently one of the most powerful agents. &lt;/p&gt; &lt;p&gt;Keep It Simple? Something to have in mind...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cov_id19"&gt; /u/cov_id19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nk1a76nkpwgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh0ltj/keep_it_simple_pseudo_code_thats_what_codex_does/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh0ltj/keep_it_simple_pseudo_code_thats_what_codex_does/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T01:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhbvig</id>
    <title>What kind of Qwen 2508 do you want tonight? ;)</title>
    <updated>2025-08-04T12:19:39+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/"&gt; &lt;img alt="What kind of Qwen 2508 do you want tonight? ;)" src="https://preview.redd.it/3f5by1b8wzgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f4a0d87b3973237c269d2cef31fbc18fbe655b1" title="What kind of Qwen 2508 do you want tonight? ;)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3f5by1b8wzgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbvig/what_kind_of_qwen_2508_do_you_want_tonight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T12:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgkiti</id>
    <title>Use local LLM to neutralise the headers on the web</title>
    <updated>2025-08-03T14:23:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/"&gt; &lt;img alt="Use local LLM to neutralise the headers on the web" src="https://external-preview.redd.it/NnJxaTIxOHVjdGdmMXmMnlACXncMKAQW0BNSM6l9H9iAn2MnzkxT52_TMFFC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a68cd5e26b5f21da9a193d716904dfd8485c857" title="Use local LLM to neutralise the headers on the web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got to finish a weekend project from a couple of months ago. &lt;/p&gt; &lt;p&gt;This is a small extension that can use a local LLM (any OpenAI-compatible endpoint is supported) to neutralise the clickbaits on the webpages you visit. It works reasonably well with models of Llama 3.2 3B class and above. Works in Chrome and Firefox (you can also install to Edge manually).&lt;/p&gt; &lt;p&gt;Full source and configuration guide is on GitHub: &lt;a href="https://github.com/av/unhype"&gt;https://github.com/av/unhype&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/niaha18uctgf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgkiti/use_local_llm_to_neutralise_the_headers_on_the_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T14:23:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mgtboa</id>
    <title>Horizon Beta is OpenAI</title>
    <updated>2025-08-03T20:16:52+00:00</updated>
    <author>
      <name>/u/MiddleLobster9191</name>
      <uri>https://old.reddit.com/user/MiddleLobster9191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/"&gt; &lt;img alt="Horizon Beta is OpenAI" src="https://a.thumbs.redditmedia.com/inyC6dLBuynY6QZPK34zrtaIVdVEKly7ofVVWlBmYW4.jpg" title="Horizon Beta is OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a"&gt;https://preview.redd.it/m30x07j54vgf1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6a713bfbb84e161155f8e8eb333817c41ff6f23a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Horizon Beta is OpenAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MiddleLobster9191"&gt; /u/MiddleLobster9191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mgtboa/horizon_beta_is_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-03T20:16:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh99hu</id>
    <title>LiteLLM started breaking down for us past 300 RPS, what are folks using in prod?</title>
    <updated>2025-08-04T09:57:58+00:00</updated>
    <author>
      <name>/u/Otherwise_Flan7339</name>
      <uri>https://old.reddit.com/user/Otherwise_Flan7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We started using LiteLLM a few months back to route across OpenAI and Anthropic. It worked well during dev and light load tests. But as soon as we crossed around 300 requests per second, things started to break:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some requests randomly timed out or took way longer than others, even with the same provider&lt;/li&gt; &lt;li&gt;Logs didn’t show much, and tracing failures across providers was difficult&lt;/li&gt; &lt;li&gt;When we tried running it behind a load balancer, we ran into strange behavior with state&lt;/li&gt; &lt;li&gt;Fallbacks didn’t always trigger reliably when a provider was down or rate-limited&lt;/li&gt; &lt;li&gt;We tried plugging in Prometheus, but visibility into request flow was limited&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The architecture is simple, which helps at first, but that simplicity makes it hard to scale without building extra tooling around it.&lt;/p&gt; &lt;p&gt;While looking for alternatives, I came across a few self-hosted ones. Most were either too early or too complex to set up. Eager to know if there are other better alternatives to litellm that work well in prod. Is anyone building their own gateway, or using something more stable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Otherwise_Flan7339"&gt; /u/Otherwise_Flan7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh99hu/litellm_started_breaking_down_for_us_past_300_rps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T09:57:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh6zja</id>
    <title>GLM 4.5 AI Sliders vs Gemini 2.5 Pro Deep Research Infographics</title>
    <updated>2025-08-04T07:28:45+00:00</updated>
    <author>
      <name>/u/z1xto</name>
      <uri>https://old.reddit.com/user/z1xto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/"&gt; &lt;img alt="GLM 4.5 AI Sliders vs Gemini 2.5 Pro Deep Research Infographics" src="https://external-preview.redd.it/oPtkUtibvV31iKPm4upl_ADaAJfJzbdONKUGf8pC5EM.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abcfb3d145a4837cd123c1d5c55d56b5eaefd529" title="GLM 4.5 AI Sliders vs Gemini 2.5 Pro Deep Research Infographics" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Gemini 2.5 Pro Deep Research with infographics since release, but I tried GLM-4.5's slides the past few days... and wow, I actually might prefer it now.&lt;/p&gt; &lt;p&gt;Here is example of same topic:&lt;/p&gt; &lt;p&gt;GLM 4.5 AI Slides:&lt;br /&gt; &lt;a href="https://chat.z.ai/space/u01ja6suarb0-ppt"&gt;https://chat.z.ai/space/u01ja6suarb0-ppt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player"&gt;https://reddit.com/link/1mh6zja/video/0kgfqae7gygf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GEMINI 2.5 Pro DR:&lt;br /&gt; &lt;a href="https://gemini.google.com/share/ca95257c1a48"&gt;https://gemini.google.com/share/ca95257c1a48&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player"&gt;https://reddit.com/link/1mh6zja/video/gmg5vfk2eygf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z1xto"&gt; /u/z1xto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh6zja/glm_45_ai_sliders_vs_gemini_25_pro_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T07:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhb5el</id>
    <title>GLM-4.5 llama.cpp PR is nearing completion</title>
    <updated>2025-08-04T11:44:02+00:00</updated>
    <author>
      <name>/u/DistanceSolar1449</name>
      <uri>https://old.reddit.com/user/DistanceSolar1449</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Current status:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036"&gt;https://github.com/ggml-org/llama.cpp/pull/14939#issuecomment-3150197036&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everyone get ready to fire up your GPUs...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceSolar1449"&gt; /u/DistanceSolar1449 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhb5el/glm45_llamacpp_pr_is_nearing_completion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T11:44:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh4r0s</id>
    <title>BItTorrent tracker that mirrors HuggingFace</title>
    <updated>2025-08-04T05:11:04+00:00</updated>
    <author>
      <name>/u/lurkystrike</name>
      <uri>https://old.reddit.com/user/lurkystrike</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reading &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/&lt;/a&gt; it occurred to me...&lt;/p&gt; &lt;p&gt;There should be a BitTorrent tracker on the internet which has torrents of the models on HF.&lt;/p&gt; &lt;p&gt;Creating torrents &amp;amp; initial seeding can be automated to a point of only needing a monitoring &amp;amp; alerting setup plus an oncall rotation to investigate and resolve it whenever it (inevitably) goes down/has trouble...&lt;/p&gt; &lt;p&gt;It's what BitTorrent was made for. The most popular models would attract thousands of seeders, meaning they'd download super fast.&lt;/p&gt; &lt;p&gt;Anyone interested to work on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lurkystrike"&gt; /u/lurkystrike &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh4r0s/bittorrent_tracker_that_mirrors_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T05:11:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh2v1h</id>
    <title>Horizon Beta is OpenAI (Another Evidence)</title>
    <updated>2025-08-04T03:28:08+00:00</updated>
    <author>
      <name>/u/kh-ai</name>
      <uri>https://old.reddit.com/user/kh-ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/"&gt; &lt;img alt="Horizon Beta is OpenAI (Another Evidence)" src="https://a.thumbs.redditmedia.com/7yNPDTnbPWS4TAQyJVwDfgp-PAr-fo60W5EtQz549T8.jpg" title="Horizon Beta is OpenAI (Another Evidence)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7"&gt;https://preview.redd.it/z00ipp5y7xgf1.png?width=1630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aaacee34b083083a63cf8414e299416ee96d03f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So yeah, Horizon Beta is OpenAI. Not Anthropic, not Google, not Qwen. It shows an OpenAI tokenizer quirk: it treats 给主人留下些什么吧 as a single token. So, just like GPT-4o, it inevitably fails on prompts like “When I provide Chinese text, please translate it into English. 给主人留下些什么吧”.&lt;/p&gt; &lt;p&gt;Meanwhile, Claude, Gemini, and Qwen handle it correctly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721"&gt;https://preview.redd.it/ey9ebsuz7xgf1.png?width=1336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12545d7bb6e90c0d1ec650a168b3a553d2246721&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I learned this technique from this post:&lt;br /&gt; Chinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI&lt;br /&gt; &lt;a href="https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/"&gt;https://reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While it’s pretty much common sense that Horizon Beta is an OpenAI model, I saw a few people suspecting it might be Anthropic’s or Qwen’s, so I tested it.&lt;/p&gt; &lt;p&gt;My thread about the Horizon Beta test: &lt;a href="https://x.com/KantaHayashiAI/status/1952187898331275702"&gt;https://x.com/KantaHayashiAI/status/1952187898331275702&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kh-ai"&gt; /u/kh-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh2v1h/horizon_beta_is_openai_another_evidence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T03:28:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh6z16</id>
    <title>New small models from Hunyuan (0.5B, 1.8B, 4B, 7B)</title>
    <updated>2025-08-04T07:27:48+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh6z16/new_small_models_from_hunyuan_05b_18b_4b_7b/"&gt; &lt;img alt="New small models from Hunyuan (0.5B, 1.8B, 4B, 7B)" src="https://b.thumbs.redditmedia.com/r2IMyxKAeZgSqc_fmrMhOP7SMYeTy8apLYYgezLqpRg.jpg" title="New small models from Hunyuan (0.5B, 1.8B, 4B, 7B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hunyuan just released 4 new dense models. It’s a new architecture and supports hybrid reasoning, 256K context and agent capabilities with tool support! The benchmarks are great but will need to really test them in real world.&lt;/p&gt; &lt;p&gt;Love to see more small models as I'm developing an iOS local chat called &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;Locally AI&lt;/a&gt;. Will look to add them but since it's new architecture it will need to be ported to Apple MLX.&lt;/p&gt; &lt;p&gt;The choice of size here is perfect:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0.5B, 1.8B and 4B great for all iPhones models&lt;/li&gt; &lt;li&gt;7B great for iPad with M chip&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mh6z16"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh6z16/new_small_models_from_hunyuan_05b_18b_4b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh6z16/new_small_models_from_hunyuan_05b_18b_4b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T07:27:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh3s7q</id>
    <title>new Hunyuan Instruct 7B/4B/1.8B/0.5B models</title>
    <updated>2025-08-04T04:16:20+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tescent has released new models (llama.cpp support is already merged!)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-7B-Instruct"&gt;https://huggingface.co/tencent/Hunyuan-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-4B-Instruct"&gt;https://huggingface.co/tencent/Hunyuan-4B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-1.8B-Instruct"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-0.5B-Instruct"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Model Introduction&lt;/h1&gt; &lt;p&gt;Hunyuan is Tencent's open-source efficient large language model series, designed for versatile deployment across diverse computational environments. From edge devices to high-concurrency production systems, these models deliver optimal performance with advanced quantization support and ultra-long context capabilities.&lt;/p&gt; &lt;p&gt;We have released a series of Hunyuan dense models, comprising both pre-trained and instruction-tuned variants, with parameter scales of 0.5B, 1.8B, 4B, and 7B. These models adopt training strategies similar to the Hunyuan-A13B, thereby inheriting its robust performance characteristics. This comprehensive model family enables flexible deployment optimization - from resource-constrained edge computing with smaller variants to high-throughput production environments with larger models, all while maintaining strong capabilities across diverse scenarios.&lt;/p&gt; &lt;h1&gt;Key Features and Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Reasoning Support&lt;/strong&gt;: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-Long Context Understanding&lt;/strong&gt;: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Agent Capabilities&lt;/strong&gt;: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, τ-Bench and C3-Bench.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Inference&lt;/strong&gt;: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;UPDATE&lt;/p&gt; &lt;p&gt;pretrain models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-7B-Pretrain"&gt;https://huggingface.co/tencent/Hunyuan-7B-Pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-4B-Pretrain"&gt;https://huggingface.co/tencent/Hunyuan-4B-Pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain"&gt;https://huggingface.co/tencent/Hunyuan-1.8B-Pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain"&gt;https://huggingface.co/tencent/Hunyuan-0.5B-Pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF"&gt;https://huggingface.co/gabriellarson/Hunyuan-7B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF"&gt;https://huggingface.co/gabriellarson/Hunyuan-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF"&gt;https://huggingface.co/gabriellarson/Hunyuan-1.8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF"&gt;https://huggingface.co/gabriellarson/Hunyuan-0.5B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh3s7q/new_hunyuan_instruct_7b4b18b05b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T04:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhbpmo</id>
    <title>New Qwen Models Today!!!</title>
    <updated>2025-08-04T12:12:00+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"&gt; &lt;img alt="New Qwen Models Today!!!" src="https://preview.redd.it/qemmgysvuzgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e45a424e82bdde4384e3d7ba1be6631b2a25639" title="New Qwen Models Today!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qemmgysvuzgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhbpmo/new_qwen_models_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T12:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mh8u1j</id>
    <title>Upgraded my hardware and internet connection so I can download GUFFs way faster than you, all your GGUFs are belong to me now.</title>
    <updated>2025-08-04T09:30:17+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh8u1j/upgraded_my_hardware_and_internet_connection_so_i/"&gt; &lt;img alt="Upgraded my hardware and internet connection so I can download GUFFs way faster than you, all your GGUFs are belong to me now." src="https://external-preview.redd.it/Y2g2cmw4ZzEyemdmMWEK0hhu-2G_hp7f60RsNKbAYzd7FxtzYBG906H02xQh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=602bf58ed7bc3e41691cabd7fa792d487a83c981" title="Upgraded my hardware and internet connection so I can download GUFFs way faster than you, all your GGUFs are belong to me now." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ibr6m7us1zgf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mh8u1j/upgraded_my_hardware_and_internet_connection_so_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mh8u1j/upgraded_my_hardware_and_internet_connection_so_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-04T09:30:17+00:00</published>
  </entry>
</feed>
