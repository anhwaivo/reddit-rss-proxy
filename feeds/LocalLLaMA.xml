<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-13T17:48:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i0b8y8</id>
    <title>What can I do with a good GPU</title>
    <updated>2025-01-13T10:40:39+00:00</updated>
    <author>
      <name>/u/_Shojaku</name>
      <uri>https://old.reddit.com/user/_Shojaku</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while back me and a cousin wanted to do some AI stuff (translation etc), but we had to put it on hold due to reasons. At that time, I became very interested in the ability to run models locally. However I knew I was held back by my computer at the time. Now I have a decent laptop, a Lenovo with ab RTX 4080 12GB. My goal is to do something useful with local AI while understanding on the low level how it works. Whhat can I do with this resource? Where do I start? Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Shojaku"&gt; /u/_Shojaku &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b8y8/what_can_i_do_with_a_good_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b8y8/what_can_i_do_with_a_good_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b8y8/what_can_i_do_with_a_good_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T10:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i04pm9</id>
    <title>What is the cheapest way to run Deepseek on a US Hosted company?</title>
    <updated>2025-01-13T03:17:22+00:00</updated>
    <author>
      <name>/u/MarsupialNo7544</name>
      <uri>https://old.reddit.com/user/MarsupialNo7544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a bit concerned about the privacy policies- especially considering PII data. I love how DeepSeek pricing is on their website- but has anyone tried to load their model in a service provider and see what costing structure works? if so, would like to hear more. thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarsupialNo7544"&gt; /u/MarsupialNo7544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04pm9/what_is_the_cheapest_way_to_run_deepseek_on_a_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04pm9/what_is_the_cheapest_way_to_run_deepseek_on_a_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i04pm9/what_is_the_cheapest_way_to_run_deepseek_on_a_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T03:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzyjsj</id>
    <title>Search-o1: Agentic Search-Enhanced Large Reasoning Models - Renmin University of China</title>
    <updated>2025-01-12T22:13:56+00:00</updated>
    <author>
      <name>/u/Singularian2501</name>
      <uri>https://old.reddit.com/user/Singularian2501</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Singularian2501"&gt; /u/Singularian2501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://search-o1.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzyjsj/searcho1_agentic_searchenhanced_large_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzyjsj/searcho1_agentic_searchenhanced_large_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T22:13:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0hpfc</id>
    <title>How to: Managing Input Data for LLMs (tamingllms.com)</title>
    <updated>2025-01-13T16:22:17+00:00</updated>
    <author>
      <name>/u/HighlanderNJ</name>
      <uri>https://old.reddit.com/user/HighlanderNJ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hpfc/how_to_managing_input_data_for_llms_tamingllmscom/"&gt; &lt;img alt="How to: Managing Input Data for LLMs (tamingllms.com)" src="https://external-preview.redd.it/iDLcYMu1x78WznSSdsxD56ghVtXw06Vn5qB7OlPcB_k.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f786a1cb395413d15ad0da00231f93e2233f99e5" title="How to: Managing Input Data for LLMs (tamingllms.com)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HighlanderNJ"&gt; /u/HighlanderNJ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://docsend.com/view/dp8b6j8jgpexkvkx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hpfc/how_to_managing_input_data_for_llms_tamingllmscom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hpfc/how_to_managing_input_data_for_llms_tamingllmscom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T16:22:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0d4pe</id>
    <title>Where to host a finetune QWEN 2.5 3b model</title>
    <updated>2025-01-13T12:45:23+00:00</updated>
    <author>
      <name>/u/ennriqe</name>
      <uri>https://old.reddit.com/user/ennriqe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am going to fine-tune a QWEN 2.5 3b model using support chat logs. I am going to do the training in a run pod.&lt;/p&gt; &lt;p&gt;What is the best option to host the model to serve inference? At first It will only be used during nights and weekends, most of the time it will not have any use, but when it does it has to reply reasonably fast&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ennriqe"&gt; /u/ennriqe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0d4pe/where_to_host_a_finetune_qwen_25_3b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0d4pe/where_to_host_a_finetune_qwen_25_3b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0d4pe/where_to_host_a_finetune_qwen_25_3b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T12:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzp789</id>
    <title>Mark Zuckerberg believes in 2025, Meta will probably have a mid-level engineer AI that can write code, and over time it will replace people engineers.</title>
    <updated>2025-01-12T15:34:50+00:00</updated>
    <author>
      <name>/u/Admirable-Star7088</name>
      <uri>https://old.reddit.com/user/Admirable-Star7088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/slow_developer/status/1877798620692422835?mx=2"&gt;https://x.com/slow_developer/status/1877798620692422835?mx=2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=USBW0ESLEK0"&gt;https://www.youtube.com/watch?v=USBW0ESLEK0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://tribune.com.pk/story/2521499/zuckerberg-announces-meta-plans-to-replace-mid-level-engineers-with-ais-this-year"&gt;https://tribune.com.pk/story/2521499/zuckerberg-announces-meta-plans-to-replace-mid-level-engineers-with-ais-this-year&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think? Is he too optimistic, or can we expect &lt;strong&gt;vastly improved&lt;/strong&gt; (coding) LLMs very soon? Will this be Llama 4? :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable-Star7088"&gt; /u/Admirable-Star7088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzp789/mark_zuckerberg_believes_in_2025_meta_will/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzp789/mark_zuckerberg_believes_in_2025_meta_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzp789/mark_zuckerberg_believes_in_2025_meta_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T15:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzkw3f</id>
    <title>DeepSeek V3 is the gift that keeps on giving!</title>
    <updated>2025-01-12T11:37:25+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"&gt; &lt;img alt="DeepSeek V3 is the gift that keeps on giving!" src="https://preview.redd.it/fj10nizoujce1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b294748a76dfcaf7f0f25300479cd3ea3b25308" title="DeepSeek V3 is the gift that keeps on giving!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fj10nizoujce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzkw3f/deepseek_v3_is_the_gift_that_keeps_on_giving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T11:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0bm84</id>
    <title>Best translation model?</title>
    <updated>2025-01-13T11:07:24+00:00</updated>
    <author>
      <name>/u/xdoso</name>
      <uri>https://old.reddit.com/user/xdoso</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm searching for the current best translation model with interest on quality and latency, and the condition that it must be real open source (allowing commercial usage). Some languages that I'm interested on are English, Japanese and Portuguese. Is there any leaderboard for this?&lt;/p&gt; &lt;p&gt;Some of the options that I'm considering are: - NLLB - mBART - madlad - opus models - LibreTranslate (openNMT behind)&lt;/p&gt; &lt;p&gt;Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xdoso"&gt; /u/xdoso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bm84/best_translation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bm84/best_translation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bm84/best_translation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T11:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0fscb</id>
    <title>Deepseek sending the same answers every time</title>
    <updated>2025-01-13T14:58:04+00:00</updated>
    <author>
      <name>/u/serendipity98765</name>
      <uri>https://old.reddit.com/user/serendipity98765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If fed a similar context (even if there are changes), Deepseek will keep sending the same response. Is there anything we can do to have more contrast and randomness to avoid sounding like a repeating robot sometimes?&lt;/p&gt; &lt;p&gt;It happens in two scenarios: &lt;/p&gt; &lt;p&gt;1) Same context 10 times = 10 times same answer&lt;/p&gt; &lt;p&gt;2) Even if you add NEW lines to the context, sometimes, it will still send the exact same answer&lt;/p&gt; &lt;p&gt;3) If your previous answer is in the context, it will take that and send it again&lt;/p&gt; &lt;p&gt;Anyone has any clues on how to make it stop from sending the same response?&lt;br /&gt; Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/serendipity98765"&gt; /u/serendipity98765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0fscb/deepseek_sending_the_same_answers_every_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0fscb/deepseek_sending_the_same_answers_every_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0fscb/deepseek_sending_the_same_answers_every_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T14:58:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0hn3f</id>
    <title>How does a generative language model still work with spelling mistakes? However BERT like models are too sensitive?</title>
    <updated>2025-01-13T16:19:29+00:00</updated>
    <author>
      <name>/u/Lazy_Wedding_1383</name>
      <uri>https://old.reddit.com/user/Lazy_Wedding_1383</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same as above. Is the issue becasue of how things are tokenized. For example in car models, you have &lt;em&gt;2.0T&lt;/em&gt; (turbocharged 2.0-liter engine), &lt;em&gt;AWD&lt;/em&gt; (All-Wheel Drive), &lt;em&gt;TDI&lt;/em&gt; (Turbocharged Direct Injection).&lt;/p&gt; &lt;p&gt;I'm trying to identify or extract individual specs of a car based on its description. If I have a string like &amp;quot;Toyota Supercharger TDI2T&amp;quot; in a list of names, and if search for &amp;quot; give me all cars that have turbocharged 2.0-liter engine&amp;quot;, a llama model can find it but if i just take consine similarty of the above promt and this string, it will be low&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lazy_Wedding_1383"&gt; /u/Lazy_Wedding_1383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hn3f/how_does_a_generative_language_model_still_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hn3f/how_does_a_generative_language_model_still_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hn3f/how_does_a_generative_language_model_still_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T16:19:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0brfj</id>
    <title>Microserving LLM engines: A multi-level architecture that provides fine-grained APIs for orchestrating llm engines.</title>
    <updated>2025-01-13T11:17:39+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0brfj/microserving_llm_engines_a_multilevel/"&gt; &lt;img alt="Microserving LLM engines: A multi-level architecture that provides fine-grained APIs for orchestrating llm engines." src="https://preview.redd.it/j0jo85w0wqce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1755e5f09f080ac61690b8f2b4ae3c3ff0d12a24" title="Microserving LLM engines: A multi-level architecture that provides fine-grained APIs for orchestrating llm engines." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j0jo85w0wqce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0brfj/microserving_llm_engines_a_multilevel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0brfj/microserving_llm_engines_a_multilevel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T11:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0edwk</id>
    <title>What are the Best Open source local models for reasoning, code and data analysis?</title>
    <updated>2025-01-13T13:51:14+00:00</updated>
    <author>
      <name>/u/devroop_saha844</name>
      <uri>https://old.reddit.com/user/devroop_saha844</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty much the title, I want to work on a use case, where I want to create an agentic system that will analyse financial stock data and give recommendations related to the analysis.&lt;/p&gt; &lt;p&gt;I mean I have done some research, recently there's some buzz about deepseek and qwq but I want something light,, like below 10GB would do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devroop_saha844"&gt; /u/devroop_saha844 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0edwk/what_are_the_best_open_source_local_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0edwk/what_are_the_best_open_source_local_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0edwk/what_are_the_best_open_source_local_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T13:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzmpuq</id>
    <title>VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?</title>
    <updated>2025-01-12T13:31:43+00:00</updated>
    <author>
      <name>/u/SpudMonkApe</name>
      <uri>https://old.reddit.com/user/SpudMonkApe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"&gt; &lt;img alt="VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?" src="https://external-preview.redd.it/aphKSMbfvfDHStraL4JSGgDfke__oze-3mdG_k4jOVQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37b50e3ca1b1a72567f853cc77c80c80b325c53a" title="VLC to add offline, real-time AI subtitles. What do you think the tech stack for this is?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpudMonkApe"&gt; /u/SpudMonkApe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pcmag.com/news/vlc-media-player-to-use-ai-to-generate-subtitles-for-videos"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzmpuq/vlc_to_add_offline_realtime_ai_subtitles_what_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T13:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i02hpf</id>
    <title>Speaches v0.6.0 - Kokoro-82M and PiperTTS API endpoints</title>
    <updated>2025-01-13T01:20:17+00:00</updated>
    <author>
      <name>/u/fedirz</name>
      <uri>https://old.reddit.com/user/fedirz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i02hpf/speaches_v060_kokoro82m_and_pipertts_api_endpoints/"&gt; &lt;img alt="Speaches v0.6.0 - Kokoro-82M and PiperTTS API endpoints" src="https://b.thumbs.redditmedia.com/mOOa_qpBkwYizQvxqZvc7QQ7OW8ESL1W7rDx6Ym6TPM.jpg" title="Speaches v0.6.0 - Kokoro-82M and PiperTTS API endpoints" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I just released Speaches v0.6.0 (previously named &lt;code&gt;faster-whisper-server&lt;/code&gt;). The main feature added in this release is support for Piper and Kokoro Text-to-Speech models. Below is a full feature list:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU and CPU support.&lt;/li&gt; &lt;li&gt;&lt;a href="https://speaches-ai.github.io/speaches/installation/"&gt;Deployable via Docker Compose / Docker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://speaches-ai.github.io/speaches/configuration/"&gt;Highly configurable&lt;/a&gt;&lt;/li&gt; &lt;li&gt;OpenAI API compatible. All tools and SDKs that work with OpenAI's API should work with &lt;code&gt;speaches&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Streaming support (transcription is sent via SSE as the audio is transcribed. You don't need to wait for the audio to fully be transcribed before receiving it). &lt;ul&gt; &lt;li&gt;LocalAgreement2 (&lt;a href="https://aclanthology.org/2023.ijcnlp-demo.3.pdf"&gt;paper&lt;/a&gt; | &lt;a href="https://github.com/ufal/whisper_streaming"&gt;original implementation&lt;/a&gt;) algorithm is used for live transcription.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Live transcription support (audio is sent via WebSocketbe fully as it's generated).&lt;/li&gt; &lt;li&gt;Dynamic model loading/offloading. In the request, specify which model you want to use. It will be loaded automatically and unloaded after a period of inactivity.&lt;/li&gt; &lt;li&gt;Text-to-Speech via &lt;code&gt;kokoro&lt;/code&gt;(Ranked #1 in the &lt;a href="https://huggingface.co/spaces/Pendrokar/TTS-Spaces-Arena"&gt;TTS Arena&lt;/a&gt;) and &lt;code&gt;piper&lt;/code&gt; models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/speaches-ai/speaches/issues/231"&gt;Coming soon&lt;/a&gt;: Audio generation (chat completions endpoint) &lt;ul&gt; &lt;li&gt;Generate a spoken audio summary of a body of text (text in, audio out)&lt;/li&gt; &lt;li&gt;Perform sentiment analysis on a recording (audio in, text out)&lt;/li&gt; &lt;li&gt;Async speech to speech interactions with a model (audio in, audio out)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/speaches-ai/speaches/issues/115"&gt;Coming soon&lt;/a&gt;: Realtime API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Project: &lt;a href="https://github.com/speaches-ai/speaches"&gt;https://github.com/speaches-ai/speaches&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the documentation to get started: &lt;a href="https://speaches-ai.github.io/speaches/"&gt;https://speaches-ai.github.io/speaches/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TTS functionality demo&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1i02hpf/video/xfqgsah1xnce1/player"&gt;https://reddit.com/link/1i02hpf/video/xfqgsah1xnce1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Generating an audio a second or third time is much faster because the model is kept in memory)&lt;/p&gt; &lt;p&gt;NOTE: The published hugging face space is currently broken, but the GradioUI should work when you spin it up locally using Docker&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fedirz"&gt; /u/fedirz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i02hpf/speaches_v060_kokoro82m_and_pipertts_api_endpoints/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i02hpf/speaches_v060_kokoro82m_and_pipertts_api_endpoints/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i02hpf/speaches_v060_kokoro82m_and_pipertts_api_endpoints/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T01:20:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i04iqo</id>
    <title>PS5 for inference</title>
    <updated>2025-01-13T03:07:15+00:00</updated>
    <author>
      <name>/u/Chemical_Mode2736</name>
      <uri>https://old.reddit.com/user/Chemical_Mode2736</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For ~$350 for the whole system is there anything better? This thing packs 3060-tier tflops, 16gb unified gddr6 with ~450gbps bandwidth with 350W PSU. not to mention that this sits in so many people's living rooms, I'm not using any llms while gaming anyways, so PS5 could actually be dual purpose.&lt;/p&gt; &lt;p&gt;Currently looking into how I could run llms on PS5, if anyone has any leads let me know.&lt;/p&gt; &lt;p&gt;I wasn't aware that systems with unified ram using gddr actually existed, let alone that amd did it 5 years ago and so they could release their own DIGITS based on strix halo but with vram instead of ddr...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chemical_Mode2736"&gt; /u/Chemical_Mode2736 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04iqo/ps5_for_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i04iqo/ps5_for_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i04iqo/ps5_for_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T03:07:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1hzuw4z</id>
    <title>Kokoro #1 on TTS leaderboard</title>
    <updated>2025-01-12T19:38:14+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After a short time and a few sabotage attempts, Kokoro is now #1 on the TTS Arena Leaderboard:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/Pendrokar/TTS-Spaces-Arena"&gt;https://huggingface.co/spaces/Pendrokar/TTS-Spaces-Arena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hadn't done any comparative tests to see whether it was better than XTTSv2 (which I was using previously) but the smaller model size and licensing was enough for me to switch after using it just for a few minutes.&lt;/p&gt; &lt;p&gt;I'd like to see work do produce a F16 and Int8 version (currently, I'm running the full F32 version). But this is a very nice model in terms of size performance when you just need simple TTS rendering of text.&lt;/p&gt; &lt;p&gt;I guess the author is busy developing, but I'd love to see a paper on this to understand how the model size was chosen and whether even smaller model sizes were explored.&lt;/p&gt; &lt;p&gt;It would be nice eventually if the full training pipeline and training data would also be open sourced to allow for reproduction, but even having the current voices and model is already very nice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzuw4z/kokoro_1_on_tts_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hzuw4z/kokoro_1_on_tts_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hzuw4z/kokoro_1_on_tts_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-12T19:38:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1i06mew</id>
    <title>How is Kokoro TTS so good with so few parameters?</title>
    <updated>2025-01-13T05:05:22+00:00</updated>
    <author>
      <name>/u/JealousAmoeba</name>
      <uri>https://old.reddit.com/user/JealousAmoeba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As I understand it, Kokoro TTS is StyleTTS 2 with some modifications to the model architecture, trained mainly on outputs from OpenAI and ElevenLabs. But the results seem to be more impressive than StyleTTS and there are only 82M params.&lt;/p&gt; &lt;p&gt;Is it that training on a sufficiently good mix of synthetic data gives you superior results?&lt;/p&gt; &lt;p&gt;Or is there something hidden in the architecture changes that unlocked this new potential?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;https://huggingface.co/hexgrad/Kokoro-82M&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JealousAmoeba"&gt; /u/JealousAmoeba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i06mew/how_is_kokoro_tts_so_good_with_so_few_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i06mew/how_is_kokoro_tts_so_good_with_so_few_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i06mew/how_is_kokoro_tts_so_good_with_so_few_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T05:05:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0hqic</id>
    <title>I Built an LLM Framework in just 100 Lines!!</title>
    <updated>2025-01-13T16:23:39+00:00</updated>
    <author>
      <name>/u/Willing-Site-8137</name>
      <uri>https://old.reddit.com/user/Willing-Site-8137</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen lots of complaints about how complex frameworks like LangChain are. Over the holidays, I wanted to explore just how minimal an LLM framework could be if we stripped away every unnecessary feature.&lt;/p&gt; &lt;p&gt;For example, why even include OpenAI wrappers in an LLM framework??&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;API Changes:&lt;/strong&gt; OpenAI API evolves (client after 0.27), and the official libraries often introduce bugs or dependency issues that are a pain to maintain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DIY Is Simple:&lt;/strong&gt; It's straightforward to generate your own wrapper—just feed the latest vendor documentation to an LLM!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extendibility:&lt;/strong&gt; By avoiding vendor-specific wrappers, developers can easily switch to the latest open-source or self-deployed models..&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Similarly, I strip out features that could be built on-demand rather than baked into the framework. The result? I created a 100-line LLM framework: &lt;a href="https://github.com/miniLLMFlow/PocketFlow/"&gt;https://github.com/miniLLMFlow/PocketFlow/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These 100 lines capture what I see as the core abstraction of most LLM frameworks: a nested directed graph that breaks down tasks into multiple LLM steps, with branching and recursion to enable agent-like decision-making. From there, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Layer On Complex Features:&lt;/strong&gt; I’ve included examples for building (&lt;a href="https://minillmflow.github.io/PocketFlow/multi_agent.html"&gt;multi-&lt;/a&gt;)&lt;a href="https://minillmflow.github.io/PocketFlow/agent.html"&gt;agents&lt;/a&gt;, &lt;a href="https://minillmflow.github.io/PocketFlow/rag.html"&gt;Retrieval-Augmented Generation (RAG)&lt;/a&gt;, &lt;a href="https://minillmflow.github.io/PocketFlow/decomp.html"&gt;task decomposition&lt;/a&gt;, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Work Seamlessly With Coding Assistants:&lt;/strong&gt; Because it’s so minimal, it integrates well with coding assistants like &lt;a href="https://chatgpt.com/g/g-677464af36588191b9eba4901946557b-mini-llm-flow-assistant"&gt;ChatGPT&lt;/a&gt;, Claude, and Cursor.ai. You only need to share the relevant &lt;a href="https://github.com/miniLLMFlow/PocketFlow/tree/main/docs"&gt;documentation &lt;/a&gt;(e.g., in the Claude project), and the assistant can help you build new workflows on the fly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m adding more examples and would love feedback. If there’s a feature you’d like to see or a specific use case you think is missing, please let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Willing-Site-8137"&gt; /u/Willing-Site-8137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hqic/i_built_an_llm_framework_in_just_100_lines/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hqic/i_built_an_llm_framework_in_just_100_lines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hqic/i_built_an_llm_framework_in_just_100_lines/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T16:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0d0qo</id>
    <title>Why do most vector databases use a NoSQL format rather than SQL?</title>
    <updated>2025-01-13T12:38:54+00:00</updated>
    <author>
      <name>/u/Available_Ad_5360</name>
      <uri>https://old.reddit.com/user/Available_Ad_5360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you've been exploring the world of vector databases, you might have noticed that most of them lean toward a NoSQL format instead of a traditional SQL approach. Why is that?&lt;/p&gt; &lt;p&gt;I'm just genuinely curious. Probably scalability?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Ad_5360"&gt; /u/Available_Ad_5360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0d0qo/why_do_most_vector_databases_use_a_nosql_format/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0d0qo/why_do_most_vector_databases_use_a_nosql_format/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0d0qo/why_do_most_vector_databases_use_a_nosql_format/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T12:38:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0hecs</id>
    <title>Researchers open source Sky-T1, a 'reasoning' AI model that can be trained for less than $450</title>
    <updated>2025-01-13T16:09:19+00:00</updated>
    <author>
      <name>/u/mr_house7</name>
      <uri>https://old.reddit.com/user/mr_house7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hecs/researchers_open_source_skyt1_a_reasoning_ai/"&gt; &lt;img alt="Researchers open source Sky-T1, a 'reasoning' AI model that can be trained for less than $450" src="https://external-preview.redd.it/CoBta77nxTwOGfkB2XmPAkIRcSe1Pm3XBChHXR_0ZNI.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7add0ea55f9158b47f0d4f1c57b35784adb6a682" title="Researchers open source Sky-T1, a 'reasoning' AI model that can be trained for less than $450" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_house7"&gt; /u/mr_house7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/01/11/researchers-open-source-sky-t1-a-reasoning-ai-model-that-can-be-trained-for-less-than-450/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hecs/researchers_open_source_skyt1_a_reasoning_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0hecs/researchers_open_source_skyt1_a_reasoning_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T16:09:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1i01k4s</id>
    <title>Llama goes off the rails if you ask it for 5 odd numbers that don’t have the letter E in them</title>
    <updated>2025-01-13T00:33:44+00:00</updated>
    <author>
      <name>/u/Applemoi</name>
      <uri>https://old.reddit.com/user/Applemoi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i01k4s/llama_goes_off_the_rails_if_you_ask_it_for_5_odd/"&gt; &lt;img alt="Llama goes off the rails if you ask it for 5 odd numbers that don’t have the letter E in them " src="https://preview.redd.it/w5j543q9pnce1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61adf904110b30f4cba98ecbd9c36a7462cf005f" title="Llama goes off the rails if you ask it for 5 odd numbers that don’t have the letter E in them " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Applemoi"&gt; /u/Applemoi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w5j543q9pnce1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i01k4s/llama_goes_off_the_rails_if_you_ask_it_for_5_odd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i01k4s/llama_goes_off_the_rails_if_you_ask_it_for_5_odd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T00:33:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0f5tt</id>
    <title>Codestral 25.01: Code at the speed of tab</title>
    <updated>2025-01-13T14:28:39+00:00</updated>
    <author>
      <name>/u/SignalCompetitive582</name>
      <uri>https://old.reddit.com/user/SignalCompetitive582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0f5tt/codestral_2501_code_at_the_speed_of_tab/"&gt; &lt;img alt="Codestral 25.01: Code at the speed of tab" src="https://external-preview.redd.it/6iQ4AGOjK4o2rzQFX3A8casUnMTrEOdxKIDWSbRrbf4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6e958373f510f16ed3c8e63d7174ab21755b11c" title="Codestral 25.01: Code at the speed of tab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SignalCompetitive582"&gt; /u/SignalCompetitive582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/codestral-2501/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0f5tt/codestral_2501_code_at_the_speed_of_tab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0f5tt/codestral_2501_code_at_the_speed_of_tab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T14:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0bsha</id>
    <title>Is this where all LLMs are going?</title>
    <updated>2025-01-13T11:19:47+00:00</updated>
    <author>
      <name>/u/omnisvosscio</name>
      <uri>https://old.reddit.com/user/omnisvosscio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bsha/is_this_where_all_llms_are_going/"&gt; &lt;img alt="Is this where all LLMs are going? " src="https://preview.redd.it/l1h02xo8wqce1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03d40d0e8695392ff6f2dbe6e68c5d8afd724e12" title="Is this where all LLMs are going? " /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omnisvosscio"&gt; /u/omnisvosscio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l1h02xo8wqce1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bsha/is_this_where_all_llms_are_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0bsha/is_this_where_all_llms_are_going/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T11:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0b289</id>
    <title>Hugging Face released a free course on agents.</title>
    <updated>2025-01-13T10:26:28+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added a chapter to smol course on agents. Naturally, using smolagents! The course cover these topics:&lt;/p&gt; &lt;p&gt;- Code agents that solve problem with code&lt;br /&gt; - Retrieval agents that supply grounded context&lt;br /&gt; - Custom functional agents that do whatever you need!&lt;/p&gt; &lt;p&gt;If you're building agent applications, this course should help.&lt;/p&gt; &lt;p&gt;Course in smol course &lt;a href="https://github.com/huggingface/smol-course/tree/main/8_agents"&gt;https://github.com/huggingface/smol-course/tree/main/8_agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0b289/hugging_face_released_a_free_course_on_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T10:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i0eio5</id>
    <title>NVidia's official statement on the Biden Administration's Ai Diffusion Rule</title>
    <updated>2025-01-13T13:57:44+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"&gt; &lt;img alt="NVidia's official statement on the Biden Administration's Ai Diffusion Rule" src="https://external-preview.redd.it/mWL0PsxkiUDwuew99qIBVAVxp2i94of5Kngrra_8DKY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f30abf9568aba3ecc9b3830767ca6d7b17d79785" title="NVidia's official statement on the Biden Administration's Ai Diffusion Rule" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blogs.nvidia.com/blog/ai-policy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i0eio5/nvidias_official_statement_on_the_biden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-13T13:57:44+00:00</published>
  </entry>
</feed>
