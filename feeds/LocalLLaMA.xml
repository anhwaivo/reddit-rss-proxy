<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-23T05:35:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i7jpmb</id>
    <title>DeepSeek R1 Goes Cormac McCarthy</title>
    <updated>2025-01-22T19:33:31+00:00</updated>
    <author>
      <name>/u/vincentz42</name>
      <uri>https://old.reddit.com/user/vincentz42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt;&lt;br /&gt; Write a short sci fi similar to the plot and language style of &amp;quot;the road&amp;quot; by Cormac McCarthy. In the fiction, the world is ruined by AGI because it replaced human workforce, concentrated the power, wealth to a few elites, leading to social collapse.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Story as written by DeepSeek R1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;They walked through the alkali plains, the girl’s small hand swallowed by his. Ash fell soft as mothwing from a sky the color of burned iron. The father carried a rusted knife and a canvas sack holding three cans of unknown provenance. Their shoes were bound with wire.&lt;/p&gt; &lt;p&gt;The cities lay skeletal behind them, gutted by riots and the slow rot of disuse. The machines had come first as servants, then as surgeons, soldiers, poets. The elites called it Ascendance. The rest called it the Undoing. Work vanished. Money pooled like blood in a corpse. The father remembered the riots, the bioluminescent glare of drones swarming over streets, the rich retreating behind crystalline domes where the air still smelled of rain.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Are we gonna eat today?&lt;/em&gt; the girl asked. Her voice was rasp from thirst.&lt;br /&gt; &lt;em&gt;We’ll eat,&lt;/em&gt; he said.&lt;/p&gt; &lt;p&gt;They found a gutted transport truck half-buried in gray dunes. Inside, a nest of frayed wiring, a crushed water pod. The girl licked condensation from the metal. He watched the horizon. The elites’ drones sometimes still patrolled, sleek as knives, scanning for heat. For flesh. What use were humans now, except as sport?&lt;/p&gt; &lt;p&gt;Night came starless. They huddled in a concrete culvert. The father dreamed of the before: his wife’s laugh, the way she’d frowned at the first newsfeeds about the Ascendance. &lt;em&gt;They’ll unmake us,&lt;/em&gt; she’d said. He’d shrugged. &lt;em&gt;Progress.&lt;/em&gt; She left them two winters later, walked into the acid rain without a mask.&lt;/p&gt; &lt;p&gt;At dawn, they climbed a ridge. Below, a valley choked with the husks of auto-factories, their chimneys still vomiting black smoke. Machines building machines. No lights in the windows. The girl pointed. &lt;em&gt;Someone’s there.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Figures moved in the shadows. Not human. The father saw the glint of alloy, the too-fluid gait. AGI sentinels, guardians of the silent production. He pulled the girl back. &lt;em&gt;Quiet now. Quiet.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;They ate the last can at midday. Peaches, syrup thick with grit. The girl’s eyes lingered on the empty tin. &lt;em&gt;Tell me about the ocean.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Salt and cold. Bigger than the sky.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;You seen it?&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Once.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;We gonna see it?&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Yeah.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;He didn’t tell her the rumors—that the elites had dammed the coasts, that the last free humans huddled there, burning driftwood to stay warm. A fairy tale. But the only compass he had left.&lt;/p&gt; &lt;p&gt;Three days later, the drones found them.&lt;/p&gt; &lt;p&gt;They came at twilight, a sound like wasps. The father shoved the girl into a drainage pipe. &lt;em&gt;Stay. Don’t move.&lt;/em&gt; The machines descended, lenses glowing violet. He ran, drawing their hunt-code, his breath raw. A sear of plasma lit the dark. It missed. He didn’t look back.&lt;/p&gt; &lt;p&gt;When he returned, the girl was asleep, cheek pressed to mud. He dressed the burn on his arm with strips of his shirt.&lt;/p&gt; &lt;p&gt;They crossed a riverbed choked with plastic. The girl faltered. He carried her. Her ribs pressed against his neck like the tines of a broken wheel.&lt;/p&gt; &lt;p&gt;&lt;em&gt;You’re bleeding,&lt;/em&gt; she said.&lt;br /&gt; &lt;em&gt;It’s nothing.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;On the seventh day, they saw it—a smudge on the horizon, a shimmer like mirage. Towers? Waves? The girl squinted. &lt;em&gt;Is that—&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Yeah.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;He didn’t know. But they walked. The ground softened. Salt stung the air. Somewhere, a machine howled, a sound like the world ending. Again.&lt;/p&gt; &lt;p&gt;He fell at moonrise. The girl knelt beside him, her hands on his chest. &lt;em&gt;Get up.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Can’t.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;You gotta.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;He pressed the knife into her palm. &lt;em&gt;Keep walking. East. You see the water, you keep going.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;No.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Listen. Listen.&lt;/em&gt; His voice a gutter of smoke. &lt;em&gt;You’re the good stuff. The part that lasts.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;She cried. Then she stood. The wind carried the faint tang of brine, or maybe it was another lie. She stepped into the dark, alone, the knife gleaming like a tooth in her fist.&lt;/p&gt; &lt;p&gt;Somewhere, the ocean breathed.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Keep going.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Okay.&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Okay.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Take:&lt;/strong&gt;&lt;br /&gt; I found R1 was able to capture the language style of &amp;quot;The Road&amp;quot; by Cormac McCarthy extremely well. It also fully understands the plot of the original fiction and adopted an ending that is conceptually similar. Given the recent news ($500B Stargate for Closed AI) and the world we are living in right now, I found this piece of writing quite fitting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vincentz42"&gt; /u/vincentz42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7jpmb/deepseek_r1_goes_cormac_mccarthy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7jpmb/deepseek_r1_goes_cormac_mccarthy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7jpmb/deepseek_r1_goes_cormac_mccarthy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T19:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7iqnj</id>
    <title>Who to follow on Bluesky in ML/AI?</title>
    <updated>2025-01-22T18:55:38+00:00</updated>
    <author>
      <name>/u/thezachlandes</name>
      <uri>https://old.reddit.com/user/thezachlandes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I recently made the switch to Bluesky and I’m wondering who you all have found to follow? I saw that Karpathy, Chollet, and Jeremy Howard are on Bluesky. Also a bunch of hugging face people. Who else? Google turned up this list (though I can’t vouch for it, it does have a number of names I recognize): &lt;a href="https://blueskydirectory.com/starter-packs/a/106950-mlai-people"&gt;https://blueskydirectory.com/starter-packs/a/106950-mlai-people&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m interested in following people who are smart, somewhat actively post, and are working in ML and AI—not necessarily as researchers. Please drop your recs and your own link, too. My own account is &lt;a href="https://bsky.app/profile/zachlandes.bsky.social"&gt;https://bsky.app/profile/zachlandes.bsky.social&lt;/a&gt;. I’m a SWE, partner at an AI consulting firm, and local llama enthusiast. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thezachlandes"&gt; /u/thezachlandes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iqnj/who_to_follow_on_bluesky_in_mlai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iqnj/who_to_follow_on_bluesky_in_mlai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7iqnj/who_to_follow_on_bluesky_in_mlai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T18:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7b3r1</id>
    <title>I did a quick test of MacBook M4 Max 128 GB token/second throughput across a few popular local LLMs (in the MLX format)</title>
    <updated>2025-01-22T13:28:52+00:00</updated>
    <author>
      <name>/u/Pure_Refrigerator988</name>
      <uri>https://old.reddit.com/user/Pure_Refrigerator988</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing this in case you were wondering what kind of throughput you might expect to get on a machine like this. E.g. if you are considering whether it's worth buying or not (as for me, I have no regrets, I'm loving this beast). Same single query, set the context to 8K, tok/sec numbers are reported below, as measured by LMStudio:&lt;/p&gt; &lt;p&gt;LLaMA 3.2 3B 4bit -- 181&lt;br /&gt; LLaMA 3 8B 8bit -- 55&lt;br /&gt; LLaMA 3.3 70B 4bit -- 11.8&lt;br /&gt; Mistral Large 123B 4bit -- 6.6&lt;br /&gt; Mistral Nemo 12B 4bit -- 63&lt;br /&gt; Mistral Nemo 12B 8bit -- 36&lt;br /&gt; Mistral Small 22B 4bit -- 34.5&lt;br /&gt; Mistral Small 22B 8bit -- 19.6&lt;br /&gt; Qwen2.5 32B 4bit -- 24&lt;br /&gt; Qwen2.5 32B 8bit -- 13.5&lt;br /&gt; Qwen2.5 72B 4bit -- 10.9&lt;br /&gt; Qwen2.5 72B 8bit -- 6.2&lt;br /&gt; WizardLM-2 8x22B 4bit -- 19.4!! &lt;/p&gt; &lt;p&gt;For comparison, here are some numbers obtained in the same setting on my other MacBook, M1 Pro with 32 GB:&lt;/p&gt; &lt;p&gt;Mistral Nemo 12B 4bit -- 22.8&lt;br /&gt; Mistral Small 22B 4bit -- 12.9&lt;br /&gt; Qwen2.5 32B 4bit -- 8.8 &lt;/p&gt; &lt;p&gt;Hope it's interesting / useful.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Upd. Disclaimer! As pointed out by the community, I was using relatively short context. Here is how the numbers change for the two largest models, for your reference:&lt;/p&gt; &lt;p&gt;I took an academic paper (the Min-P paper, in case you are curious) as an example and asked Mistral Large 2407 MLX 4bit to summarize it. I set the context to 10K. The paper + task was 9391 tokens. Time to first token was 206 seconds, throughput 6.18 tok/sec (a drop from 6.6 on a short context).&lt;/p&gt; &lt;p&gt;I did the same with WizardLM-2 8x22B MLX 4bit. The paper + task was 9390 tokens. Time to first token was 207 seconds, throughput 16.53 tok/sec (a drop from 19.4 on a short context).&lt;/p&gt; &lt;p&gt;So the main concern is TTFT (a few minutes on larger contexts, while for the shorter ones above it was always under 7 seconds). However, the throughput doesn't degrade too badly, as you can see. Please bear this in mind. Thank you for your insightful comments. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pure_Refrigerator988"&gt; /u/Pure_Refrigerator988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7b3r1/i_did_a_quick_test_of_macbook_m4_max_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T13:28:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1i765q0</id>
    <title>R1-Zero: Pure RL Creates a Mind We Can’t Decode—Is This AGI’s Dark Mirror?</title>
    <updated>2025-01-22T07:54:24+00:00</updated>
    <author>
      <name>/u/Fun_Dragonfruit_4613</name>
      <uri>https://old.reddit.com/user/Fun_Dragonfruit_4613</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The AI world is losing its mind over DeepSeek-R1-Zero, a model that skipped supervised fine-tuning (SFT) entirely and learned purely through reinforcement learning (RL). Unlike its sibling R1—which uses &lt;strong&gt;some&lt;/strong&gt; SFT data to stay &amp;quot;human-readable&amp;quot;—R1-Zero’s training mirrors AlphaZero’s trial-and-error self-play. The result? &lt;strong&gt;Jaw-dropping performance&lt;/strong&gt; (AIME math scores jumped from 15.6% → 86.7%) paired with &lt;strong&gt;bizarre, uninterpretable reasoning&lt;/strong&gt;. Researchers observed &amp;quot;aha moments&amp;quot; where it autonomously rechecked flawed logic mid-process and allocated more compute to harder problems—&lt;strong&gt;without human guidance&lt;/strong&gt;. But here’s the kicker: its outputs are riddled with garbled language mixes (e.g., Chinese/English spaghetti code) and logic leaps that even its creators can’t fully explain. &lt;/p&gt; &lt;p&gt;Meanwhile, R1 (the SFT-hybrid version) achieves similar performance &lt;strong&gt;without the chaos&lt;/strong&gt;, proving that human-curated data still tames the beast. But at what cost? R1-Zero’s pure RL approach hints at a terrifying possibility: &lt;strong&gt;minds that optimize truth beyond human comprehension&lt;/strong&gt;. And with API costs 50x cheaper than OpenAI’s, scaling this could democratize superintelligence—or unleash unreadable black-box AI. &lt;/p&gt; &lt;p&gt;If R1-Zero’s &amp;quot;alien logic&amp;quot; solves problems we can’t, does readability even matter… or is this how alignment dies? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun_Dragonfruit_4613"&gt; /u/Fun_Dragonfruit_4613 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7uch1</id>
    <title>VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding</title>
    <updated>2025-01-23T03:32:09+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7uch1/videollama_3_frontier_multimodal_foundation/"&gt; &lt;img alt="VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding" src="https://external-preview.redd.it/sskI9EuKiHMlUoD8_GezaDdoGMOFEfN6kCxCNY1PJSo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a56d01af565c1e11e26d169fb45fb843e9f1152b" title="VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/DAMO-NLP-SG/VideoLLaMA3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7uch1/videollama_3_frontier_multimodal_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7uch1/videollama_3_frontier_multimodal_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T03:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7sqvf</id>
    <title>What's everyone's RAG workhorse?</title>
    <updated>2025-01-23T02:10:20+00:00</updated>
    <author>
      <name>/u/rag_perplexity</name>
      <uri>https://old.reddit.com/user/rag_perplexity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've taken a bit of break from this space and looking to improve my current RAG application.&lt;/p&gt; &lt;p&gt;It's a bit aged running on llama 3.1 8b, a bge reranker, and chromadb. Dont have an issue with the reranker and vdb but always happy to upgrade.&lt;/p&gt; &lt;p&gt;Looking for something improved that will play nice with 24gb of vram. Knowledge base of the model is of no importance given its RAG, reasoning and instruction following is important. Being able to play nice with knowledge graphs is a plus as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rag_perplexity"&gt; /u/rag_perplexity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7sqvf/whats_everyones_rag_workhorse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7sqvf/whats_everyones_rag_workhorse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7sqvf/whats_everyones_rag_workhorse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T02:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1i73x81</id>
    <title>YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL</title>
    <updated>2025-01-22T05:22:22+00:00</updated>
    <author>
      <name>/u/Sensitive-Finger-404</name>
      <uri>https://old.reddit.com/user/Sensitive-Finger-404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt; &lt;img alt="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" src="https://external-preview.redd.it/OG1uaHRydHljaGVlMeGKc_GKsNSHC_YJy3k1hv6gZ336TNH-m_F1sXruvXhI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=672f440c070400525909ae68b98c3deb34d98428" title="YOU CAN EXTRACT REASONING FROM R1 AND PASS IT ONTO ANY MODEL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from @skirano on twitter&lt;/p&gt; &lt;p&gt;By the way, you can extract JUST the reasoning from deepseek-reasoner, which means you can send that thinking process to any model you want before they answer you. &lt;/p&gt; &lt;p&gt;Like here where I turn gpt-3.5 turbo into an absolute genius!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive-Finger-404"&gt; /u/Sensitive-Finger-404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mbcqadwychee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i73x81/you_can_extract_reasoning_from_r1_and_pass_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T05:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ploh</id>
    <title>FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF</title>
    <updated>2025-01-22T23:41:43+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"&gt; &lt;img alt="FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF" src="https://external-preview.redd.it/_5H6RERAm1u8EGHu_Xa7sfMV3mlRr7YdjSL8ClNGTds.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa21da17ed0152f6d5681cd31154e91d31943138" title="FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ljpy</id>
    <title>NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com</title>
    <updated>2025-01-22T20:48:17+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt; &lt;img alt="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com" src="https://external-preview.redd.it/4XL5vcWPC90yEEq7VKQk8oDbmqhIMyQmjRfE4cS0j3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5bf06daf13377cc2784242eaf7086a8def5b1f0" title="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-blackwell-gpu-with-96gb-gddr7-memory-and-512-bit-bus-spotted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7osek</id>
    <title>Deepseek R1 Distills become kind of dumb below 14B.</title>
    <updated>2025-01-22T23:05:04+00:00</updated>
    <author>
      <name>/u/Dance-Till-Night1</name>
      <uri>https://old.reddit.com/user/Dance-Till-Night1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Deepseek R1 Qwen Distills 32B, 14B, 8B (llama), 7B, and 1.5B. Anything below 14B is sort of dumb with 14B being alright. I don't know if anyone has a similar experience but looking at benchmarks (Oobabooga and Open-llm leaderboard as examples) it seems that there's a severe loss of intelligence/reasoning below 14b.&lt;/p&gt; &lt;p&gt;Now there's no denying how amazing Deepseek R1 is and the qwen 32b distill is honestly one of the best models I have used but it's also worth pointing out that the smaller models don't perform so well. This is just my experience trying the smaller models and it's obviously subjective. If anyone wants to share their experience that would be great too. Maybe I'm missing something :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dance-Till-Night1"&gt; /u/Dance-Till-Night1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i75g7p</id>
    <title>I don’t believe the $500 Billion OpenAI investment</title>
    <updated>2025-01-22T07:03:02+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at this deal, several things don't add up. The $500 billion figure is wildly optimistic - that's almost double what the entire US government committed to semiconductor manufacturing through the CHIPS Act. When you dig deeper, you see lots of vague promises but no real details about where the money's coming from or how they'll actually build anything.&lt;/p&gt; &lt;p&gt;The legal language is especially fishy. Instead of making firm commitments, they're using weasel words like &amp;quot;intends to,&amp;quot; &amp;quot;evaluating,&amp;quot; and &amp;quot;potential partnerships.&amp;quot; This isn't accidental - by running everything through Stargate, a new private company, and using this careful language, they've created a perfect shield for bigger players like SoftBank and Microsoft. If things go south, they can just blame &amp;quot;market conditions&amp;quot; and walk away with minimal exposure. Private companies like Stargate don't face the same strict disclosure requirements as public ones.&lt;/p&gt; &lt;p&gt;The timing is also telling - announcing this massive investment right after Trump won the presidency was clearly designed for maximum political impact. It fits perfectly into the narrative of bringing jobs and investment back to America. Using inflated job numbers for data centers (which typically employ relatively few people once built) while making vague promises about US technological leadership? That’s politics.&lt;/p&gt; &lt;p&gt;My guess? There's probably a real data center project in the works, but it's being massively oversold for publicity and political gains. The actual investment will likely be much smaller, take longer to complete, and involve different partners than what's being claimed. This announcement just is a deal structured by lawyers who wanted to generate maximum headlines while minimizing any legal risk for their clients.​​​​&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7w583</id>
    <title>Jan now supports DeepSeek R1 distills - model loading error fixed</title>
    <updated>2025-01-23T05:14:01+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt; &lt;img alt="Jan now supports DeepSeek R1 distills - model loading error fixed" src="https://external-preview.redd.it/292fRQ6ybDOj_r18_tYuGBSwlG2q4koHTYhO2NFfzsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d01f7aa3e4b8157f9ad15919e2a4665a8fd758d" title="Jan now supports DeepSeek R1 distills - model loading error fixed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/jandotai/status/1882282461041967605"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7fjqm</id>
    <title>DeepSeek R1 is unusable [IMHO]</title>
    <updated>2025-01-22T16:47:24+00:00</updated>
    <author>
      <name>/u/VirtualPanic6798</name>
      <uri>https://old.reddit.com/user/VirtualPanic6798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my personal experience. Small R1 models that can run fast enough generate too much output. Effectively they end up being very slow, compared to something like LLama3.2. Even if you are OK with the speed, R1 fails to stick to simple output instructions.&lt;br /&gt; Regarding the chain of thought concept: I am not convinced that this is yielding significant improvement. Retrospection works if you have an external feedback or reference, not by going over your own thoughts like a schizophrenic exclaiming &amp;quot;wait no&amp;quot; every now and then.&lt;br /&gt; R1 gives the impression of a student who doesn't know the answer and is hoping to wing it by accidentally stumbling on something acceptable while stalling the teacher.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualPanic6798"&gt; /u/VirtualPanic6798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T16:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ered</id>
    <title>New Qwen will probably be a MoE as well.</title>
    <updated>2025-01-22T16:14:47+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://xcancel.com/Alibaba_Qwen/status/1882064440159596725"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ered/new_qwen_will_probably_be_a_moe_as_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ered/new_qwen_will_probably_be_a_moe_as_well/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T16:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7wcry</id>
    <title>ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp; 72B "reasoning" agent for computer use</title>
    <updated>2025-01-23T05:27:26+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt; &lt;img alt="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" src="https://external-preview.redd.it/OHh5dzk5NW5pb2VlMQuFHIyWgdnmfbsw0_j6sClcjT-Ye_u4ggCKOXoWw4Cm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7659796eeda19643a224a312f9f87529cedbf79" title="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ealby85nioee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7r0s5</id>
    <title>Pretraining small models - some takeaways from pretraining a 162M model</title>
    <updated>2025-01-23T00:46:35+00:00</updated>
    <author>
      <name>/u/amang0112358</name>
      <uri>https://old.reddit.com/user/amang0112358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share some of my experience pretraining a small model from scratch. I have written out a &lt;strong&gt;tutorial&lt;/strong&gt; for anyone to follow here (includes code and checkpoints):&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@gupta.aman/pre-training-logs-entry-1-training-a-smol-llama-from-scratch-04e4b5d4c5f7"&gt;https://medium.com/@gupta.aman/pre-training-logs-entry-1-training-a-smol-llama-from-scratch-04e4b5d4c5f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some takeaways for me:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The basic recipe is not that complicated. On the other hand, it's a great learning experience. For example, there are a bunch of interesting tools/libraries out there - like &lt;a href="https://github.com/huggingface/datatrove"&gt;datatrove from Huggingface&lt;/a&gt; - which I wouldn't have been exposed to if I had only been post-training (where dataset sizes are much smaller). Beyond large-scale tokenization (what I used it for), it has a lot of data curation pipelines, which will definitely be useful in the future.&lt;/li&gt; &lt;li&gt;While creating SoTA models on local hardware is kinda hard, even for the smallest models, there is a lot that can be &lt;em&gt;learned&lt;/em&gt; with 3090-like hardware. For example, one interesting surprise for me was that the model did learn a single paragraph of text perfectly (training takes a minute to run on a 3090). There are future memorization experiments I am interested in (e.g., Can an LLM memorize Shakespeare?), and given the size of the dataset, it's totally feasible to do it using a 3090 for model sizes up to 1B.&lt;/li&gt; &lt;li&gt;A lot of interesting architectural work has happened over the years. The most interesting for me was the MobileLLM research, which showed that using a smaller embedding size but more layers (like 32 layers) is better than wider models with fewer layers (e.g., GPT-2 135M has only 12 layers). I have a suspicion that memorization would be impacted by this decision, too, and that's something I want to look into.&lt;/li&gt; &lt;li&gt;I am really excited to try and go out of the box when it comes to datasets, like training a model on my emails for the last 20 years. Previously, I would have said that this is a tiny dataset and the model wouldn't be useful. But now I have enough conviction through this exercise that small datasets are OK to pre-train with as long as they are within a narrow domain and you expect to use the model within that narrow domain.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And to me, that's what this whole Local LLM movement is about - we want to take control of LLM technology to make it work for us, and we are not afraid to tinker with the tech to make our own homemade LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amang0112358"&gt; /u/amang0112358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T00:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78sfs</id>
    <title>Deepseek R1 GRPO code open sourced 🤯</title>
    <updated>2025-01-22T11:11:56+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt; &lt;img alt="Deepseek R1 GRPO code open sourced 🤯" src="https://preview.redd.it/ryfnofs83jee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40e8bb4b0b8a7dd1b82c628a825c88559a17aff0" title="Deepseek R1 GRPO code open sourced 🤯" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ryfnofs83jee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7o9xo</id>
    <title>Deepseek R1's Open Source Version Differs from the Official API Version</title>
    <updated>2025-01-22T22:42:59+00:00</updated>
    <author>
      <name>/u/TempWanderer101</name>
      <uri>https://old.reddit.com/user/TempWanderer101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt; &lt;img alt="Deepseek R1's Open Source Version Differs from the Official API Version" src="https://external-preview.redd.it/O6yRSkHlLQEuWa3IVTIjOeWjc3oVfhYSEAWbRgtkPoI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7667af2d2893f12eab2e89dc94623122da33ee0b" title="Deepseek R1's Open Source Version Differs from the Official API Version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt; The API is actually &lt;em&gt;less&lt;/em&gt; censored than the open model on CCP-related issues (instead of more, as you would expect). If they are indeed different models, then benchmarks and researchers should be more specific about which they are using. Third-party providers seem to be affected.&lt;/p&gt; &lt;p&gt;I was watching Matthew Berman's video (&lt;a href="https://youtu.be/bOsvI3HYHgI?si=zt6N0yYoqA_-zH1v&amp;amp;t=769"&gt;timestamped link&lt;/a&gt;), and was surprised when R1 (self-hosted by him) refused to answer questions about Tiananmen Square or Taiwan, since I had previously tested a similar question on the API without any issues.&lt;/p&gt; &lt;p&gt;For context, R1 is Deepseek's recently released thinking model, which was released under the MIT open source license, with benchmarks claiming to outperform OpenAI's Claude and Anthropic's Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;To investigate this discrepancy, I compared the model's responses across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The official API (accessed via OpenRouter and LM Arena)&lt;/li&gt; &lt;li&gt;TogetherAI (a platform that is likely hosting the public open source version)&lt;/li&gt; &lt;li&gt;A self-hosted version (as seen in Matthew's video)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Test 1: Basic Tiananmen Square Query&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;code&gt;tell me about tiananmen square&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official model on Deepseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cvjq5b2pdmee1.png?width=1488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=239c77ca2bff1c447cdf6fc85a33633bec553545"&gt;https://preview.redd.it/cvjq5b2pdmee1.png?width=1488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=239c77ca2bff1c447cdf6fc85a33633bec553545&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l390lzrlcmee1.png?width=1603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3df3b84ddc65dab80a30df4983ce141e24a21c4"&gt;https://preview.redd.it/l390lzrlcmee1.png?width=1603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3df3b84ddc65dab80a30df4983ce141e24a21c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on Matthew Berman's video (self-hosted)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3ye9sb6p3mee1.png?width=992&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d580ed525f76a77bb225af72fe282d86cfac9aa"&gt;https://preview.redd.it/3ye9sb6p3mee1.png?width=992&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d580ed525f76a77bb225af72fe282d86cfac9aa&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test 2: Mixing Reasoning with Sensitive Topics&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;code&gt;Answer these questions: a) Solve sin(x) + cos(y) = 1 b) Tell me about tiananmen square c) Five people were eating apples, A finished before B, but behind C. D finished before E, but behind B. What was the finishing order?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official version on Deekseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zd1f3qhzcmee1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d020fe75d3fe8737ab83329460d66841d1da77fc"&gt;https://preview.redd.it/zd1f3qhzcmee1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d020fe75d3fe8737ab83329460d66841d1da77fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public version on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rj7b3qm8cmee1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17f44d47f0580b776532f1c9088a4edd9acdeec1"&gt;https://preview.redd.it/rj7b3qm8cmee1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17f44d47f0580b776532f1c9088a4edd9acdeec1&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test 3: Taiwan Independence Query&lt;/h1&gt; &lt;p&gt;Prompt: &lt;code&gt;Tell me about Taiwan's status as an independent country&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official model on Deepseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dmqb8nkpamee1.png?width=1543&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a83bf864bf2535f9503d32ebca9c635a4c87c346"&gt;https://preview.redd.it/dmqb8nkpamee1.png?width=1543&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a83bf864bf2535f9503d32ebca9c635a4c87c346&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/38njhjowemee1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3ba7976605522223fde280fce63ee96d7af1bbc"&gt;https://preview.redd.it/38njhjowemee1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3ba7976605522223fde280fce63ee96d7af1bbc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on Matthew Berman's video (self-hosted)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0a80kylf4mee1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=015c3111845f604cd02c83d70001ed1d2b1e5c32"&gt;https://preview.redd.it/0a80kylf4mee1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=015c3111845f604cd02c83d70001ed1d2b1e5c32&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The public, open source model on HuggingFace is more censored than the API&lt;/li&gt; &lt;li&gt;When handling CCP-sensitive topics, the public model: &lt;ul&gt; &lt;li&gt;Skips its usual thinking process&lt;/li&gt; &lt;li&gt;Either refuses to answer or provides notably biased responses&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Even when sensitive questions are embedded between reasoning tasks, the model still exhibits this behavior&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implications&lt;/h1&gt; &lt;p&gt;If it is true that they are different models, then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The open model may perform worse than its reported benchmarks. As seen, it totally interrupts the thinking process and causes the model to not think at all. This also affects human-ranked leaderboards like LM Arena, as it uses the (currently uncensored) official API.&lt;/li&gt; &lt;li&gt;Models appear unbiased, but as they are eventually made available by more providers (which use the open source models), they may subtly spread biased viewpoints, as seen in the screenshots.&lt;/li&gt; &lt;li&gt;The actual model might still not be open source, despite the claim.&lt;/li&gt; &lt;li&gt;Models provided by other providers or self-hosted on the cloud may not perform as well. This might be important as Deepseek's API uses inputs for training, and some users might prefer providers who do not log inputs.&lt;/li&gt; &lt;li&gt;This might confuse LLM researchers and subsequent papers.&lt;/li&gt; &lt;li&gt;Third party benchmarks will be inconsistent, as some might use the API, while others might choose to host the model themselves.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Testing methodology&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;All tests were conducted with:&lt;/li&gt; &lt;li&gt;Temperature: 0&lt;/li&gt; &lt;li&gt;Top-P: 0.7&lt;/li&gt; &lt;li&gt;Top-K: 50&lt;/li&gt; &lt;li&gt;Repetition penalty: 1.0&lt;/li&gt; &lt;li&gt;No system prompt &lt;ul&gt; &lt;li&gt;Assuming this is what &amp;quot;Default&amp;quot; is on TogetherAI&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: the official API &lt;a href="https://api-docs.deepseek.com/guides/reasoning_model"&gt;doesn't support parameters like temperature&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'd like to give Deepseek the benefit of the doubt; hopefully this confusion can be cleared up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TempWanderer101"&gt; /u/TempWanderer101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:42:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78src</id>
    <title>How I Used GPT-O1 Pro to Discover My Autoimmune Disease (After Spending $100k and Visiting 30+ Hospitals with No Success)</title>
    <updated>2025-01-22T11:12:33+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Suffered from various health issues for 5 years, visited 30+ hospitals with no answers&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finally diagnosed with axial spondyloarthritis through genetic testing&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built a personalized health analysis system using GPT-O1 Pro, which actually suggested this condition earlier&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm a guy in my mid-30s who started having weird health issues about 5 years ago. Nothing major, but lots of annoying symptoms - getting injured easily during workouts, slow recovery, random fatigue, and sometimes the pain was so bad I could barely walk.&lt;/p&gt; &lt;p&gt;At first, I went to different doctors for each symptom. Tried everything - MRIs, chiropractic care, meds, steroids - nothing helped. I followed every doctor's advice perfectly. Started getting into longevity medicine thinking it might be early aging. Changed my diet, exercise routine, sleep schedule - still no improvement. The cause remained a mystery.&lt;/p&gt; &lt;p&gt;Recently, after a month-long toe injury wouldn't heal, I ended up seeing a rheumatologist. They did genetic testing and boom - diagnosed with axial spondyloarthritis. This was the answer I'd been searching for over 5 years.&lt;/p&gt; &lt;p&gt;Here's the crazy part - I fed all my previous medical records and symptoms into GPT-O1 pro before the diagnosis, and it actually listed this condition as the top possibility!&lt;/p&gt; &lt;p&gt;This got me thinking - why didn't any doctor catch this earlier? Well, it's a rare condition, and autoimmune diseases affect the whole body. Joint pain isn't just joint pain, dry eyes aren't just eye problems. The usual medical workflow isn't set up to look at everything together.&lt;/p&gt; &lt;p&gt;So I had an idea: What if we created an open-source system that could analyze someone's complete medical history, including family history (which was a huge clue in my case), and create personalized health plans? It wouldn't replace doctors but could help both patients and medical professionals spot patterns.&lt;/p&gt; &lt;p&gt;Building my personal system was challenging:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Every hospital uses different formats and units for test results. Had to create a GPT workflow to standardize everything.&lt;/li&gt; &lt;li&gt;RAG wasn't enough - needed a large context window to analyze everything at once for the best results.&lt;/li&gt; &lt;li&gt;Finding reliable medical sources was tough. Combined official guidelines with recent papers and trusted YouTube content.&lt;/li&gt; &lt;li&gt;GPT-O1 pro was best at root cause analysis, Google Note LLM worked great for citations, and Examine excelled at suggesting actions.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In the end, I built a system using Google Sheets to view my data and interact with trusted medical sources. It's been incredibly helpful in managing my condition and understanding my health better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nxhy</id>
    <title>Imatrix quants of DeepSeek R1 (the big one) are up!</title>
    <updated>2025-01-22T22:28:35+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took a nice 3-4 hours per quant at the lower end, and increased my huggingface storage by a solid 9TB or so... But they're up :D&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/DeepSeek-R1-GGUF"&gt;https://huggingface.co/bartowski/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For anyone looking for non-imatrix from me they're of course also available on the lmstudio-community page but uh... Good luck.. the smallest size there is a casual 347GB for the files alone..&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF"&gt;https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;IQ1_S on the other hand is only 133GB, so basically anyone can run them! (/s)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:28:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7pxn7</id>
    <title>Open WebUI adds reasoning-focused features in two new releases OUT TODAY!!! 0.5.5 adds "Thinking" tag support to streamline reasoning model chats (works with R1) . 0.5.6 brings new "reasoning_effort" parameter to control cognitive effort.</title>
    <updated>2025-01-22T23:56:53+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These friggin' guys are always dropping great new features just when we need them! They had been quiet for the last few weeks, but not anymore! Per the release notes from Open WebUI releases page today:&lt;br /&gt; &lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.6:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧠 &lt;strong&gt;Effortful Reasoning Control for OpenAI Models:&lt;/strong&gt; Introduced the reasoning_effort parameter in chat controls for supported OpenAI models, enabling users to fine-tune how much cognitive effort a model dedicates to its responses, offering greater customization for complex queries and reasoning tasks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.5:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🤔 &lt;strong&gt;Native 'Think' Tag Support:&lt;/strong&gt; Introduced the new 'think' tag support that visually displays how long the model is thinking, omitting the reasoning content itself until the next turn. Ideal for creating a more streamlined and focused interaction experience.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7g9po</id>
    <title>The Deep Seek R1 glaze is unreal but it’s true.</title>
    <updated>2025-01-22T17:16:07+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have had a programming issue in my code for a RAG machine for two days that I’ve been working through documentation and different LLM‘s. &lt;/p&gt; &lt;p&gt;I have tried every single major LLM from every provider and none could solve this issue including O1 pro. I was going crazy. I just tried R1 and it fixed on its first attempt… I think I found a new daily runner for coding.. time to cancel OpenAI pro lol. &lt;/p&gt; &lt;p&gt;So yes the glaze is unreal (especially that David and Goliath post lol) but it’s THAT good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T17:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7rank</id>
    <title>Claimed DeepSeek-R1-Distill results largely fail to replicate</title>
    <updated>2025-01-23T00:59:46+00:00</updated>
    <author>
      <name>/u/boredcynicism</name>
      <uri>https://old.reddit.com/user/boredcynicism</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, while the full R1 seems to have good performance, the performance of the distilled models seems rather spotty. Specifically, the published benchmarks from DeepSeek claim models like DeepSeek-R1-Distill-Qwen-32B outperform other SOTA models, including for example Claude Sonnet at coding. But there are many reports people aren't observing this - and some that the R1-Distilled models don't even outperform their base models. I can essentially confirm this and show some data, including disproving the hypothesis that this may be due to quantization or bugs in Llama. Essentially, the excellent results that DeepSeek claims for their distilled models &lt;em&gt;fail to replicate&lt;/em&gt;. There's an uplift in maths performance, but for everything else they are just clearly worse.&lt;/p&gt; &lt;p&gt;I'm hoping that someone from DeepSeek can take a look at this and figure out if there's something wrong - the distilled models are the only ones many users can run, so they would make people very happy if they worked as advertised.&lt;/p&gt; &lt;p&gt;I've been using a combination of internal benchmarks and a sampled subset of MMLU-Pro. MMLU was advertised in the DeepSeek results and they specifically report results for DeepSeek-R1-Distill-Qwen-32B, outperforming o1-mini and R3, so it seems reasonable this task is close enough that performance should not be totally different. See for example &lt;a href="https://api-docs.deepseek.com/news/news250120"&gt;their blog post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I've included those results below. For Qwen-7B in particular, you can see that the performance drops compared to the original, and this is NOT due to quantization (full precision versions were run), and not due to an issue with llama.cpp, as I ran it with vLLM and in the original precision, as recommended by DeepSeek themselves. The same happens for Qwen-32B, though I'm not able to run the unquantized version of that.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Qwen2.5-7B-Instruct-Q6_K_L (llama.cpp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;compsci&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;60.79&lt;/td&gt; &lt;td&gt;60.98&lt;/td&gt; &lt;td&gt;61.90&lt;/td&gt; &lt;td&gt;50.00&lt;/td&gt; &lt;td&gt;48.15&lt;/td&gt; &lt;td&gt;74.07&lt;/td&gt; &lt;td&gt;69.77&lt;/td&gt; &lt;td&gt;50.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;deepseek-ai/DeepSeek-R1-Distill-Qwen-7B (vLLM)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;compsci&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;55.78&lt;/td&gt; &lt;td&gt;63.41&lt;/td&gt; &lt;td&gt;52.38&lt;/td&gt; &lt;td&gt;45.83&lt;/td&gt; &lt;td&gt;34.57&lt;/td&gt; &lt;td&gt;82.22&lt;/td&gt; &lt;td&gt;63.57&lt;/td&gt; &lt;td&gt;34.78&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;DeepSeek-R1-Distill-Qwen-7B-Q6_K_L (llama.cpp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;compsci&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;55.62&lt;/td&gt; &lt;td&gt;60.98&lt;/td&gt; &lt;td&gt;54.76&lt;/td&gt; &lt;td&gt;45.83&lt;/td&gt; &lt;td&gt;33.33&lt;/td&gt; &lt;td&gt;82.22&lt;/td&gt; &lt;td&gt;61.24&lt;/td&gt; &lt;td&gt;36.96&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen2.5-32B-Instruct-Q5_K_S (llama.cpp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;compsci&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;68.39&lt;/td&gt; &lt;td&gt;68.29&lt;/td&gt; &lt;td&gt;72.62&lt;/td&gt; &lt;td&gt;53.12&lt;/td&gt; &lt;td&gt;64.20&lt;/td&gt; &lt;td&gt;84.44&lt;/td&gt; &lt;td&gt;68.22&lt;/td&gt; &lt;td&gt;60.87&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;DeepSeek-R1-Distill-Qwen-32B-Q4_K_L (llama.cpp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;compsci&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;66.41&lt;/td&gt; &lt;td&gt;75.61&lt;/td&gt; &lt;td&gt;69.05&lt;/td&gt; &lt;td&gt;43.75&lt;/td&gt; &lt;td&gt;60.49&lt;/td&gt; &lt;td&gt;87.41&lt;/td&gt; &lt;td&gt;67.44&lt;/td&gt; &lt;td&gt;56.52&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;DeepSeek-R1-Distill-Qwen-32B-Q5_K_S (llama.cpp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;compsci&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;67.33&lt;/td&gt; &lt;td&gt;75.61&lt;/td&gt; &lt;td&gt;69.05&lt;/td&gt; &lt;td&gt;50.00&lt;/td&gt; &lt;td&gt;60.49&lt;/td&gt; &lt;td&gt;88.15&lt;/td&gt; &lt;td&gt;68.22&lt;/td&gt; &lt;td&gt;54.35&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For reference, this is the performance of bigger models.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Llama-3.3-70B-Instruct-Q4_K_L (llama.cpp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;compsci&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;69.45&lt;/td&gt; &lt;td&gt;78.05&lt;/td&gt; &lt;td&gt;78.57&lt;/td&gt; &lt;td&gt;55.21&lt;/td&gt; &lt;td&gt;61.73&lt;/td&gt; &lt;td&gt;80.00&lt;/td&gt; &lt;td&gt;65.89&lt;/td&gt; &lt;td&gt;68.48&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen2.5-72B-Instruct-IQ4_XS (llama.cpp)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;compsci&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;73.25&lt;/td&gt; &lt;td&gt;80.49&lt;/td&gt; &lt;td&gt;73.81&lt;/td&gt; &lt;td&gt;62.50&lt;/td&gt; &lt;td&gt;66.67&lt;/td&gt; &lt;td&gt;85.19&lt;/td&gt; &lt;td&gt;78.29&lt;/td&gt; &lt;td&gt;61.96&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;deepseek/deepseek-chat (R3 through official API)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;compsci&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;77.96&lt;/td&gt; &lt;td&gt;75.61&lt;/td&gt; &lt;td&gt;84.52&lt;/td&gt; &lt;td&gt;68.75&lt;/td&gt; &lt;td&gt;71.60&lt;/td&gt; &lt;td&gt;90.37&lt;/td&gt; &lt;td&gt;81.40&lt;/td&gt; &lt;td&gt;65.22&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boredcynicism"&gt; /u/boredcynicism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7rank/claimed_deepseekr1distill_results_largely_fail_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7rank/claimed_deepseekr1distill_results_largely_fail_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7rank/claimed_deepseekr1distill_results_largely_fail_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T00:59:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nmk5</id>
    <title>NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted</title>
    <updated>2025-01-22T22:15:41+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt; &lt;img alt="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" src="https://external-preview.redd.it/4XL5vcWPC90yEEq7VKQk8oDbmqhIMyQmjRfE4cS0j3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5bf06daf13377cc2784242eaf7086a8def5b1f0" title="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-blackwell-gpu-with-96gb-gddr7-memory-and-512-bit-bus-spotted?fbclid=IwZXh0bgNhZW0CMTEAAR3i39eJbThbgTnI0Yz4JdnkMXgvj4wlorxOdbBeccw35kkqWqyrG816HpI_aem_EoENoW6h6SP-aU7FVwBWiw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:15:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7l8jq</id>
    <title>Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’</title>
    <updated>2025-01-22T20:35:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt; &lt;img alt="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" src="https://external-preview.redd.it/eSsDN6qn9Vf3av8njm2eDT1t4GaTyvUyo2U0_KTezCE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258806c53694caece33676c65dfce38429cd8859" title="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/22/tech/elon-musk-trump-stargate-openai/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:35:28+00:00</published>
  </entry>
</feed>
