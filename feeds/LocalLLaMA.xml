<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-28T00:28:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kwzrh4</id>
    <title>State of open-source computer using agents (2025)?</title>
    <updated>2025-05-27T21:42:27+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a new domain to dig into after spending time on language, music, and speech.&lt;/p&gt; &lt;p&gt;I played around with &lt;a href="https://openai.com/index/computer-using-agent/"&gt;OpenAI's CUA&lt;/a&gt; and think it's a cool idea. What are the best open-source CUA models available today to build on and improve? I'm looking for something hackable and with a good community (or a dev/team open to reasonable pull requests).&lt;/p&gt; &lt;p&gt;I thought I'd make a post here to crowdsource your experiences.&lt;/p&gt; &lt;p&gt;Edit: Answering my own question, it seems TARS-UI from Bytedance is the open-source SoTA in compute using agents right now. I was able to get their &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;7B model&lt;/a&gt; running through VLLM (hogs 86GB of VRAM just for the weights) and use their &lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;desktop app&lt;/a&gt; on my laptop. I couldn't get it to do anything useful beyond generating a single &amp;quot;thought&amp;quot;. Cool, now I have something fun to play with!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwzrh4/state_of_opensource_computer_using_agents_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwzrh4/state_of_opensource_computer_using_agents_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwzrh4/state_of_opensource_computer_using_agents_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T21:42:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwwlyv</id>
    <title>most hackable coding agent</title>
    <updated>2025-05-27T19:37:44+00:00</updated>
    <author>
      <name>/u/mnze_brngo_7325</name>
      <uri>https://old.reddit.com/user/mnze_brngo_7325</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find with local models coding agents need quite a lot of guidance and fail at tasks that are too complex. Also adherence to style and other rules is often not easy to achieve.&lt;/p&gt; &lt;p&gt;I use agents to do planing, requirement engineering, software architecture stuff etc., which is usually very specific to my domain and tailoring low resource LLMs to my use cases is often surprisingly effective. Only missing piece in my agentic chain is the actual coding part. I don't want to reinvent the wheel, when others have figured that out better than I ever could.&lt;/p&gt; &lt;p&gt;Aider seems to be the option closest to what I want. They have python bindings but they also kind of advise against using it.&lt;/p&gt; &lt;p&gt;Any experience and recommendations for integrating coding agents in your own agent workflows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mnze_brngo_7325"&gt; /u/mnze_brngo_7325 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwlyv/most_hackable_coding_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwlyv/most_hackable_coding_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwlyv/most_hackable_coding_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T19:37:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kw7n6w</id>
    <title>DIA 1B Podcast Generator - With Consistent Voices and Script Generation</title>
    <updated>2025-05-26T22:36:14+00:00</updated>
    <author>
      <name>/u/Smartaces</name>
      <uri>https://old.reddit.com/user/Smartaces</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw7n6w/dia_1b_podcast_generator_with_consistent_voices/"&gt; &lt;img alt="DIA 1B Podcast Generator - With Consistent Voices and Script Generation" src="https://external-preview.redd.it/NG1pdDduNDFlNzNmMUcfJmyGLBoX3HGWzWW7GBEQ5TlU9sPw-Gkkjhi-K8NK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07c6ebc8b3055e1fcfc4b4a856d4bdb99beffb3f" title="DIA 1B Podcast Generator - With Consistent Voices and Script Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm pleased to share üêê GOATBookLM üêê... &lt;/p&gt; &lt;p&gt;A dual voice Open Source podcast generator powered by &lt;a href="https://www.linkedin.com/search/results/all/?keywords=%23narilabs&amp;amp;origin=HASH_TAG_FROM_FEED"&gt;hashtag#NariLabs&lt;/a&gt; &lt;a href="https://www.linkedin.com/search/results/all/?keywords=%23dia&amp;amp;origin=HASH_TAG_FROM_FEED"&gt;hashtag#Dia&lt;/a&gt; 1B audio model (with a little sprinkling of &lt;a href="https://www.linkedin.com/company/googledeepmind/"&gt;Google DeepMind&lt;/a&gt;'s Gemini Flash 2.5 and &lt;a href="https://www.linkedin.com/company/anthropicresearch/"&gt;Anthropic&lt;/a&gt; Sonnet 4) &lt;/p&gt; &lt;p&gt;What started as an evening playing around with a new open source audio model on &lt;a href="https://www.linkedin.com/company/huggingface/"&gt;Hugging Face&lt;/a&gt; ended up as a week building an open source podcast generator.&lt;/p&gt; &lt;p&gt;Out of the box Dia 1B, the model powering the audio, is a rather unpredictable model, with random voices spinning up for every audio generation.&lt;/p&gt; &lt;p&gt;With a little exploration and testing I was able to fix this, and optimize the speaker dialogue format for pretty strong results.&lt;/p&gt; &lt;p&gt;Running entirely in Google colab üêê GOATBookLM üêê includes:&lt;/p&gt; &lt;p&gt;üîä Dual voice/ speaker podcast script creation from any text input file&lt;/p&gt; &lt;p&gt;üîä Full consistency in Dia 1B voices using a selection of demo cloned voices&lt;/p&gt; &lt;p&gt;üîä Full preview and regeneration of audio files (for quick corrections)&lt;/p&gt; &lt;p&gt;üîä Full final output in .wav or .mp3&lt;/p&gt; &lt;p&gt;Link to the Notebook: &lt;a href="https://github.com/smartaces/dia_podcast_generator"&gt;https://github.com/smartaces/dia_podcast_generator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smartaces"&gt; /u/Smartaces &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4ym9al41e73f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kw7n6w/dia_1b_podcast_generator_with_consistent_voices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kw7n6w/dia_1b_podcast_generator_with_consistent_voices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T22:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kvz322</id>
    <title>Qwen 3 30B A3B is a beast for MCP/ tool use &amp; Tiny Agents + MCP @ Hugging Face! üî•</title>
    <updated>2025-05-26T16:44:22+00:00</updated>
    <author>
      <name>/u/vaibhavs10</name>
      <uri>https://old.reddit.com/user/vaibhavs10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heya everyone, I'm VB from Hugging Face, we've been experimenting with MCP (Model Context Protocol) quite a bit recently. In our (vibe) tests, Qwen 3 30B A3B gives the best performance overall wrt size and tool calls! Seriously underrated.&lt;/p&gt; &lt;p&gt;The most recent &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12379"&gt;streamable tool calling support&lt;/a&gt; in llama.cpp makes it even more easier to use it locally for MCP. Here's how you can try it out too:&lt;/p&gt; &lt;p&gt;Step 1: Start the llama.cpp server `llama-server --jinja -fa -hf unsloth/Qwen3-30B-A3B-GGUF:Q4_K_M -c 16384`&lt;/p&gt; &lt;p&gt;Step 2: Define an `agent.json` file w/ MCP server/s&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;model&amp;quot;: &amp;quot;unsloth/Qwen3-30B-A3B-GGUF:Q4_K_M&amp;quot;, &amp;quot;endpointUrl&amp;quot;: &amp;quot;http://localhost:8080/v1&amp;quot;, &amp;quot;servers&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;sse&amp;quot;, &amp;quot;config&amp;quot;: { &amp;quot;url&amp;quot;: &amp;quot;https://evalstate-flux1-schnell.hf.space/gradio_api/mcp/sse&amp;quot; } } ] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Step 3: Run it &lt;/p&gt; &lt;pre&gt;&lt;code&gt;npx @huggingface/tiny-agents run ./local-image-gen &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details here: &lt;a href="https://github.com/Vaibhavs10/experiments-with-mcp"&gt;https://github.com/Vaibhavs10/experiments-with-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To make it easier for tinkerers like you, we've been experimenting around tooling for MCP and registry:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MCP Registry - you can now host spaces as MCP server on Hugging Face (with just one line of code): &lt;a href="https://huggingface.co/spaces?filter=mcp-server"&gt;https://huggingface.co/spaces?filter=mcp-server&lt;/a&gt; (all the spaces that are MCP compatible)&lt;/li&gt; &lt;li&gt;MCP Clients - we've created &lt;a href="https://github.com/huggingface/huggingface.js/tree/main/packages/tiny-agents"&gt;TypeScript&lt;/a&gt; and &lt;a href="https://huggingface.co/blog/python-tiny-agents"&gt;Python interfaces&lt;/a&gt; for you to experiment local and deployed models directly w/ MCP&lt;/li&gt; &lt;li&gt;MCP Course - learn more about MCP in an applied manner directly here: &lt;a href="https://huggingface.co/learn/mcp-course/en/unit0/introduction"&gt;https://huggingface.co/learn/mcp-course/en/unit0/introduction&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We're experimenting a lot more with open models, local + remote workflows for MCP, do let us know what you'd like to see. Moore so keen to hear your feedback on all!&lt;/p&gt; &lt;p&gt;Cheers,&lt;/p&gt; &lt;p&gt;VB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavs10"&gt; /u/vaibhavs10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvz322/qwen_3_30b_a3b_is_a_beast_for_mcp_tool_use_tiny/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kvz322/qwen_3_30b_a3b_is_a_beast_for_mcp_tool_use_tiny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kvz322/qwen_3_30b_a3b_is_a_beast_for_mcp_tool_use_tiny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-26T16:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwynyt</id>
    <title>Your favourite non-English/Chinese model</title>
    <updated>2025-05-27T20:58:59+00:00</updated>
    <author>
      <name>/u/JohnnyOR</name>
      <uri>https://old.reddit.com/user/JohnnyOR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Much like English is the lingua franca for programming, it seems to also be the same preferred language for, well, language models (plus Chinese, obviously). For those generating content or using models in languages that are not Chinese or English, what is your model or models of choice? &lt;/p&gt; &lt;p&gt;Gemma 3 and Qwen 3 boast, on paper, some of the highest numbers of languages &amp;quot;officially&amp;quot; supported (except Gemma 3 1B, which Google decided to neuter entirely) but honestly outside of high resources languages they often leave a lot to be desired imo. Don't even get me started on forgetting to turn off thinking on Qwen when attempting something outside of English and Chinese. That being said, it is fun to see labs and universities in Europe and Asia put out finetunes of these models for local languages, but it is a bit sad to see true multilingual excellence still kinda locked behind APIs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JohnnyOR"&gt; /u/JohnnyOR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwynyt/your_favourite_nonenglishchinese_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwynyt/your_favourite_nonenglishchinese_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwynyt/your_favourite_nonenglishchinese_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T20:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwhr56</id>
    <title>Why LLM Agents Still Hallucinate (Even with Tool Use and Prompt Chains)</title>
    <updated>2025-05-27T08:04:06+00:00</updated>
    <author>
      <name>/u/Mountain-Insect-2153</name>
      <uri>https://old.reddit.com/user/Mountain-Insect-2153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You‚Äôd think calling external tools would ‚Äúfix‚Äù hallucinations in LLM agents, but even with tools integrated (LangChain, ReAct, etc.), the bots still confidently invent or misuse tool outputs.&lt;/p&gt; &lt;p&gt;Part of the problem is that most pipelines treat the LLM like a black box between prompt ‚Üí tool ‚Üí response. There's no consistent &lt;em&gt;reasoning checkpoint&lt;/em&gt; before the final output. So even if the tool gives the right data, the model might still mess up interpreting it or worse, hallucinate extra ‚Äúcontext‚Äù to justify a bad answer.&lt;/p&gt; &lt;p&gt;What‚Äôs missing is a self-check step before the response is finalized. Like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Did this answer follow the intended logic?&lt;/li&gt; &lt;li&gt;Did the tool result get used properly?&lt;/li&gt; &lt;li&gt;Are we sticking to domain constraints?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Without that, you're just crossing your fingers and hoping the model doesn't go rogue. This matters a ton in customer support, healthcare, or anything regulated.&lt;/p&gt; &lt;p&gt;Also, tool use is only as good as your control over &lt;em&gt;when and how&lt;/em&gt; tools are triggered. I‚Äôve seen bots misfire APIs just because the prompt hinted at it vaguely. Unless you gate tool calls with precise logic, you get weird or premature tool usage that ruins the UX.&lt;/p&gt; &lt;p&gt;Curious what others are doing to get more reliable LLM behavior around tools + reasoning. Are you layering on more verification? Custom wrappers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mountain-Insect-2153"&gt; /u/Mountain-Insect-2153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhr56/why_llm_agents_still_hallucinate_even_with_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhr56/why_llm_agents_still_hallucinate_even_with_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhr56/why_llm_agents_still_hallucinate_even_with_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T08:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwyfnq</id>
    <title>Local RAG for PDF questions</title>
    <updated>2025-05-27T20:49:45+00:00</updated>
    <author>
      <name>/u/Overall_Advantage750</name>
      <uri>https://old.reddit.com/user/Overall_Advantage750</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am looking for some feedback one a simple project I put together for asking questions about PDFs. Anyone have experience with chromadb and langchain in combination with Ollama?&lt;br /&gt; &lt;a href="https://github.com/Mschroeder95/ai-rag-setup"&gt;https://github.com/Mschroeder95/ai-rag-setup&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Overall_Advantage750"&gt; /u/Overall_Advantage750 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwyfnq/local_rag_for_pdf_questions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwyfnq/local_rag_for_pdf_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwyfnq/local_rag_for_pdf_questions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T20:49:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwer9z</id>
    <title>Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration</title>
    <updated>2025-05-27T04:46:15+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwer9z/omnir1_reinforcement_learning_for_omnimodal/"&gt; &lt;img alt="Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration" src="https://external-preview.redd.it/Mslr5FmgDa5Wl6TVAGHIe-yyfpC8KB7GpupP6mmM8Ko.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2aefa740a7c49432c821d22fe05c260150bb95bc" title="Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Long-horizon video-audio reasoning and fine-grained pixel understanding impose conflicting requirements on omnimodal models: dense temporal coverage demands many low-resolution frames, whereas precise grounding calls for high-resolution inputs. We tackle this trade-off with a two-system architecture: a Global Reasoning System selects informative keyframes and rewrites the task at low spatial cost, while a Detail Understanding System performs pixel-level grounding on the selected high-resolution snippets. Because ``optimal'' keyframe selection and reformulation are ambiguous and hard to supervise, we formulate them as a reinforcement learning (RL) problem and present Omni-R1, an end-to-end RL framework built on Group Relative Policy Optimization. Omni-R1 trains the Global Reasoning System through hierarchical rewards obtained via online collaboration with the Detail Understanding System, requiring only one epoch of RL on small task splits.&lt;br /&gt; Experiments on two challenging benchmarks, namely Referring Audio-Visual Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show that Omni-R1 not only surpasses strong supervised baselines but also outperforms specialized state-of-the-art models, while substantially improving out-of-domain generalization and mitigating multimodal hallucination. Our results demonstrate the first successful application of RL to large-scale omnimodal reasoning and highlight a scalable path toward universally foundation models.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Haoz0206/Omni-R1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwer9z/omnir1_reinforcement_learning_for_omnimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwer9z/omnir1_reinforcement_learning_for_omnimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T04:46:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwwgon</id>
    <title>How to think about ownership of my personal AI system</title>
    <updated>2025-05-27T19:31:53+00:00</updated>
    <author>
      <name>/u/davidtwaring</name>
      <uri>https://old.reddit.com/user/davidtwaring</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm working on building my own personal AI system, and thinking about what it means to own my own AI system. Here‚Äôs how I‚Äôm thinking about it and would appreciate thoughts from the community on where you think I am on or off base here. &lt;/p&gt; &lt;p&gt;I think ownership lies on spectrum between running on ChatGPT which I clearly don‚Äôt own or running a 100% MIT licensed setup locally that I clearly do own. &lt;/p&gt; &lt;p&gt;Hosting: Let‚Äôs say I‚Äôm running an MIT-licensed AI system but instead of hosting it locally, I run it on Google Cloud. I don‚Äôt own the cloud infrastructure, but I‚Äôd still consider this my AI system. Why? Because I retain full control. I can leave anytime, move to another host, or run it locally without losing anything. The cloud host is a service that I am using to host my AI system. &lt;/p&gt; &lt;p&gt;AI Models: I also don‚Äôt believe I need to own or self-host every model I use in order to own my AI system. I think about this like my physical mind. I control my intelligence, but I routinely consult other minds you don‚Äôt own like mentors, books, and specialists. So if I use a third-party model (say, for legal or health advice), that doesn‚Äôt compromise ownership so long as I choose when and how to use it, and I‚Äôm not locked into it.&lt;/p&gt; &lt;p&gt;Interface: Where I draw a harder line is the interface. Whether it‚Äôs a chatbox, wearable, or voice assistant, this is the entry point to my digital mind. If I don‚Äôt own and control this, someone else could reshape how I experience or access my system. So if I don‚Äôt own the interface I don‚Äôt believe I own my own AI system. &lt;/p&gt; &lt;p&gt;Storage &amp;amp; Memory: As memory in AI systems continues to improve, this is what is going to make AI systems truly personal. And this will be what makes my AI system truly my AI system. As unique to me as my physical memory, and exponentially more powerful. The more I use my personal AI system the more memory it will have, and the better and more personalized it will be at helping me. Over time losing access to the memory of my AI system would be as bad or potentially even worse than losing access to my physical memory.&lt;/p&gt; &lt;p&gt;Do you agree, disagree or think I am missing components from the above? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davidtwaring"&gt; /u/davidtwaring &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwgon/how_to_think_about_ownership_of_my_personal_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwgon/how_to_think_about_ownership_of_my_personal_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwgon/how_to_think_about_ownership_of_my_personal_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T19:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx24nq</id>
    <title>GitHub - som1tokmynam/FusionQuant: FusionQuant Model Merge &amp; GGUF Conversion Pipeline - Your Free Toolkit for Custom LLMs!</title>
    <updated>2025-05-27T23:26:01+00:00</updated>
    <author>
      <name>/u/Som1tokmynam</name>
      <uri>https://old.reddit.com/user/Som1tokmynam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Just dropped &lt;strong&gt;FusionQuant v1.4&lt;/strong&gt;! a Docker-based toolkit to easily merge LLMs (with Mergekit) and convert them to GGUF (Llama.cpp) or the newly supported EXL2 format (Exllamav2) for local use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt;&lt;a href="https://github.com/som1tokmynam/FusionQuant"&gt;https://github.com/som1tokmynam/FusionQuant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key v1.4 Updates:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ú® &lt;strong&gt;EXL2 Quantization:&lt;/strong&gt; Now supports Exllamav2 for efficient EXL2 model creation.&lt;/li&gt; &lt;li&gt;üöÄ &lt;strong&gt;Optimized Docker:&lt;/strong&gt; Uses custom precompiled &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;exl2&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;üíæ &lt;strong&gt;Local Cache for Merges:&lt;/strong&gt; Save models locally to speed up future merges.&lt;/li&gt; &lt;li&gt;‚öôÔ∏è &lt;strong&gt;More GGUF Options:&lt;/strong&gt; Expanded GGUF quantization choices.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Merge models with YAML, upload to Hugging Face.&lt;/li&gt; &lt;li&gt;Convert to GGUF or EXL2 with many quantization options.&lt;/li&gt; &lt;li&gt;User-friendly Gradio Web UI.&lt;/li&gt; &lt;li&gt;Run as a pipeline or use steps standalone.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get Started (Docker):&lt;/strong&gt; Check the Github for the full &lt;code&gt;docker run&lt;/code&gt; command and requirements (NVIDIA GPU recommended for EXL2/GGUF).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Som1tokmynam"&gt; /u/Som1tokmynam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx24nq/github_som1tokmynamfusionquant_fusionquant_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx24nq/github_som1tokmynamfusionquant_fusionquant_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx24nq/github_som1tokmynamfusionquant_fusionquant_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T23:26:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwile2</id>
    <title>Engineers who work in companies that have embraced AI coding, how has your worklife changed?</title>
    <updated>2025-05-27T09:03:54+00:00</updated>
    <author>
      <name>/u/thezachlandes</name>
      <uri>https://old.reddit.com/user/thezachlandes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on my own since just before GPT 4, so I never experienced AI in the workplace. How has the job changed? How are sprints run? Is more of your time spent reviewing pull requests? Has the pace of releases increased? Do things break more often?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thezachlandes"&gt; /u/thezachlandes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwile2/engineers_who_work_in_companies_that_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwile2/engineers_who_work_in_companies_that_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwile2/engineers_who_work_in_companies_that_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T09:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwws3n</id>
    <title>B-score: Detecting Biases in Large Language Models Using Response History</title>
    <updated>2025-05-27T19:44:41+00:00</updated>
    <author>
      <name>/u/Substantial-Air-1285</name>
      <uri>https://old.reddit.com/user/Substantial-Air-1285</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; When LLMs can see their own previous answers, their biases significantly decrease. We introduce B-score, a metric that detects bias by comparing responses between single-turn and multi-turn conversations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper, Code &amp;amp; Data:&lt;/strong&gt; &lt;a href="https://b-score.github.io"&gt;https://b-score.github.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial-Air-1285"&gt; /u/Substantial-Air-1285 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwws3n/bscore_detecting_biases_in_large_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwws3n/bscore_detecting_biases_in_large_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwws3n/bscore_detecting_biases_in_large_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T19:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwhw20</id>
    <title>Cognito: Your AI Sidekick for Chrome. A MIT licensed very lightweight Web UI with multitools.</title>
    <updated>2025-05-27T08:13:51+00:00</updated>
    <author>
      <name>/u/Asleep-Ratio7535</name>
      <uri>https://old.reddit.com/user/Asleep-Ratio7535</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Easiest Setup: No python, no docker, no endless dev packages.&lt;/strong&gt; Just download it from &lt;a href="https://chromewebstore.google.com/detail/pphjdjdoclkedgiaahmiahladgcpohca?utm_source=item-share-cb"&gt;Chrome&lt;/a&gt; or my &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick"&gt;Github&lt;/a&gt; (Same with the store, just the latest release). You don't need an exe.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No privacy issue: you can check the code yourself.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless AI Integration:&lt;/strong&gt; Connect to a wide array of powerful AI models: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Models:&lt;/strong&gt; Ollama, LM Studio, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloud Services: several&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Connections:&lt;/strong&gt; all OpenAI compatible endpoints.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Content Interaction:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Instant Summaries:&lt;/strong&gt; Get the gist of any webpage in seconds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Contextual Q&amp;amp;A:&lt;/strong&gt; Ask questions about the current page, PDFs, selected text in the notes or you can simply send the urls directly to the bot, the scrapper will give the bot context to use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Web Search with scrapper:&lt;/strong&gt; Conduct context-aware searches using Google, DuckDuckGo, and Wikipedia, with the ability to fetch and analyze content from search results.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable Personas (system prompts):&lt;/strong&gt; Choose from 7 pre-built AI personalities (Researcher, Strategist, etc.) or create your own.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text-to-Speech (TTS):&lt;/strong&gt; Hear AI responses read aloud (supports browser TTS and integration with external services like Piper).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat History:&lt;/strong&gt; You can search it (also planed to be used in RAG).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don't know how to post image here, tried links, markdown links or directly upload, all failed to display. Screenshots gifs links below: &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/web.gif&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif"&gt;https://github.com/3-ark/Cognito-AI_Sidekick/blob/main/docs/local.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asleep-Ratio7535"&gt; /u/Asleep-Ratio7535 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwhw20/cognito_your_ai_sidekick_for_chrome_a_mit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T08:13:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwfp8v</id>
    <title>Used A100 80 GB Prices Don't Make Sense</title>
    <updated>2025-05-27T05:44:37+00:00</updated>
    <author>
      <name>/u/fakebizholdings</name>
      <uri>https://old.reddit.com/user/fakebizholdings</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can someone explain what I'm missing? The median price of the A100 80GB PCIe on eBay is $18,502 RTX 6000 Pro Blackwell cards can be purchased new for $8500. &lt;/p&gt; &lt;p&gt;What am I missing here? Is there something about the A100s that justifies the price difference? The only thing I can think of is 200w less power consumption and NVlink.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakebizholdings"&gt; /u/fakebizholdings &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfp8v/used_a100_80_gb_prices_dont_make_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfp8v/used_a100_80_gb_prices_dont_make_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwfp8v/used_a100_80_gb_prices_dont_make_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T05:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwn27n</id>
    <title>FairyR1 32B / 14B</title>
    <updated>2025-05-27T13:19:11+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwn27n/fairyr1_32b_14b/"&gt; &lt;img alt="FairyR1 32B / 14B" src="https://external-preview.redd.it/W-qV0BV1voPJhiTsOQdsGmcAlL-lVFIkzu14DCr59cA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b257239b7a4b96c72e2b478eb3665269afe6ea4" title="FairyR1 32B / 14B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/PKU-DS-LAB/fairy-r1-6834014fe8fd45bc211c6dd7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwn27n/fairyr1_32b_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwn27n/fairyr1_32b_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T13:19:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwlxvb</id>
    <title>Run qwen 30b-a3b on Android local with Alibaba MNN Chat</title>
    <updated>2025-05-27T12:26:03+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwlxvb/run_qwen_30ba3b_on_android_local_with_alibaba_mnn/"&gt; &lt;img alt="Run qwen 30b-a3b on Android local with Alibaba MNN Chat" src="https://external-preview.redd.it/aGZnZW1ma2hpYjNmMebcV0-OYASONSRSOZTsoevngxFFIFBRatfx4SVyyBoC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5360d08748a08af161ba7604536a366b958cba1" title="Run qwen 30b-a3b on Android local with Alibaba MNN Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-050"&gt;https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-050&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/aafvzgkhib3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwlxvb/run_qwen_30ba3b_on_android_local_with_alibaba_mnn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwlxvb/run_qwen_30ba3b_on_android_local_with_alibaba_mnn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T12:26:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwmlos</id>
    <title>mtmd : support Qwen 2.5 Omni (input audio+vision, no audio output) by ngxson ¬∑ Pull Request #13784 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-05-27T12:58:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13784"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwmlos/mtmd_support_qwen_25_omni_input_audiovision_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwmlos/mtmd_support_qwen_25_omni_input_audiovision_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T12:58:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kx077t</id>
    <title>Deepseek R2 Release?</title>
    <updated>2025-05-27T22:00:34+00:00</updated>
    <author>
      <name>/u/Old-Medicine2445</name>
      <uri>https://old.reddit.com/user/Old-Medicine2445</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Didn‚Äôt Deepseek say they were accelerating the timeline to release R2 before the original May release date shooting for April? Now that it‚Äôs almost June, have they said anything about R2 or when they will be releasing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-Medicine2445"&gt; /u/Old-Medicine2445 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx077t/deepseek_r2_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kx077t/deepseek_r2_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kx077t/deepseek_r2_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T22:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwnv4o</id>
    <title>Switched from a PC to Mac for LLM dev - One week Later</title>
    <updated>2025-05-27T13:54:23+00:00</updated>
    <author>
      <name>/u/ETBiggs</name>
      <uri>https://old.reddit.com/user/ETBiggs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ks5sh4/broke_down_and_bought_a_mac_mini_my_processes_run/"&gt;Broke down and bought a Mac Mini - my processes run 5x faster : r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Exactly a week ago I tromped to the Apple Store and bought a Mac Mini M4 Pro with 24gb memory - the model they usually stock in store. I really *didn't* want to move from Windows because I've used Windows since 3.0 and while it has its annoyances, I know the platform and didn't want to stall my development to go down a rabbit hole of new platform hassles - and I'm not a Windows, Mac or Linux 'fan' - they're tools to me - I've used them all - but always thought the MacOS was the least enjoyable to use. &lt;/p&gt; &lt;p&gt;Despite my reservations I bought the thing - and a week later - I'm glad I did - it's a keeper. &lt;/p&gt; &lt;p&gt;It took about 2 hours to set up my simple-as-possible free stack. Anaconda, Ollama, VScode. Download models, build model files, and maybe an hour of cursing to adjust the code for the Mac and I was up and running. I have a few python libraries that complain a bit but still run fine - no issues there. &lt;/p&gt; &lt;p&gt;The unified memory is a game-changer. It's not like having a gamer box with multiple slots having Nvidia cards, but it fits my use-case perfectly - I need to be able to travel with it in a backpack. I run a 13b model 5x faster than my CPU-constrained MiniPC did with an 8b model. I do need to use a free Mac utility to speed my fans up to full blast when running so I don't melt my circuit boards and void my warranty - but this box is the sweet-spot for me. &lt;/p&gt; &lt;p&gt;Still not a big lover of the MacOS but it works - and the hardware and unified memory architecture jams a lot into a small package. &lt;/p&gt; &lt;p&gt;I was hesitant to make the switch because I thought it would be a hassle - but it wasn't all that bad. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ETBiggs"&gt; /u/ETBiggs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwnv4o/switched_from_a_pc_to_mac_for_llm_dev_one_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwnv4o/switched_from_a_pc_to_mac_for_llm_dev_one_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwnv4o/switched_from_a_pc_to_mac_for_llm_dev_one_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T13:54:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwwwil</id>
    <title>We build Curie: The Open-sourced AI Co-Scientist Making ML More Accessible for Your Research</title>
    <updated>2025-05-27T19:49:33+00:00</updated>
    <author>
      <name>/u/Pleasant-Type2044</name>
      <uri>https://old.reddit.com/user/Pleasant-Type2044</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwwil/we_build_curie_the_opensourced_ai_coscientist/"&gt; &lt;img alt="We build Curie: The Open-sourced AI Co-Scientist Making ML More Accessible for Your Research" src="https://b.thumbs.redditmedia.com/5WB6xaADo5SQjBMeBNh63ISBduIh2t7sRigkPfA69Lo.jpg" title="We build Curie: The Open-sourced AI Co-Scientist Making ML More Accessible for Your Research" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After personally seeing many researchers in fields like biology, materials science, and chemistry struggle to &lt;strong&gt;apply machine learning&lt;/strong&gt; to &lt;strong&gt;their valuable domain datasets&lt;/strong&gt; to accelerate scientific discovery and gain deeper insights, often due to the lack of specialized ML knowledge needed to select the right algorithms, tune hyperparameters, or interpret model outputs, we knew we had to help.&lt;/p&gt; &lt;p&gt;That's why we're so excited to introduce the new AutoML feature in &lt;a href="https://github.com/Just-Curieous/Curie"&gt;Curie&lt;/a&gt; üî¨, our AI research experimentation co-scientist designed to &lt;strong&gt;make ML more accessible&lt;/strong&gt;! Our goal is to empower researchers like them to &lt;strong&gt;rapidly test hypotheses and extract deep insights from their data&lt;/strong&gt;. Curie automates the aforementioned complex ML pipeline ‚Äì taking the tedious yet critical work.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k2aq1wo6zd3f1.png?width=1455&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a8c186cf9224019b4e3964adff49d7ee612cf05"&gt;https://preview.redd.it/k2aq1wo6zd3f1.png?width=1455&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a8c186cf9224019b4e3964adff49d7ee612cf05&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, Curie can generate highly performant models, achieving a 0.99 AUC (top 1% performance) for a melanoma (cancer) detection task. We're passionate about open science and invite you to try Curie and even contribute to making it better for everyone!&lt;/p&gt; &lt;p&gt;Check out our post: &lt;a href="https://www.just-curieous.com/machine-learning/research/2025-05-27-automl-co-scientist.html"&gt;https://www.just-curieous.com/machine-learning/research/2025-05-27-automl-co-scientist.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pleasant-Type2044"&gt; /u/Pleasant-Type2044 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwwil/we_build_curie_the_opensourced_ai_coscientist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwwil/we_build_curie_the_opensourced_ai_coscientist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwwwil/we_build_curie_the_opensourced_ai_coscientist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T19:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwrv8g</id>
    <title>Hunyuan releases HunyuanPortrait</title>
    <updated>2025-05-27T16:34:10+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwrv8g/hunyuan_releases_hunyuanportrait/"&gt; &lt;img alt="Hunyuan releases HunyuanPortrait" src="https://preview.redd.it/66xgi7lrqc3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=457d1dce333f1637875489b18ba0f1081aa38b7a" title="Hunyuan releases HunyuanPortrait" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üéâ Introducing HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation&lt;/p&gt; &lt;p&gt;üëâWhat's New?&lt;/p&gt; &lt;p&gt;1‚É£Turn static images into living art! üñº‚û°üé•&lt;/p&gt; &lt;p&gt;2‚É£Unparalleled realism with Implicit Control + Stable Video Diffusion&lt;/p&gt; &lt;p&gt;3‚É£SoTA temporal consistency &amp;amp; crystal-clear fidelity&lt;/p&gt; &lt;p&gt;This breakthrough method outperforms existing techniques, effectively disentangling appearance and motion under various image styles.&lt;/p&gt; &lt;p&gt;üëâWhy Matters?&lt;/p&gt; &lt;p&gt;With this method, animators can now create highly controllable and vivid animations by simply using a single portrait image and video clips as driving templates.&lt;/p&gt; &lt;p&gt;‚úÖ One-click animation üñ±: Single image + video template = hyper-realistic results! üéû&lt;/p&gt; &lt;p&gt;‚úÖ Perfectly synced facial dynamics &amp;amp; head movements&lt;/p&gt; &lt;p&gt;‚úÖ Identity consistency locked across all styles&lt;/p&gt; &lt;p&gt;üëâA Game-changer for Fields likeÔºö&lt;/p&gt; &lt;p&gt;‚ñ∂Ô∏èVirtual Reality + AR experiences üëì&lt;/p&gt; &lt;p&gt;‚ñ∂Ô∏èNext-gen gaming Characters üéÆ&lt;/p&gt; &lt;p&gt;‚ñ∂Ô∏èHuman-AI interactions ü§ñüí¨&lt;/p&gt; &lt;p&gt;üìöDive Deeper&lt;/p&gt; &lt;p&gt;Check out our paper to learn more about the magic behind HunyuanPortrait and how it‚Äôs setting a new standard for portrait animation!&lt;/p&gt; &lt;p&gt;üîó Project Page: &lt;a href="https://kkakkkka.github.io/HunyuanPortrait/"&gt;https://kkakkkka.github.io/HunyuanPortrait/&lt;/a&gt; üîó Research Paper: &lt;a href="https://arxiv.org/abs/2503.18860"&gt;https://arxiv.org/abs/2503.18860&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://x.com/tencenthunyuan/status/1912109205525528673?s=46"&gt;https://x.com/tencenthunyuan/status/1912109205525528673?s=46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üåü Rewriting the rules of digital humans one frame at a time!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66xgi7lrqc3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwrv8g/hunyuan_releases_hunyuanportrait/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwrv8g/hunyuan_releases_hunyuanportrait/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T16:34:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwj2p2</id>
    <title>The Aider LLM Leaderboards were updated with benchmark results for Claude 4, revealing that Claude 4 Sonnet didn't outperform Claude 3.7 Sonnet</title>
    <updated>2025-05-27T09:37:08+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwj2p2/the_aider_llm_leaderboards_were_updated_with/"&gt; &lt;img alt="The Aider LLM Leaderboards were updated with benchmark results for Claude 4, revealing that Claude 4 Sonnet didn't outperform Claude 3.7 Sonnet" src="https://preview.redd.it/ls92grf5oa3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e89933d9870d06458186daafb142b31f9c95830f" title="The Aider LLM Leaderboards were updated with benchmark results for Claude 4, revealing that Claude 4 Sonnet didn't outperform Claude 3.7 Sonnet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ls92grf5oa3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwj2p2/the_aider_llm_leaderboards_were_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwj2p2/the_aider_llm_leaderboards_were_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T09:37:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwqt64</id>
    <title>[Research] AutoThink: Adaptive reasoning technique that improves local LLM performance by 43% on GPQA-Diamond</title>
    <updated>2025-05-27T15:53:20+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;I wanted to share a technique we've been working on called &lt;strong&gt;AutoThink&lt;/strong&gt; that significantly improves reasoning performance on local models through adaptive resource allocation and steering vectors.&lt;/p&gt; &lt;h1&gt;What is AutoThink?&lt;/h1&gt; &lt;p&gt;Instead of giving every query the same amount of &amp;quot;thinking time,&amp;quot; AutoThink:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Classifies query complexity&lt;/strong&gt; (HIGH/LOW) using an adaptive classifier&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamically allocates thinking tokens&lt;/strong&gt; based on complexity (70-90% for hard problems, 20-40% for simple ones)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Uses steering vectors&lt;/strong&gt; to guide reasoning patterns during generation&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Think of it as making your local model &amp;quot;think harder&amp;quot; on complex problems and &amp;quot;think faster&amp;quot; on simple ones.&lt;/p&gt; &lt;h1&gt;Performance Results&lt;/h1&gt; &lt;p&gt;Tested on &lt;strong&gt;DeepSeek-R1-Distill-Qwen-1.5B&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPQA-Diamond&lt;/strong&gt;: 31.06% vs 21.72% baseline (+9.34 points, 43% relative improvement)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MMLU-Pro&lt;/strong&gt;: 26.38% vs 25.58% baseline (+0.8 points)&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;fewer tokens&lt;/strong&gt; than baseline approaches&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technical Approach&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Steering Vectors&lt;/strong&gt;: We use Pivotal Token Search (PTS) - a technique from Microsoft's Phi-4 paper that we implemented and enhanced. These vectors modify activations to encourage specific reasoning patterns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;depth_and_thoroughness&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;numerical_accuracy&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;self_correction&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;exploration&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;organization&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;: Built on our adaptive classifier that can learn new complexity categories without retraining.&lt;/p&gt; &lt;h1&gt;Model Compatibility&lt;/h1&gt; &lt;p&gt;Works with any local reasoning model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-R1 variants&lt;/li&gt; &lt;li&gt;Qwen models&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How to Try It&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Install optillm pip install optillm # Basic usage from optillm.autothink import autothink_decode response = autothink_decode( model, tokenizer, messages, { &amp;quot;steering_dataset&amp;quot;: &amp;quot;codelion/Qwen3-0.6B-pts-steering-vectors&amp;quot;, &amp;quot;target_layer&amp;quot;: 19 # adjust based on your model } ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Full examples in the repo: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/autothink"&gt;https://github.com/codelion/optillm/tree/main/optillm/autothink&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Research Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327"&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AutoThink Code&lt;/strong&gt;: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/autothink"&gt;https://github.com/codelion/optillm/tree/main/optillm/autothink&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PTS Implementation&lt;/strong&gt;: &lt;a href="https://github.com/codelion/pts"&gt;https://github.com/codelion/pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HuggingFace Blog&lt;/strong&gt;: &lt;a href="https://huggingface.co/blog/codelion/pts"&gt;https://huggingface.co/blog/codelion/pts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive Classifier&lt;/strong&gt;: &lt;a href="https://github.com/codelion/adaptive-classifier"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Requires models that support thinking tokens (&lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Need to tune &lt;code&gt;target_layer&lt;/code&gt; parameter for different model architectures&lt;/li&gt; &lt;li&gt;Steering vector datasets are model-specific (though we provide some pre-computed ones)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next&lt;/h1&gt; &lt;p&gt;We're working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for more model architectures&lt;/li&gt; &lt;li&gt;Better automatic layer detection&lt;/li&gt; &lt;li&gt;Community-driven steering vector datasets&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Discussion&lt;/h1&gt; &lt;p&gt;Has anyone tried similar approaches with local models? I'm particularly interested in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How different model families respond to steering vectors&lt;/li&gt; &lt;li&gt;Alternative ways to classify query complexity&lt;/li&gt; &lt;li&gt;Ideas for extracting better steering vectors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your thoughts and results if you try it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwqt64/research_autothink_adaptive_reasoning_technique/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwqt64/research_autothink_adaptive_reasoning_technique/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwqt64/research_autothink_adaptive_reasoning_technique/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T15:53:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwk1jm</id>
    <title>Wife isn‚Äôt home, that means H200 in the living room ;D</title>
    <updated>2025-05-27T10:40:11+00:00</updated>
    <author>
      <name>/u/Flintbeker</name>
      <uri>https://old.reddit.com/user/Flintbeker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt; &lt;img alt="Wife isn‚Äôt home, that means H200 in the living room ;D" src="https://a.thumbs.redditmedia.com/CHdnIbD-SLsvZOKpoU7Rs4hqE0GREYpW_lt-IICeGd0.jpg" title="Wife isn‚Äôt home, that means H200 in the living room ;D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got our H200 System, until it‚Äôs going in the datacenter next week that means localLLaMa with some extra power :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flintbeker"&gt; /u/Flintbeker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kwk1jm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwk1jm/wife_isnt_home_that_means_h200_in_the_living_room/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T10:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kwucpn</id>
    <title>üòûNo hate but claude-4 is disappointing</title>
    <updated>2025-05-27T18:10:17+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"&gt; &lt;img alt="üòûNo hate but claude-4 is disappointing" src="https://preview.redd.it/9dngmfww7d3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d89328b58759f0c926b5258c859b6fbfcf5a5b32" title="üòûNo hate but claude-4 is disappointing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean how the heck literally Is Qwen-3 better than claude-4(the Claude who used to dog walk everyone). this is just disappointing ü´†&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9dngmfww7d3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kwucpn/no_hate_but_claude4_is_disappointing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-27T18:10:17+00:00</published>
  </entry>
</feed>
