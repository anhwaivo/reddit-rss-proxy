<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-30T00:28:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kybtri</id>
    <title>Small open models are more cost effective than closed ones (score from artifical analysis).</title>
    <updated>2025-05-29T14:12:37+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kybtri/small_open_models_are_more_cost_effective_than/"&gt; &lt;img alt="Small open models are more cost effective than closed ones (score from artifical analysis)." src="https://preview.redd.it/hwn90facbq3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e7240aaf83de86ee6bec64e336e5d5e8f0b8703" title="Small open models are more cost effective than closed ones (score from artifical analysis)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sampled only the most cost efficient models that were above a score threshold. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hwn90facbq3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kybtri/small_open_models_are_more_cost_effective_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kybtri/small_open_models_are_more_cost_effective_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T14:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky847t</id>
    <title>Another benchmark result is in for Deepseek r1.1: big jump in nyt word connections</title>
    <updated>2025-05-29T11:13:10+00:00</updated>
    <author>
      <name>/u/_Nils-</name>
      <uri>https://old.reddit.com/user/_Nils-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky847t/another_benchmark_result_is_in_for_deepseek_r11/"&gt; &lt;img alt="Another benchmark result is in for Deepseek r1.1: big jump in nyt word connections" src="https://preview.redd.it/h9qjhjmbfp3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c435c0734d4925ffa167706d837e08be7b2a0870" title="Another benchmark result is in for Deepseek r1.1: big jump in nyt word connections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_Nils-"&gt; /u/_Nils- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h9qjhjmbfp3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky847t/another_benchmark_result_is_in_for_deepseek_r11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky847t/another_benchmark_result_is_in_for_deepseek_r11/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T11:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky6hxy</id>
    <title>MNN is quite something, Qwen3-32B on a OnePlus 13 24GB</title>
    <updated>2025-05-29T09:32:44+00:00</updated>
    <author>
      <name>/u/VickWildman</name>
      <uri>https://old.reddit.com/user/VickWildman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky6hxy/mnn_is_quite_something_qwen332b_on_a_oneplus_13/"&gt; &lt;img alt="MNN is quite something, Qwen3-32B on a OnePlus 13 24GB" src="https://preview.redd.it/432wqex5vo3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5718621f13c3fd8412aaeb9d5f7bca2fe1dfa8d3" title="MNN is quite something, Qwen3-32B on a OnePlus 13 24GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the settings for the model mmap needs to be enabled for this to not crash. It's not that fast, but works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VickWildman"&gt; /u/VickWildman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/432wqex5vo3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky6hxy/mnn_is_quite_something_qwen332b_on_a_oneplus_13/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky6hxy/mnn_is_quite_something_qwen332b_on_a_oneplus_13/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T09:32:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kypm3g</id>
    <title>Noticed Deepseek-R1-0528 mirrors user language in reasoning tokens—interesting!</title>
    <updated>2025-05-29T23:35:28+00:00</updated>
    <author>
      <name>/u/Sparkyu222</name>
      <uri>https://old.reddit.com/user/Sparkyu222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kypm3g/noticed_deepseekr10528_mirrors_user_language_in/"&gt; &lt;img alt="Noticed Deepseek-R1-0528 mirrors user language in reasoning tokens—interesting!" src="https://b.thumbs.redditmedia.com/WRHf27QKCY7p3CNCAxsLaRHX273gAWeS2cN-dG5ubhk.jpg" title="Noticed Deepseek-R1-0528 mirrors user language in reasoning tokens—interesting!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally, Deepseek-R1's reasoning tokens were only in English by default. Now it adapts to the user's language—pretty cool!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sparkyu222"&gt; /u/Sparkyu222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kypm3g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kypm3g/noticed_deepseekr10528_mirrors_user_language_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kypm3g/noticed_deepseekr10528_mirrors_user_language_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T23:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyppno</id>
    <title>SLM RAG Arena</title>
    <updated>2025-05-29T23:40:14+00:00</updated>
    <author>
      <name>/u/unseenmarscai</name>
      <uri>https://old.reddit.com/user/unseenmarscai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyppno/slm_rag_arena/"&gt; &lt;img alt="SLM RAG Arena" src="https://external-preview.redd.it/8r6quPLwJFB6N88XAbM_zURMnRPY5GnGjCbHS1kK7Gk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3318f2724918bae90ef20995728f679fe2fdbe6d" title="SLM RAG Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unseenmarscai"&gt; /u/unseenmarscai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/aizip-dev/SLM-RAG-Arena"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyppno/slm_rag_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyppno/slm_rag_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T23:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyo9df</id>
    <title>new gemma3 abliterated models from mlabonne</title>
    <updated>2025-05-29T22:32:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-4b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-4b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-1b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-1b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-27b-it-qat-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-27b-it-qat-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-12b-it-qat-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-12b-it-qat-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-4b-it-qat-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-4b-it-qat-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-1b-it-qat-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-1b-it-qat-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyo9df/new_gemma3_abliterated_models_from_mlabonne/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyo9df/new_gemma3_abliterated_models_from_mlabonne/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyo9df/new_gemma3_abliterated_models_from_mlabonne/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T22:32:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyjh6f</id>
    <title>Paper page - GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</title>
    <updated>2025-05-29T19:15:48+00:00</updated>
    <author>
      <name>/u/AutomataManifold</name>
      <uri>https://old.reddit.com/user/AutomataManifold</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyjh6f/paper_page_gralora_granular_lowrank_adaptation/"&gt; &lt;img alt="Paper page - GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning" src="https://external-preview.redd.it/bn2Vlujw06Mp0sbZ3JHB5bJ2z1rjiqN_uvV6LZmy5wg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=184973529c8b48cb5ff5b70dc76ccaec9db3387c" title="Paper page - GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This looks pretty promising for getting closer to a full finetuning. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AutomataManifold"&gt; /u/AutomataManifold &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2505.20355"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyjh6f/paper_page_gralora_granular_lowrank_adaptation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyjh6f/paper_page_gralora_granular_lowrank_adaptation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T19:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyofth</id>
    <title>Rough observations about the updated Deepseek R1</title>
    <updated>2025-05-29T22:41:07+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- It has much more patience for some reasons. It doesn't mind actually &amp;quot;giving a try&amp;quot; on very hard problems, like, it doesn't look so lazy now.&lt;/p&gt; &lt;p&gt;- Thinks longer and spends good amount of time on each of it's hypothesized thoughts. The previous version had one flaw, at least in my opinion - while it's initial thinking, it used to just give a hint of idea, thought or an approach to solve the problem without actually exploring it fully, now it just seems like it's selectively deep, it's not shy and it &amp;quot;curiously&amp;quot; proceed along.&lt;/p&gt; &lt;p&gt;- There is still thought retention issue during it's thinking i.e. suppose, it thought about something like for 35 seconds initially and then it left that by saying it's not worth spending time on, and then spent another 3 mins on some other idea/ideas or thought but then again came back to the thought it already spent 35 seconds on initially, then while coming back like this again, it is not able to actually recall what it inferred or maybe calculated during that 35 seconds, so it'll either spend another 35 seconds on it but again stuck in same loop until it realizes... or it just remembers it just doesn't work from it's previous intuition and forgets why it actually thought about this approach &amp;quot;again&amp;quot; after 4 mins to begin with.&lt;/p&gt; &lt;p&gt;- For some reasons, it's much better at calculations. I told it to raw approximate the values of some really hard definite integrals, and it was pretty precise. Other models, first of all use python to approximate that, and if i tell them to do a raw calculation, without using tools, then what they come up with is really far from the actual value. Idk how it got good at raw calculations, but that's very impressive.&lt;/p&gt; &lt;p&gt;- Another fundamental flaw still remains -- Making assumptions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyofth/rough_observations_about_the_updated_deepseek_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyofth/rough_observations_about_the_updated_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyofth/rough_observations_about_the_updated_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T22:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyfcky</id>
    <title>LLM benchmarks for AI MAX+ 395 (HP laptop)</title>
    <updated>2025-05-29T16:34:01+00:00</updated>
    <author>
      <name>/u/BerryGloomy4215</name>
      <uri>https://old.reddit.com/user/BerryGloomy4215</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyfcky/llm_benchmarks_for_ai_max_395_hp_laptop/"&gt; &lt;img alt="LLM benchmarks for AI MAX+ 395 (HP laptop)" src="https://external-preview.redd.it/tr4JiOsuiWwYXrmqb9qpQBRMgBXV0gjIlFHUHTS_EpE.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c64c43c5bd85f57a3e86b9335903ac1c1ac8709f" title="LLM benchmarks for AI MAX+ 395 (HP laptop)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not my video. &lt;/p&gt; &lt;p&gt;Even knowing the bandwidth in advance, the tokens per second are still a bit underwhelming. Can't beat physics I guess.&lt;/p&gt; &lt;p&gt;The Framework Desktop will have a higher TDP, but don't think it's gonna help much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BerryGloomy4215"&gt; /u/BerryGloomy4215 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=-HJ-VipsuSk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyfcky/llm_benchmarks_for_ai_max_395_hp_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyfcky/llm_benchmarks_for_ai_max_395_hp_laptop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T16:34:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kymlon</id>
    <title>Deep Seek R1 0528 FP on Mac Studio M3U 512GB</title>
    <updated>2025-05-29T21:21:51+00:00</updated>
    <author>
      <name>/u/redragtop99</name>
      <uri>https://old.reddit.com/user/redragtop99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using deep seek R1 to do a coding project I’ve been trying to do with O-Mini for a couple weeks and DS528 nailed it. It’s more up to date. &lt;/p&gt; &lt;p&gt;It’s using about 360 GB of ram, and I’m only getting 10TKS max, but using more experts. I also have full 138K context. Taking me longer and running the studio hotter than I’ve felt it before, but it’s chugging it out accurate at least. &lt;/p&gt; &lt;p&gt;Got a 8500 token response which is the longest I’ve had yet. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redragtop99"&gt; /u/redragtop99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymlon/deep_seek_r1_0528_fp_on_mac_studio_m3u_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymlon/deep_seek_r1_0528_fp_on_mac_studio_m3u_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kymlon/deep_seek_r1_0528_fp_on_mac_studio_m3u_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T21:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kymxtq</id>
    <title>Qwen finetune from NVIDIA...?</title>
    <updated>2025-05-29T21:35:46+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymxtq/qwen_finetune_from_nvidia/"&gt; &lt;img alt="Qwen finetune from NVIDIA...?" src="https://external-preview.redd.it/7jWrZJp8baLd-q8ftwpfQNg5hnORpIkQUQ5gwiCsnDY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4948da5a2139d58521457051a127ca863465ac4d" title="Qwen finetune from NVIDIA...?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Qwen-2.5-32B-HS3-RM_20250501"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymxtq/qwen_finetune_from_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kymxtq/qwen_finetune_from_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T21:35:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kxxmdr</id>
    <title>DeepSeek R1 05 28 Tested. It finally happened. The ONLY model to score 100% on everything I threw at it.</title>
    <updated>2025-05-29T00:48:50+00:00</updated>
    <author>
      <name>/u/Ok-Contribution9043</name>
      <uri>https://old.reddit.com/user/Ok-Contribution9043</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ladies and gentlemen, It finally happened. &lt;/p&gt; &lt;p&gt;I knew this day was coming. I knew that one day, a model would come along that would be able to score a 100% on every single task I throw at it. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=4CXkmFbgV28"&gt;https://www.youtube.com/watch?v=4CXkmFbgV28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Past few weeks have been busy - OpenAI 4.1, Gemini 2.5, Claude 4 - They all did very well, but none were able to score a perfect 100% across every single test. DeepSeek R1 05 28 is the FIRST model ever to do this. &lt;/p&gt; &lt;p&gt;And mind you, these aren't impractical tests like you see many folks on youtube doing. Like number of rs in strawberry or write a snake game etc. These are tasks that we actively use in real business applications, and from those, we chose the edge cases on the more complex side of things. &lt;/p&gt; &lt;p&gt;I feel like I am Anton from Ratatouille (if you have seen the movie). I am deeply impressed (pun intended) but also a little bit numb, and having a hard time coming up with the right words. That a free, MIT licensed model from a largely unknown lab until last year has done better than the commercial frontier is wild.&lt;/p&gt; &lt;p&gt;Usually in my videos, I explain the test, and then talk about the mistakes the models are making. But today, since there ARE NO mistakes, I am going to do something different. For each test, i am going to show you a couple of examples of the model's responses - and how hard these questions are, and I hope that gives you a deep sense of appreciation of what a powerful model this is. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Contribution9043"&gt; /u/Ok-Contribution9043 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T00:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyeo4z</id>
    <title>When to Fine-Tune LLMs (and When Not To) - A Practical Guide</title>
    <updated>2025-05-29T16:07:09+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building fine-tunes for 9 years (at my own startup, then at Apple, now at a second startup) and learned a lot along the way. I thought most of this was common knowledge, but I've been told it's helpful so wanted to write up a rough guide for when to (and when not to) fine-tune, what to expect, and which models to consider. Hopefully it's helpful!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Fine-tuning can solve specific, measurable problems: inconsistent outputs, bloated inference costs, prompts that are too complex, and specialized behavior you can't achieve through prompting alone. However, you should pick the goals of fine-tuning before you start, to help you select the right base models.&lt;/p&gt; &lt;p&gt;Here's a quick overview of what fine-tuning can (and can't) do:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quality Improvements&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Task-specific scores&lt;/strong&gt;: Teaching models how to respond through examples (way more effective than just prompting)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Style conformance&lt;/strong&gt;: A bank chatbot needs different tone than a fantasy RPG agent&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JSON formatting&lt;/strong&gt;: Seen format accuracy jump from &amp;lt;5% to &amp;gt;99% with fine-tuning vs base model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Other formatting requirements&lt;/strong&gt;: Produce consistent function calls, XML, YAML, markdown, etc&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cost, Speed and Privacy Benefits&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Shorter prompts&lt;/strong&gt;: Move formatting, style, rules from prompts into the model itself &lt;ul&gt; &lt;li&gt;Formatting instructions → fine-tuning&lt;/li&gt; &lt;li&gt;Tone/style → fine-tuning&lt;/li&gt; &lt;li&gt;Rules/logic → fine-tuning&lt;/li&gt; &lt;li&gt;Chain of thought guidance → fine-tuning&lt;/li&gt; &lt;li&gt;Core task prompt → keep this, but can be much shorter&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smaller models&lt;/strong&gt;: Much smaller models can offer similar quality for specific tasks, once fine-tuned. Example: Qwen 14B runs 6x faster, costs ~3% of GPT-4.1.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local deployment&lt;/strong&gt;: Fine-tune small models to run locally and privately. If building for others, this can drop your inference cost to zero.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Specialized Behaviors&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tool calling&lt;/strong&gt;: Teaching when/how to use specific tools through examples&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logic/rule following&lt;/strong&gt;: Better than putting everything in prompts, especially for complex conditional logic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt;: Add examples of failure modes with correct outputs to eliminate them&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distillation&lt;/strong&gt;: Get large model to teach smaller model (surprisingly easy, takes ~20 minutes)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learned reasoning patterns&lt;/strong&gt;: Teach specific thinking patterns for your domain instead of using expensive general reasoning models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What NOT to Use Fine-Tuning For&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Adding knowledge really isn't a good match for fine-tuning. Use instead:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG for searchable info&lt;/li&gt; &lt;li&gt;System prompts for context&lt;/li&gt; &lt;li&gt;Tool calls for dynamic knowledge&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can combine these with fine-tuned models for the best of both worlds.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Base Model Selection by Goal&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mobile local&lt;/strong&gt;: Gemma 3 3n/1B, Qwen 3 1.7B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Desktop local&lt;/strong&gt;: Qwen 3 4B/8B, Gemma 3 2B/4B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost/speed optimization&lt;/strong&gt;: Try 1B-32B range, compare tradeoff of quality/cost/speed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Max quality&lt;/strong&gt;: Gemma 3 27B, Qwen3 large, Llama 70B, GPT-4.1, Gemini flash/Pro (yes - you can fine-tune closed OpenAI/Google models via their APIs)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Pro Tips&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Iterate and experiment&lt;/strong&gt; - try different base models, training data, tuning with/without reasoning tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Set up evals&lt;/strong&gt; - you need metrics to know if fine-tuning worked&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Start simple&lt;/strong&gt; - supervised fine-tuning usually sufficient before trying RL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Synthetic data works well for most use cases&lt;/strong&gt; - don't feel like you need tons of human-labeled data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The process of fine-tuning involves a few steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Pick specific goals from above&lt;/li&gt; &lt;li&gt;Generate/collect training examples (few hundred to few thousand)&lt;/li&gt; &lt;li&gt;Train on a range of different base models&lt;/li&gt; &lt;li&gt;Measure quality with evals&lt;/li&gt; &lt;li&gt;Iterate, trying more models and training modes&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Tool to Create and Evaluate Fine-tunes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've been building a free and open tool called &lt;a href="https://getkiln.ai"&gt;Kiln&lt;/a&gt; which makes this process easy. It has several major benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Complete&lt;/strong&gt;: Kiln can do every step including defining schemas, creating synthetic data for training, fine-tuning, creating evals to measure quality, and selecting the best model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intuitive&lt;/strong&gt;: anyone can use Kiln. The UI will walk you through the entire process.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: We never have access to your data. Kiln runs locally. You can choose to fine-tune locally (unsloth) or use a service (Fireworks, Together, OpenAI, Google) using your own API keys&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wide range of models&lt;/strong&gt;: we support training over 60 models including open-weight models (Gemma, Qwen, Llama) and closed models (GPT, Gemini)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Evals&lt;/strong&gt;: fine-tuning many models is easy, but selecting the best one can be hard. Our evals will help you figure out which model works best.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to check out the tool or our guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://getkiln.ai"&gt;Kiln AI on Github - over 3500 stars&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.getkiln.ai/docs/fine-tuning-guide"&gt;Guide: How to Fine Tune LLMs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.getkiln.ai/docs/guide-train-a-reasoning-model"&gt;Guide: How to distill LLMs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/blog/why_fine_tune_LLM_models_and_how_to_get_started"&gt;Blog post on when to fine-tune (same ideas as above in more depth)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai"&gt;Kiln AI - Overview and Docs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to answer questions if anyone wants to dive deeper on specific aspects!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyeo4z/when_to_finetune_llms_and_when_not_to_a_practical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyeo4z/when_to_finetune_llms_and_when_not_to_a_practical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyeo4z/when_to_finetune_llms_and_when_not_to_a_practical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T16:07:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyakcp</id>
    <title>DeepSeek-R1-0528 distill on Qwen3 8B</title>
    <updated>2025-05-29T13:17:51+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyakcp/deepseekr10528_distill_on_qwen3_8b/"&gt; &lt;img alt="DeepSeek-R1-0528 distill on Qwen3 8B" src="https://preview.redd.it/nrkr44ek1q3f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7ae61c0111aa5a48e0895ada14976d096d88746" title="DeepSeek-R1-0528 distill on Qwen3 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nrkr44ek1q3f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyakcp/deepseekr10528_distill_on_qwen3_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyakcp/deepseekr10528_distill_on_qwen3_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:17:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kya3c2</id>
    <title>Deepseek R1.1 dominates gemini 2.5 flash on price vs performance</title>
    <updated>2025-05-29T12:56:01+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"&gt; &lt;img alt="Deepseek R1.1 dominates gemini 2.5 flash on price vs performance" src="https://b.thumbs.redditmedia.com/8YFi4IXtXKswD8wkWDjiNDtDIgia7lDEYcmacZzlgVk.jpg" title="Deepseek R1.1 dominates gemini 2.5 flash on price vs performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/di952wumxp3f1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f234fe0c11a5f42bd407698bf0640a3d3d9b18fa"&gt;https://preview.redd.it/di952wumxp3f1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f234fe0c11a5f42bd407698bf0640a3d3d9b18fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: Artifical Analysis&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kya3c2/deepseek_r11_dominates_gemini_25_flash_on_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T12:56:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyap9q</id>
    <title>deepseek-ai/DeepSeek-R1-0528-Qwen3-8B · Hugging Face</title>
    <updated>2025-05-29T13:24:05+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyap9q/deepseekaideepseekr10528qwen38b_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B · Hugging Face" src="https://external-preview.redd.it/8hRwXI0dhC0uoSc2zQ6TvHX1Aw9zshcTMnuDtSCd7AY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdf654415d883f00f7930d8548353332a4e97f3a" title="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyap9q/deepseekaideepseekr10528qwen38b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyap9q/deepseekaideepseekr10528qwen38b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyac9f</id>
    <title>New DeepSeek R1 8B Distill that's "matching the performance of Qwen3-235B-thinking" may be incoming!</title>
    <updated>2025-05-29T13:07:22+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyac9f/new_deepseek_r1_8b_distill_thats_matching_the/"&gt; &lt;img alt="New DeepSeek R1 8B Distill that's &amp;quot;matching the performance of Qwen3-235B-thinking&amp;quot; may be incoming!" src="https://preview.redd.it/8vwdjpcxyp3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16361f0824e9b22cc2a7a8bb532724773abb7a72" title="New DeepSeek R1 8B Distill that's &amp;quot;matching the performance of Qwen3-235B-thinking&amp;quot; may be incoming!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-R1-0528-Qwen3-8B incoming? Oh yeah, gimme that, thank you! 😂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8vwdjpcxyp3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyac9f/new_deepseek_r1_8b_distill_thats_matching_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyac9f/new_deepseek_r1_8b_distill_thats_matching_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:07:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyk9nf</id>
    <title>Always nice to get something open from the closed AI labs. This time from Anthropic, not a model but pretty cool research/exploration tool.</title>
    <updated>2025-05-29T19:47:35+00:00</updated>
    <author>
      <name>/u/indicava</name>
      <uri>https://old.reddit.com/user/indicava</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyk9nf/always_nice_to_get_something_open_from_the_closed/"&gt; &lt;img alt="Always nice to get something open from the closed AI labs. This time from Anthropic, not a model but pretty cool research/exploration tool." src="https://external-preview.redd.it/VhgQC2k4JrTeuPSzZf3YPcyvHE4Tk7RPdF2DBlFwjUY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b0a93a9d20ac4bf6173a53bdc3f6120b8384d723" title="Always nice to get something open from the closed AI labs. This time from Anthropic, not a model but pretty cool research/exploration tool." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indicava"&gt; /u/indicava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.anthropic.com/research/open-source-circuit-tracing"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyk9nf/always_nice_to_get_something_open_from_the_closed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyk9nf/always_nice_to_get_something_open_from_the_closed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T19:47:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky54kq</id>
    <title>PLEASE LEARN BASIC CYBERSECURITY</title>
    <updated>2025-05-29T07:59:20+00:00</updated>
    <author>
      <name>/u/eastwindtoday</name>
      <uri>https://old.reddit.com/user/eastwindtoday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stumbled across a project doing about $30k a month with their OpenAI API key exposed in the frontend.&lt;/p&gt; &lt;p&gt;Public key, no restrictions, fully usable by anyone.&lt;/p&gt; &lt;p&gt;At that volume someone could easily burn through thousands before it even shows up on a billing alert.&lt;/p&gt; &lt;p&gt;This kind of stuff doesn’t happen because people are careless. It happens because things feel like they’re working, so you keep shipping without stopping to think through the basics.&lt;/p&gt; &lt;p&gt;Vibe coding is fun when you’re moving fast. But it’s not so fun when it costs you money, data, or trust.&lt;/p&gt; &lt;p&gt;Add just enough structure to keep things safe. That’s it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eastwindtoday"&gt; /u/eastwindtoday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky54kq/please_learn_basic_cybersecurity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky54kq/please_learn_basic_cybersecurity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky54kq/please_learn_basic_cybersecurity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T07:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kya8kq</id>
    <title>DeepSeek-R1-0528 Official Benchmark</title>
    <updated>2025-05-29T13:02:45+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya8kq/deepseekr10528_official_benchmark/"&gt; &lt;img alt="DeepSeek-R1-0528 Official Benchmark" src="https://preview.redd.it/ph8ccp8vyp3f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a26aecb4cde21d947b429d105d49de5b484adce2" title="DeepSeek-R1-0528 Official Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source：&lt;a href="https://mp.weixin.qq.com/s/U5fnTRW4cGvXYJER__YBiw"&gt;https://mp.weixin.qq.com/s/U5fnTRW4cGvXYJER__YBiw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ph8ccp8vyp3f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kya8kq/deepseekr10528_official_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kya8kq/deepseekr10528_official_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T13:02:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1kyca0p</id>
    <title>Deepseek is the 4th most intelligent AI in the world.</title>
    <updated>2025-05-29T14:31:15+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyca0p/deepseek_is_the_4th_most_intelligent_ai_in_the/"&gt; &lt;img alt="Deepseek is the 4th most intelligent AI in the world." src="https://b.thumbs.redditmedia.com/X0KPg8JXBmXy8-1eNeyHrK1UyUGXMwCXW0z4zs4FhwE.jpg" title="Deepseek is the 4th most intelligent AI in the world." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/t3s1i8o0eq3f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f2f51d1daafe540bca8f70181486a635e78bc0f"&gt;https://preview.redd.it/t3s1i8o0eq3f1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f2f51d1daafe540bca8f70181486a635e78bc0f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And yes, that's Claude-4 all the way at the bottom.&lt;br /&gt; &lt;br /&gt; i love Deepseek&lt;br /&gt; i mean look at the price to performance &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyca0p/deepseek_is_the_4th_most_intelligent_ai_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kyca0p/deepseek_is_the_4th_most_intelligent_ai_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kyca0p/deepseek_is_the_4th_most_intelligent_ai_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T14:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kykez2</id>
    <title>PSA: Don't waste electricity when running vllm. Use this patch</title>
    <updated>2025-05-29T19:53:38+00:00</updated>
    <author>
      <name>/u/pmur12</name>
      <uri>https://old.reddit.com/user/pmur12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was annoyed by vllm using 100% CPU on as many cores as there are connected GPUs even when there's no activity. I have 8 GPUs connected connected to a single machine, so this is 8 CPU cores running at full utilization. Due to turbo boost idle power usage was almost double compared to optimal arrangement.&lt;/p&gt; &lt;p&gt;I went forward and fixed this: &lt;a href="https://github.com/vllm-project/vllm/pull/16226"&gt;https://github.com/vllm-project/vllm/pull/16226&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;The PR to vllm is getting ages to be merged, so if you want to reduce your power cost today, you can use instructions outlined here &lt;a href="https://github.com/vllm-project/vllm/pull/16226#issuecomment-2839769179"&gt;https://github.com/vllm-project/vllm/pull/16226#issuecomment-2839769179&lt;/a&gt; to apply fix. This only works when deploying vllm in a container.&lt;/p&gt; &lt;p&gt;There's similar patch to sglang as well: &lt;a href="https://github.com/sgl-project/sglang/pull/6026"&gt;https://github.com/sgl-project/sglang/pull/6026&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By the way, thumbsup reactions is a relatively good way to make it known that the issue affects lots of people and thus the fix is more important. Maybe the maintainers will merge the PRs sooner.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmur12"&gt; /u/pmur12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kykez2/psa_dont_waste_electricity_when_running_vllm_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kykez2/psa_dont_waste_electricity_when_running_vllm_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kykez2/psa_dont_waste_electricity_when_running_vllm_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T19:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ky8vlm</id>
    <title>DeepSeek-R1-0528 Official Benchmarks Released!!!</title>
    <updated>2025-05-29T11:55:06+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8vlm/deepseekr10528_official_benchmarks_released/"&gt; &lt;img alt="DeepSeek-R1-0528 Official Benchmarks Released!!!" src="https://external-preview.redd.it/G2g_zbuPp_sknOUdQv6ufEg8e0xJC81xbpHlzy2plQU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2851bfb3532bcd96cf4e16cbef4ae32c4943a665" title="DeepSeek-R1-0528 Official Benchmarks Released!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8vlm/deepseekr10528_official_benchmarks_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ky8vlm/deepseekr10528_official_benchmarks_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T11:55:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kymbcn</id>
    <title>DeepSeek-R1-0528-Qwen3-8B on iPhone 16 Pro</title>
    <updated>2025-05-29T21:10:08+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymbcn/deepseekr10528qwen38b_on_iphone_16_pro/"&gt; &lt;img alt="DeepSeek-R1-0528-Qwen3-8B on iPhone 16 Pro" src="https://external-preview.redd.it/NXIzbTE5bXRkczNmMTPgNQxrmyDrsqQqm5XEPHINTq7pqExK0opX4bhpHRYD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62607f45a99cf2231166bccc6235669ff6c4e8dc" title="DeepSeek-R1-0528-Qwen3-8B on iPhone 16 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added the updated DeepSeek-R1-0528-Qwen3-8B with 4bit quant in my app to test it on iPhone. It's running with MLX.&lt;/p&gt; &lt;p&gt;It runs which is impressive but too slow to be usable, the model is thinking for too long and the phone get really hot. I wonder if 8B models will be usable when the iPhone 17 drops.&lt;/p&gt; &lt;p&gt;That said, I will add the model on iPad with M series chip.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mb6zoiqtds3f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kymbcn/deepseekr10528qwen38b_on_iphone_16_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kymbcn/deepseekr10528qwen38b_on_iphone_16_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T21:10:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kynytt</id>
    <title>DeepSeek is THE REAL OPEN AI</title>
    <updated>2025-05-29T22:19:53+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every release is great. I am only dreaming to run the 671B beast locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kynytt/deepseek_is_the_real_open_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-29T22:19:53+00:00</published>
  </entry>
</feed>
