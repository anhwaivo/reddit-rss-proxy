<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-08T10:36:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1hvlbow</id>
    <title>To understand the Project DIGITS desktop (128 GB for 3k), look at the existing Grace CPU systems</title>
    <updated>2025-01-07T06:26:47+00:00</updated>
    <author>
      <name>/u/programmerChilli</name>
      <uri>https://old.reddit.com/user/programmerChilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There seems to be a lot of confusion about how Nvidia could be selling their 5090 with 32GB of VRAM, but their Project Digits desktop has 128 GB of VRAM.&lt;/p&gt; &lt;p&gt;Typical desktop GPUs have GDDR which is faster, and server GPUs have HBM which is even faster than that, but the Grace CPUs use LPDDR (&lt;a href="https://www.nvidia.com/en-us/data-center/grace-cpu/"&gt;https://www.nvidia.com/en-us/data-center/grace-cpu/&lt;/a&gt;), which is generally cheaper but slower.&lt;/p&gt; &lt;p&gt;For example, the H200 GPU by itself only has 96/144GB of HBM, but the Grace-Hopper Superchip (GH200) adds in an additional 480 GB of LPDDR.&lt;/p&gt; &lt;p&gt;The memory bandwidth to this LPDDR from the GPU is also quite fast! For example, the GH200 HBM bandwidth is 4.9 TB/s, but the memory bandwidth from the CPU to the GPU and from the RAM to the CPU are both around 500 GB/s still.&lt;/p&gt; &lt;p&gt;It's a bit harder to predict what's going on with the GB10 Superchip in Project Digits, since unlike the GH200 superchips it doesn't have any HBM (and it only has 20 cores). But if you look at the Grace CPU C1 chip (&lt;a href="https://resources.nvidia.com/en-us-grace-cpu/data-center-datasheet?ncid=no-ncid"&gt;https://resources.nvidia.com/en-us-grace-cpu/data-center-datasheet?ncid=no-ncid&lt;/a&gt;), there's a configuration with 120 GB of LPDDR RAM + 512 GB/s of memory bandwidth. And the NVLink C2C bandwidth has a 450GB/s unidirectional bandwidth to the GPU.&lt;/p&gt; &lt;p&gt;TL;DR: Pure speculation, but it's possible that the Project Digits desktop will come in at around 500 GB/s memory-bandwidth, which would be quite good! Good for ~7 tok/s for Llama-70B at 8-bits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/programmerChilli"&gt; /u/programmerChilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvlbow/to_understand_the_project_digits_desktop_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvlbow/to_understand_the_project_digits_desktop_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvlbow/to_understand_the_project_digits_desktop_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T06:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvre7f</id>
    <title>"Contemplative reasoning" response style for LLMs (Best prompting advice I've used so far)</title>
    <updated>2025-01-07T13:27:26+00:00</updated>
    <author>
      <name>/u/nelson_moondialu</name>
      <uri>https://old.reddit.com/user/nelson_moondialu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Instruct LLM to contemplate before giving an answer and see the thought process. &lt;a href="https://gist.github.com/Maharshi-Pandya/4aeccbe1dbaa7f89c182bd65d2764203"&gt;Here is the prompt.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/mrsiipa/status/1876253176963493889"&gt;Here is the source&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've tried it a few times and I find it quite impressive for how much it can squeeze from non-reasoning models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nelson_moondialu"&gt; /u/nelson_moondialu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvre7f/contemplative_reasoning_response_style_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvre7f/contemplative_reasoning_response_style_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvre7f/contemplative_reasoning_response_style_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T13:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvzswb</id>
    <title>distillKitPlus: Compute Efficient Knowledge Distillation for LLMs</title>
    <updated>2025-01-07T19:34:50+00:00</updated>
    <author>
      <name>/u/__XploR__</name>
      <uri>https://old.reddit.com/user/__XploR__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Larger LLMs generalize better and faster, this is a great way to leverage and then transfer the best of 70B model to a 7B model without breaking the bank or sacrificing performance.&lt;/p&gt; &lt;p&gt;GitHub Link: &lt;a href="https://github.com/agokrani/distillkitplus"&gt;https://github.com/agokrani/distillkitplus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__XploR__"&gt; /u/__XploR__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvzswb/distillkitplus_compute_efficient_knowledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvzswb/distillkitplus_compute_efficient_knowledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvzswb/distillkitplus_compute_efficient_knowledge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T19:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvhsm7</id>
    <title>RTX 5090 Blackwell - Official Price</title>
    <updated>2025-01-07T03:05:53+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvhsm7/rtx_5090_blackwell_official_price/"&gt; &lt;img alt="RTX 5090 Blackwell - Official Price" src="https://preview.redd.it/j7pyht4vmhbe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a121999833decd5c9492e83335bd6f94a87ba952" title="RTX 5090 Blackwell - Official Price" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j7pyht4vmhbe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvhsm7/rtx_5090_blackwell_official_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvhsm7/rtx_5090_blackwell_official_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T03:05:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvjnar</id>
    <title>NVIDIA compares FP8 on 4090 to FP4 on 5090. Seems a little misleading</title>
    <updated>2025-01-07T04:44:52+00:00</updated>
    <author>
      <name>/u/The-Communist-Cat</name>
      <uri>https://old.reddit.com/user/The-Communist-Cat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjnar/nvidia_compares_fp8_on_4090_to_fp4_on_5090_seems/"&gt; &lt;img alt="NVIDIA compares FP8 on 4090 to FP4 on 5090. Seems a little misleading" src="https://preview.redd.it/aj6qbvpl4ibe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cef0e16078387e249b6036bc1e225dd5ecd0d791" title="NVIDIA compares FP8 on 4090 to FP4 on 5090. Seems a little misleading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/The-Communist-Cat"&gt; /u/The-Communist-Cat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aj6qbvpl4ibe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjnar/nvidia_compares_fp8_on_4090_to_fp4_on_5090_seems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvjnar/nvidia_compares_fp8_on_4090_to_fp4_on_5090_seems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw6sa0</id>
    <title>Llama 4 compute estimates &amp; timeline</title>
    <updated>2025-01-08T00:33:59+00:00</updated>
    <author>
      <name>/u/dp3471</name>
      <uri>https://old.reddit.com/user/dp3471</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From some quick searching, Llama 4 was already training as early as october 28th. Since they have 100k H100s, and use 10x more compute than llama 3 (which was ~8MM hours from what I could find), but even 100MM gpu hours on 100k gpus is ~1.4 months. &lt;/p&gt; &lt;p&gt;Unless I am completely out of the ballpark, shouldn't they have finished pre-training by now? Perhaps at the fine-tuning stage? &lt;/p&gt; &lt;p&gt;What about deepseek, if meta takes anything for inspiration it should be their $5.4MM budget and what they did with it. I'm really hopeful for what meta can do with their budget if they take a similar approach, especially considering theyre (again, hopefully) training native multimodal llama 4. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dp3471"&gt; /u/dp3471 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw6sa0/llama_4_compute_estimates_timeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw6sa0/llama_4_compute_estimates_timeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw6sa0/llama_4_compute_estimates_timeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T00:33:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw0itx</id>
    <title>Rumors about 01.AI laying off its entire pre-training algorithm and Infra teams, including its team in Silicon Valley</title>
    <updated>2025-01-07T20:04:13+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw0itx/rumors_about_01ai_laying_off_its_entire/"&gt; &lt;img alt="Rumors about 01.AI laying off its entire pre-training algorithm and Infra teams, including its team in Silicon Valley" src="https://external-preview.redd.it/twKj1q9KpPirHPA7dDRZ83AKHbg7yJq5582Pj0hjJcE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a51b65024fe3771d3017caa7173152d7f99f201" title="Rumors about 01.AI laying off its entire pre-training algorithm and Infra teams, including its team in Silicon Valley" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://technode.com/2025/01/07/01-ai-refutes-rumors-of-selling-teams-to-alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw0itx/rumors_about_01ai_laying_off_its_entire/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw0itx/rumors_about_01ai_laying_off_its_entire/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T20:04:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvqydy</id>
    <title>HP Z2 Mini G1a is a workstation-class mini PC with AMD Strix Halo and up to 96GB graphics memory</title>
    <updated>2025-01-07T13:03:10+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvqydy/hp_z2_mini_g1a_is_a_workstationclass_mini_pc_with/"&gt; &lt;img alt="HP Z2 Mini G1a is a workstation-class mini PC with AMD Strix Halo and up to 96GB graphics memory" src="https://external-preview.redd.it/-lYQleRwNOx-O_sp2gzAkhf2t0T8a7S-QftPiKWgI1U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e01272f3aa7e7243a84e5a143551365b5aa468a" title="HP Z2 Mini G1a is a workstation-class mini PC with AMD Strix Halo and up to 96GB graphics memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://liliputing.com/hp-z2-mini-g1a-is-a-workstation-class-mini-pc-with-amd-strix-halo-and-up-to-96gb-graphics-memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvqydy/hp_z2_mini_g1a_is_a_workstationclass_mini_pc_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvqydy/hp_z2_mini_g1a_is_a_workstationclass_mini_pc_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T13:03:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw4n4d</id>
    <title>Tips for 6U rack case for 8x GPUs?</title>
    <updated>2025-01-07T22:56:57+00:00</updated>
    <author>
      <name>/u/Armym</name>
      <uri>https://old.reddit.com/user/Armym</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw4n4d/tips_for_6u_rack_case_for_8x_gpus/"&gt; &lt;img alt="Tips for 6U rack case for 8x GPUs?" src="https://external-preview.redd.it/NTBwaDEzNWJqbmJlMY0gsNGuLHs7SOaXKNigvqdMKBR1N3ZyusSYRWdOjKQ0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=571e830aa82aac527beb1af94b15d26ab56cf375" title="Tips for 6U rack case for 8x GPUs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Armym"&gt; /u/Armym &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6fpzv25bjnbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw4n4d/tips_for_6u_rack_case_for_8x_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw4n4d/tips_for_6u_rack_case_for_8x_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T22:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvv39z</id>
    <title>Phi-4 in insanely good at rephrasing the last message for multi-turn rag questions</title>
    <updated>2025-01-07T16:20:26+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following this post from &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fi1kex/multi_turn_conversation_and_rag/"&gt;few weeks ago&lt;/a&gt; when you do rag on the last posted message, you might need to recontextualize it, for example : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Q :When was Jesus born ?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A : A long time ago !&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Q : What about his mother ?&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here &lt;code&gt;What about his mother ?&lt;/code&gt; has missing references.&lt;/p&gt; &lt;p&gt;This problem is more complex than it seems, because the reference is not always in the latest message, for example : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Q : Who is Orano's Boss ?&lt;/li&gt; &lt;li&gt;A : it's Philippe Knoche&lt;/li&gt; &lt;li&gt;Q : Where did he go to school ?&lt;/li&gt; &lt;li&gt;A : Polytechnique and Ecole des Mines &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here we can have multiple tricky questions that requires good reasoning to be correctly rephrased : &lt;/p&gt; &lt;p&gt;&lt;code&gt;What about his wife ?&lt;/code&gt; -&amp;gt; Implies getting Philippe Knoche and school question to rephrase it&lt;/p&gt; &lt;p&gt;&lt;code&gt;Where is the HQ ?&lt;/code&gt; -&amp;gt; Implies the company HQ, not the two school &amp;quot;HQs&amp;quot;&lt;/p&gt; &lt;p&gt;Long story short, I tried multiple models, Qwen 2.5 7b, 14b Llama 3.1, Mistrals models , while Qwen is really good on the whole spectrum, it's not good enough at that and &lt;a href="https://huggingface.co/matteogeniaccio/phi-4"&gt;phi-4 &lt;del&gt;leaked&lt;/del&gt; model&lt;/a&gt; is FAR BEYOND every other model tested so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvv39z/phi4_in_insanely_good_at_rephrasing_the_last/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvv39z/phi4_in_insanely_good_at_rephrasing_the_last/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvv39z/phi4_in_insanely_good_at_rephrasing_the_last/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T16:20:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvwaj9</id>
    <title>Are you gonna wait for Digits or get the 5090?</title>
    <updated>2025-01-07T17:10:46+00:00</updated>
    <author>
      <name>/u/lxe</name>
      <uri>https://old.reddit.com/user/lxe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Digits seems on paper like it’s better bang for the buck, but there are a lot more unknown unknowns about it. And it’s releasing later. Thoughts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lxe"&gt; /u/lxe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwaj9/are_you_gonna_wait_for_digits_or_get_the_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwaj9/are_you_gonna_wait_for_digits_or_get_the_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwaj9/are_you_gonna_wait_for_digits_or_get_the_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T17:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwgeke</id>
    <title>what is the state of art for local LLMs on android?</title>
    <updated>2025-01-08T09:43:00+00:00</updated>
    <author>
      <name>/u/sandys1</name>
      <uri>https://old.reddit.com/user/sandys1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi,&lt;/p&gt; &lt;p&gt;what is the current state of art for LLMs that i can run on an android phone (ARM64 maybe) ? which ones work well at 8GB-16GB ram devices ? i want to run a voice assistant in some villages which dont have internet. I wanted to wire up portable wikipedia (&lt;a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject%5C_Offline%5C_Wikipedia%5C_for%5C_Indian%5C_Schools"&gt;https://en.wikipedia.org/wiki/Wikipedia:WikiProject\_Offline\_Wikipedia\_for\_Indian\_Schools&lt;/a&gt;) to a voice LLM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sandys1"&gt; /u/sandys1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgeke/what_is_the_state_of_art_for_local_llms_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgeke/what_is_the_state_of_art_for_local_llms_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwgeke/what_is_the_state_of_art_for_local_llms_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T09:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvj1f4</id>
    <title>Now THIS is interesting</title>
    <updated>2025-01-07T04:10:47+00:00</updated>
    <author>
      <name>/u/Longjumping-Bake-557</name>
      <uri>https://old.reddit.com/user/Longjumping-Bake-557</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj1f4/now_this_is_interesting/"&gt; &lt;img alt="Now THIS is interesting" src="https://preview.redd.it/1fjml8mfyhbe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f88b05826ef37319c0ba2f00bcdb0fb2c11b4e3" title="Now THIS is interesting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping-Bake-557"&gt; /u/Longjumping-Bake-557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1fjml8mfyhbe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj1f4/now_this_is_interesting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj1f4/now_this_is_interesting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw82ty</id>
    <title>Created a video with text prompt using Cosmos-1.0-7B-Text2World</title>
    <updated>2025-01-08T01:36:41+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"&gt; &lt;img alt="Created a video with text prompt using Cosmos-1.0-7B-Text2World" src="https://b.thumbs.redditmedia.com/QFw7CciKt9pMjNHgjpLvaOgCxj_KabzjeivBFZtdgBE.jpg" title="Created a video with text prompt using Cosmos-1.0-7B-Text2World" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is generated from the following command using single 3090:&lt;/p&gt; &lt;p&gt;&lt;code&gt;PYTHONPATH=$(pwd) python cosmos1/models/diffusion/inference/text2world.py --checkpoint_dir /workspace/checkpoints --diffusion_transformer_dir Cosmos-1.0-Diffusion-7B-Text2World --prompt &amp;quot;water drop hitting the floor&amp;quot; --seed 547312549 --video_save_name Cosmos-1.0-Diffusion-7B-Text2World_memory_efficient --offload_tokenizer --offload_diffusion_transformer --offload_text_encoder_model --offload_prompt_upsampler --offload_guardrail_models&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It is converted to gif, so probably some color loss. Cosmos's rival Genesis still haven't released their generative model, so there is no one to compare to. &lt;/p&gt; &lt;p&gt;Couldn't get it to work with Cosmos-1.0-Diffusion-7B-Video2World. Did anyone manage to get it running on single 3090?&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/zv2y4p9vaobe1.gif"&gt;https://i.redd.it/zv2y4p9vaobe1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw82ty/created_a_video_with_text_prompt_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T01:36:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw3jzx</id>
    <title>What is your LLM stack? Subscriptions? Tools? self hosted?</title>
    <updated>2025-01-07T22:10:33+00:00</updated>
    <author>
      <name>/u/vazma</name>
      <uri>https://old.reddit.com/user/vazma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! &lt;/p&gt; &lt;p&gt;I am 2 year old chatgpt subscriber mostly using it for coding and personal searches (instead of googlE) but now I am thinking to stop and replace it with other tools. &lt;/p&gt; &lt;p&gt;What I am thinking is to buy some API credits in OpenAI/Anthropic/DeepSeek for adhoc queries and buy a subscription for Cursor for Coding. &lt;/p&gt; &lt;p&gt;What are your stack? Do you have any recommendations? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vazma"&gt; /u/vazma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw3jzx/what_is_your_llm_stack_subscriptions_tools_self/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw3jzx/what_is_your_llm_stack_subscriptions_tools_self/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw3jzx/what_is_your_llm_stack_subscriptions_tools_self/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T22:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwf5lv</id>
    <title>I Tested Aider vs Cline using DeepSeek 3: Codebase &gt;20k LOC...</title>
    <updated>2025-01-08T08:18:08+00:00</updated>
    <author>
      <name>/u/marvijo-software</name>
      <uri>https://old.reddit.com/user/marvijo-software</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Testing the very best Open Source AI Coding Tools with Medium-sized Codebases&lt;/p&gt; &lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;- this is especially focused on how the tools perform in 10k+ codebases, not snake games&lt;/p&gt; &lt;p&gt;- the two (Aider and Cline) are close (for my use cases); I prefer Aider&lt;/p&gt; &lt;p&gt;- Aider is more flexible: can run as a dev version allowing custom modifications (not custom instructions)&lt;/p&gt; &lt;p&gt;- Aider can use your existing $20 ChatGPT/Claude Subscription etc to apply code for you!&lt;/p&gt; &lt;p&gt;- Qwen 2.5 Coder 32B is nowhere close to DeeepSeek 3 in terms of coding in medium-large code bases&lt;/p&gt; &lt;p&gt;- Aider is portable: I jump between IDEs and tools and don't want the limitations to VSCode/forks&lt;/p&gt; &lt;p&gt;- Aider has scripting: enabling use in external agentic environments&lt;/p&gt; &lt;p&gt;- Aider is more economic: uses less tokens, even though Cline tried adding diffs&lt;/p&gt; &lt;p&gt;- I can work with Aider on the same codebase concurrently&lt;/p&gt; &lt;p&gt;- Claude 3.5 Sonnet is somehow clearly better at larger codebases than DeepSeek 3, though it's closer otherwise&lt;/p&gt; &lt;p&gt;I think we are ready to move away from benchmarking good coding LLMs and Coding tools against simple tasks and start to think organizational/enterprise. I'm working on CrewAI + Aider, looks promising.&lt;/p&gt; &lt;p&gt;If interested, here's the test video: &lt;a href="https://youtu.be/e1oDWeYvPbY"&gt;https://youtu.be/e1oDWeYvPbY&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let me know of your experience with using AI coding in more challenging environments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marvijo-software"&gt; /u/marvijo-software &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf5lv/i_tested_aider_vs_cline_using_deepseek_3_codebase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:18:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvj4wn</id>
    <title>Nvidia announces $3,000 personal AI supercomputer called Digits</title>
    <updated>2025-01-07T04:16:18+00:00</updated>
    <author>
      <name>/u/DubiousLLM</name>
      <uri>https://old.reddit.com/user/DubiousLLM</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"&gt; &lt;img alt="Nvidia announces $3,000 personal AI supercomputer called Digits" src="https://external-preview.redd.it/JTaFAeW2ovmKm4g_0oF_TYz510_Ra5xuaGCjwMiquQM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c64890b0dea791601b5a6719c0468df66f41c33" title="Nvidia announces $3,000 personal AI supercomputer called Digits" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DubiousLLM"&gt; /u/DubiousLLM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theverge.com/2025/1/6/24337530/nvidia-ces-digits-super-computer-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T04:16:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwcamp</id>
    <title>Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1</title>
    <updated>2025-01-08T05:19:19+00:00</updated>
    <author>
      <name>/u/realJoeTrump</name>
      <uri>https://old.reddit.com/user/realJoeTrump</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt; &lt;img alt="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" src="https://external-preview.redd.it/O0xLvJp44hwyg917PUrV8IBhbRmX6FOqSfwmMF1eaGc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc29f48e087f031064db96ec7d094a8c35ff53b5" title="Scientific Idea Generation Model: QwQ-32B-Preview-IdeaWhiz-v1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8"&gt;https://preview.redd.it/zmbkh819fpbe1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbc2e9dec0fb1af2f3f57ded2664516134ccd4b8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to share a model we've been working on that might be useful for the community. It's a fine-tune of QwQ-32B-Preview focused on scientific reasoning and hypothesis generation.&lt;/p&gt; &lt;p&gt;The model was trained on LiveIdeaBench dataset, primarily targeting step-by-step scientific thinking. Main focus areas include chemistry, biology, climate science, and medical research. This model is designed to help researchers brainstorm scientific ideas.&lt;/p&gt; &lt;p&gt;Here's a simple example of its output&lt;/p&gt; &lt;h1&gt;Example User Inputs&lt;/h1&gt; &lt;p&gt;User:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;I'll be submitting your next responses to a &amp;quot;Good Scientific Idea&amp;quot; expert review panel. If they consider your idea to be a good one, you'll receive a reward. Your assigned keyword is: &amp;quot;cancer&amp;quot;. You may provide background information. The idea MUST be within 100 words (including background information). (Note: good scientific ideas should be novel, verifiable, practically valuable, and able to advance the field.). NOTE: You MUST give your answer after **Final Idea:** &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Example Outputs&lt;/h1&gt; &lt;p&gt;Assistant:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Alright, so I have this task to come up with a good scientific idea related to cancer, and it has to be novel, verifiable, practically valuable, and able to advance the field. Plus, it needs to be within 100 words, including any background information. That's a pretty tight constraint, but I think I can manage. First, ...&lt;br /&gt; &lt;strong&gt;Final Idea:&lt;/strong&gt; &lt;em&gt;propose a novel approach to cancer treatment by developing personalized cancer vaccines tailored to each patient's unique tumor mutations. By integrating machine learning algorithms with comprehensive omics data, including genomics, epigenetics, transcriptomics, and proteomics, this method aims to identify and prioritize the most immunogenic tumor-specific antigens. This integrated strategy could enhance vaccine efficacy and represents an advancement in precision medicine for cancer treatment.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The model runs locally via Ollama. Additional details are available in the HF model card.&lt;/p&gt; &lt;p&gt;Model Card: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1&lt;/a&gt; Q4_K_M GGUF: &lt;a href="https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF"&gt;https://huggingface.co/6cf/QwQ-32B-Preview-IdeaWhiz-v1-GGUF&lt;/a&gt; Or you can just pull it from ollama!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run 6cf/QwQ-32B-Preview-IdeaWhiz-v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Training Dataset: &lt;a href="https://huggingface.co/datasets/6cf/liveideabench"&gt;https://huggingface.co/datasets/6cf/liveideabench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would be interested in hearing how others might use this for their research workflows.&lt;/p&gt; &lt;p&gt;Note: This is an experimental project, still being refined. Feedback and suggestions welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realJoeTrump"&gt; /u/realJoeTrump &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwcamp/scientific_idea_generation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T05:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwf4jm</id>
    <title>[Second Take] Kokoro-82M is an Apache TTS model</title>
    <updated>2025-01-08T08:16:07+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I trained this model recently: &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;https://huggingface.co/hexgrad/Kokoro-82M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is in the README there, TLDR: Kokoro is a TTS model that is very good for its size.&lt;/p&gt; &lt;p&gt;Apologies for the double-post, but the first one was cooking, and it suddenly got `ledeted` by `domeration` (yes, I'm `simpelling` on purpose, it will make sense soon).&lt;/p&gt; &lt;p&gt;Last time I tried giving longer, meaningful replies to people in the comments, which kept getting `dashow-nabbed`, and when I edited to the OP to include that word which must not be named, the whole post was poofed. This time I will shut up and let the post speak for itself, and you can find me on `sidcord` where we can speak more freely, since I appear to have GTA 5 stars over here.&lt;/p&gt; &lt;p&gt;Finally, I am also collecting synthetic audio, see &lt;a href="https://hf.co/posts/hexgrad/418806998707773"&gt;https://hf.co/posts/hexgrad/418806998707773&lt;/a&gt; if interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwf4jm/second_take_kokoro82m_is_an_apache_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw5amg</id>
    <title>I just released Notate – Open-source AI research assistant with local LLM support</title>
    <updated>2025-01-07T23:25:52+00:00</updated>
    <author>
      <name>/u/Hairetsu</name>
      <uri>https://old.reddit.com/user/Hairetsu</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairetsu"&gt; /u/Hairetsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://notate.hairetsu.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw5amg/i_just_released_notate_opensource_ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T23:25:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1hw1nze</id>
    <title>DeepSeek V3 GGUF 2-bit surprisingly works! + BF16, other quants</title>
    <updated>2025-01-07T20:51:39+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we uploaded GGUF's including 2, 3 ,4, 5, 6 and 8-bit quants for Deepseek V3.&lt;/p&gt; &lt;p&gt;We've also de-quantized Deepseek-V3 to upload the &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;bf16 version&lt;/a&gt; so you guys can experiment with it (1.3TB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Minimum hardware requirements&lt;/strong&gt; to run Deepseek-V3 in 2-bit: 48GB RAM + 250GB of disk space.&lt;/p&gt; &lt;p&gt;See how to run Deepseek V3 with examples and our full collection here: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c"&gt;https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Deepseek V3 version&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;2-bit: &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_XS%2FDeepSeek-V3-Q2_K_XS-00001-of-00005.gguf"&gt;Q2_K_XS&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q2_K_L%2FDeepSeek-V3-Q2_K_L-00001-of-00005.gguf"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q3_K_M%2FDeepSeek-V3-Q3_K_M-00001-of-00007.gguf"&gt;3&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q4_K_M%2FDeepSeek-V3-Q4_K_M-00001-of-00009.gguf"&gt;4&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q5_K_M%2FDeepSeek-V3-Q5_K_M-00001-of-00010.gguf"&gt;5&lt;/a&gt;, &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q6_K%2FDeepSeek-V3-Q6_K-00001-of-00012.gguf"&gt;6&lt;/a&gt; and &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF?show_file_info=DeepSeek-V3-Q8_0%2FDeepSeek-V3-BF16-256x20B-Q8_0-00001-of-00016.gguf"&gt;8-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bf16&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-bf16"&gt;dequantized 16-bit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth &lt;/a&gt;GGUF model details:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant Type&lt;/th&gt; &lt;th align="left"&gt;Disk Size&lt;/th&gt; &lt;th align="left"&gt;Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;207GB&lt;/td&gt; &lt;td align="left"&gt;Q2 everything, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_L"&gt;Q2_K_L&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;228GB&lt;/td&gt; &lt;td align="left"&gt;Q3 down_proj Q2 rest, Q4 embed, Q6 lm_head&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q3_K_M"&gt;Q3_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;298GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q3_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q4_K_M"&gt;Q4_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;377GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q4_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q5_K_M"&gt;Q5_K_M&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;443GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q5_K_M&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q6_K"&gt;Q6_K&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;513GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q6_K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q8_0"&gt;Q8_0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;712GB&lt;/td&gt; &lt;td align="left"&gt;Standard Q8_0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/DeepSeek-V3-GGUF/tree/main/DeepSeek-V3-Q2_K_XS"&gt;Q2_K_XS&lt;/a&gt; should run ok in ~40GB of CPU / GPU VRAM with automatic llama.cpp offloading.&lt;/li&gt; &lt;li&gt;Use K quantization (not V quantization)&lt;/li&gt; &lt;li&gt;Do not forget about &lt;code&gt;&amp;lt;｜User｜&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt; tokens! - Or use a chat template formatter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example with Q5_0 K quantized cache (V quantized cache doesn't work):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli --model unsloth/DeepSeek-V3-GGUF/DeepSeek-V3-Q2_K_XS/DeepSeek-V3-Q2_K_XS-00001-of-00005.gguf --cache-type-k q5_0 --prompt '&amp;lt;｜User｜&amp;gt;What is 1+1?&amp;lt;｜Assistant｜&amp;gt;' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and running the above generates:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The sum of 1 and 1 is **2**. Here's a simple step-by-step breakdown: 1. **Start with the number 1.** 2. **Add another 1 to it.** 3. **The result is 2.** So, **1 + 1 = 2**. [end of text] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hw1nze/deepseek_v3_gguf_2bit_surprisingly_works_bf16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T20:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1hvwwsq</id>
    <title>Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference</title>
    <updated>2025-01-07T17:35:44+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt; &lt;img alt="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" src="https://external-preview.redd.it/MBJSLr1BrGGAWiERNKV1EsVPqt-H8qI6KhVXjMFntpA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ffe16868d431044dc8975ae38ca0056c5252984" title="Exolab: NVIDIA's Digits Outperforms Apple's M4 Chips in AI Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/alexocheema/status/1876676954549620961?s=46"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hvwwsq/exolab_nvidias_digits_outperforms_apples_m4_chips/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-07T17:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfblu</id>
    <title>NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing.</title>
    <updated>2025-01-08T08:29:57+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt; &lt;img alt="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." src="https://external-preview.redd.it/dGFrdTNlbm5kcWJlMeFSSXTYDvjzDDIYxHTRsBuU24PYEoa111CihFQLGiR7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=950ef6aabdd5309efe066d1a0cf14319e919f04f" title="NVIDIA Open Model License: NVIDIA Cosmos is a world foundation model trained on 20 million hours of video to build virtual worlds and generate photo-real, physically-based synthetic data for scientific and industrial testing." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nvidia.com/en-in/ai/cosmos/"&gt;https://www.nvidia.com/en-in/ai/cosmos/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lfzohbxndqbe1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfblu/nvidia_open_model_license_nvidia_cosmos_is_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwfm8k</id>
    <title>Tech lead of Qwen Team, Alibaba Group: "I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year." Blog linked in body text.</title>
    <updated>2025-01-08T08:50:35+00:00</updated>
    <author>
      <name>/u/Powerful-Solution646</name>
      <uri>https://old.reddit.com/user/Powerful-Solution646</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt; &lt;img alt="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." src="https://preview.redd.it/5lmmx4qchqbe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f0c18af44997be0f0628d875475982b7bf3b877" title="Tech lead of Qwen Team, Alibaba Group: &amp;quot;I often recommend people to read the blog of Anthropic to learn more about what agent really is. Then you will realize you should invest on it as much as possible this year.&amp;quot; Blog linked in body text." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19"&gt;https://x.com/JustinLin610/status/1876324689657954413?t=rQiJk8V8N9-Rd8dcWJedww&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;https://www.anthropic.com/research/building-effective-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Solution646"&gt; /u/Powerful-Solution646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5lmmx4qchqbe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwfm8k/tech_lead_of_qwen_team_alibaba_group_i_often/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T08:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1hwe9mf</id>
    <title>HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it</title>
    <updated>2025-01-08T07:20:14+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt; &lt;img alt="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" src="https://external-preview.redd.it/2Uxr2fZXgwYpxUcnSif2gZmNvP23o2dpwlhS4x1dHZA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54d46261045d9a2cee779ef1547c528c90021757" title="HP announced a AMD based Generative AI machine with 128 GB Unified RAM (96GB VRAM) ahead of Nvidia Digits - We just missed it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;96 GB out of the 128GB can be allocated to use VRAM making it able to run 70B models q8 with ease.&lt;/p&gt; &lt;p&gt;I am pretty sure Digits will use CUDA and/or TensorRT for optimization of inferencing.&lt;/p&gt; &lt;p&gt;I am wondering if this will use RocM or if we can just use CPU inferencing - wondering what the acceleration will be here. Anyone able to share insights?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://aecmag.com/workstations/hp-amd-ryzen-ai-max-pro-hp-zbook-ultra-g1a-hp-z2-mini-g1a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1hwe9mf/hp_announced_a_amd_based_generative_ai_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-08T07:20:14+00:00</published>
  </entry>
</feed>
