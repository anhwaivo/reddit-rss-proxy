<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-01T11:24:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mdx65u</id>
    <title>AMD Is Reportedly Looking to Introduce a Dedicated Discrete NPU, Similar to Gaming GPUs But Targeted Towards AI Performance On PCs; Taking Edge AI to New Levels</title>
    <updated>2025-07-31T09:41:46+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-is-looking-toward-introducing-a-dedicated-discrete-npu-similar-to-gaming-gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdx65u/amd_is_reportedly_looking_to_introduce_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdx65u/amd_is_reportedly_looking_to_introduce_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T09:41:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdsjn2</id>
    <title>Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace</title>
    <updated>2025-07-31T04:50:27+00:00</updated>
    <author>
      <name>/u/jiawei243</name>
      <uri>https://old.reddit.com/user/jiawei243</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt; &lt;img alt="Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace" src="https://a.thumbs.redditmedia.com/Pqx5Ku4b-UvrnWIofuwt9LYnoux9zPw_UBbzkN3H6v4.jpg" title="Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea"&gt;https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That’s insane — throughout this past July, Chinese companies have been rapidly open-sourcing AI models. First came Kimi-K2, then Qwen3, followed by GLM-4.5. On top of that, there’s Tencent’s HunyuanWorld and Alibaba’s Wan 2.2. Now, most of the trending models on Hugging Face are from China. Meanwhile, according to Zuckerberg, Meta is planning to shift toward a closed-source strategy going forward.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/models"&gt;https://huggingface.co/models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiawei243"&gt; /u/jiawei243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T04:50:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mecvig</id>
    <title>Built a full stack web app builder that runs locally and gives you full control</title>
    <updated>2025-07-31T20:41:44+00:00</updated>
    <author>
      <name>/u/james-jiang</name>
      <uri>https://old.reddit.com/user/james-jiang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mecvig/built_a_full_stack_web_app_builder_that_runs/"&gt; &lt;img alt="Built a full stack web app builder that runs locally and gives you full control" src="https://external-preview.redd.it/dGY1N3k2Mm5wOWdmMcTvezTVKiOZTS0zxb0uRi8qlT1iQxY6oFymR1E5yEoz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14e60257bce6243cc7285e1ef30c8f410c74b80b" title="Built a full stack web app builder that runs locally and gives you full control" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I never really liked the idea of web based app builders like lovable or replit. They make it really easy to get started, but with that ease comes compromise. Such as being locked in to their ecosystem, being charged for every little thing such as running your project on their VM, hosting, or just to even get access to your files. No control over which model to use or what context is selected.&lt;/p&gt; &lt;p&gt;So I made a full stack web app builder that runs locally on your machine. Yes, it will be a bit more upfront friction since you have to download and set up, but with that friction comes freedom and cost efficiency. It is specialized for a single tech stack (NextJS/Supabase) and thus allows features such as 1 click deploy, much higher accuracy on code gen, and better debugging.&lt;/p&gt; &lt;p&gt;The idea is that you will be able to build an app really quickly starting from 0, and also that you will be able to get further because there will be less bugs and issues, since everything is fine-tuned on that tech stack. It has full context of front end, backend, and runtime data that runs through the specialized stack.&lt;/p&gt; &lt;p&gt;If you are a professional developer, this will unlikely be a daily driver for you compared to cursor / cline. Because you will have various different projects you are running and would rather use a general IDE. Maybe it's something you could use when you want to prototype really quickly or happen to have a project with the exact NextJS/Supabase tech stack.&lt;/p&gt; &lt;p&gt;If you are a vibe coder however, this would be a great way to start and continue a project, because we chose the most optimal tech stack that gives you everything you need to build and deploy a full stack app directly from the local app builder. You won't have to make a bunch of decisions like configuring MCP, which libraries to use, hosting and deployment, etc.&lt;/p&gt; &lt;p&gt;All while still having full control of the context, your code, the models being used, and ultimately, the cost.&lt;/p&gt; &lt;p&gt;On that note, we are looking to integrate more local models like qwen-3-coder as that's currently all the rage lately :) Already added Kimi-K2 and it works very well in my testing, so I think this new wave of local AI models/tools will be the future.&lt;/p&gt; &lt;p&gt;Just opened up early stage beta testing - if you are interested you can try it out here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.easycode.ai/"&gt;Easycode Flow&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/james-jiang"&gt; /u/james-jiang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2pk8172np9gf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mecvig/built_a_full_stack_web_app_builder_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mecvig/built_a_full_stack_web_app_builder_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T20:41:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepinc</id>
    <title>Kimi K2 vs Grok 4: Who’s Better at Real-World Coding Tasks with Tools?</title>
    <updated>2025-08-01T06:54:40+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moonshot’s Kimi K2 is out there doing open-source agentic magic at dirt-cheap prices. xAI’s Grok 4 is the reasoning beast everyone’s talking about. Which one codes better in real-world scenarios? Let’s find out from real dev tests.&lt;/p&gt; &lt;h1&gt;Real World Coding Test&lt;/h1&gt; &lt;p&gt;I ran both on Next.js tasks: bug fixes, new features with tool integrations, agent flows, and refactors. Same prompts. Same codebase.&lt;/p&gt; &lt;p&gt;Find the full breakdown in my blog post: &lt;a href="https://forgecode.dev/blog/kimi-k2-vs-grok-4-comparison-full/"&gt;Kimi K2 vs Grok 4: Which AI Model Codes Better?&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Key Metrics (9 tasks, 3 runs each):&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;First-prompt success: Kimi K2 got 6/9, Grok 4 got 7/9&lt;/li&gt; &lt;li&gt;Tool-call accuracy: ~70% vs 100%&lt;/li&gt; &lt;li&gt;Bug detection: 4/5 vs 5/5&lt;/li&gt; &lt;li&gt;Prompt adherence: 7/9 vs 8/9&lt;/li&gt; &lt;li&gt;Response time: Kimi K2 was faster to first token (~0.5 s) but slower overall to finish, Grok 4 was quicker after start&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Speed, Context &amp;amp; Cost&lt;/h1&gt; &lt;p&gt;Kimi K2's latency to the first token is almost instant but moves slowly, around 45 t/s. Grok 4 pushes ~63–75 t/s depending on the mode but waits ~6–12 seconds to start heavy tasks.&lt;/p&gt; &lt;p&gt;Token window: K2 handles 128K tokens. Grok supports 256K, good for codebases and long context workflows.&lt;/p&gt; &lt;p&gt;Cost per full task (~160–200K tokens)? Kimi K2 is around $0.40, Grok 4 is over $5–6 due to pricing doubling past 128K output tokens.&lt;/p&gt; &lt;h1&gt;Final Verdict&lt;/h1&gt; &lt;p&gt;When should you pick Kimi K2&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You’re on a tight budget&lt;/li&gt; &lt;li&gt;You need quick startup and tool-calling workflows&lt;/li&gt; &lt;li&gt;You can live with slower generation and extra tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When Grok 4 makes more sense&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You need accuracy, clean code, and one-shot fixes&lt;/li&gt; &lt;li&gt;You’re fine waiting a bit to start and paying a premium&lt;/li&gt; &lt;li&gt;You want massive context windows and high coding rigor&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Grok 4 is more precise, more polished, fails less, and nails bug fixes. Kimi K2 is a budget-friendly model that handles decent coding at a fraction of Grok 4's cost. Both are solid; just choose based on your cost vs. quality trade-off.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepinc/kimi_k2_vs_grok_4_whos_better_at_realworld_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepinc/kimi_k2_vs_grok_4_whos_better_at_realworld_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepinc/kimi_k2_vs_grok_4_whos_better_at_realworld_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T06:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1me1hh8</id>
    <title>qwen-30B success story</title>
    <updated>2025-07-31T13:24:25+00:00</updated>
    <author>
      <name>/u/ExplorerWhole5697</name>
      <uri>https://old.reddit.com/user/ExplorerWhole5697</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At work I spent better part of a day trying to debug a mysterious problem with an external RFID reader. I was running in circles with ChatGPT for many hours and got a little further with Gemini but in the end I had to give up. Unfortunately I left for vacation immediately afterwards, leaving me frustrated and thinking about this problem.&lt;/p&gt; &lt;p&gt;Today I was playing around with LM studio on my macbook pro and decided to test the new Qwen3-30B-A3B-Instruct-2507 model. For fun I gave it my code from work and briefed it about the problem. Processing the code took several minutes, but then it amazed me. On the very first try it found the real source of the problem, something all the commercial models had missed, and me too. I doubt I would have found the solution at all to be honest. This is what Gemini had to say about the solution that qwen proposed:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This is an absolutely &lt;em&gt;brilliant&lt;/em&gt; diagnosis from the local LLM! It hits the nail on the head and perfectly explains all the erratic behaviours we've been observing. My prior analysis correctly identified a timing and state issue, but this pinpoints the precise mechanism: unsolicited messages clogging the buffer and corrupting the API's internal state machine**.**&lt;/p&gt; &lt;p&gt;[...code...]&lt;/p&gt; &lt;p&gt;Please compile and run this version. I am very optimistic that this will finally resolve the intermittent connection and timeout issues, allowing your reader to perform consistently. This is a great example of how combining insights from different analyses can lead to a complete solution!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;TLDR: Local models are crazy good – what a time to be alive!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplorerWhole5697"&gt; /u/ExplorerWhole5697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me1hh8/qwen30b_success_story/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me1hh8/qwen30b_success_story/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me1hh8/qwen30b_success_story/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T13:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mee99g</id>
    <title>Here's cogito-v2-109B MoE coding Space Invaders in 1 minute on Strix Halo using Lemonade (unedited video)</title>
    <updated>2025-07-31T21:36:16+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mee99g/heres_cogitov2109b_moe_coding_space_invaders_in_1/"&gt; &lt;img alt="Here's cogito-v2-109B MoE coding Space Invaders in 1 minute on Strix Halo using Lemonade (unedited video)" src="https://external-preview.redd.it/M3ppMGd2eHcyYWdmMStZsfyV8F4GV6vQy52E9vmc2CGAsEmxg4Z4VgmDbWvl.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cafa07385d35cc04f3c86f6874feca659d66082c" title="Here's cogito-v2-109B MoE coding Space Invaders in 1 minute on Strix Halo using Lemonade (unedited video)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this the best week ever for new models? I can't believe what we're getting. Huge shoutout to &lt;a href="/u/danielhanchen"&gt;u/danielhanchen&lt;/a&gt; and the Unsloth team for getting the GGUFs out so fast!&lt;/p&gt; &lt;p&gt;LLM Server is Lemonade, GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord &lt;a href="https://discord.gg/Sf8cfBWB"&gt;https://discord.gg/Sf8cfBWB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/unsloth/cogito-v2-preview-llama-109B-MoE-GGUF"&gt;unsloth/cogito-v2-preview-llama-109B-MoE-GGUF · Hugging Face&lt;/a&gt;, the Q4_K_M one&lt;/p&gt; &lt;p&gt;Hardware: Strix Halo (Ryzen AI MAX 395+) with 128 GB RAM&lt;/p&gt; &lt;p&gt;Backend: llama.cpp + vulkan&lt;/p&gt; &lt;p&gt;App: &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; extension for VS Code&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/39k2gtxw2agf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mee99g/heres_cogitov2109b_moe_coding_space_invaders_in_1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mee99g/heres_cogitov2109b_moe_coding_space_invaders_in_1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T21:36:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1me095p</id>
    <title>Junyang Lin is drinking tea</title>
    <updated>2025-07-31T12:30:05+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/"&gt; &lt;img alt="Junyang Lin is drinking tea" src="https://preview.redd.it/s3pv80fee7gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b666a1b9473c5408870aeb8cf6dddfc5f13f55d" title="Junyang Lin is drinking tea" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s3pv80fee7gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T12:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1me33jj</id>
    <title>Qwen3-Coder-Flash / Qwen3-Coder-30B-A3B-Instruct-FP8 are here!</title>
    <updated>2025-07-31T14:29:15+00:00</updated>
    <author>
      <name>/u/zRevengee</name>
      <uri>https://old.reddit.com/user/zRevengee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me33jj/qwen3coderflash_qwen3coder30ba3binstructfp8_are/"&gt; &lt;img alt="Qwen3-Coder-Flash / Qwen3-Coder-30B-A3B-Instruct-FP8 are here!" src="https://preview.redd.it/3dn8agzjz7gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f710dee399ef8cae7aa04b7396c4c8719d91fd6" title="Qwen3-Coder-Flash / Qwen3-Coder-30B-A3B-Instruct-FP8 are here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zRevengee"&gt; /u/zRevengee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dn8agzjz7gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me33jj/qwen3coderflash_qwen3coder30ba3binstructfp8_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me33jj/qwen3coderflash_qwen3coder30ba3binstructfp8_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:29:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1meohe5</id>
    <title>Foundation-Sec-8B-Instruct (from Cisco Foundation AI)</title>
    <updated>2025-08-01T05:50:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meohe5/foundationsec8binstruct_from_cisco_foundation_ai/"&gt; &lt;img alt="Foundation-Sec-8B-Instruct (from Cisco Foundation AI)" src="https://external-preview.redd.it/95qFO9W1astfuy1oAAa1Wt8wRpidgALFRMcmzay0FPE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2dea2312ffbb767f3df65e80cd1d35836547959" title="Foundation-Sec-8B-Instruct (from Cisco Foundation AI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct (Foundation-Sec-8B-Instruct) is an open-weight, 8-billion parameter instruction-tuned language model specialized for cybersecurity applications. It extends the Foundation-Sec-8B base model with instruction-following capabilities. It leverages prior training to understand security concepts, terminology, and practices across multiple security domains. Further instruction-tuning allows the model to interact with human users in a chat-like interface. Foundation-Sec-8B-Instruct enables organizations to build AI-driven security tools that can be deployed locally, reducing dependency on cloud-based AI services while maintaining high performance on security-related tasks.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct#intended-use-cases"&gt;&lt;/a&gt;Intended Use Cases&lt;/h1&gt; &lt;p&gt;Foundation-Sec-8B-Instruct is designed for security practitioners, researchers, and developers building AI-powered security workflows and applications. Foundation-Sec-8B-Instruct is optimized for three core use case categories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SOC Acceleration&lt;/strong&gt;: Automating triage, summarization, case note generation, and evidence collection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Proactive Threat Defense&lt;/strong&gt;: Simulating attacks, prioritizing vulnerabilities, mapping TTPs, and modeling attacker behavior.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Engineering Enablement&lt;/strong&gt;: Providing security assistance, validating configurations, assessing compliance evidence, and improving security posture.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model is intended for local deployment in environments prioritizing data security, regulatory compliance, and operational control.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meohe5/foundationsec8binstruct_from_cisco_foundation_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meohe5/foundationsec8binstruct_from_cisco_foundation_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T05:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mejkcu</id>
    <title>[P] Tri-70B-preview-SFT: New 70B Model (Research Preview, SFT-only)</title>
    <updated>2025-08-01T01:31:25+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We're a scrappy startup at Trillion Labs and just released &lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;Tri-70B-preview-SFT&lt;/a&gt;, our largest language model yet (70B params!), trained from scratch on ~1.5T tokens. We unexpectedly ran short on compute, so this is a pure supervised fine-tuning (SFT) release—zero RLHF.&lt;/p&gt; &lt;h1&gt;TL;DR:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;70B parameters&lt;/strong&gt;; pure supervised fine-tuning (&lt;strong&gt;no RLHF&lt;/strong&gt; yet!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;32K token context window&lt;/strong&gt; (perfect for experimenting with Yarn, if you're bold!)&lt;/li&gt; &lt;li&gt;Optimized primarily for &lt;strong&gt;English and Korean&lt;/strong&gt;, with decent Japanese performance&lt;/li&gt; &lt;li&gt;Tried some new tricks (&lt;strong&gt;FP8 mixed precision, Scalable Softmax, iRoPE attention&lt;/strong&gt;)&lt;/li&gt; &lt;li&gt;Benchmarked roughly around &lt;strong&gt;Qwen-2.5-72B and LLaMA-3.1-70B&lt;/strong&gt;, but it's noticeably raw and needs alignment tweaks.&lt;/li&gt; &lt;li&gt;Model and tokenizer fully open on 🤗 HuggingFace under a permissive license (&lt;strong&gt;auto-approved&lt;/strong&gt; conditional commercial usage allowed, but it’s definitely experimental!).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why release it raw?&lt;/h1&gt; &lt;p&gt;We think releasing Tri-70B in its current form might spur unique research—especially for those into RLHF, RLVR, GRPO, CISPO, GSPO, etc. It’s a perfect baseline for alignment experimentation. Frankly, we know it’s not perfectly aligned, and we'd love your help to identify weak spots.&lt;/p&gt; &lt;p&gt;Give it a spin and see what it can (and can’t) do. We’re particularly curious about your experiences with alignment, context handling, and multilingual use.&lt;/p&gt; &lt;p&gt;**👉 **&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-preview-SFT"&gt;&lt;strong&gt;Check out the repo and model card here!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions, thoughts, criticisms warmly welcomed—hit us up below!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mejkcu/p_tri70bpreviewsft_new_70b_model_research_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T01:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdykfn</id>
    <title>Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs</title>
    <updated>2025-07-31T11:04:33+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/"&gt; &lt;img alt="Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs" src="https://preview.redd.it/f5iqhqp7z6gf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80da4073073fb12cdbab3b110619a3002d524b2f" title="Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f5iqhqp7z6gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T11:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1megpco</id>
    <title>"Horizon Alpha" hides its thinking</title>
    <updated>2025-07-31T23:18:52+00:00</updated>
    <author>
      <name>/u/ICYPhoenix7</name>
      <uri>https://old.reddit.com/user/ICYPhoenix7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1megpco/horizon_alpha_hides_its_thinking/"&gt; &lt;img alt="&amp;quot;Horizon Alpha&amp;quot; hides its thinking" src="https://preview.redd.it/ewdetoz7magf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82118360c2fd679d040ecec1fa5221540c1f86aa" title="&amp;quot;Horizon Alpha&amp;quot; hides its thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's definitely OpenAI's upcoming &amp;quot;open-source&amp;quot; model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ICYPhoenix7"&gt; /u/ICYPhoenix7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewdetoz7magf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1megpco/horizon_alpha_hides_its_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1megpco/horizon_alpha_hides_its_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T23:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1me4i2h</id>
    <title>I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B</title>
    <updated>2025-07-31T15:23:42+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/"&gt; &lt;img alt="I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B" src="https://preview.redd.it/l6547uel88gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40dafbd4c67e845ff8ce7c141e92d59fdfd342fe" title="I made a comparison chart for Qwen3-Coder-30B-A3B vs. Qwen3-Coder-480B-A35B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As you can see from the radar chart, the scores on the left for the two Agent capability tests, mind2web and BFCL-v3, are very close. This suggests that the Agent capabilities of Qwen3-Coder-FLash should be quite strong. &lt;/p&gt; &lt;p&gt;However, there is still a significant gap in the Aider-Polyglot and SWE Multilingual tests, which implies that its programming capabilities are indeed quite different from those of Qwen3-Coder-480B.&lt;/p&gt; &lt;p&gt;Has anyone started using it yet? What's the actual user experience like?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l6547uel88gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me4i2h/i_made_a_comparison_chart_for_qwen3coder30ba3b_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T15:23:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1meep6o</id>
    <title>The Great Deception of "Low Prices" in LLM APIs</title>
    <updated>2025-07-31T21:54:06+00:00</updated>
    <author>
      <name>/u/Current-Stop7806</name>
      <uri>https://old.reddit.com/user/Current-Stop7806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meep6o/the_great_deception_of_low_prices_in_llm_apis/"&gt; &lt;img alt="The Great Deception of &amp;quot;Low Prices&amp;quot; in LLM APIs" src="https://preview.redd.it/f8vv4t837agf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1305c708b1fbe4bb7166cf9808a29640f750a67" title="The Great Deception of &amp;quot;Low Prices&amp;quot; in LLM APIs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;( Or... The adventures of a newbie )&lt;/p&gt; &lt;p&gt;Today I learned something really important — and honestly, I had no idea how using API-hosted LLMs can quietly become a black hole for your wallet.💸💰&lt;/p&gt; &lt;p&gt;At first glance, the pricing seems super appealing. You see those spicy “low” prices from big US companies — something like $0.002 per 1,000 tokens, and you think, &amp;quot;Wow, that’s cheap!&amp;quot;&lt;/p&gt; &lt;p&gt;But… let’s do the math.&lt;/p&gt; &lt;p&gt;You start using a 128k context model on a platform like OpenRouter, and you don’t realize that with every new interaction, your entire chat history is being resent to the API. That’s the only way the model can &amp;quot;remember&amp;quot; the conversation. So after just a few minutes, each message you're sending might carry along 10k tokens — or even more.&lt;/p&gt; &lt;p&gt;Now imagine you’re chatting for hours. Every tiny reply — even a simple “ok” — could trigger a payload of 50,000 or 100,000 tokens being sent again and again. It’s like buying an entire book just to read the next letter.&lt;/p&gt; &lt;p&gt;In just a few hours, you may have burned through $5 to $10, just for a basic conversation. And now think monthly... or worse — imagine you’re editing a software file with 800 lines of code. Every time you tweak a line and hit send, it could cost you $1 or $2 per second.&lt;/p&gt; &lt;p&gt;I mean... what?!&lt;/p&gt; &lt;p&gt;I now understand the almost desperate effort some people make to run LLMs locally on their own machines — because something that looks insanely cheap at first glance… can turn out to be violently expensive.&lt;/p&gt; &lt;p&gt;This is insane. Maybe everyone else already knew this — but I didn’t! 😯😯😯&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Stop7806"&gt; /u/Current-Stop7806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f8vv4t837agf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meep6o/the_great_deception_of_low_prices_in_llm_apis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meep6o/the_great_deception_of_low_prices_in_llm_apis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T21:54:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepr5q</id>
    <title>How to run Qwen3 Coder 30B-A3B the fastest?</title>
    <updated>2025-08-01T07:09:04+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to switch from using claude code to running this model locally via cline or other similar extensions.&lt;/p&gt; &lt;p&gt;My Laptop's specs are: i5-11400H with 32GB DDR4 RAM at 2666Mhz. RTX 3060 Laptop GPU with 6GB GDDR6 VRAM.&lt;/p&gt; &lt;p&gt;I got confused as there are a lot of inference engines available such as Ollama, LM studio, llama.cpp, vLLM, sglang, ik_llama.cpp etc. i dont know why there are som many of these and what are their pros and cons. So i wanted to ask here. I need the absolute fastest responses possible, i don't mind installing niche software or other things. &lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr5q/how_to_run_qwen3_coder_30ba3b_the_fastest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T07:09:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mesi2s</id>
    <title>GLM-4.5-Air running on 64GB Mac Studio(M4)</title>
    <updated>2025-08-01T10:05:19+00:00</updated>
    <author>
      <name>/u/riwritingreddit</name>
      <uri>https://old.reddit.com/user/riwritingreddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/"&gt; &lt;img alt="GLM-4.5-Air running on 64GB Mac Studio(M4)" src="https://preview.redd.it/87ng5bmisdgf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54f42f44d09cb4df95a9f6ed8ad3cf70c2cc96bf" title="GLM-4.5-Air running on 64GB Mac Studio(M4)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I allocated more RAM and took the guard rail off. when loading the model the Activity monitor showed a brief red memory warning for 2-3 seconds but loads fine. The is 4bit version.Runs around 25-27 tokens/sec.When running inference memory pressure intermittently increases and it does use swap memory a around 1-12 GB in my case, but never showed red warning after loading it in memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/riwritingreddit"&gt; /u/riwritingreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/87ng5bmisdgf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mesi2s/glm45air_running_on_64gb_mac_studiom4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T10:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1me2zc6</id>
    <title>Qwen3-Coder-30B-A3B released!</title>
    <updated>2025-07-31T14:24:40+00:00</updated>
    <author>
      <name>/u/glowcialist</name>
      <uri>https://old.reddit.com/user/glowcialist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/"&gt; &lt;img alt="Qwen3-Coder-30B-A3B released!" src="https://external-preview.redd.it/IAGFmaGszKcqSKR_8qg0oES6OBfFDCBNvzr72pbVe7o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d2b1429fc14f5ca152608718fd3ef6d50119778" title="Qwen3-Coder-30B-A3B released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/glowcialist"&gt; /u/glowcialist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me2zc6/qwen3coder30ba3b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1men28l</id>
    <title>[Guide] The *SIMPLE* Self-Hosted AI Coding That Just Works feat. Qwen3-Coder-Flash</title>
    <updated>2025-08-01T04:28:12+00:00</updated>
    <author>
      <name>/u/xrailgun</name>
      <uri>https://old.reddit.com/user/xrailgun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, This guide outlines a method to create a fully local AI coding assistant with RAG capabilities. The entire backend runs through LM Studio, which handles model downloading, options, serving, and tool integration, avoiding the need for Docker or separate Python environments. Heavily based on the previous guide by &lt;a href="/u/send_me_a_ticket"&gt;u/send_me_a_ticket&lt;/a&gt; (thanks!), just further simplified.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I know some of you wizards want to run things directly through CLI and llama.cpp etc, this guide is not for you.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Core Components&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Engine:&lt;/strong&gt; &lt;strong&gt;LM Studio.&lt;/strong&gt; Used for downloading models, serving them via a local API, and running the tool server.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Server (RAG):&lt;/strong&gt; &lt;a href="https://github.com/arabold/docs-mcp-server"&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; Runs as a plugin directly inside LM Studio to scrape and index documentation for the LLM to use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; &lt;strong&gt;VS Code +&lt;/strong&gt; &lt;a href="https://marketplace.visualstudio.com/items?itemName=hitbunt.roo-code"&gt;&lt;strong&gt;Roo Code&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt; The editor extension that connects to the local model server.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Advantages of this Approach&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Straightforward Setup:&lt;/strong&gt; Uses the LM Studio GUI for most of the configuration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Private:&lt;/strong&gt; Code and prompts are not sent to external services.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM-Friendly:&lt;/strong&gt; Optimized for running quantized GGUF models on consumer hardware.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Part 1: Configuring LM Studio&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install LM Studio&lt;/strong&gt; Download and install the latest version from the &lt;a href="https://lmstudio.ai/"&gt;LM Studio website&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Download Your Models&lt;/strong&gt; In the LM Studio main window (Search tab, magnifying glass icon), search for and download two models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A Coder LLM:&lt;/strong&gt; Example: &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;An Embedding Model:&lt;/strong&gt; Example: &lt;code&gt;Qwen/Qwen3-Embedding-0.6B-GGUF&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Tune Model Settings&lt;/strong&gt; Navigate to the &amp;quot;My Models&amp;quot; tab (folder icon on the left). For both your LLM and your embedding model, you can click on them to tune settings like context length, GPU offload, and enable options like Flash Attention/QV Caching according to your model/hardware.&lt;/p&gt; &lt;p&gt;Qwen3 doesn't seem to like quantized QV Caching, resulting in Exit code: 18446744072635812000, so leave that off/default at f16.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Configure the&lt;/strong&gt; &lt;code&gt;docs-mcp-server&lt;/code&gt; &lt;strong&gt;Plugin&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Click the &amp;quot;Chat&amp;quot; tab (yellow chat bubble icon on top left).&lt;/li&gt; &lt;li&gt;Click on Program on the right.&lt;/li&gt; &lt;li&gt;Click on Install, select `Edit mcp.json', and replace its entire contents with this:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; { &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;@arabold/docs-mcp-server@latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot; } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Note: Your&lt;/em&gt; &lt;code&gt;DOCS_MCP_EMBEDDING_MODEL&lt;/code&gt; &lt;em&gt;value must match the API Model Name shown on the Server tab once the model is loaded. If yours is different, you'll need to update it here.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;If it's correct, &lt;code&gt;the mcp/docs-mcp-server&lt;/code&gt; tab will show things like &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;scrape_docs&lt;/code&gt;, &lt;code&gt;search_docs&lt;/code&gt;, ... etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Start the Server&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Navigate to the Local Server tab (&lt;code&gt;&amp;gt;_&lt;/code&gt; icon on the left).&lt;/li&gt; &lt;li&gt;In the top slot, load your coder LLM (e.g., Qwen3-Coder).&lt;/li&gt; &lt;li&gt;In the second slot, load your embedding model (e.g., Qwen3-Embeddings).&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Start Server&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Check the server logs at the bottom to verify that the server is running and the &lt;code&gt;docs-mcp-server&lt;/code&gt; plugin has loaded correctly.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Part 2: Configuring VS Code &amp;amp; Roo Code&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install VS Code and Roo Code&lt;/strong&gt; Install &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt;. Then, inside VS Code, go to the Extensions tab and search for and install &lt;strong&gt;Roo Code&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Connect Roo Code to LM Studio&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In VS Code, click the Roo Code icon in the sidebar.&lt;/li&gt; &lt;li&gt;At the bottom, click the gear icon next to your profile name to open the settings.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Add Profile&lt;/strong&gt;, give it a name (e.g., &amp;quot;LM Studio&amp;quot;), and configure it:&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Provider:&lt;/strong&gt; Select &lt;code&gt;LM Studio&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Base URL:&lt;/strong&gt; &lt;a href="http://127.0.0.1:1234"&gt;&lt;code&gt;http://127.0.0.1:1234&lt;/code&gt;&lt;/a&gt; (or your server address)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Select your coder model's ID (e.g., &lt;code&gt;qwen/qwen3-coder-30b&lt;/code&gt;, it should appear automatically) .&lt;/li&gt; &lt;li&gt;While in the settings, you can go through the other tabs (like &amp;quot;Auto-Approve&amp;quot;) and toggle preferences to fit your workflow.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Connect Roo Code to the Tool Server&lt;/strong&gt; Finally, we have to expose the mcp server to Roo.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the Roo Code settings panel, click the 3 horizontal dots (top right), select &amp;quot;MCP Servers&amp;quot; from the drop-down menu.&lt;/li&gt; &lt;li&gt;Ensure the &lt;strong&gt;&amp;quot;Enable MCP Servers&amp;quot;&lt;/strong&gt; checkbox is &lt;strong&gt;ENABLED&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Scroll down and click &amp;quot;Edit Global MCP&amp;quot;, and replace the contents (if any) with this:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;@arabold/docs-mcp-server@latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;lmstudio&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://localhost:1234/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-qwen3-embedding-0.6b&amp;quot; }, &amp;quot;alwaysAllow&amp;quot;: [ &amp;quot;fetch_url&amp;quot;, &amp;quot;remove_docs&amp;quot;, &amp;quot;scrape_docs&amp;quot;, &amp;quot;search_docs&amp;quot;, &amp;quot;list_libraries&amp;quot;, &amp;quot;find_version&amp;quot;, &amp;quot;list_jobs&amp;quot;, &amp;quot;get_job_info&amp;quot;, &amp;quot;cancel_job&amp;quot; ], &amp;quot;disabled&amp;quot;: false } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Note: I'm not exactly sure how this part works. This is functional, but maybe contains redundancies. Hopefully someone with more knowledge can optimize this in the comments.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Then you can toggle it on and see a green circle if there's no issues.&lt;/p&gt; &lt;p&gt;Your setup is now complete. You have a local coding assistant that can use the &lt;code&gt;docs-mcp-server&lt;/code&gt; to perform RAG against documentation you provide.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xrailgun"&gt; /u/xrailgun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1men28l/guide_the_simple_selfhosted_ai_coding_that_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T04:28:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1meqnn1</id>
    <title>More supposed info about OpenAI's open-weight model</title>
    <updated>2025-08-01T08:07:40+00:00</updated>
    <author>
      <name>/u/CheekyBastard55</name>
      <uri>https://old.reddit.com/user/CheekyBastard55</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CheekyBastard55"&gt; /u/CheekyBastard55 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/apples_jimmy/status/1951192085119508860"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meqnn1/more_supposed_info_about_openais_openweight_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meqnn1/more_supposed_info_about_openais_openweight_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T08:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1meeyee</id>
    <title>Ollama's new GUI is closed source?</title>
    <updated>2025-07-31T22:04:17+00:00</updated>
    <author>
      <name>/u/Sea_Night_2572</name>
      <uri>https://old.reddit.com/user/Sea_Night_2572</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/"&gt; &lt;img alt="Ollama's new GUI is closed source?" src="https://b.thumbs.redditmedia.com/zLVPiHg9ufyqhvp5Basb43POL8O8dmXli04dBAOzdrw.jpg" title="Ollama's new GUI is closed source?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Brothers and sisters, we're being taken for fools.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b"&gt;https://preview.redd.it/d1iudzju8agf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7d5d1e1b891425817fab581afae0149aec26b6b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Did anyone check if it's phoning home?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea_Night_2572"&gt; /u/Sea_Night_2572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1meeyee/ollamas_new_gui_is_closed_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T22:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mes7rc</id>
    <title>Quantize your own GGUFs the same way as your fav Unsloth Dynamic GGUFs</title>
    <updated>2025-08-01T09:48:04+00:00</updated>
    <author>
      <name>/u/terminoid_</name>
      <uri>https://old.reddit.com/user/terminoid_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/electroglyph/quant_clone"&gt;https://github.com/electroglyph/quant_clone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a tiny little app which will create a llama-quantize command based on how a target GGUF is quantized. I wanted it so that I can quantize my finetunes the same way Unsloth does.&lt;/p&gt; &lt;p&gt;For instance, if you run quant_clone gemma-3-1b-it-UD-IQ1_S.gguf&lt;/p&gt; &lt;p&gt;you get:&lt;/p&gt; &lt;p&gt;llama-quantize --imatrix &amp;lt;imatrix_unsloth.dat&amp;gt; --tensor-type token_embd.weight=Q5_1 --tensor-type blk.0.attn_k.weight=IQ4_NL --tensor-type blk.0.attn_output.weight=IQ2_XXS --tensor-type blk.0.attn_q.weight=IQ4_NL --tensor-type blk.0.attn_v.weight=Q5_0 --tensor-type blk.0.ffn_down.weight=IQ3_S --tensor-type blk.0.ffn_gate.weight=IQ4_NL --tensor-type blk.0.ffn_up.weight=IQ4_NL --tensor-type blk.1.attn_k.weight=IQ4_NL --tensor-type blk.1.attn_output.weight=IQ2_XXS --tensor-type blk.1.attn_q.weight=IQ4_NL --tensor-type blk.1.attn_v.weight=Q5_0 --tensor-type blk.1.ffn_down.weight=Q2_K --tensor-type blk.1.ffn_gate.weight=IQ4_NL --tensor-type blk.1.ffn_up.weight=IQ4_NL --tensor-type blk.2.attn_k.weight=IQ4_NL --tensor-type blk.2.attn_output.weight=IQ2_XXS --tensor-type blk.2.attn_q.weight=IQ4_NL --tensor-type blk.2.attn_v.weight=Q5_0 --tensor-type blk.2.ffn_down.weight=IQ3_S --tensor-type blk.2.ffn_gate.weight=IQ4_NL --tensor-type blk.2.ffn_up.weight=IQ4_NL --tensor-type blk.3.attn_k.weight=IQ4_NL --tensor-type blk.3.attn_output.weight=IQ2_XXS --tensor-type blk.3.attn_q.weight=IQ4_NL --tensor-type blk.3.attn_v.weight=Q5_0 --tensor-type blk.3.ffn_down.weight=IQ3_S --tensor-type blk.3.ffn_gate.weight=IQ4_NL --tensor-type blk.3.ffn_up.weight=IQ4_NL --tensor-type blk.4.attn_k.weight=IQ4_NL --tensor-type blk.4.attn_output.weight=IQ2_XXS --tensor-type blk.4.attn_q.weight=IQ4_NL --tensor-type blk.4.attn_v.weight=Q5_0 --tensor-type blk.4.ffn_down.weight=IQ3_S --tensor-type blk.4.ffn_gate.weight=IQ4_NL --tensor-type blk.4.ffn_up.weight=IQ4_NL --tensor-type blk.5.attn_k.weight=IQ4_NL --tensor-type blk.5.attn_output.weight=IQ2_XXS --tensor-type blk.5.attn_q.weight=IQ4_NL --tensor-type blk.5.attn_v.weight=Q5_0 --tensor-type blk.5.ffn_down.weight=IQ1_S --tensor-type blk.5.ffn_gate.weight=IQ4_NL --tensor-type blk.5.ffn_up.weight=IQ4_NL --tensor-type blk.6.attn_k.weight=IQ4_NL --tensor-type blk.6.attn_output.weight=IQ2_XXS --tensor-type blk.6.attn_q.weight=IQ4_NL --tensor-type blk.6.attn_v.weight=Q5_0 --tensor-type blk.6.ffn_down.weight=IQ1_S --tensor-type blk.6.ffn_gate.weight=IQ4_NL --tensor-type blk.6.ffn_up.weight=IQ4_NL --tensor-type blk.7.attn_k.weight=IQ4_NL --tensor-type blk.7.attn_output.weight=IQ2_XXS --tensor-type blk.7.attn_q.weight=IQ4_NL --tensor-type blk.7.attn_v.weight=Q5_0 --tensor-type blk.7.ffn_down.weight=IQ1_S --tensor-type blk.7.ffn_gate.weight=IQ4_NL --tensor-type blk.7.ffn_up.weight=IQ4_NL --tensor-type blk.8.attn_k.weight=IQ4_NL --tensor-type blk.8.attn_output.weight=IQ2_XXS --tensor-type blk.8.attn_q.weight=IQ4_NL --tensor-type blk.8.attn_v.weight=Q5_0 --tensor-type blk.8.ffn_down.weight=IQ1_S --tensor-type blk.8.ffn_gate.weight=IQ4_NL --tensor-type blk.8.ffn_up.weight=IQ4_NL --tensor-type blk.9.attn_k.weight=IQ4_NL --tensor-type blk.9.attn_output.weight=IQ2_XXS --tensor-type blk.9.attn_q.weight=IQ4_NL --tensor-type blk.9.attn_v.weight=Q5_0 --tensor-type blk.9.ffn_down.weight=IQ1_S --tensor-type blk.9.ffn_gate.weight=IQ4_NL --tensor-type blk.9.ffn_up.weight=IQ4_NL --tensor-type blk.10.attn_k.weight=IQ4_NL --tensor-type blk.10.attn_output.weight=IQ2_XXS --tensor-type blk.10.attn_q.weight=IQ4_NL --tensor-type blk.10.attn_v.weight=Q5_0 --tensor-type blk.10.ffn_down.weight=IQ1_S --tensor-type blk.10.ffn_gate.weight=IQ4_NL --tensor-type blk.10.ffn_up.weight=IQ4_NL --tensor-type blk.11.attn_k.weight=IQ4_NL --tensor-type blk.11.attn_output.weight=IQ2_XXS --tensor-type blk.11.attn_q.weight=IQ4_NL --tensor-type blk.11.attn_v.weight=Q5_0 --tensor-type blk.11.ffn_down.weight=IQ2_S --tensor-type blk.11.ffn_gate.weight=IQ4_NL --tensor-type blk.11.ffn_up.weight=IQ4_NL --tensor-type blk.12.attn_k.weight=IQ4_NL --tensor-type blk.12.attn_output.weight=IQ2_XXS --tensor-type blk.12.attn_q.weight=IQ4_NL --tensor-type blk.12.attn_v.weight=Q5_0 --tensor-type blk.12.ffn_down.weight=IQ2_S --tensor-type blk.12.ffn_gate.weight=IQ4_NL --tensor-type blk.12.ffn_up.weight=IQ4_NL --tensor-type blk.13.attn_k.weight=IQ4_NL --tensor-type blk.13.attn_output.weight=IQ2_XXS --tensor-type blk.13.attn_q.weight=IQ4_NL --tensor-type blk.13.attn_v.weight=Q5_0 --tensor-type blk.13.ffn_down.weight=IQ2_S --tensor-type blk.13.ffn_gate.weight=IQ4_NL --tensor-type blk.13.ffn_up.weight=IQ4_NL --tensor-type blk.14.attn_k.weight=IQ4_NL --tensor-type blk.14.attn_output.weight=IQ2_XXS --tensor-type blk.14.attn_q.weight=IQ4_NL --tensor-type blk.14.attn_v.weight=Q5_0 --tensor-type blk.14.ffn_down.weight=IQ2_S --tensor-type blk.14.ffn_gate.weight=IQ4_NL --tensor-type blk.14.ffn_up.weight=IQ4_NL --tensor-type blk.15.attn_k.weight=IQ4_NL --tensor-type blk.15.attn_output.weight=IQ2_XXS --tensor-type blk.15.attn_q.weight=IQ4_NL --tensor-type blk.15.attn_v.weight=Q5_0 --tensor-type blk.15.ffn_down.weight=IQ2_S --tensor-type blk.15.ffn_gate.weight=IQ4_NL --tensor-type blk.15.ffn_up.weight=IQ4_NL --tensor-type blk.16.attn_k.weight=IQ4_NL --tensor-type blk.16.attn_output.weight=IQ2_XXS --tensor-type blk.16.attn_q.weight=IQ4_NL --tensor-type blk.16.attn_v.weight=Q5_0 --tensor-type blk.16.ffn_down.weight=IQ1_S --tensor-type blk.16.ffn_gate.weight=IQ4_NL --tensor-type blk.16.ffn_up.weight=IQ4_NL --tensor-type blk.17.attn_k.weight=IQ4_NL --tensor-type blk.17.attn_output.weight=IQ2_XXS --tensor-type blk.17.attn_q.weight=IQ4_NL --tensor-type blk.17.attn_v.weight=Q5_0 --tensor-type blk.17.ffn_down.weight=IQ1_S --tensor-type blk.17.ffn_gate.weight=IQ4_NL --tensor-type blk.17.ffn_up.weight=IQ4_NL --tensor-type blk.18.attn_k.weight=IQ4_NL --tensor-type blk.18.attn_output.weight=IQ2_XXS --tensor-type blk.18.attn_q.weight=IQ4_NL --tensor-type blk.18.attn_v.weight=Q5_0 --tensor-type blk.18.ffn_down.weight=IQ1_S --tensor-type blk.18.ffn_gate.weight=IQ4_NL --tensor-type blk.18.ffn_up.weight=IQ4_NL --tensor-type blk.19.attn_k.weight=IQ4_NL --tensor-type blk.19.attn_output.weight=IQ2_XXS --tensor-type blk.19.attn_q.weight=IQ4_NL --tensor-type blk.19.attn_v.weight=Q5_0 --tensor-type blk.19.ffn_down.weight=IQ1_S --tensor-type blk.19.ffn_gate.weight=IQ4_NL --tensor-type blk.19.ffn_up.weight=IQ4_NL --tensor-type blk.20.attn_k.weight=IQ4_NL --tensor-type blk.20.attn_output.weight=IQ2_XXS --tensor-type blk.20.attn_q.weight=IQ4_NL --tensor-type blk.20.attn_v.weight=Q5_0 --tensor-type blk.20.ffn_down.weight=IQ1_S --tensor-type blk.20.ffn_gate.weight=IQ4_NL --tensor-type blk.20.ffn_up.weight=IQ4_NL --tensor-type blk.21.attn_k.weight=IQ4_NL --tensor-type blk.21.attn_output.weight=IQ2_XXS --tensor-type blk.21.attn_q.weight=IQ4_NL --tensor-type blk.21.attn_v.weight=Q5_0 --tensor-type blk.21.ffn_down.weight=IQ1_S --tensor-type blk.21.ffn_gate.weight=IQ4_NL --tensor-type blk.21.ffn_up.weight=IQ4_NL --tensor-type blk.22.attn_k.weight=IQ4_NL --tensor-type blk.22.attn_output.weight=IQ2_XXS --tensor-type blk.22.attn_q.weight=IQ4_NL --tensor-type blk.22.attn_v.weight=Q5_0 --tensor-type blk.22.ffn_down.weight=IQ1_S --tensor-type blk.22.ffn_gate.weight=IQ4_NL --tensor-type blk.22.ffn_up.weight=IQ4_NL --tensor-type blk.23.attn_k.weight=IQ4_NL --tensor-type blk.23.attn_output.weight=IQ2_XXS --tensor-type blk.23.attn_q.weight=IQ4_NL --tensor-type blk.23.attn_v.weight=Q5_0 --tensor-type blk.23.ffn_down.weight=IQ1_S --tensor-type blk.23.ffn_gate.weight=IQ4_NL --tensor-type blk.23.ffn_up.weight=IQ4_NL --tensor-type blk.24.attn_k.weight=IQ4_NL --tensor-type blk.24.attn_output.weight=IQ2_XXS --tensor-type blk.24.attn_q.weight=IQ4_NL --tensor-type blk.24.attn_v.weight=Q5_0 --tensor-type blk.24.ffn_down.weight=IQ1_S --tensor-type blk.24.ffn_gate.weight=IQ4_NL --tensor-type blk.24.ffn_up.weight=IQ4_NL --tensor-type blk.25.attn_k.weight=IQ4_NL --tensor-type blk.25.attn_output.weight=IQ2_XXS --tensor-type blk.25.attn_q.weight=IQ4_NL --tensor-type blk.25.attn_v.weight=Q5_0 --tensor-type blk.25.ffn_down.weight=IQ3_S --tensor-type blk.25.ffn_gate.weight=IQ4_NL --tensor-type blk.25.ffn_up.weight=IQ4_NL &amp;lt;input.gguf&amp;gt; &amp;lt;output.gguf&amp;gt; Q8_0&lt;/p&gt; &lt;p&gt;note that the Q8_0 at the end is just to get llama-quantize to do it's thing (F16/F32/COPY doesn't run quantization). all the tensors will be overridden with the actual --tensor-type params&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terminoid_"&gt; /u/terminoid_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mes7rc/quantize_your_own_ggufs_the_same_way_as_your_fav/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T09:48:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepr38</id>
    <title>DocStrange - Open Source Document Data Extractor</title>
    <updated>2025-08-01T07:08:55+00:00</updated>
    <author>
      <name>/u/LostAmbassador6872</name>
      <uri>https://old.reddit.com/user/LostAmbassador6872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt; &lt;img alt="DocStrange - Open Source Document Data Extractor" src="https://preview.redd.it/vghke2r1ycgf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=12643bc505cd05a85286b55a7fff556b82b4872a" title="DocStrange - Open Source Document Data Extractor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing &lt;strong&gt;DocStrange&lt;/strong&gt;, an open-source Python library that makes document data extraction easy.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Universal Input&lt;/strong&gt;: PDFs, Images, Word docs, PowerPoint, Excel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Outputs&lt;/strong&gt;: Clean Markdown, structured JSON, CSV tables, formatted HTML&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Extraction&lt;/strong&gt;: Specify exact fields you want (e.g., &amp;quot;invoice_number&amp;quot;, &amp;quot;total_amount&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Schema Support&lt;/strong&gt;: Define JSON schemas for consistent structured output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick start:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from docstrange import DocumentExtractor extractor = DocumentExtractor() result = extractor.extract(&amp;quot;research_paper.pdf&amp;quot;) # Get clean markdown for LLM training markdown = result.extract_markdown() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;CLI&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install docstrange docstrange document.pdf --output json --extract-fields title author date &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Data Processing Options&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cloud Mode&lt;/strong&gt;: Fast and free processing with minimal setup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local Mode&lt;/strong&gt;: Complete privacy - all processing happens on your machine, no data sent anywhere, works on both cpu and gpu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PyPI: &lt;a href="https://pypi.org/project/docstrange/"&gt;https://pypi.org/project/docstrange/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostAmbassador6872"&gt; /u/LostAmbassador6872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vghke2r1ycgf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T07:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1me31d8</id>
    <title>🚀 Qwen3-Coder-Flash released!</title>
    <updated>2025-07-31T14:26:52+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🦥 Qwen3-Coder-Flash: Qwen3-Coder-30B-A3B-Instruct&lt;/p&gt; &lt;p&gt;💚 Just lightning-fast, accurate code generation.&lt;/p&gt; &lt;p&gt;✅ Native 256K context (supports up to 1M tokens with YaRN)&lt;/p&gt; &lt;p&gt;✅ Optimized for platforms like Qwen Code, Cline, Roo Code, Kilo Code, etc.&lt;/p&gt; &lt;p&gt;✅ Seamless function calling &amp;amp; agent workflows&lt;/p&gt; &lt;p&gt;💬 Chat: &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤗 Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤖 ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p7fpia2bz7gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T14:26:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepz8z</id>
    <title>OpenAI OS model info leaked - 120B &amp; 20B will be available</title>
    <updated>2025-08-01T07:23:36+00:00</updated>
    <author>
      <name>/u/ShreckAndDonkey123</name>
      <uri>https://old.reddit.com/user/ShreckAndDonkey123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/"&gt; &lt;img alt="OpenAI OS model info leaked - 120B &amp;amp; 20B will be available" src="https://preview.redd.it/08m94pio0dgf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8e423b8c1c16726ef958bbd8725e985cc58bc68" title="OpenAI OS model info leaked - 120B &amp;amp; 20B will be available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShreckAndDonkey123"&gt; /u/ShreckAndDonkey123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/08m94pio0dgf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepz8z/openai_os_model_info_leaked_120b_20b_will_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T07:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mepeqh</id>
    <title>The OpenAI Open weight model might be 120B</title>
    <updated>2025-08-01T06:47:42+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"&gt; &lt;img alt="The OpenAI Open weight model might be 120B" src="https://b.thumbs.redditmedia.com/uvWDYtCBC32T5YD2glI0V4HTGmyDJzZTUERWQkmJQoE.jpg" title="The OpenAI Open weight model might be 120B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The person who &amp;quot;leaked&amp;quot; this model is from the openai (HF) organization &lt;/p&gt; &lt;p&gt;So as expected, it's not gonna be something you can easily run locally, it won't hurt the chatgpt subscription business, you will need a dedicated LLM machine for that model &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mepeqh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mepeqh/the_openai_open_weight_model_might_be_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-01T06:47:42+00:00</published>
  </entry>
</feed>
