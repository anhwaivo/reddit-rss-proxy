<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-29T08:49:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jmg2wj</id>
    <title>Time to shine — what are your local LLM workflows you couldn’t live without?</title>
    <updated>2025-03-29T06:16:56+00:00</updated>
    <author>
      <name>/u/vel_is_lava</name>
      <uri>https://old.reddit.com/user/vel_is_lava</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is your setup and how do you use it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vel_is_lava"&gt; /u/vel_is_lava &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmg2wj/time_to_shine_what_are_your_local_llm_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmg2wj/time_to_shine_what_are_your_local_llm_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmg2wj/time_to_shine_what_are_your_local_llm_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T06:16:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlwpz9</id>
    <title>llama.cpp parameters for QwQ-32B with 128k expanded context</title>
    <updated>2025-03-28T14:44:54+00:00</updated>
    <author>
      <name>/u/H3PO</name>
      <uri>https://old.reddit.com/user/H3PO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got 48GB of VRAM and the Q4_K_M model fits alongside 128k context using q4_0 value cache quantization. Which parameters do I need to give to llama.cpp to correctly expand the context from 32k to 128k? &lt;a href="https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/tutorial-how-to-run-qwq-32b-effectively"&gt;This unsloth blog post&lt;/a&gt; mentions how they tried setting some --override-kv options, but from what I understand that was in an attempt to fix issues with repetitions, which they then solved with the --sampler paramter.&lt;/p&gt; &lt;p&gt;Below are the parameters I used in my naive attempt to copy those that unsloth suggest, but with yarn rope scaling added. Using the &amp;quot;Create a Flappy Bird game in Python....&amp;quot; prompt from the blog post, QwQ thinks for for a long time and outputs a working flappy bird pygame script (about 150 lines), but only after thinking for about 20.000 tokens.&lt;/p&gt; &lt;p&gt;Should I set the various --yarn-* parameters differently? I notice llama.cpp logs &amp;quot;qwen2.context_length u32 = 131072&amp;quot; and &amp;quot;n_ctx_train = 131072&amp;quot;, which are wrong afaik.&lt;br /&gt; Also, can someone suggest a long-context test prompt I could use to test if the context expansion is working correctly?&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./build/bin/llama-cli \ --threads 32 --prio 2 \ --model ~/llm/models/QwQ-32B-Q4_K_M.gguf \ --n-gpu-layers 99 \ --temp 0.6 --repeat-penalty 1.1 --dry-multiplier 0.5 \ --min-p 0.01 --top-k 40 --top-p 0.95 \ --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \ --ctx-size 131072 --rope-scaling yarn --rope-scale 4 \ --cache-type-v q4_0 --flash-attn \ -no-cnv --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;lt;think&amp;gt;\n&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/H3PO"&gt; /u/H3PO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlwpz9/llamacpp_parameters_for_qwq32b_with_128k_expanded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlwpz9/llamacpp_parameters_for_qwq32b_with_128k_expanded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlwpz9/llamacpp_parameters_for_qwq32b_with_128k_expanded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T14:44:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmeqmt</id>
    <title>Looking for open source projects that DEVOUR LLM tokens</title>
    <updated>2025-03-29T04:46:01+00:00</updated>
    <author>
      <name>/u/lightdreamscape</name>
      <uri>https://old.reddit.com/user/lightdreamscape</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have $330 Claude credits expiring in 1 week. &lt;/p&gt; &lt;p&gt;What are some projects you guys like that are&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open source and can use local and API LLMs&lt;/li&gt; &lt;li&gt;Requires a smarter or more eloquent LLM&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I try to only use Claude API for tasks that require smart LLMs since for dumb ones I just use Gemini api.&lt;/p&gt; &lt;p&gt;I use cursor for coding, OpenAI subscription for deep research.&lt;/p&gt; &lt;p&gt;What do I need Claude for anymore... It's 2-3x the price of Gemini.&lt;/p&gt; &lt;p&gt;Is there a cool open source project I should try out that requires a smarter model? Is there an app idea/workflow that requires using a smarter model that I can add to my workflow in the next week?&lt;/p&gt; &lt;p&gt;What would you use it for?&lt;/p&gt; &lt;p&gt;Is there a way to sell these credits?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lightdreamscape"&gt; /u/lightdreamscape &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmeqmt/looking_for_open_source_projects_that_devour_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmeqmt/looking_for_open_source_projects_that_devour_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmeqmt/looking_for_open_source_projects_that_devour_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T04:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmdwit</id>
    <title>Core ML body segmentation to replace the background in real-time on iOS devices.</title>
    <updated>2025-03-29T03:53:59+00:00</updated>
    <author>
      <name>/u/AdLegitimate1066</name>
      <uri>https://old.reddit.com/user/AdLegitimate1066</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ochornenko/virtual-background-ios"&gt;https://github.com/ochornenko/virtual-background-ios&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This project leverages Core ML body segmentation to replace the background in real-time on iOS devices. Using deep learning models, it accurately detects and segments the human figure, allowing users to apply custom virtual backgrounds. Optimized for performance, it utilizes &lt;strong&gt;Metal&lt;/strong&gt; for efficient GPU-based rendering and &lt;strong&gt;vImage&lt;/strong&gt;for high-performance image processing, ensuring smooth and responsive background replacement on mobile devices. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdLegitimate1066"&gt; /u/AdLegitimate1066 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmdwit/core_ml_body_segmentation_to_replace_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmdwit/core_ml_body_segmentation_to_replace_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmdwit/core_ml_body_segmentation_to_replace_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T03:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm5tcb</id>
    <title>People who bought the tinybox, what is your review?</title>
    <updated>2025-03-28T21:12:39+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to recommend the tinybox green or pro made by tinygrad to one of my customer to do inference for about 100 concurrent users a day, but I didn't find any customers review. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm5tcb/people_who_bought_the_tinybox_what_is_your_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm5tcb/people_who_bought_the_tinybox_what_is_your_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm5tcb/people_who_bought_the_tinybox_what_is_your_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T21:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmhojo</id>
    <title>AI Development Made Simple for Web Developers!</title>
    <updated>2025-03-29T08:17:37+00:00</updated>
    <author>
      <name>/u/No-Section4169</name>
      <uri>https://old.reddit.com/user/No-Section4169</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmhojo/ai_development_made_simple_for_web_developers/"&gt; &lt;img alt="AI Development Made Simple for Web Developers!" src="https://external-preview.redd.it/m3gtLS0JZbW7GJ2cc-3nQ3a_-FaUgKKoyGfQ-eQ4eFA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a3a9ad1dd189777b8d7fb8bbeb5f10bf020a258" title="AI Development Made Simple for Web Developers!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Section4169"&gt; /u/No-Section4169 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wrtnlabs.io/agentica/articles/AI-development-made-simple-for-web-developers.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmhojo/ai_development_made_simple_for_web_developers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmhojo/ai_development_made_simple_for_web_developers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T08:17:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmaii0</id>
    <title>Could Google's search engine supercharge RAG?</title>
    <updated>2025-03-29T00:52:02+00:00</updated>
    <author>
      <name>/u/Nervous-Positive-431</name>
      <uri>https://old.reddit.com/user/Nervous-Positive-431</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wouldn't whatever Google uses for their search engine blow any current RAG implementations?&lt;/p&gt; &lt;p&gt;I tied both of the keyword-based (BM25) and vector-based search routes, and none of them delivered the most relevant top chunks (BM25 did good when always selecting the top 40 chunks, as for vector search, it did not do any good, not even within top 150 chunks)!&lt;/p&gt; &lt;p&gt;So, I thought maybe Google can provide a service where we can upload our documents or chunks; and let whatever magic they have to fetch the most relevant chunk/document to pass as a context to the LLM!&lt;/p&gt; &lt;p&gt;I am sure someone perfected the best semantic/lexical recipe combination, but I keep getting futile results. The problem also lays with the fact that I am dealing with legal documents, coupled with the fact that most embeddings are not well optimized for the language I am using for the said legal documents. &lt;/p&gt; &lt;p&gt;But I believe RAG's whole point is retrieving the most relevant documents/chunks. If anyone would pioneer and excel in said area, it would be Google, not?&lt;/p&gt; &lt;p&gt;I am also familiar with KAG, but a lot criticized it for being too slow and burns relatively high amounts of tokens. Then there is CAG, which tries to take advantage of the whole context window; not const-effective. And the traditional RAG, which did not perform any good.&lt;/p&gt; &lt;p&gt;Curious about your thoughts about the matter and whether or not have managed to pull a successful pipeline!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nervous-Positive-431"&gt; /u/Nervous-Positive-431 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmaii0/could_googles_search_engine_supercharge_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmaii0/could_googles_search_engine_supercharge_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmaii0/could_googles_search_engine_supercharge_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T00:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm8wzt</id>
    <title>GitHub - lenankamp/AITextADV - Text Adventure Front End for LLM/SDAPI</title>
    <updated>2025-03-28T23:33:37+00:00</updated>
    <author>
      <name>/u/lenankamp</name>
      <uri>https://old.reddit.com/user/lenankamp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm8wzt/github_lenankampaitextadv_text_adventure_front/"&gt; &lt;img alt="GitHub - lenankamp/AITextADV - Text Adventure Front End for LLM/SDAPI" src="https://external-preview.redd.it/zIbfCUEiFlYwNlzn58xawBE0HIq802E-Tbon0tW8nuk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8dbb694c7296ff2ae5c7c5ef0bc71ba9409d9161" title="GitHub - lenankamp/AITextADV - Text Adventure Front End for LLM/SDAPI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lenankamp"&gt; /u/lenankamp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/lenankamp/AITextADV"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm8wzt/github_lenankampaitextadv_text_adventure_front/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm8wzt/github_lenankampaitextadv_text_adventure_front/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T23:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmf16i</id>
    <title>Are there reliable DeepSeek V3 API providers?</title>
    <updated>2025-03-29T05:04:54+00:00</updated>
    <author>
      <name>/u/kappaappa</name>
      <uri>https://old.reddit.com/user/kappaappa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently the official DeepSeek v3 api has really bad reliability, so I looked on openrouter for alternatives - when I tried fireworks / nebius they performed noticeably worse (than the official API) on our internal evals across several runs (even though they claim to use an un-quantized model).&lt;/p&gt; &lt;p&gt;I used the same temperature, top-p etc. These tests were run on the old v3 (not the recent 0324 model since those aren’t out yet across all providers).&lt;/p&gt; &lt;p&gt;It could be there are some settings or system prompts that each provider injects that I don’t know about which leads to the discrepancy though. Has anybody run into the same issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kappaappa"&gt; /u/kappaappa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmf16i/are_there_reliable_deepseek_v3_api_providers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmf16i/are_there_reliable_deepseek_v3_api_providers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmf16i/are_there_reliable_deepseek_v3_api_providers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T05:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm0mui</id>
    <title>reddacted v0.2 - put your local llm to work cleaning up your reddit history</title>
    <updated>2025-03-28T17:32:51+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm0mui/reddacted_v02_put_your_local_llm_to_work_cleaning/"&gt; &lt;img alt="reddacted v0.2 - put your local llm to work cleaning up your reddit history" src="https://external-preview.redd.it/Y2w4ZGdzamN1Z3JlMVHhkJTjGeoJ9CRgVjEh_zDclb6MyQghkK1Bc0VYi3qQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df126dd57a0eaf75c3cd491d9219874f788251d6" title="reddacted v0.2 - put your local llm to work cleaning up your reddit history" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9r4fisjcugre1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm0mui/reddacted_v02_put_your_local_llm_to_work_cleaning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm0mui/reddacted_v02_put_your_local_llm_to_work_cleaning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T17:32:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlsi6h</id>
    <title>nsfw orpheus tts - update</title>
    <updated>2025-03-28T11:04:52+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ok since the last post captured quite a bit of interest&lt;/p&gt; &lt;p&gt;Overall Total Duration: 31624380.29850002 seconds&lt;br /&gt; Overall Total Duration: 8784.55 hours &lt;/p&gt; &lt;p&gt;Total audio events found: 1317991&lt;/p&gt; &lt;p&gt;that's where we are - i think i can cut it short to 10-15k hours and then we should have something interesting . sadly 95% only female for the time being.&lt;/p&gt; &lt;p&gt;i should have enough high quality data in about a week to push a first finetune and then release it oss-nc&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jhgpew/nsfw_orpheus_tts/"&gt;old reddit post as ref&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlsi6h/nsfw_orpheus_tts_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlsi6h/nsfw_orpheus_tts_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlsi6h/nsfw_orpheus_tts_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T11:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmgkxh</id>
    <title>Broke down some of the design principles we think about when building agents!</title>
    <updated>2025-03-29T06:53:48+00:00</updated>
    <author>
      <name>/u/ComfortableArm121</name>
      <uri>https://old.reddit.com/user/ComfortableArm121</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been thinking a lot about needing formal, structured methods to accurately define the crucial semantics (meaning, logic, behavior) of complex AI systems.&lt;/p&gt; &lt;p&gt;Wrote about some of these principles &lt;a href="https://theaiworld.substack.com/p/the-thoughts-we-have-when-bringing"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Workflow Design (Patterns like RAG, Agents)&lt;/li&gt; &lt;li&gt;Connecting to the World (Utilities &amp;amp; Tools)&lt;/li&gt; &lt;li&gt;Managing State &amp;amp; Data Flow&lt;/li&gt; &lt;li&gt;Robust Execution (Retries, Fallbacks)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComfortableArm121"&gt; /u/ComfortableArm121 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgkxh/broke_down_some_of_the_design_principles_we_think/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgkxh/broke_down_some_of_the_design_principles_we_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgkxh/broke_down_some_of_the_design_principles_we_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T06:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmg7td</id>
    <title>Best models to run with 8GB VRAM, 16GB RAM</title>
    <updated>2025-03-29T06:27:04+00:00</updated>
    <author>
      <name>/u/Qxz3</name>
      <uri>https://old.reddit.com/user/Qxz3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been experimenting with local LLMs on my gaming laptop (RTX 4070 8GB, 16GB of RAM). My use cases have been coding and creative writing. Models that work well and that I like:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gemma 3 12B&lt;/strong&gt; - low quantization (IQ3_XS), 100% offloaded to GPU, spilling into RAM. ~10t/s. Great at following instructions and general knowledge. &lt;em&gt;This is the sweet spot and my main model&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Gemma 3 4B&lt;/strong&gt; - full quantization (Q8), 100% offloaded to GPU, minimal spill. ~30-40t/s. Still smart and competent but more limited knowledge. &lt;em&gt;This is an amazing model at this performance level&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MN GRAND Gutenburg Lyra4 Lyra 23.5B,&lt;/strong&gt; medium quant (Q4) (lower quants are just too wonky) about 50% offloaded to GPU, 2-3t/s. When quality of prose and writing a captivating story matters. Tends to break down so needs some supervision, but it's in another league entirely - Gemma 3 just cannot write like this &lt;em&gt;whatsoever&lt;/em&gt; (although Gemma follows instructions more closely). Great companion for creative writing. &lt;strong&gt;12B version&lt;/strong&gt; of this is way faster (100% GPU, 15t/s) and still strong stylistically, although its stories aren't nearly as engaging so I tend to be patient and wait for the 23.5B.&lt;/p&gt; &lt;p&gt;I was disappointed with:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Llama 3.1 8B&lt;/strong&gt; - runs fast, but responses are short, superficial and uninteresting compared with Gemma 3 4B.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral Small 3.1&lt;/strong&gt; - Can barely run on my machine, and for the extreme slowness, wasn't impressed with the responses. Would rather run &lt;strong&gt;Gemma 3 27B&lt;/strong&gt; instead.&lt;/p&gt; &lt;p&gt;I wish I could run:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;QWQ 32B&lt;/strong&gt; - doesn't do well at the lower quants that would allow it to run on my system, just too slow.&lt;br /&gt; &lt;strong&gt;Gemma 3 27B&lt;/strong&gt; - it runs but the jump in quality compared to &lt;strong&gt;12B&lt;/strong&gt; hasn't been worth going down to 2t/s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qxz3"&gt; /u/Qxz3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmg7td/best_models_to_run_with_8gb_vram_16gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmg7td/best_models_to_run_with_8gb_vram_16gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmg7td/best_models_to_run_with_8gb_vram_16gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T06:27:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmh2wj</id>
    <title>Does anyone know about the model code name: 'Spider' in LM arena??</title>
    <updated>2025-03-29T07:30:55+00:00</updated>
    <author>
      <name>/u/Harsh2588</name>
      <uri>https://old.reddit.com/user/Harsh2588</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spider model is somewhat more human-like and its answers are quite different compared to other LLM. It so far told me that it is a GPT-4 model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Harsh2588"&gt; /u/Harsh2588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmh2wj/does_anyone_know_about_the_model_code_name_spider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmh2wj/does_anyone_know_about_the_model_code_name_spider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmh2wj/does_anyone_know_about_the_model_code_name_spider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T07:30:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmgj6z</id>
    <title>is there a future for diffusion language models ?</title>
    <updated>2025-03-29T06:50:17+00:00</updated>
    <author>
      <name>/u/superNova-best</name>
      <uri>https://old.reddit.com/user/superNova-best</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;there's this new shiny type of models that are diffusion based not autoregressive, said to be faster cheaper and better, i've seen one called mercury by inception labs, what you think guys about those ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superNova-best"&gt; /u/superNova-best &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgj6z/is_there_a_future_for_diffusion_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgj6z/is_there_a_future_for_diffusion_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgj6z/is_there_a_future_for_diffusion_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T06:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm36yg</id>
    <title>CXL: Slot RAM into your PCIE slot, great for running Deepseek on your CPU</title>
    <updated>2025-03-28T19:20:28+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm36yg/cxl_slot_ram_into_your_pcie_slot_great_for/"&gt; &lt;img alt="CXL: Slot RAM into your PCIE slot, great for running Deepseek on your CPU" src="https://external-preview.redd.it/57C9ha6egNvnO5W3Odr1a-BMcv_qWXnF4sDFPfNBsCc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5550e16a355dc853fe379c1f46ba3e7e29f64aa" title="CXL: Slot RAM into your PCIE slot, great for running Deepseek on your CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=W5X8MEZVqzM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm36yg/cxl_slot_ram_into_your_pcie_slot_great_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm36yg/cxl_slot_ram_into_your_pcie_slot_great_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T19:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmfksn</id>
    <title>Cline with gemini-2.5-pro-exp-03-25, Not yet missed Claude after 30 min usage</title>
    <updated>2025-03-29T05:41:35+00:00</updated>
    <author>
      <name>/u/prabhic</name>
      <uri>https://old.reddit.com/user/prabhic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After reading multiple reviews wanted to give a try. Finally,&lt;/p&gt; &lt;p&gt;I see with model from google, gemini-2.5-pro-exp-03-25 , I able to generate useful code. and continue to use. Not sure how long it is free. So far with Previous models after a quick try I remember Claude. with this after 30 min of code generation, have not yet missed Claude. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prabhic"&gt; /u/prabhic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmfksn/cline_with_gemini25proexp0325_not_yet_missed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmfksn/cline_with_gemini25proexp0325_not_yet_missed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmfksn/cline_with_gemini25proexp0325_not_yet_missed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T05:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlw5hb</id>
    <title>New TTS model from bytedance</title>
    <updated>2025-03-28T14:18:47+00:00</updated>
    <author>
      <name>/u/bio_risk</name>
      <uri>https://old.reddit.com/user/bio_risk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlw5hb/new_tts_model_from_bytedance/"&gt; &lt;img alt="New TTS model from bytedance" src="https://external-preview.redd.it/uKiEmAEQx3Gdvnl2sNzEW0QEbrYYFLxWFMibNhAEifw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=813b2b376a8aba99e7464ee633d5a6f6d97c2749" title="New TTS model from bytedance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bio_risk"&gt; /u/bio_risk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/bytedance/MegaTTS3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlw5hb/new_tts_model_from_bytedance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlw5hb/new_tts_model_from_bytedance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T14:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlptqu</id>
    <title>Reverse engineering GPT-4o image gen via Network tab - here's what I found</title>
    <updated>2025-03-28T07:46:37+00:00</updated>
    <author>
      <name>/u/seicaratteri</name>
      <uri>https://old.reddit.com/user/seicaratteri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt; &lt;img alt="Reverse engineering GPT-4o image gen via Network tab - here's what I found" src="https://b.thumbs.redditmedia.com/6ICxPu6L9VzJwgZpZB11Ryf2Jzo9ZxiBnaamsutC34E.jpg" title="Reverse engineering GPT-4o image gen via Network tab - here's what I found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am very intrigued about this new model; I have been working in the image generation space a lot, and I want to understand what's going on&lt;/p&gt; &lt;p&gt;I found interesting details when opening the network tab to see what the BE was sending - here's what I found. I tried with few different prompts, let's take this as a starter:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;An image of happy dog running on the street, studio ghibli style&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here I got four intermediate images, as follows:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6q6f9b9naere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=586bd8d4f0cdb7b03c76492891bec5df0c0dbea9"&gt;https://preview.redd.it/6q6f9b9naere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=586bd8d4f0cdb7b03c76492891bec5df0c0dbea9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We can see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The BE is actually returning the image as we see it in the UI&lt;/li&gt; &lt;li&gt;It's not really clear wether the generation is autoregressive or not - we see &lt;em&gt;some&lt;/em&gt; details and a faint global structure of the image, this could mean two things: &lt;ul&gt; &lt;li&gt;Like usual diffusion processes, we first generate the global structure and then add details&lt;/li&gt; &lt;li&gt;OR - The image is &lt;em&gt;actually&lt;/em&gt; generated autoregressively&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If we analyze the 100% zoom of the first and last frame, we can see details are being added to high frequency textures like the trees&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vxdt6m8oaere1.png?width=2608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00b4f056ed1d4b6e363146438b951e59e2279965"&gt;https://preview.redd.it/vxdt6m8oaere1.png?width=2608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00b4f056ed1d4b6e363146438b951e59e2279965&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what we would typically expect from a diffusion model. This is further accentuated in this other example, where I prompted specifically for a high frequency detail texture (&amp;quot;create the image of a grainy texture, abstract shape, very extremely highly detailed&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4sd80u4paere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29a87f794c0041801bc825e32cebcbcbed8a3ddf"&gt;https://preview.redd.it/4sd80u4paere1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29a87f794c0041801bc825e32cebcbcbed8a3ddf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, I got only three images here from the BE; and the details being added is obvious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nuoeccupaere1.png?width=2058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c0ffd1869939b6a7cc24167cd69ad7bd94ad728"&gt;https://preview.redd.it/nuoeccupaere1.png?width=2058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c0ffd1869939b6a7cc24167cd69ad7bd94ad728&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This could be done of course as a separate post processing step too, for example like SDXL introduced the refiner model back in the days that was specifically trained to add details to the VAE latent representation before decoding it to pixel space.&lt;/p&gt; &lt;p&gt;It's also unclear if I got less images with this prompt due to availability (i.e. the BE could give me more flops), or to some kind of specific optimization (eg: latent caching).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So where I am at now:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's probably a multi step process pipeline&lt;/li&gt; &lt;li&gt;OpenAI in the model card is stating that &amp;quot;Unlike DALL·E, which operates as a diffusion model, 4o image generation is an autoregressive model natively embedded within ChatGPT&amp;quot;&lt;/li&gt; &lt;li&gt;This makes me think of this recent paper: &lt;a href="https://arxiv.org/pdf/2409.11340"&gt;OmniGen&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There they directly connect the VAE of a Latent Diffusion architecture to an LLM and learn to model jointly both text and images; they observe few shot capabilities and emerging properties too which would explain the vast capabilities of GPT4-o, and it makes even more sense if we consider the usual OAI formula:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;More / higher quality data&lt;/li&gt; &lt;li&gt;More flops&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The architecture proposed in OmniGen has &lt;em&gt;great&lt;/em&gt; potential to scale given that is purely transformer based - and if we know one thing is surely that transformers scale well, and that OAI is especially good at that&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What do you think?&lt;/strong&gt; would love to take this as a space to investigate together! Thanks for reading and let's get to the bottom of this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seicaratteri"&gt; /u/seicaratteri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T07:46:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmgum6</id>
    <title>Test results of gemini 2.5 pro exp on ARC AGI 2</title>
    <updated>2025-03-29T07:13:29+00:00</updated>
    <author>
      <name>/u/flysnowbigbig</name>
      <uri>https://old.reddit.com/user/flysnowbigbig</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgum6/test_results_of_gemini_25_pro_exp_on_arc_agi_2/"&gt; &lt;img alt="Test results of gemini 2.5 pro exp on ARC AGI 2" src="https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12fe1cebb1de52a5010be00250423f821bd40d0a" title="Test results of gemini 2.5 pro exp on ARC AGI 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source:&lt;a href="https://arcprize.org/leaderboard"&gt;https://arcprize.org/leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When it was first launched, I used my own tests to determine that its &lt;strong&gt;generalization reasoning was significantly weaker than that of O3 mini high.&lt;/strong&gt; It seems that ARC AGI is still a things.&lt;/p&gt; &lt;p&gt;Livebench Publicly accessible reasoning problem stays at 2024-10-22&lt;/p&gt; &lt;p&gt;I don't know what they use now&lt;/p&gt; &lt;p&gt;Assuming it still uses the same type of zebra reasoning, web of lies, but just changes the name, number, and other parameters? Then it is easy to target training, so it may not be so reliable anymore&lt;/p&gt; &lt;p&gt;Of all the models Provider, Sam seems to be the only one who is reluctant to provide detailed COT. It seems that there is a reason for this.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ppg2vyxn2lre1.png?width=1069&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e8904afd4bd1d45d63eec92481c25b33a2fc70d"&gt;https://preview.redd.it/ppg2vyxn2lre1.png?width=1069&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e8904afd4bd1d45d63eec92481c25b33a2fc70d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/flysnowbigbig"&gt; /u/flysnowbigbig &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgum6/test_results_of_gemini_25_pro_exp_on_arc_agi_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgum6/test_results_of_gemini_25_pro_exp_on_arc_agi_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgum6/test_results_of_gemini_25_pro_exp_on_arc_agi_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T07:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmgzoj</id>
    <title>Local LLM test M3 Ultra vs RTX 5090</title>
    <updated>2025-03-29T07:24:11+00:00</updated>
    <author>
      <name>/u/Careless_Garlic1438</name>
      <uri>https://old.reddit.com/user/Careless_Garlic1438</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think some of us have been waiting for this&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=nwIZ5VI3Eus"&gt;https://www.youtube.com/watch?v=nwIZ5VI3Eus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Careless_Garlic1438"&gt; /u/Careless_Garlic1438 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgzoj/local_llm_test_m3_ultra_vs_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgzoj/local_llm_test_m3_ultra_vs_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmgzoj/local_llm_test_m3_ultra_vs_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T07:24:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmcdgy</id>
    <title>[Proprietary Model] I "Vibe Coded" An ML model From Scratch Without Any Solid Experience, Gemini-2.5</title>
    <updated>2025-03-29T02:28:43+00:00</updated>
    <author>
      <name>/u/Few_Ask683</name>
      <uri>https://old.reddit.com/user/Few_Ask683</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmcdgy/proprietary_model_i_vibe_coded_an_ml_model_from/"&gt; &lt;img alt="[Proprietary Model] I &amp;quot;Vibe Coded&amp;quot; An ML model From Scratch Without Any Solid Experience, Gemini-2.5" src="https://b.thumbs.redditmedia.com/YDbfH5XQRfduI0VuE3UVLcuur80RphfxKjdAIanrr-A.jpg" title="[Proprietary Model] I &amp;quot;Vibe Coded&amp;quot; An ML model From Scratch Without Any Solid Experience, Gemini-2.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using the model via Google Studio for a while and I just can't wrap my head around it. I said fuck it, why not push it further, but in a meaningful way. I don't expect it to write Crysis from scratch or spell out the R's in the word STRAWBERRY, but I wonder, what's the limit of pure prompting here?&lt;/p&gt; &lt;p&gt;This was my third rendition of a sloppily engineered prompt after a couple of successful but underperforming results:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/urp4vl2lfjre1.png?width=1256&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97d211afbe14b3f3b40c124665d433dc0b4e30a"&gt;The generated code worked first try.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Then, I wanted to improve the logic:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u0l1334ufjre1.png?width=1241&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a1a827c48ba2ed5dc9fc06b281ad41485f61364"&gt;It gave a single error due to huber loss implementation, which was solved by adding a single line of code.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is way too long to share as a screenshot, sorry. But don't worry, I will give you a pastebin link.&lt;/p&gt; &lt;p&gt;At this point I wondered, are we trying to train a model without any meaningful input? Because I did not necessarily specify a certain workflow or method. Just average geek person words.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lhwmovg4gjre1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3fd7d45b2b687e8ac14cb356081a4e6ad08fd800"&gt;It in fact is not random, according to Gemini.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now, the model uses pygame to run the simulation, but it's annoying to run pygame on colab, in a cell. So, it saves the best results as a video. There is no way it just works, right?&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jmcdgy/video/0et9mjq1hjre1/player"&gt;Epoch 3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here is the Epoch 23!!!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1jmcdgy/video/hzl0gofahjre1/player"&gt;https://reddit.com/link/1jmcdgy/video/hzl0gofahjre1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;## Final Thoughts&lt;/p&gt; &lt;p&gt;Please use as much as free Gemini possible and save the outputs. We can create a state of the art dataset together. The pastebin link is in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Ask683"&gt; /u/Few_Ask683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmcdgy/proprietary_model_i_vibe_coded_an_ml_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmcdgy/proprietary_model_i_vibe_coded_an_ml_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmcdgy/proprietary_model_i_vibe_coded_an_ml_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T02:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jmaauq</id>
    <title>QwenPhi-4-0.5b-Draft</title>
    <updated>2025-03-29T00:41:28+00:00</updated>
    <author>
      <name>/u/das_rdsm</name>
      <uri>https://old.reddit.com/user/das_rdsm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmaauq/qwenphi405bdraft/"&gt; &lt;img alt="QwenPhi-4-0.5b-Draft" src="https://external-preview.redd.it/st6SxzZLy5bw__kD-hkAlJzI_Tjlnzg5MffUlguivIA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70d9159e9a9e077f42f3fd0ee897f79ccf57e374" title="QwenPhi-4-0.5b-Draft" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, inspired on the recently shared here Mistral Small Draft model, I used the same technique to make this &lt;strong&gt;draft model for the Phi 4 model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I also made a MLX 8bit version available of this model. &lt;/p&gt; &lt;p&gt;On my local lmstudio it caused Phi 4 - 4 bit Token generation to increase from 10tk/s to 20tk/s (MLX , mac m4 , low context , coding task)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/das_rdsm"&gt; /u/das_rdsm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/rdsm/QwenPhi-4-0.5b-Draft"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jmaauq/qwenphi405bdraft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jmaauq/qwenphi405bdraft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T00:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm9l6q</id>
    <title>New release of EQ-Bench creative writing leaderboard w/ new prompts, more headroom, &amp; cozy sample reader</title>
    <updated>2025-03-29T00:05:35+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm9l6q/new_release_of_eqbench_creative_writing/"&gt; &lt;img alt="New release of EQ-Bench creative writing leaderboard w/ new prompts, more headroom, &amp;amp; cozy sample reader" src="https://b.thumbs.redditmedia.com/1wR0A4z7POWQrz_UIslETNDq8PPe15red_LwAnuSiTU.jpg" title="New release of EQ-Bench creative writing leaderboard w/ new prompts, more headroom, &amp;amp; cozy sample reader" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Find the leaderboard here: &lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A nice long writeup: &lt;a href="https://eqbench.com/about.html#creative-writing-v3"&gt;https://eqbench.com/about.html#creative-writing-v3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source code: &lt;a href="https://github.com/EQ-bench/creative-writing-bench"&gt;https://github.com/EQ-bench/creative-writing-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1jm9l6q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm9l6q/new_release_of_eqbench_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm9l6q/new_release_of_eqbench_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-29T00:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1jm4agx</id>
    <title>Qwen-2.5-72b is now the best open source OCR model</title>
    <updated>2025-03-28T20:07:08+00:00</updated>
    <author>
      <name>/u/Tylernator</name>
      <uri>https://old.reddit.com/user/Tylernator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This has been a big week for open source LLMs. In the last few days we got:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 2.5 VL (72b and 32b)&lt;/li&gt; &lt;li&gt;Gemma-3 (27b)&lt;/li&gt; &lt;li&gt;DeepSeek-v3-0324&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And a couple weeks ago we got the new mistral-ocr model. We updated our OCR benchmark to include the new models.&lt;/p&gt; &lt;p&gt;We evaluated 1,000 documents for JSON extraction accuracy. Major takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen 2.5 VL (72b and 32b) are by far the most impressive. Both landed right around 75% accuracy (equivalent to GPT-4o’s performance). Qwen 72b was only 0.4% above 32b. Within the margin of error.&lt;/li&gt; &lt;li&gt;Both Qwen models passed mistral-ocr (72.2%), which is specifically trained for OCR.&lt;/li&gt; &lt;li&gt;Gemma-3 (27B) only scored 42.9%. Particularly surprising given that it's architecture is based on Gemini 2.0 which still tops the accuracy chart.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The data set and benchmark runner is fully open source. You can check out the code and reproduction steps here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/getomni-ai/benchmark"&gt;https://github.com/getomni-ai/benchmark&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/getomni-ai/ocr-benchmark"&gt;https://huggingface.co/datasets/getomni-ai/ocr-benchmark&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tylernator"&gt; /u/Tylernator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://getomni.ai/blog/benchmarking-open-source-models-for-ocr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jm4agx/qwen2572b_is_now_the_best_open_source_ocr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jm4agx/qwen2572b_is_now_the_best_open_source_ocr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T20:07:08+00:00</published>
  </entry>
</feed>
