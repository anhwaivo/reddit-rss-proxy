<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-20T19:48:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mv01ls</id>
    <title>Daily driving GLM 4.5 for 10 days, kinda insane how good it is at half the size of other frontier models</title>
    <updated>2025-08-20T00:27:19+00:00</updated>
    <author>
      <name>/u/susmitds</name>
      <uri>https://old.reddit.com/user/susmitds</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been running GLM 4.5 (355B) locally for about 10 days, and it’s basically replaced my old setup. I used to juggle GPT-4o/4.1 for general tasks and o3 for heavier reasoning, but after GPT-5 struggled with long research paper convos I moved to GLM 4.5, and it covers both use cases in one.&lt;/p&gt; &lt;p&gt;Using Unsloth’s GGUF builds in llama.cpp: ud-iq2_m for 128k context (K/V cache q8) and ud-q2_k_xl when 30k okay. What’s impressive is how well it holds up under 2-bit quant + cache quant. I expected obvious degradation, but it’s solid. Non-thinking mode feels like GPT-4o in fluency, and thinking mode sits between o4-mini-high and o3—better than most open weights I’ve tried at this scale.&lt;/p&gt; &lt;p&gt;Benchmarks aside, real usage matches the claims. I’ve thrown workloads at it I’d normally reserve for GPT-4.1 or o3, and it keeps up or beats them, without the usual quantization artifacts. Knowledge depth and overall competence are ahead of GPT-4.1/4o in many areas, and it’s one of the most stock, uncensored releases I’ve seen.&lt;/p&gt; &lt;p&gt;Running on RTX 6000 Ada + 6000 Pro Blackwell with a llama.cpp fork for GLM 4.5 tool support. Frontend is Open WebUI with some filter functions to toggle thinking on/off, plus a fixed chat template so /nothink only applies to the last message (avoids re-prefill issues).&lt;/p&gt; &lt;p&gt;First time I’ve daily-driven a local model this large and felt it genuinely outclass closed ones.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/susmitds"&gt; /u/susmitds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv01ls/daily_driving_glm_45_for_10_days_kinda_insane_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv01ls/daily_driving_glm_45_for_10_days_kinda_insane_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv01ls/daily_driving_glm_45_for_10_days_kinda_insane_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T00:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvnetu</id>
    <title>New Trainable Sparsity Method I've been working on!</title>
    <updated>2025-08-20T18:41:56+00:00</updated>
    <author>
      <name>/u/nano-tech-warrior</name>
      <uri>https://old.reddit.com/user/nano-tech-warrior</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnetu/new_trainable_sparsity_method_ive_been_working_on/"&gt; &lt;img alt="New Trainable Sparsity Method I've been working on!" src="https://preview.redd.it/mpxhgfb1y7kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1be78ec5f54af38b4819955caf62751a49149e6" title="New Trainable Sparsity Method I've been working on!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing CWIC a trainable sparsity paradigm that beats SOTA methods, enabling 80% sparsity and 4x+ speedups on CPU.&lt;/p&gt; &lt;p&gt;Something I've been working on with friends at &lt;a href="http://crystalai.org"&gt;crystalai.org&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;It works on models as small as 1b, outperforming TEAL R-sparse and friends.&lt;br /&gt; We are releasing code at &lt;a href="https://github.com/crystal-ai-org/cwic"&gt;https://github.com/crystal-ai-org/cwic&lt;/a&gt;&lt;br /&gt; read more at the blog &lt;a href="https://crystalai.org/blog/2025-08-18-compute-where-it-counts"&gt;https://crystalai.org/blog/2025-08-18-compute-where-it-counts&lt;/a&gt;&lt;br /&gt; if your interested in our our work feel free to reach out at &lt;a href="https://x.com/crystalAIorg"&gt;https://x.com/crystalAIorg&lt;/a&gt;, we love collaboration!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nano-tech-warrior"&gt; /u/nano-tech-warrior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mpxhgfb1y7kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnetu/new_trainable_sparsity_method_ive_been_working_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnetu/new_trainable_sparsity_method_ive_been_working_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv4et3</id>
    <title>DeepSeek V3.1 BASE Q4_K_M available</title>
    <updated>2025-08-20T03:53:13+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm making imatrix calculations from Q4_K_M so figured might as well upload it in the meantime for anyone who wants to use it&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-GGUF"&gt;https://huggingface.co/bartowski/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As noted in the model card, it's good to keep in mind this is a &lt;em&gt;BASE&lt;/em&gt; model&lt;/p&gt; &lt;p&gt;Typically to use base models for general conversation, you want to feed it a couple of turns to teach if what a conversation looks like&lt;/p&gt; &lt;p&gt;I simply gave it a system message and a couple turns of each user/assistant and it seemed capable:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;./llama-cli -m /models/deepseek-ai_DeepSeek-V3.1-Base-Q4_K_M-00001-of-00011.gguf -p &amp;quot;You are a helpful assistant.&amp;lt;User&amp;gt;Hello, who are you?&amp;lt;Assistant&amp;gt;I am DeepSeek, a helpful AI assistant.&amp;lt;User&amp;gt;How are you today?&amp;lt;Assistant&amp;gt;I'm doing well! Is there anything I can assist you with?&amp;lt;User&amp;gt;Can you explain the laws of thermodynamics?&amp;lt;Assistant&amp;gt;&amp;quot; -no-cnv -ngl 0 --reverse-prompt &amp;quot;&amp;lt;User&amp;gt;&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;blockquote&gt; &lt;p&gt;Sure, here's a brief explanation of the laws of thermodynamics: 1. Zeroth Law of Thermodynamics: If two thermodynamic systems are each in thermal equilibrium with a third system, then they are in thermal equilibrium with each other. 2. First Law of Thermodynamics: The total energy of an isolated system is constant; energy can be transformed from one form to another, but cannot be created or destroyed. 3. Second Law of Thermodynamics: The entropy of an isolated system not in equilibrium will tend to increase over time, approaching a maximum value at equilibrium. 4. Third Law of Thermodynamics: As the temperature of a system approaches absolute zero, the entropy of the system approaches a minimum value. Would you like more details on any of these laws?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Yes, I am using &lt;code&gt;&amp;lt;User&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;Assistant&amp;gt;&lt;/code&gt; as opposed to the special tokens &lt;code&gt;&amp;lt;｜User｜&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;｜Assistant｜&amp;gt;&lt;/code&gt;, for some reason this seems to be more stable? Using the proper tokens tended to result in endless incoherent generation.&lt;/p&gt; &lt;p&gt;Can't comment on quality in any way, but figured someone would want to play early! Feel free to share multi-turn prompts that give good results, mine are likely far from ideal but at least they seem to work :)&lt;/p&gt; &lt;p&gt;384GB btw...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4et3/deepseek_v31_base_q4_k_m_available/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4et3/deepseek_v31_base_q4_k_m_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv4et3/deepseek_v31_base_q4_k_m_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T03:53:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvol0o</id>
    <title>Running Qwen3-Coder-30B-A3 Q4_LM in Cursor with Agent Mode unlocked</title>
    <updated>2025-08-20T19:24:29+00:00</updated>
    <author>
      <name>/u/ConfidentDinner6648</name>
      <uri>https://old.reddit.com/user/ConfidentDinner6648</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing ways to make Cursor usable without relying only on their default “auto” model (which honestly feels pretty bad). While experimenting, I noticed something interesting:&lt;/p&gt; &lt;p&gt;If you run a model locally and just register it under the name &lt;code&gt;gpt-4o&lt;/code&gt;, Cursor unlocks &lt;strong&gt;Agent Mode&lt;/strong&gt; (function calling, todo list, etc.) and everything works as if it were an official endpoint.&lt;/p&gt; &lt;p&gt;I tried this with &lt;strong&gt;Qwen3-Coder-30B-A3 Q4_LM&lt;/strong&gt; (through LM Studio + ngrok) and here’s what I got:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outperforms Gemini Flash and Gemini Pro on many coding tasks&lt;/li&gt; &lt;li&gt;In some cases, feels close to Sonnet 4 (which is wild for a quantized 30B)&lt;/li&gt; &lt;li&gt;Function calling works smoothly, no errors so far&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This obviously isn’t official support, but it shows that Cursor &lt;em&gt;could&lt;/em&gt; support local/self-hosted models natively without much issue.&lt;/p&gt; &lt;p&gt;Anyone else tried running Qwen3 (or others) inside Cursor like this? Curious to hear results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ConfidentDinner6648"&gt; /u/ConfidentDinner6648 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvol0o/running_qwen3coder30ba3_q4_lm_in_cursor_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvol0o/running_qwen3coder30ba3_q4_lm_in_cursor_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvol0o/running_qwen3coder30ba3_q4_lm_in_cursor_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T19:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1muxbqj</id>
    <title>Understanding DeepSeek-V3.1-Base Updates at a Glance</title>
    <updated>2025-08-19T22:32:24+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"&gt; &lt;img alt="Understanding DeepSeek-V3.1-Base Updates at a Glance" src="https://preview.redd.it/mqcnus8py1kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=489f12b61d95cf93fb3d2849e85440310d44f38d" title="Understanding DeepSeek-V3.1-Base Updates at a Glance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek officially released DeepSeek-V3.1-Base a few hours ago. The model card has not been uploaded yet, so performance data is not available. &lt;/p&gt; &lt;p&gt;I have directly reviewed the model's configuration files, tokenizer, and other data, and combined this with test data published by the community to create a summary for everyone. &lt;/p&gt; &lt;p&gt;This should give you a quick overview of what has been updated in DeepSeek-V3.1-Base. Please point out any errors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mqcnus8py1kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1muxbqj/understanding_deepseekv31base_updates_at_a_glance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T22:32:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvl1dg</id>
    <title>My open source AI activity tracker project</title>
    <updated>2025-08-20T17:17:46+00:00</updated>
    <author>
      <name>/u/adnan-kaya</name>
      <uri>https://old.reddit.com/user/adnan-kaya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I wanted to share my latest project. &lt;strong&gt;Bilge&lt;/strong&gt; is a wise activity tracker that runs completely on your machine. Instead of sending your data to a cloud server, it uses a local LLM to understand your digital habits and gently nudge you to take breaks.&lt;/p&gt; &lt;p&gt;It's a great example of what's possible with local AI, and I'd love to get your feedback on the project. It's still a work in progress, but I think it could be useful for some people who wants to work on similar project.&lt;/p&gt; &lt;p&gt;Feel free to check out the code, open an issue, or even make your first pull request. All contributions are welcome!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/adnankaya/bilge"&gt;https://github.com/adnankaya/bilge&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adnan-kaya"&gt; /u/adnan-kaya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl1dg/my_open_source_ai_activity_tracker_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl1dg/my_open_source_ai_activity_tracker_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl1dg/my_open_source_ai_activity_tracker_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T17:17:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mukl2a</id>
    <title>deepseek-ai/DeepSeek-V3.1-Base · Hugging Face</title>
    <updated>2025-08-19T14:49:14+00:00</updated>
    <author>
      <name>/u/xLionel775</name>
      <uri>https://old.reddit.com/user/xLionel775</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" src="https://external-preview.redd.it/TF0v-SFT5DAKs6neF39KH5oR_BZ__J6Srmsxz1t_P1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7432a9894d4ead34a34aab111e0acba5a8647c40" title="deepseek-ai/DeepSeek-V3.1-Base · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xLionel775"&gt; /u/xLionel775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mukl2a/deepseekaideepseekv31base_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T14:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvfuqn</id>
    <title>What other MOE models are you using?</title>
    <updated>2025-08-20T14:09:18+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for MOE models under 50B(Active upto 5B). Our laptop has 8GB VRAM &amp;amp; 32GB RAM.&lt;/p&gt; &lt;p&gt;I know that most of us do use Qwen MOE models(Qwen3-30B-A3B particularly). Mistral, recently GPT-OSS-20B. &lt;strong&gt;What else we have? Share your favorites. Recommend under appreciated/overlooked MOE models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It would be great to have MOE models under 20B since I have only 8GB VRAM so it could be faster on our laptop.&lt;/p&gt; &lt;p&gt;Use case : Content Creation, Writing, Learnings, Coding&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------------------------------------- &lt;/p&gt; &lt;p&gt;&lt;sup&gt;Though HuggingFace has an option to filter models MOE wise, unfortunately some MOE models don't carry MOE label(Ex: Qwen MOE models&lt;/sup&gt;.)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Below HuggingFace URL is for MOE models sorted by Downloads. Many models are missing because those don't carry MOE label.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/models?other=moe&amp;amp;sort=downloads"&gt;https://huggingface.co/models?other=moe&amp;amp;sort=downloads&lt;/a&gt;&lt;/p&gt; &lt;p&gt;--------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One question on picking quants&lt;/strong&gt; (I don't want to open another thread for this since it's related to MOE). I'm getting 15 t/s for Q4 of Qwen3-30B-A3B.&lt;/p&gt; &lt;p&gt;How much t/s will I get for other quants? If it's same t/s, I'll download Q6 or Q8. Otherwise I'll download suitable quant(Ex: Q5 or keeping Q4) depends on t/s. Downloading big double digit GB size files multiple times are too much for me here so want to ensure the quant before download.&lt;/p&gt; &lt;p&gt;Q4_K_XL - 17.7GB&lt;/p&gt; &lt;p&gt;Q5_K_XL - 21.7GB&lt;/p&gt; &lt;p&gt;Q6_K_XL - 26.3GB&lt;/p&gt; &lt;p&gt;Q8_K_XL - 36GB&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfuqn/what_other_moe_models_are_you_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfuqn/what_other_moe_models_are_you_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfuqn/what_other_moe_models_are_you_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T14:09:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv1c96</id>
    <title>PSA: before spending 5k€ on GPUs, you might want to test the models online first</title>
    <updated>2025-08-20T01:25:56+00:00</updated>
    <author>
      <name>/u/e79683074</name>
      <uri>https://old.reddit.com/user/e79683074</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can do so on &lt;a href="https://lmarena.ai/?mode=direct"&gt;https://lmarena.ai/?mode=direct&lt;/a&gt; or any other place you know. Local models have come a huge, long way since the first Llama appearances, and the amount of progress done is unbelievable.&lt;/p&gt; &lt;p&gt;However, don't expect to be able to unsub from Gemini\ChatGPT\Claude soon. Test them first, before you spend money on hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/e79683074"&gt; /u/e79683074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1c96/psa_before_spending_5k_on_gpus_you_might_want_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1c96/psa_before_spending_5k_on_gpus_you_might_want_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv1c96/psa_before_spending_5k_on_gpus_you_might_want_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T01:25:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv7zdl</id>
    <title>Deepseek V3.1 is bad at creative writing, way worse than 0324</title>
    <updated>2025-08-20T07:19:09+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've tried 3.1 on chat.deepseek.com, and boy it is very very bad at conversation and creative writing; it does not understand prompt nuances V3 0324 does, it has very high slop cliche output, and generally feels like switch from Mistral Small 2409 to 2501. &lt;/p&gt; &lt;p&gt;Let me know your impression.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7zdl/deepseek_v31_is_bad_at_creative_writing_way_worse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7zdl/deepseek_v31_is_bad_at_creative_writing_way_worse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7zdl/deepseek_v31_is_bad_at_creative_writing_way_worse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T07:19:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6wwe</id>
    <title>nvidia/parakeet-tdt-0.6b-v3 (now multilingual)</title>
    <updated>2025-08-20T06:14:15+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"&gt; &lt;img alt="nvidia/parakeet-tdt-0.6b-v3 (now multilingual)" src="https://external-preview.redd.it/12PzLvQjZXrvyzotsfsH7vxtU3vJRsRc5ZD3WiNviO0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3a6ac17d880f0cd5d00920d58cc8f2aa4530205" title="nvidia/parakeet-tdt-0.6b-v3 (now multilingual)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;parakeet-tdt-0.6b-v3 is a 600-million-parameter multilingual automatic speech recognition (ASR) model designed for high-throughput speech-to-text transcription. It extends the parakeet-tdt-0.6b-v2 model by expanding language support from English to 25 European languages. The model automatically detects the language of the audio and transcribes it without requiring additional prompting. It is part of a series of models that leverage the Granary [1, 2] multilingual corpus as their primary training dataset.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6wwe/nvidiaparakeettdt06bv3_now_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:14:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mviuzq</id>
    <title>Cluster of two AMD Strix Halo machines (HP Z2 Mini G1a)</title>
    <updated>2025-08-20T16:00:19+00:00</updated>
    <author>
      <name>/u/aquarat</name>
      <uri>https://old.reddit.com/user/aquarat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mviuzq/cluster_of_two_amd_strix_halo_machines_hp_z2_mini/"&gt; &lt;img alt="Cluster of two AMD Strix Halo machines (HP Z2 Mini G1a)" src="https://external-preview.redd.it/MaUSRoNLAJ1LyKjA9wGag9Te4pvp93JxB__k_YVVUBI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95a08347bd804d947ea5c5b82647272167dc6fca" title="Cluster of two AMD Strix Halo machines (HP Z2 Mini G1a)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd really like to get something decent running locally, like one of the Deepseek models. I figure this will need 600 GBs of VRAM to run comfortably with one of the Unsloth models. Buying this amount of VRAM via Nvidia GPUs isn't workable for me, but the AMD Strix Halo 395+ machines should make this possible, eventually 😅.&lt;/p&gt; &lt;p&gt;An option is the Framework Desktop, but about 2 months back HP ran a very short-lived special on their HP Z2 Mini G1a Strix machines... so I bought two. I found the ROCM system/libs to be pretty unusable but things are improving.&lt;/p&gt; &lt;p&gt;One machine runs Ubuntu (kernel 6.14.0-27) and another runs Fedora Rawhide (FR is really nice...) (kernel 6.17.0-0.rc0.250808g37816488247d.14.fc43.x86_64)&lt;/p&gt; &lt;p&gt;I recently found this repository via a post on Framework's forum: &lt;a href="https://github.com/kyuz0/amd-strix-halo-toolboxes"&gt;https://github.com/kyuz0/amd-strix-halo-toolboxes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I modified the Dockerfile slightly to also build llama.cpp's RPC server and then tried out the result. To my surprise it worked with GPT OSS 120b (Unsloth Q4_K_XL variant).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/92p4bsxv17kf1.png?width=1710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ed952d17fbb988151abf8a2523ed8cf84759f15"&gt;https://preview.redd.it/92p4bsxv17kf1.png?width=1710&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ed952d17fbb988151abf8a2523ed8cf84759f15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some caveats: one machine is running Ubuntu, another is running Fedora Rawhide with a bleeding edge kernel. They're connected via 2.5 gigabit ethernet. &lt;code&gt;nvtop&lt;/code&gt; doesn't work very well on the Ubuntu machine, I assume because the kernel is old.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 5294.35 ms / 2491 tokens ( 2.13 ms per token, 470.50 tokens per second) eval time = 3586.47 ms / 109 tokens ( 32.90 ms per token, 30.39 tokens per second) total time = 8880.82 ms / 2600 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and a follow-up prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 5082.28 ms / 2313 tokens ( 2.20 ms per token, 455.11 tokens per second) eval time = 152837.44 ms / 4085 tokens ( 37.41 ms per token, 26.73 tokens per second) total time = 157919.71 ms / 6398 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm using a modified version of the &lt;code&gt;rocm-7rc-rocwmma&lt;/code&gt; docker image:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;RUN cmake -S . -B build \ -DGGML_HIP=ON \ -DAMDGPU_TARGETS=gfx1151 \ -DCMAKE_BUILD_TYPE=Release \ -DLLAMA_HIP_UMA=ON \ -DGGML_HIP_ROCWMMA_FATTN=ON \ -DGGML_RPC=ON \ &amp;amp;&amp;amp; cmake --build build --config Release -- -j$(nproc) \ &amp;amp;&amp;amp; cmake --install build --config Release RUN find build -type f -name 'libggm*.so' -exec cp -v {} /opt/rocm-7.0/lib/ \; RUN cp ./build/bin/rpc-server /usr/local/bin/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and I run it like so:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Build the image docker build -t &amp;lt;your account on docker hub&amp;gt;/llama-rocm-7rc-rocwmma:2508201616 -f Dockerfile.rocm-7rc-rocwmma --push . // secondary node / start first docker run --rm -ti --device /dev/dri --device /dev/kfd --group-add video --group-add render -p 50052:50052 --name llama --security-opt seccomp=unconfined docker.io/aquarat/llama-rocm-7rc-rocwmma:2508201616 rpc-server -H 0.0.0.0 // primary node docker run --rm -ti -v /home/user/models-llama:/models --name llama --device /dev/dri --device /dev/kfd --group-add video --group-add render -p 11434:11434 --security-opt seccomp=unconfined docker.io/aquarat/llama-rocm-7rc-rocwmma:2508201616 llama-server --no-mmap -ngl 999 -fa -m /models/gpt-oss/gpt-oss-120b-UD-Q4_K_XL-00001-of-00002.gguf --port 11434 --host 0.0.0.0 -c 100000 --jinja --reasoning-format auto --rpc 192.168.0.39:50052 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I thought this might be usable to someone 🤷‍♂️&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l6z8a6ak67kf1.png?width=631&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f13cb6fb46388ad264eb42481ad735465aeb751"&gt;https://preview.redd.it/l6z8a6ak67kf1.png?width=631&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f13cb6fb46388ad264eb42481ad735465aeb751&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aquarat"&gt; /u/aquarat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mviuzq/cluster_of_two_amd_strix_halo_machines_hp_z2_mini/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mviuzq/cluster_of_two_amd_strix_halo_machines_hp_z2_mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mviuzq/cluster_of_two_amd_strix_halo_machines_hp_z2_mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T16:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvjjxe</id>
    <title>guide : running gpt-oss with llama.cpp -ggerganov</title>
    <updated>2025-08-20T16:24:30+00:00</updated>
    <author>
      <name>/u/onwardforward</name>
      <uri>https://old.reddit.com/user/onwardforward</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjjxe/guide_running_gptoss_with_llamacpp_ggerganov/"&gt; &lt;img alt="guide : running gpt-oss with llama.cpp -ggerganov" src="https://external-preview.redd.it/0MtZInNIGRZV6H6dIhZ8EGkGtej94yvJcqiqENXoH1U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e1421e8cea5aa0f8f2819555a163e1481ff84d0" title="guide : running gpt-oss with llama.cpp -ggerganov" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onwardforward"&gt; /u/onwardforward &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15396"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjjxe/guide_running_gptoss_with_llamacpp_ggerganov/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjjxe/guide_running_gptoss_with_llamacpp_ggerganov/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T16:24:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvo9pq</id>
    <title>My open-source project on building production-level AI agents just hit 10K stars on GitHub</title>
    <updated>2025-08-20T19:13:02+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My Agents-Towards-Production GitHub repository just crossed 10,000 stars in only two months! Here's what's inside:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; 33 detailed tutorials on building the components needed for production-level agents &lt;/li&gt; &lt;li&gt; Tutorials organized by category &lt;/li&gt; &lt;li&gt; Clear, high-quality explanations with diagrams and step-by-step code implementations &lt;/li&gt; &lt;li&gt; New tutorials are added regularly &lt;/li&gt; &lt;li&gt; I'll keep sharing updates about these tutorials here A huge thank you to all contributors who made this possible!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;the link is in the first comment&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9pq/my_opensource_project_on_building_productionlevel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9pq/my_opensource_project_on_building_productionlevel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvo9pq/my_opensource_project_on_building_productionlevel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T19:13:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mve5hp</id>
    <title>Datarus-R1-14B-Preview, an adaptive multi-step reasoning LLM for automated data analysis</title>
    <updated>2025-08-20T13:01:02+00:00</updated>
    <author>
      <name>/u/Educational_Cry_7951</name>
      <uri>https://old.reddit.com/user/Educational_Cry_7951</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mve5hp/datarusr114bpreview_an_adaptive_multistep/"&gt; &lt;img alt="Datarus-R1-14B-Preview, an adaptive multi-step reasoning LLM for automated data analysis" src="https://a.thumbs.redditmedia.com/bAUgORU1f1Mo2dKJy7wG0byeBvquE5LZqpWQak3Vgr0.jpg" title="Datarus-R1-14B-Preview, an adaptive multi-step reasoning LLM for automated data analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you’ve used modern reasoning-focused LLMs, you’ve probably seen it happen: the model starts solving your problem, then analyzes its own reasoning, then re-analyzes that, spiraling into thousands of tokens of circular “thinking.” It’s expensive, slow, and sometimes worse than a non reasoning model.&lt;/p&gt; &lt;p&gt;Today, we’re excited to share &lt;strong&gt;Datarus-R1-14B-Preview&lt;/strong&gt;, a new &lt;strong&gt;open-weight reasoning model&lt;/strong&gt; designed to avoid this overthinking trap while hitting state-of-the-art results on coding and reasoning benchmarks.&lt;/p&gt; &lt;h1&gt;Key points:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;14B parameters — but outperforms much larger models.&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;18–49% fewer tokens&lt;/strong&gt; than competitors for the same reasoning tasks.&lt;/li&gt; &lt;li&gt;New training method focused on &lt;strong&gt;adaptive&lt;/strong&gt; multi-step reasoning.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it out &amp;amp; resources:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Chat and test the model: &lt;a href="https://chat.datarus.ai"&gt;chat.datarus.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Website: &lt;a href="https://datarus.ai"&gt;datarus.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jupyter Agent for interactive workflows: &lt;a href="https://github.com/DatarusAI/Datarus-JupyterAgent"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model weights (open): &lt;a href="https://huggingface.co/DatarusAI/Datarus-R1-14B-preview"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Preprint: &lt;a href="https://arxiv.org/pdf/2508.13382"&gt;ArXiv 2508.13382&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear what you all think, especially if you give the &lt;strong&gt;Preview&lt;/strong&gt; a spin or integrate the Jupyter agent into your workflows!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z0nb5gw4a6kf1.png?width=1172&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b5295d0021f25a5efefb6c6ea98ef8c5a53e367"&gt;https://preview.redd.it/z0nb5gw4a6kf1.png?width=1172&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b5295d0021f25a5efefb6c6ea98ef8c5a53e367&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Cry_7951"&gt; /u/Educational_Cry_7951 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mve5hp/datarusr114bpreview_an_adaptive_multistep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mve5hp/datarusr114bpreview_an_adaptive_multistep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mve5hp/datarusr114bpreview_an_adaptive_multistep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T13:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvbzvh</id>
    <title>Qwen 30B Instruct vs GPT-OSS 20B for real life coding</title>
    <updated>2025-08-20T11:20:38+00:00</updated>
    <author>
      <name>/u/Mobile_Ice1759</name>
      <uri>https://old.reddit.com/user/Mobile_Ice1759</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Would like some opinions besides benchmarks for those 2 models (or maybe additional one) from people who use it for production applications. Web (PHP/JS), iOS (Swift). As Im GPU poor and have 1x3090 these are the best local options for me now.&lt;/p&gt; &lt;p&gt;Both models sucks with the whole codebases (qwen cli, aider), so I'm making some summaries which then I give to it along with some context.&lt;/p&gt; &lt;p&gt;Naturally GPT works a bit faster, but I encounter a problem where I have to switch models for different problems, like UI or back-end, even though they are not consistently better versus each other. I'm looking for anyone who can get me along the way with models parameters, workflow, etc with going on this setup.&lt;/p&gt; &lt;p&gt;Mostly all my problems are solved via paid services, but there are 2 projects now, where I can't/won't share data and trying to think of solution without spending half a budget on making a lab or purchasing cloud gpu.&lt;/p&gt; &lt;p&gt;thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mobile_Ice1759"&gt; /u/Mobile_Ice1759 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvbzvh/qwen_30b_instruct_vs_gptoss_20b_for_real_life/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvbzvh/qwen_30b_instruct_vs_gptoss_20b_for_real_life/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvbzvh/qwen_30b_instruct_vs_gptoss_20b_for_real_life/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T11:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv3hcr</id>
    <title>GPT 4.5 vs DeepSeek V3.1</title>
    <updated>2025-08-20T03:06:43+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"&gt; &lt;img alt="GPT 4.5 vs DeepSeek V3.1" src="https://preview.redd.it/5c3gbyx3c3kf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcee32647c7f5d8e6d02cf6e6eb7d06bb63cacab" title="GPT 4.5 vs DeepSeek V3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5c3gbyx3c3kf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv3hcr/gpt_45_vs_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T03:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv7kk2</id>
    <title>Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528</title>
    <updated>2025-08-20T06:54:36+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"&gt; &lt;img alt="Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528" src="https://b.thumbs.redditmedia.com/H3XSTIYwTWYE6zf6TJkmBkaHUW2BDiBZVxdTdMDbz8s.jpg" title="Deepseek V3.1 improved token efficiency in reasoning mode over R1 and R1-0528" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See &lt;a href="https://github.com/cpldcpu/LRMTokenEconomy"&gt;here &lt;/a&gt;for more background information on the evaluation.&lt;/p&gt; &lt;p&gt;It appears they significantly reduced overthinking for prompts that can can be answered from model knowledge and math problems. There are still some cases where it creates very long CoT though for logic puzzles.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mv7kk2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv7kk2/deepseek_v31_improved_token_efficiency_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T06:54:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvgw9k</id>
    <title>DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC</title>
    <updated>2025-08-20T14:48:46+00:00</updated>
    <author>
      <name>/u/alexmrv</name>
      <uri>https://old.reddit.com/user/alexmrv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/"&gt; &lt;img alt="DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC" src="https://external-preview.redd.it/FW1JlH9sSss0Pq8rzoWxjFpJJZK922NxO1y6uOe6VUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d145871e4a70112dabd181298b3fe1e73ada2a35" title="DiffMem: Using Git as a Differential Memory Backend for AI Agents - Open-Source PoC" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've been experimenting with memory systems for AI agents, and I wanted to share a prototype I've built: DiffMem. It's a lightweight, Git-based memory backend that stores &amp;quot;current state&amp;quot; knowledge in Markdown files while using Git's commit history for tracking evolution. The goal is efficient, scalable memory for long-horizon agents. Think personal assistants that accumulate knowledge over years without bloating queries.&lt;/p&gt; &lt;h1&gt;Why Git for AI Memory?&lt;/h1&gt; &lt;p&gt;Traditional approaches (databases, vector stores) work, but they can get messy with evolving personal data. DiffMem flips this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Current-State Focus&lt;/strong&gt;: Only the &amp;quot;now&amp;quot; view is in active files (e.g., current relationships or facts). This keeps search/indexing lean. BM25 queries hit a compact surface, reducing token overhead in LLM contexts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;History in the Background&lt;/strong&gt;: Changes live in Git diffs/logs. Agents query the present by default but can dive into &amp;quot;how did this evolve?&amp;quot; via targeted diffs (e.g., &lt;code&gt;git diff HEAD~1 file.md&lt;/code&gt;), without loading full histories.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benefits for Engineers&lt;/strong&gt;: No schemas/migrations. Just edit Markdown. Git handles versioning, branching (e.g., monthly timelines), and audits for free. It's durable (plaintext, distributed) and hackable.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How It Works&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt;: Analyzes transcripts, creates/updates entities, stages in Git's working tree (commit explicit for atomicity).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Manager&lt;/strong&gt;: Assembles depth-based context (basic: core blocks; deep: full files; temporal: with Git history).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Searcher&lt;/strong&gt;: LLM-orchestrated BM25 for semantic-aware retrieval.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a PoC. Functional but rough (e.g., manual index rebuilds, basic error handling). Full code, examples, and repo guide on GitHub.&lt;/p&gt; &lt;h1&gt;Why Share This?&lt;/h1&gt; &lt;p&gt;This is R&amp;amp;D exploring how version control can power evolvable agents. We're not claiming it's revolutionary, but it solves real pain points like memory sprawl and temporal reasoning. Future ideas: agent-driven pruning (archive low-strength memories to branches), collaborative repos for multi-agent systems, or hybrid with embeddings.&lt;/p&gt; &lt;p&gt;I'd love honest feedback: Does this resonate? What breaks? Ideas for improvements/collaborations? PRs welcome.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Growth-Kinetics/DiffMem"&gt;https://github.com/Growth-Kinetics/DiffMem&lt;/a&gt;&lt;br /&gt; License: MIT&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alexmrv"&gt; /u/alexmrv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Growth-Kinetics/DiffMem"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvgw9k/diffmem_using_git_as_a_differential_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T14:48:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvn50l</id>
    <title>Guys it's official, the nano banana model on lm arena is Google's</title>
    <updated>2025-08-20T18:32:13+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/OfficialLoganK/status/1957908528925909391"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvn50l/guys_its_official_the_nano_banana_model_on_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvn50l/guys_its_official_the_nano_banana_model_on_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvl0zk</id>
    <title>Qwen-Image-Edit #6 overall on LMArena, best open model image editor</title>
    <updated>2025-08-20T17:17:24+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"&gt; &lt;img alt="Qwen-Image-Edit #6 overall on LMArena, best open model image editor" src="https://preview.redd.it/90yj5wnyj7kf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c77694f269b30f417ff8568342a83f0ba81a1ec2" title="Qwen-Image-Edit #6 overall on LMArena, best open model image editor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Surprised they didn't vote this one higher, I felt like the edits I saw Qwen make online were pretty good&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90yj5wnyj7kf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvl0zk/qwenimageedit_6_overall_on_lmarena_best_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T17:17:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvnmjo</id>
    <title>My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834</title>
    <updated>2025-08-20T18:49:36+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt; &lt;img alt="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" src="https://external-preview.redd.it/bruJaed8mpWclO3rYYnLL_4tpIRSDSNQT1lxjc08864.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de2bdb3583ed7a714be2ea7450e90270d3b3e83" title="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’ve posted on here a couple times sharing my project. I'm training LLM’s from scratch on 1800’s London texts (no fine tune/modern data). I built a dataset using 7,000 texts published between 1800 to 1875 in the city of London, and also trained a custom tokenizer on the dataset itself to get rid of modern vocab. &lt;/p&gt; &lt;p&gt;So far I’ve trained 3 models, 2 with nanoGPT and the latest using Phi 1.5. After training, I messed around with some prompts and used this one:&lt;/p&gt; &lt;p&gt;&amp;quot;It was the year of our Lord 1834&amp;quot; &lt;/p&gt; &lt;p&gt;Here’s the output:&lt;/p&gt; &lt;p&gt; &amp;quot;It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity&amp;quot; (The last sentence is weird but stuff like that shows up a lot probably due to heavy biblical influence)&lt;/p&gt; &lt;p&gt;I was interested to see if a protest had actually occurred in 1834 London and it really did happen but I thought it was maybe just a coincidence. The output also brought up “Lord Palmerston” and after a google search I learned that his actions resulted in the 1834 protests. So this idea is past just mimicking 1800s text and can now actually recall real historical events. &lt;/p&gt; &lt;p&gt;This is all from just 5-6GB of data, imagine the results with 30GB or more. I’m not sure if just scaling the data up will ever result in reasoning but even now it kinda feels like digital time travel. I want to eventually try different cities also, maybe a Chinese, Russian or Indian or even just another English city model. I’m just doing this for fun so if anyone would like to collaborate let me know, I’m open to anything really. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e"&gt;https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvjj8q</id>
    <title>Seed-OSS-36B-Instruct</title>
    <updated>2025-08-20T16:23:50+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct"&gt;https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Introduction:&lt;/p&gt; &lt;p&gt;Seed-OSS is a series of open-source large language models developed by ByteDance's Seed Team, designed for powerful long-context, reasoning, agent and general capabilities, and versatile developer-friendly features. Although trained with only 12T tokens, Seed-OSS achieves excellent performance on several popular open benchmarks.&lt;/p&gt; &lt;p&gt;We release this series of models to the open-source community under the Apache-2.0 license.&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Flexible Control of Thinking Budget&lt;/strong&gt;: Allowing users to flexibly adjust the reasoning length as needed. This capability of dynamically controlling the reasoning length enhances inference efficiency in practical application scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Reasoning Capability&lt;/strong&gt;: Specifically optimized for reasoning tasks while maintaining balanced and excellent general capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Intelligence&lt;/strong&gt;: Performs exceptionally well in agentic tasks such as tool-using and issue resolving.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research-Friendly&lt;/strong&gt;: Given that the inclusion of synthetic instruction data in pre-training may affect the post-training research, we released pre-trained models both with and without instruction data, providing the research community with more diverse options.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native Long Context&lt;/strong&gt;: Trained with up-to-512K long context natively.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvjj8q/seedoss36binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T16:23:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvfdja</id>
    <title>IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit</title>
    <updated>2025-08-20T13:51:04+00:00</updated>
    <author>
      <name>/u/AskGpts</name>
      <uri>https://old.reddit.com/user/AskGpts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"&gt; &lt;img alt="IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit" src="https://preview.redd.it/moddapg5j6kf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6316f1a44b3898869b91f28f4d1774a35db2491" title="IBM and NASA just dropped Surya: an open‑source AI to forecast solar storms before they hit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Solar storms don’t just make pretty auroras—they can scramble GPS, disrupt flights, degrade satellite comms, and stress power grids. To get ahead of that, IBM and NASA have open‑sourced Surya on Hugging Face: a foundation model trained on years of Solar Dynamics Observatory (SDO) data to make space‑weather forecasting more accurate and accessible.&lt;/p&gt; &lt;p&gt;What Surya is&lt;/p&gt; &lt;p&gt;A mid‑size foundation model for heliophysics that learns general “features of the Sun” from large SDO image archives.&lt;/p&gt; &lt;p&gt;Built to support zero/few‑shot tasks like flare probability, CME risk, and geomagnetic indices (e.g., Kp/Dst) with fine‑tuning.&lt;/p&gt; &lt;p&gt;Released with open weights and recipes so labs, universities, and startups can adapt it without massive compute.&lt;/p&gt; &lt;p&gt;Why this matters&lt;/p&gt; &lt;p&gt;Early, reliable alerts help airlines reroute, satellite operators safe‑mode hardware, and grid operators harden the network before a hit.&lt;/p&gt; &lt;p&gt;Open sourcing lowers the barrier for regional forecasters and fosters reproducible science (shared baselines, comparable benchmarks).&lt;/p&gt; &lt;p&gt;We’re in an active solar cycle—better lead times now can prevent expensive outages and service disruptions.&lt;/p&gt; &lt;p&gt;How to try it (technical)&lt;/p&gt; &lt;p&gt;Pull the model from Hugging Face and fine‑tune on your target label: flare class prediction, Kp nowcasting, or satellite anomaly detection.&lt;/p&gt; &lt;p&gt;Start with SDO preprocessing pipelines; add lightweight adapters/LoRA for event‑specific fine‑tuning to keep compute modest.&lt;/p&gt; &lt;p&gt;Evaluate on public benchmarks (Kp/Dst) and report lead time vs. skill scores; stress test on extreme events.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AskGpts"&gt; /u/AskGpts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/moddapg5j6kf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvfdja/ibm_and_nasa_just_dropped_surya_an_opensource_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T13:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mv6go1</id>
    <title>We beat Google Deepmind but got killed by a chinese lab</title>
    <updated>2025-08-20T05:46:26+00:00</updated>
    <author>
      <name>/u/Connect-Employ-4708</name>
      <uri>https://old.reddit.com/user/Connect-Employ-4708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt; &lt;img alt="We beat Google Deepmind but got killed by a chinese lab" src="https://external-preview.redd.it/eG8yNGJoZWQyNGtmMVo0YW9szsCgDSDYpHIZftteA0dldCtHqInQOZXGentR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7913b23ef6b3d159bc028db814e051ecf2742451" title="We beat Google Deepmind but got killed by a chinese lab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two months ago, my friends in AI and I asked: What if an AI could actually use a phone like a human?&lt;/p&gt; &lt;p&gt;So we built an agentic framework that taps, swipes, types… and somehow it’s outperforming giant labs like &lt;strong&gt;Google DeepMind&lt;/strong&gt; and &lt;strong&gt;Microsoft Research&lt;/strong&gt; on the AndroidWorld benchmark.&lt;/p&gt; &lt;p&gt;We were thrilled about our results until a massive Chinese lab (Zhipu AI) released its results last week to take the top spot.&lt;/p&gt; &lt;p&gt;They’re slightly ahead, but they have an army of 50+ phds and I don't see how a team like us can compete with them, that does not seem realistic... except that they're closed source.&lt;/p&gt; &lt;p&gt;And we decided to open-source everything. That way, even as a small team, we can make our work count.&lt;/p&gt; &lt;p&gt;We’re currently building our own custom mobile RL gyms, training environments made to push this agent further and get closer to 100% on the benchmark.&lt;/p&gt; &lt;p&gt;What do you think can make a small team like us compete against such giants?&lt;/p&gt; &lt;p&gt;Repo’s here if you want to check it out or contribute: &lt;a href="https://github.com/minitap-ai/mobile-use"&gt;github.com/minitap-ai/mobile-use&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Connect-Employ-4708"&gt; /u/Connect-Employ-4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qvewe6nd24kf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mv6go1/we_beat_google_deepmind_but_got_killed_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T05:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
