<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-03-28T08:38:41+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1jlmduh</id>
    <title>Resume Tailor - an AI-powered tool that helps job seekers customize their resumes for specific positions! 💼</title>
    <updated>2025-03-28T03:53:48+00:00</updated>
    <author>
      <name>/u/Maleficent-Penalty50</name>
      <uri>https://old.reddit.com/user/Maleficent-Penalty50</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlmduh/resume_tailor_an_aipowered_tool_that_helps_job/"&gt; &lt;img alt="Resume Tailor - an AI-powered tool that helps job seekers customize their resumes for specific positions! 💼" src="https://external-preview.redd.it/aW9lOG95ZGRzY3JlMcOCn68Ug8MSclXcs9Aau_EFApnS4CTmSgUVZy2v_uPC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96f3c43da906af9bc987a0c86330871804a6ed9f" title="Resume Tailor - an AI-powered tool that helps job seekers customize their resumes for specific positions! 💼" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maleficent-Penalty50"&gt; /u/Maleficent-Penalty50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t3zuzvddscre1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlmduh/resume_tailor_an_aipowered_tool_that_helps_job/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlmduh/resume_tailor_an_aipowered_tool_that_helps_job/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T03:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlpbge</id>
    <title>Deep research</title>
    <updated>2025-03-28T07:06:49+00:00</updated>
    <author>
      <name>/u/arivar</name>
      <uri>https://old.reddit.com/user/arivar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. Since OpenAI made deep research available I’ve changed my subscription to pro and its really been great for many things (from simple to more complex requests), but I am wondering if there open source projects that do the same (I have 56gb vram) or if there is any other paid one, but cheaper than $200.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arivar"&gt; /u/arivar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlpbge/deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlpbge/deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlpbge/deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T07:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlpiz8</id>
    <title>If you could run any model at home for free (open or closed), which one would you choose?</title>
    <updated>2025-03-28T07:22:43+00:00</updated>
    <author>
      <name>/u/eposnix</name>
      <uri>https://old.reddit.com/user/eposnix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's your ideal model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eposnix"&gt; /u/eposnix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlpiz8/if_you_could_run_any_model_at_home_for_free_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlpiz8/if_you_could_run_any_model_at_home_for_free_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlpiz8/if_you_could_run_any_model_at_home_for_free_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T07:22:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl5uu5</id>
    <title>New unit in the Hugging Face LLM course. We dive deep into RL with an advanced and hands-on guide to interpreting GRPO.</title>
    <updated>2025-03-27T14:57:15+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NEW UNIT in the Hugging Face Reasoning course. We dive deep into the algorithm behind DeepSeek R1 with an advanced and hands-on guide to interpreting GRPO.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://huggingface.co/reasoning-course"&gt;https://huggingface.co/reasoning-course&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This unit is super useful if you’re tuning models with reinforcement learning. It will help with:&lt;/p&gt; &lt;p&gt;- interpreting loss and reward progression during training runs&lt;/p&gt; &lt;p&gt;- selecting effective parameters for training&lt;/p&gt; &lt;p&gt;- reviewing and defining effective reward functions&lt;/p&gt; &lt;p&gt;This unit also works up smoothly toward the existing practical exercises form Maxime Labonne and Unsloth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5uu5/new_unit_in_the_hugging_face_llm_course_we_dive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5uu5/new_unit_in_the_hugging_face_llm_course_we_dive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5uu5/new_unit_in_the_hugging_face_llm_course_we_dive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T14:57:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlmlj0</id>
    <title>Cool tool for coding with LLMs: Prompt-Tower</title>
    <updated>2025-03-28T04:05:30+00:00</updated>
    <author>
      <name>/u/arthurwolf</name>
      <uri>https://old.reddit.com/user/arthurwolf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The link: &lt;a href="https://github.com/backnotprop/prompt-tower"&gt;https://github.com/backnotprop/prompt-tower&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's an extension for VSCode, that lets you easily create prompts to copy/paste into your favorite LLM, from a selection of copy/pasted text, or from entire files you select in your file tree.&lt;/p&gt; &lt;p&gt;It saves a ton of time, and I figured maybe it could save time to others.&lt;/p&gt; &lt;p&gt;If you look at the issues, there is a lot of discutions of interresting possible ways it could be extended too, and it's open-source so you can participate in making it better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arthurwolf"&gt; /u/arthurwolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlmlj0/cool_tool_for_coding_with_llms_prompttower/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlmlj0/cool_tool_for_coding_with_llms_prompttower/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlmlj0/cool_tool_for_coding_with_llms_prompttower/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T04:05:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlofc7</id>
    <title>Performance regression in CUDA workloads with modern drivers</title>
    <updated>2025-03-28T06:01:38+00:00</updated>
    <author>
      <name>/u/karurochari</name>
      <uri>https://old.reddit.com/user/karurochari</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all. For the last few hours I have been trying to debug a performance regression on my 3090 of ~ 35% in cuda workloads. Same machine, same hardware, just a fresh install of the OS and new drivers. &lt;/p&gt; &lt;p&gt;Before I was running 535.104.05 and 12.2 for the cuda SDK.&lt;br /&gt; Now it is 535.216.03 and same 12.2. I also tested 570.124.06 with sdk version 12.8, but results are similar.&lt;/p&gt; &lt;p&gt;Does anyone have an idea of what is going on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karurochari"&gt; /u/karurochari &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlofc7/performance_regression_in_cuda_workloads_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlofc7/performance_regression_in_cuda_workloads_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlofc7/performance_regression_in_cuda_workloads_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T06:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlfpiw</id>
    <title>What's the best hardware to run ~30b models?</title>
    <updated>2025-03-27T22:28:05+00:00</updated>
    <author>
      <name>/u/NationalMushroom7938</name>
      <uri>https://old.reddit.com/user/NationalMushroom7938</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I was really hyped when Nvidia announced project digits back in January. I'm a ml-student and don't have a big gaming PC or something with some good gpus, also I want something that's portable. Project Digits/Spark would be simply perfect.&lt;/p&gt; &lt;p&gt;Now I saw that many here say that this dgx spark would be completely unuseable because of the 273gb/s bandwidth. Is it that bad? &lt;/p&gt; &lt;p&gt;My goal is to use it as kind of research lab. I would like to run ~30b models with a good generationspeed, but also do some finetuning or something.&lt;/p&gt; &lt;p&gt;What do you guys think? Would you buy the dgx spark? What are the alternatives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NationalMushroom7938"&gt; /u/NationalMushroom7938 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlfpiw/whats_the_best_hardware_to_run_30b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlfpiw/whats_the_best_hardware_to_run_30b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlfpiw/whats_the_best_hardware_to_run_30b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T22:28:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl4amv</id>
    <title>A closer look at the NVIDIA DGX Station GB300</title>
    <updated>2025-03-27T13:47:13+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4amv/a_closer_look_at_the_nvidia_dgx_station_gb300/"&gt; &lt;img alt="A closer look at the NVIDIA DGX Station GB300" src="https://external-preview.redd.it/3kMZ_XjxFOw3ZXQvJxmGumZXY5uPpAA35n9ELNs2oWg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d81ba8815cb5a0ec0d51068cff351b711fc0590a" title="A closer look at the NVIDIA DGX Station GB300" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.servethehome.com/nvidia-dgx-station-gb300-edition-arm-launched/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4amv/a_closer_look_at_the_nvidia_dgx_station_gb300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4amv/a_closer_look_at_the_nvidia_dgx_station_gb300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T13:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlk03r</id>
    <title>Fine-tuning Gemma 1B with PEFT, how much VRAM and how long?</title>
    <updated>2025-03-28T01:48:33+00:00</updated>
    <author>
      <name>/u/Qdr-91</name>
      <uri>https://old.reddit.com/user/Qdr-91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soon after doing the research and settling on the methodolgy, I'll start working on my master's thesis project. The topic is memory-efficient fine-tuning of LLMs. I've already worked on a similar topic but with DistilBERT and I only experimented with different optimizers and hyperparameters. For the thesis I'll use different PEFT adapters, quantizations, optimizers and fine-tune on larger datasets, all to benchmark performance vs. memory efficiency. I'll have to do many runs.&lt;/p&gt; &lt;p&gt;has anyone fine-tuned a model with a similar size locally? How long does it take and what's the required VRAM with vanilla LoRA? I'll be using the cloud to fine-tune. I have an RTX 3070 laptop and it won't serve me for such a task, but still I'd like to have an estimate of the VRAM requirement and the time a run will take.&lt;/p&gt; &lt;p&gt;Thanks everyone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qdr-91"&gt; /u/Qdr-91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlk03r/finetuning_gemma_1b_with_peft_how_much_vram_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlk03r/finetuning_gemma_1b_with_peft_how_much_vram_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlk03r/finetuning_gemma_1b_with_peft_how_much_vram_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T01:48:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jldrdn</id>
    <title>V3 2.42 oneshot snake game</title>
    <updated>2025-03-27T21:08:01+00:00</updated>
    <author>
      <name>/u/getmevodka</name>
      <uri>https://old.reddit.com/user/getmevodka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldrdn/v3_242_oneshot_snake_game/"&gt; &lt;img alt="V3 2.42 oneshot snake game" src="https://external-preview.redd.it/dnJwbDl1Z2xyYXJlMfVmROp4L1DkunUvunpfnNbpr-mP5aJzdPtvTcGrT5BH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=218c4f03db89efb69e9f1e9c1dfe61df9dc2ca99" title="V3 2.42 oneshot snake game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i simply asked it to generate a fully functional snake game including all features and what is around the game like highscores, buttons and wanted it in a single script including html css and javascript, while behaving like it was a fullstack dev. Consider me impressed both to the guys of deepseek devs and the unsloth guys making it usable. i got about 13 tok/s in generation speed and the code is about 3300 tokens long. temperature was .3 min p 0.01 top p 0.95 , top k 35. fully ran in vram of my m3 ultra base model with 256gb vram, taking up about 250gb with 6.8k context size. more would break the system. deepseek devs themselves advise temp of 0.0 for coding though. hope you guys like it, im truly impressed for a singleshot. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getmevodka"&gt; /u/getmevodka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/tpp20ytlrare1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldrdn/v3_242_oneshot_snake_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jldrdn/v3_242_oneshot_snake_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T21:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl1yk4</id>
    <title>DeepSeek V3 0324 on livebench surpasses Claude 3.7</title>
    <updated>2025-03-27T11:44:33+00:00</updated>
    <author>
      <name>/u/MrPiradoHD</name>
      <uri>https://old.reddit.com/user/MrPiradoHD</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"&gt; &lt;img alt="DeepSeek V3 0324 on livebench surpasses Claude 3.7" src="https://b.thumbs.redditmedia.com/rn27tkiEJGK7lj3Fd5ifzNLt6PhUdToT0IvAY2kN6gM.jpg" title="DeepSeek V3 0324 on livebench surpasses Claude 3.7" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw the latest LiveBench results and DeepSeek's V3 (0324) is showing some impressive performance! It's currently sitting at 10th place overall, but what's really interesting is that it's the second highest non-thinking model, only behind GPT-4.5 Preview, while outperforming Claude 3.7 Sonnet (base model, not the thinking version).&lt;/p&gt; &lt;p&gt;We will have to wait, but this suggests that R2 might be a stupidly great model if V3 is already outperforming Claude 3.7 (base), this next version could seriously challenge to the big ones.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cvzv13s3z7re1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10ff7be91e85ed4fd15e1efa97f8c271791cd296"&gt;https://preview.redd.it/cvzv13s3z7re1.png?width=1956&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=10ff7be91e85ed4fd15e1efa97f8c271791cd296&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPiradoHD"&gt; /u/MrPiradoHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl1yk4/deepseek_v3_0324_on_livebench_surpasses_claude_37/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T11:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl7dd9</id>
    <title>What is currently the best Uncensored LLM for 24gb of VRAM?</title>
    <updated>2025-03-27T16:01:27+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for recommendations. I have been using APIs but itching getting back to locallama. &lt;/p&gt; &lt;p&gt;Will be running Ollama with OpenWebUI and the model's use case being simply general purpose with the occasional sketchy request.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7dd9/what_is_currently_the_best_uncensored_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7dd9/what_is_currently_the_best_uncensored_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl7dd9/what_is_currently_the_best_uncensored_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T16:01:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlbrjk</id>
    <title>QVQ-Max: Think with Evidence</title>
    <updated>2025-03-27T19:11:59+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwenlm.github.io/blog/qvq-max-preview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlbrjk/qvqmax_think_with_evidence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlbrjk/qvqmax_think_with_evidence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T19:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1jln2th</id>
    <title>If money was no object, what kind of system would you seek out in order to run Llama 3.3?</title>
    <updated>2025-03-28T04:34:17+00:00</updated>
    <author>
      <name>/u/TokenBearer</name>
      <uri>https://old.reddit.com/user/TokenBearer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A Mac Studio with 256GB unified ram, or maybe 512GB to run DeepSeek as well? Both should handle full precision.&lt;/p&gt; &lt;p&gt;Or would you go cluster together GPUs? If so, which ones and why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenBearer"&gt; /u/TokenBearer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jln2th/if_money_was_no_object_what_kind_of_system_would/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jln2th/if_money_was_no_object_what_kind_of_system_would/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jln2th/if_money_was_no_object_what_kind_of_system_would/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T04:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlj7qm</id>
    <title>Video of 48GB 4090d teardown and test.</title>
    <updated>2025-03-28T01:09:01+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a video that shows a teardown of a 48GB 4090. They also show various tests including a LLM run at around the 12:40 mark. It's in Russian so turn on CC with autotranslate to your language of choice.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=m9YszWQenII"&gt;https://www.youtube.com/watch?v=m9YszWQenII&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlj7qm/video_of_48gb_4090d_teardown_and_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlj7qm/video_of_48gb_4090d_teardown_and_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlj7qm/video_of_48gb_4090d_teardown_and_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T01:09:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1jkzjve</id>
    <title>Microsoft develop a more efficient way to add knowledge into LLMs</title>
    <updated>2025-03-27T08:55:51+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"&gt; &lt;img alt="Microsoft develop a more efficient way to add knowledge into LLMs" src="https://external-preview.redd.it/aCGhAR6FEKRX-h5rqecZAFckea8B8CJ4kaRGE3aJoC0.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ed759c95a14ac48041ed2121cc23df6c9a4808d" title="Microsoft develop a more efficient way to add knowledge into LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jkzjve/microsoft_develop_a_more_efficient_way_to_add/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T08:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlec7i</id>
    <title>Microsoft developed this technique which combines RAG and Fine-tuning for better domain adaptation</title>
    <updated>2025-03-27T21:31:07+00:00</updated>
    <author>
      <name>/u/Ambitious_Anybody855</name>
      <uri>https://old.reddit.com/user/Ambitious_Anybody855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlec7i/microsoft_developed_this_technique_which_combines/"&gt; &lt;img alt="Microsoft developed this technique which combines RAG and Fine-tuning for better domain adaptation" src="https://preview.redd.it/do8skr38sare1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09863117ee9d202e6c101b49e612ece45d06fde5" title="Microsoft developed this technique which combines RAG and Fine-tuning for better domain adaptation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been exploring Retrieval Augmented Fine-Tuning (RAFT). Combines RAG and finetuning for better domain adaptation. Along with the question, the doc that gave rise to the context (called the oracle doc) is added, along with other distracting documents. Then, with a certain probability, the oracle document is not included. Has there been any successful use cases of RAFT in the wild? Or has it been overshadowed. In that case, by what?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Anybody855"&gt; /u/Ambitious_Anybody855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/do8skr38sare1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlec7i/microsoft_developed_this_technique_which_combines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlec7i/microsoft_developed_this_technique_which_combines/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T21:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1jldt1s</id>
    <title>I looked up "Qwen 3" on duckduck go and found something interesting</title>
    <updated>2025-03-27T21:09:55+00:00</updated>
    <author>
      <name>/u/Flat_Jelly_3581</name>
      <uri>https://old.reddit.com/user/Flat_Jelly_3581</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldt1s/i_looked_up_qwen_3_on_duckduck_go_and_found/"&gt; &lt;img alt="I looked up &amp;quot;Qwen 3&amp;quot; on duckduck go and found something interesting" src="https://b.thumbs.redditmedia.com/9BzPKrc0IjjvNWCIjcjCohoKMpTPqOmRfuRMvx_uJeY.jpg" title="I looked up &amp;quot;Qwen 3&amp;quot; on duckduck go and found something interesting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xwv0yfxyrare1.png?width=1414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cff8e124dd1984d9ea2d583ed97e7805a6a33db"&gt;https://preview.redd.it/xwv0yfxyrare1.png?width=1414&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cff8e124dd1984d9ea2d583ed97e7805a6a33db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Did someone make a mistake? I think someone made a mistake. That or someones baiting me. Also the link is obviously not made public, but here it will be when its released &lt;a href="https://huggingface.co/FalconNet/Qwen3.0"&gt;https://huggingface.co/FalconNet/Qwen3.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Im stupid, this is early april fools. :/&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flat_Jelly_3581"&gt; /u/Flat_Jelly_3581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldt1s/i_looked_up_qwen_3_on_duckduck_go_and_found/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldt1s/i_looked_up_qwen_3_on_duckduck_go_and_found/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jldt1s/i_looked_up_qwen_3_on_duckduck_go_and_found/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T21:09:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1jldzbn</id>
    <title>Is there something better than Ollama?</title>
    <updated>2025-03-27T21:16:43+00:00</updated>
    <author>
      <name>/u/Timziito</name>
      <uri>https://old.reddit.com/user/Timziito</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't mind Ollama but i assume something more optimized is out there maybe? :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timziito"&gt; /u/Timziito &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldzbn/is_there_something_better_than_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jldzbn/is_there_something_better_than_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jldzbn/is_there_something_better_than_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T21:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1jla08h</id>
    <title>Orpheus.cpp - Fast Audio Generation without a GPU</title>
    <updated>2025-03-27T17:50:44+00:00</updated>
    <author>
      <name>/u/freddyaboulton</name>
      <uri>https://old.reddit.com/user/freddyaboulton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! I've been spending the last couple of months trying to build real-time audio/video assistants in python and got frustrated by the lack of good text-to-speech models that are easy to use and can run decently fast without a GPU on my macbook.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/freddyaboulton/orpheus-cpp"&gt;orpheus.cpp&lt;/a&gt; - a llama.cpp port of CanopyAI's &lt;a href="https://github.com/canopyai/Orpheus-TTS"&gt;Orpheus TTS model&lt;/a&gt; with an easy python API.&lt;/p&gt; &lt;p&gt;Orpheus is cool because it's a llama backbone that generates tokens that can be independently decoded to audio. So it lends itself well to this kind of hardware optimizaiton.&lt;/p&gt; &lt;p&gt;Anyways, hope you find it useful!&lt;/p&gt; &lt;p&gt;𝚙𝚒𝚙 𝚒𝚗𝚜𝚝𝚊𝚕𝚕 𝚘𝚛𝚙𝚑𝚎𝚞𝚜-𝚌𝚙𝚙&lt;br /&gt; 𝚙𝚢𝚝𝚑𝚘𝚗 -𝚖 𝚘𝚛𝚙𝚑𝚎𝚞𝚜_𝚌𝚙𝚙&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freddyaboulton"&gt; /u/freddyaboulton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jla08h/orpheuscpp_fast_audio_generation_without_a_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jla08h/orpheuscpp_fast_audio_generation_without_a_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jla08h/orpheuscpp_fast_audio_generation_without_a_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T17:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlgvbv</id>
    <title>I built a very easy to use lightweight fully C++ desktop UI for whisper.cpp</title>
    <updated>2025-03-27T23:17:57+00:00</updated>
    <author>
      <name>/u/mehtabmahir</name>
      <uri>https://old.reddit.com/user/mehtabmahir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released a lightweight local desktop UI for whisper.cpp, and added several thoughtful features that makes the whisper experience very easy and noob friendly.&lt;/p&gt; &lt;p&gt;It’s a lightweight, native desktop interface for &lt;a href="https://github.com/ggerganov/whisper.cpp"&gt;whisper.cpp&lt;/a&gt;, built entirely in C++ using Qt. No Python, no browser, and no heavy dependencies — just a smooth and fast UI that runs locally on Windows.&lt;/p&gt; &lt;h1&gt;🔧 Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fully C++ implementation — &lt;strong&gt;no Python required&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;Vulkan&lt;/strong&gt; for cross platform GPU acceleration (via whisper.cpp)&lt;/li&gt; &lt;li&gt;Drag &amp;amp; drop or use “Open With” to load audio&lt;/li&gt; &lt;li&gt;Auto-converts audio if needed to &lt;code&gt;.mp3&lt;/code&gt; with FFmpeg&lt;/li&gt; &lt;li&gt;Model selector with automatic downloading&lt;/li&gt; &lt;li&gt;Real-time logs in a built-in console box&lt;/li&gt; &lt;li&gt;Opens the final transcript in Notepad&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;💡 Why I built it&lt;/h1&gt; &lt;p&gt;I wanted something that just worked — no virtual environments, no setup steps — just a small program you can drop on your desktop and use right away. Whisper is amazing, but I felt the experience could be simpler for everyday users.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mehtabmahir/easy-whisper-ui/releases/"&gt;https://github.com/mehtabmahir/easy-whisper-ui/releases/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think — feedback, feature ideas, and bug reports welcome! I'm planning to add more features very soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehtabmahir"&gt; /u/mehtabmahir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlgvbv/i_built_a_very_easy_to_use_lightweight_fully_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlgvbv/i_built_a_very_easy_to_use_lightweight_fully_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlgvbv/i_built_a_very_easy_to_use_lightweight_fully_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T23:17:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlaeuw</id>
    <title>New QVQ-Max on Qwen Chat</title>
    <updated>2025-03-27T18:07:42+00:00</updated>
    <author>
      <name>/u/MrPLotor</name>
      <uri>https://old.reddit.com/user/MrPLotor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlaeuw/new_qvqmax_on_qwen_chat/"&gt; &lt;img alt="New QVQ-Max on Qwen Chat" src="https://preview.redd.it/vlz8vwxsv9re1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f5aac48465e4c10b54c5dbc92a2a67b80abc921" title="New QVQ-Max on Qwen Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPLotor"&gt; /u/MrPLotor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vlz8vwxsv9re1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlaeuw/new_qvqmax_on_qwen_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlaeuw/new_qvqmax_on_qwen_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T18:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlptqu</id>
    <title>Reverse engineering GPT-4o image gen via Network tab - here's what I found</title>
    <updated>2025-03-28T07:46:37+00:00</updated>
    <author>
      <name>/u/seicaratteri</name>
      <uri>https://old.reddit.com/user/seicaratteri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt; &lt;img alt="Reverse engineering GPT-4o image gen via Network tab - here's what I found" src="https://b.thumbs.redditmedia.com/m14HJMUTNABG7YbJvH-v0bUEJS6S9yY56n4S06ct2IU.jpg" title="Reverse engineering GPT-4o image gen via Network tab - here's what I found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am very intrigued about this new model; I have been working in the image generation space a lot, and I want to understand what's going on&lt;/p&gt; &lt;p&gt;I found interesting details when opening the network tab to see what the BE was sending - here's what I found. I tried with few different prompts, let's take this as a starter:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;An image of happy dog running on the street, studio ghibli style&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here I got four intermediate images, as follows:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qvx91aksxdre1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8bb7ddfca20ff1a8a80b8bdb5089a6f1e5173cc"&gt;https://preview.redd.it/qvx91aksxdre1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8bb7ddfca20ff1a8a80b8bdb5089a6f1e5173cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We can see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The BE is actually returning the image as we see it in the UI&lt;/li&gt; &lt;li&gt;It's not really clear wether the generation is autoregressive or not - we see &lt;em&gt;some&lt;/em&gt; details and a faint global structure of the image, this could mean two things: &lt;ul&gt; &lt;li&gt;1. Like usual diffusion processes, we first generate the global structure and then add details&lt;/li&gt; &lt;li&gt;2. The image is &lt;em&gt;actually&lt;/em&gt; generated autoregressively&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At first I was believing on the 2nd option, but thinking about it I am not so sure. If we analyze the 100% zoom of the first and last frame, we can see details are being added to high frequency textures like the trees&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rg7kxx4kzdre1.png?width=2608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c70c9ff61a4632ca5872feb991a3415913bd8200"&gt;https://preview.redd.it/rg7kxx4kzdre1.png?width=2608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c70c9ff61a4632ca5872feb991a3415913bd8200&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what we would typically expect from a diffusion model. This is further accentuated in this other example, where I prompted specifically for a high frequency detail texture (&amp;quot;create the image of a grainy texture, abstract shape, very extremely highly detailed&amp;quot;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o5bmeyyvxdre1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01e8feb207422127661f188c860bcf5f929fa914"&gt;https://preview.redd.it/o5bmeyyvxdre1.png?width=2048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01e8feb207422127661f188c860bcf5f929fa914&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, I got only three images here from the BE; and the details being added is obvious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aix1uw7xxdre1.png?width=2058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ff81a90e83c608b291080a404712806deea4b61"&gt;https://preview.redd.it/aix1uw7xxdre1.png?width=2058&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ff81a90e83c608b291080a404712806deea4b61&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This could be done of course as a separate post processing step too, for example like SDXL introduced the refiner model back in the days that was specifically trained to add details to the VAE latent representation before decoding it to pixel space.&lt;/p&gt; &lt;p&gt;It's also unclear if I got less images with this prompt due to availability (i.e. the BE could give me more flops), or to some kind of specific optimization (eg: latent caching).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So where I am at now:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I am inclined to think that the generation is still based on diffusion primarily&lt;/li&gt; &lt;li&gt;There might be some refiner model as post processing in place&lt;/li&gt; &lt;li&gt;I &lt;strong&gt;think the real difference&lt;/strong&gt; comes from how they connected the input and output space of images and text; it makes me think of this recent paper: &lt;a href="https://arxiv.org/pdf/2409.11340"&gt;OmniGen&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There they directly connect the VAE of a Latent Diffusion architecture to an LLM and learn to model jointly both text and images; they observe few shot capabilities and emerging properties too which would explain the vast capabilities of GPT4-o, and it makes even more sense if we consider the usual OAI formula:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;More / higher quality data&lt;/li&gt; &lt;li&gt;More flops&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The architecture proposed in OmniGen has &lt;em&gt;great&lt;/em&gt; potential to scale given that is purely transformer based - and if we know one thing is surely that transformers scale well, and that OAI is especially good at that&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What do you think?&lt;/strong&gt; would love to take this as a space to investigate together! Thanks for reading and let's get to the bottom of this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seicaratteri"&gt; /u/seicaratteri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlptqu/reverse_engineering_gpt4o_image_gen_via_network/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-28T07:46:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1jlgrik</id>
    <title>Gemini 2.5 Pro is amazing!</title>
    <updated>2025-03-27T23:13:15+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a PSA: if you haven't yet tried 2.5 Pro. Go try it now!&lt;/p&gt; &lt;p&gt;I'm blown away by the quality of the thinking for coding problems. I've only tested for a single coding task (I've been working half the day with it) so far but it is incredible. The thinking steps are logical and wisely chosen, not a scatter gun &amp;quot;no but wait!&amp;quot; random fest. &lt;/p&gt; &lt;p&gt;It is helping me solve real problems and saving me days of work!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlgrik/gemini_25_pro_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jlgrik/gemini_25_pro_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jlgrik/gemini_25_pro_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T23:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1jl5jea</id>
    <title>My LLMs are all free thinking and locally-sourced.</title>
    <updated>2025-03-27T14:43:35+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5jea/my_llms_are_all_free_thinking_and_locallysourced/"&gt; &lt;img alt="My LLMs are all free thinking and locally-sourced." src="https://preview.redd.it/s6mrolmfv8re1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4fc11e662f92489410497cde8c31a6b140e9bde" title="My LLMs are all free thinking and locally-sourced." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s6mrolmfv8re1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5jea/my_llms_are_all_free_thinking_and_locallysourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1jl5jea/my_llms_are_all_free_thinking_and_locallysourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-03-27T14:43:35+00:00</published>
  </entry>
</feed>
