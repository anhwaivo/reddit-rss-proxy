<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-28T05:06:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1izazyk</id>
    <title>Kokoro TTS 1.1</title>
    <updated>2025-02-27T08:18:53+00:00</updated>
    <author>
      <name>/u/incognataa</name>
      <uri>https://old.reddit.com/user/incognataa</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incognataa"&gt; /u/incognataa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M-v1.1-zh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izazyk/kokoro_tts_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izazyk/kokoro_tts_11/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T08:18:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz54du</id>
    <title>DeepSeek Realse 4th Bomb! DualPipe an innovative bidirectional pipeline parallism algorithm</title>
    <updated>2025-02-27T02:20:47+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"&gt; &lt;img alt="DeepSeek Realse 4th Bomb! DualPipe an innovative bidirectional pipeline parallism algorithm" src="https://external-preview.redd.it/8TUylBdHG6G-RlVpDiMU8uIUEktXyGwSKK-R9XNOIZE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de464dc178abc09c8c46cb861ec265373d57c26" title="DeepSeek Realse 4th Bomb! DualPipe an innovative bidirectional pipeline parallism algorithm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DualPipe is an innovative bidirectional pipeline parallism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/DualPipe"&gt;https://github.com/deepseek-ai/DualPipe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qzu9ol3cdlle1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a9c17d6008c619f5b01ce6d145949f0ebe675b"&gt;https://preview.redd.it/qzu9ol3cdlle1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a9c17d6008c619f5b01ce6d145949f0ebe675b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz54du/deepseek_realse_4th_bomb_dualpipe_an_innovative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T02:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm8k6</id>
    <title>Phi-4-Mini performance metrics on Intel PCs</title>
    <updated>2025-02-27T18:07:59+00:00</updated>
    <author>
      <name>/u/intofuture</name>
      <uri>https://old.reddit.com/user/intofuture</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm8k6/phi4mini_performance_metrics_on_intel_pcs/"&gt; &lt;img alt="Phi-4-Mini performance metrics on Intel PCs" src="https://external-preview.redd.it/pOPuW_iNdIa10dhRtc1cNC57z3SNqjd2uyv5Nypw2CQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc8bb08e89753226b2ff85d39d197ef70f2b7abc" title="Phi-4-Mini performance metrics on Intel PCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Intel posted &lt;a href="https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-microsoft-phi-4-small-language-models.html"&gt;an article&lt;/a&gt; with inference speed benchmarks of Phi-4-Mini (4-bit weights + OpenVINO hardware acceleration) running on a couple of their chips. &lt;/p&gt; &lt;p&gt;It's cool to see hard performance data with an SLM announcement for once. (At least, it's saving my team from one on-device benchmark ðŸ˜…) &lt;/p&gt; &lt;p&gt;On an Asus Zenbook S 14, which has an Intel Core Ultra 9 inside with 32GB RAM, they're getting ~30 toks/s for 1024 tokens in/out.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kl5e00430qle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc6f692ccdd0d4293f87668a6d4471439c92dae7"&gt;https://preview.redd.it/kl5e00430qle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc6f692ccdd0d4293f87668a6d4471439c92dae7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Exciting to see the progress with local inference on typical consumer hardware :) &lt;/p&gt; &lt;p&gt;They also ran a benchmark on a PC with an Core i9-149000K and a discrete Arc B580 GPU, which was hitting &amp;gt;90 toks/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y0mrilz70qle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=040651dffabfde774b87c8571af6d53fe050393d"&gt;https://preview.redd.it/y0mrilz70qle1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=040651dffabfde774b87c8571af6d53fe050393d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/intofuture"&gt; /u/intofuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm8k6/phi4mini_performance_metrics_on_intel_pcs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm8k6/phi4mini_performance_metrics_on_intel_pcs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izm8k6/phi4mini_performance_metrics_on_intel_pcs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:07:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1izu1yc</id>
    <title>Not having luck with Aider+Qwen-Coder, any tips?</title>
    <updated>2025-02-27T23:44:43+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Qwen-Coder 32b Q6 served via Llama CPP with the latest version of aider.&lt;/p&gt; &lt;p&gt;Context for these services never goes very high.&lt;/p&gt; &lt;p&gt;It takes a lot of iteration to make it do what I want. I can't seem to recreate others' benchmark success. Sometimes it does amazing but it seems random.&lt;/p&gt; &lt;p&gt;Does anyone have any tips for settings? Running it at temp 0.6&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izu1yc/not_having_luck_with_aiderqwencoder_any_tips/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izu1yc/not_having_luck_with_aiderqwencoder_any_tips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izu1yc/not_having_luck_with_aiderqwencoder_any_tips/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T23:44:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz1fv4</id>
    <title>Microsoft announces Phi-4-multimodal and Phi-4-mini</title>
    <updated>2025-02-26T23:22:15+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz1fv4/microsoft_announces_phi4multimodal_and_phi4mini/"&gt; &lt;img alt="Microsoft announces Phi-4-multimodal and Phi-4-mini" src="https://external-preview.redd.it/QxVX6RZwkbebYL7yNK-C4tfRXCDplq8w2ZjdBvIh-2c.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4f4d685b401cb23df7ee1e63ea0579a77eea2bc" title="Microsoft announces Phi-4-multimodal and Phi-4-mini" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz1fv4/microsoft_announces_phi4multimodal_and_phi4mini/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz1fv4/microsoft_announces_phi4multimodal_and_phi4mini/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-26T23:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iz9fpc</id>
    <title>Phi Model Family: The rise of The Small Language Models (SLMs)!</title>
    <updated>2025-02-27T06:24:39+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz9fpc/phi_model_family_the_rise_of_the_small_language/"&gt; &lt;img alt="Phi Model Family: The rise of The Small Language Models (SLMs)!" src="https://preview.redd.it/1218qwefkmle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0047de65b619a85493b7afdeb217512daf64f0b" title="Phi Model Family: The rise of The Small Language Models (SLMs)!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1218qwefkmle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iz9fpc/phi_model_family_the_rise_of_the_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iz9fpc/phi_model_family_the_rise_of_the_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T06:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1izy2l1</id>
    <title>Web Search using Local LLMs/We have Perplexity at home.</title>
    <updated>2025-02-28T03:09:45+00:00</updated>
    <author>
      <name>/u/Tokamakium</name>
      <uri>https://old.reddit.com/user/Tokamakium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use the Page Assist browser plugin as frontend, it has Web Search built-in.&lt;/li&gt; &lt;li&gt;Any model good at following instructions will be good at web search.&lt;/li&gt; &lt;li&gt;The number of pages and the search engine used will be more important. For my testing, I searched 10 pages and used Google. You can change those in the Page Assist settings.&lt;/li&gt; &lt;li&gt;Keep it brief. Ask only one question. Be as specific as possible.&lt;/li&gt; &lt;li&gt;Hallucinations/Incomplete information is to be expected.&lt;/li&gt; &lt;li&gt;Always start a new chat for a new question.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Uses:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When you want to know about something new but don't have the time to dig in.&lt;/li&gt; &lt;li&gt;Quickly checking the news.&lt;/li&gt; &lt;li&gt;That's pretty much it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Testing Parameters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4k context length. Rest of the Ollama settings at default.&lt;/li&gt; &lt;li&gt;Models: Llama 3.1 8b q6_k, Gemma 9b, Phi 4 14b, Qwen 2.5-Coder 14b, DeepSeek r1 14b. Default quantizations available on Ollama, except for the Llama model.&lt;/li&gt; &lt;li&gt;3060 12GB with 16 GB RAM. Naturally, Llama 3.1 is the quickest and I can use up to 16k context length without using the CPU.&lt;/li&gt; &lt;li&gt;Tested with 2 pages/DDG and then 10 pages/Google. Made the largest difference.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Questions Asked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What are the latest gameplay changes and events in Helldivers 2?&lt;/li&gt; &lt;li&gt;Summarize the latest Rust in Linux drama.&lt;/li&gt; &lt;li&gt;What is the best LLM I can run on a 3060 12GB?&lt;/li&gt; &lt;li&gt;What is the new Minion protocol for LLMs?&lt;/li&gt; &lt;li&gt;Give me a detailed summary of the latest Framework Company launch, including their specs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Summary of the replies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.1 8b is the quickest and performs almost at par with the other top models, so this will be my go-to.&lt;/li&gt; &lt;li&gt;Other models that performed well were DeepSeek and Qwen. After that was Phi and lastly Gemma.&lt;/li&gt; &lt;li&gt;No model recommended a specific model to run on my GPU.&lt;/li&gt; &lt;li&gt;The Framework question was the trickiest. Unless I mentioned that Framework is a company, models didn't know what to do with the question. Almost no model mentioned the new desktop launch, so I had to edit the question to get the answer I was seeking.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tokamakium"&gt; /u/Tokamakium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy2l1/web_search_using_local_llmswe_have_perplexity_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy2l1/web_search_using_local_llmswe_have_perplexity_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izy2l1/web_search_using_local_llmswe_have_perplexity_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T03:09:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1iztn9l</id>
    <title>New Karpathy's video: How I use LLMs</title>
    <updated>2025-02-27T23:25:34+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iztn9l/new_karpathys_video_how_i_use_llms/"&gt; &lt;img alt="New Karpathy's video: How I use LLMs" src="https://external-preview.redd.it/eYKZhXcOfzaddtzPaAUR7x_cqHQEGFWwSvoLcPORhV0.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f77784dc12aee9b44f5def7b81176eaff04e2195" title="New Karpathy's video: How I use LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not as techical as his past videos, but still lots of nice insights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/EWvNQjAaOHw?si=lixNIZJRppLshiw9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iztn9l/new_karpathys_video_how_i_use_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iztn9l/new_karpathys_video_how_i_use_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T23:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1izbmbb</id>
    <title>Perplexity R1 1776 performs worse than DeepSeek R1 for complex problems.</title>
    <updated>2025-02-27T09:06:25+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perplexity claims the reasoning abilities of R1 1776 are not affected by the decensoring process, but after testing it in &lt;a href="https://github.com/fairydreaming/lineage-bench/"&gt;lineage-bench&lt;/a&gt; I found that for very complex problems there are significant differences in the model performance.&lt;/p&gt; &lt;p&gt;Below you can see benchmark results for different problem sizes:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;lineage-8&lt;/th&gt; &lt;th align="left"&gt;lineage-16&lt;/th&gt; &lt;th align="left"&gt;lineage-32&lt;/th&gt; &lt;th align="left"&gt;lineage-64&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1&lt;/td&gt; &lt;td align="left"&gt;0.965&lt;/td&gt; &lt;td align="left"&gt;0.980&lt;/td&gt; &lt;td align="left"&gt;0.945&lt;/td&gt; &lt;td align="left"&gt;0.780&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;R1 1776&lt;/td&gt; &lt;td align="left"&gt;0.980&lt;/td&gt; &lt;td align="left"&gt;0.975&lt;/td&gt; &lt;td align="left"&gt;0.675&lt;/td&gt; &lt;td align="left"&gt;0.205&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While for lineage-8 and lineage-16 problem sizes the model performance matches or even exceeds the original DeepSeek R1, for lineage-32 we can already observe difference in scores, while for lineage-64 R1 1776 score reached random guessing level.&lt;/p&gt; &lt;p&gt;So it looks like Perplexity claims about reasoning abilities not being affected by the decensoring process are not true.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We also ensured that the modelâ€™s math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the decensoring had no impact on its core reasoning capabilities.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Edit: here's one example prompt for lineage-64 and the model output generated in Perplexity Labs playground in case anyone is interested: &lt;a href="https://pastebin.com/EPy06bqp"&gt;https://pastebin.com/EPy06bqp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also Perplexity staff noticed my findings and are looking into the problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izbmbb/perplexity_r1_1776_performs_worse_than_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T09:06:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1izsmu7</id>
    <title>Anyone tried Granite3.2 yet?</title>
    <updated>2025-02-27T22:39:18+00:00</updated>
    <author>
      <name>/u/Hujkis9</name>
      <uri>https://old.reddit.com/user/Hujkis9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izsmu7/anyone_tried_granite32_yet/"&gt; &lt;img alt="Anyone tried Granite3.2 yet?" src="https://external-preview.redd.it/0c14HAxPkb4aDj5vcS7W2_hQsWUHo_3ZSJZ9EO23nF8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb6c6699299da5a1633be975fbfd08d1875472c9" title="Anyone tried Granite3.2 yet?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hujkis9"&gt; /u/Hujkis9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.ibm.com/blog/inference-scaling-reasoning-ai-model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izsmu7/anyone_tried_granite32_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izsmu7/anyone_tried_granite32_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T22:39:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1izy4by</id>
    <title>2 diffusion LLMs in one day -&gt; don't undermine the underdog</title>
    <updated>2025-02-28T03:12:18+00:00</updated>
    <author>
      <name>/u/dp3471</name>
      <uri>https://old.reddit.com/user/dp3471</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First, its awesome that we're getting frequent and amazing model releases - seemingly by the day right now. &lt;/p&gt; &lt;p&gt;Inception labs released&lt;a href="https://www.inceptionlabs.ai/news"&gt; mercury coder&lt;/a&gt;, a (by my testing) somewhat competent model which can code on a 1 to 2 year old SOTA (as good as the best models 1-2 years ago), having the benefit of being really cool to see the diffusion process. Really scratches an itch (perhaps one of some interpretability?). &lt;strong&gt;Promises 700-1000 t/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Reason why I say time period instead of model - it suffers from many of the same issues I remember GPT 4 (and turbo) suffering from. You should check it out anyways.&lt;/p&gt; &lt;p&gt;And, for some reason on the same day (at least model weights uploaded, preprint earlier), we get &lt;a href="https://arxiv.org/pdf/2502.09992"&gt;LLaDA&lt;/a&gt;, an open-source diffusion model which seems to be somewhat of a contender for llama 3 8b with benchmarks, and gives some degree of freedom in terms of guiding (not forcing, sometimes doesn't work) the nth word to be a specified one. I found the quality in the &lt;a href="https://huggingface.co/spaces/multimodalart/LLaDA"&gt;demo &lt;/a&gt;to be much worse than any recent models, but I also noticed it improved a TON as I played around and adjusted certain prompting (and word targets, really cool). Check this out too - its different from mercury. &lt;/p&gt; &lt;p&gt;TLDR; 2 cool new diffusion-based LLMs, a closed-source one comparable to GPT-4 (based on my vibe checking) promising 700-1000 t/s (technically 2 different models by size), and an open-source one reported to be LLaMa3.1-8b-like, but testing (again, mine only) shows more testing is needed lol.&lt;/p&gt; &lt;p&gt;Don't let the open source model be overshadowed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dp3471"&gt; /u/dp3471 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy4by/2_diffusion_llms_in_one_day_dont_undermine_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izy4by/2_diffusion_llms_in_one_day_dont_undermine_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izy4by/2_diffusion_llms_in_one_day_dont_undermine_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T03:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm9pu</id>
    <title>I created this tool I named Reddit Thread Analyzer â€“ just paste a link, tweak a few settings, and get a detailed thread analysis. It's open-source and freely hosted.</title>
    <updated>2025-02-27T18:09:20+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm9pu/i_created_this_tool_i_named_reddit_thread/"&gt; &lt;img alt="I created this tool I named Reddit Thread Analyzer â€“ just paste a link, tweak a few settings, and get a detailed thread analysis. It's open-source and freely hosted." src="https://external-preview.redd.it/bWhwMHFib2gycWxlMYR5n1H2-KAhKVY699t1y87JffT7MDLWeztGuBbhiJoR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ce3d64ee8a302c6a140ce04850b2b4f5ed3d45c" title="I created this tool I named Reddit Thread Analyzer â€“ just paste a link, tweak a few settings, and get a detailed thread analysis. It's open-source and freely hosted." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rs0obaoh2qle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm9pu/i_created_this_tool_i_named_reddit_thread/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izm9pu/i_created_this_tool_i_named_reddit_thread/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1izdrsd</id>
    <title>vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days</title>
    <updated>2025-02-27T11:38:48+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"&gt; &lt;img alt="vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days" src="https://preview.redd.it/wnphfz5s4ole1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d23c35465c203ce3194ca69901bcbe56c0961102" title="vLLM just landed FlashMLA (DeepSeek - day 1) in vLLM and it is already boosting output throughput 2-16% - expect more improvements in the coming days" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wnphfz5s4ole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izdrsd/vllm_just_landed_flashmla_deepseek_day_1_in_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T11:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1izm42j</id>
    <title>What is Aider?</title>
    <updated>2025-02-27T18:03:05+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm42j/what_is_aider/"&gt; &lt;img alt="What is Aider?" src="https://preview.redd.it/6kgjr75i1qle1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a96cf91fad0adc3fea1292d175b92cbc64800ec" title="What is Aider?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seriously, what is Aider? Is it a model? Or a benchmark? Or a cli? Or a browser extension? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6kgjr75i1qle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izm42j/what_is_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izm42j/what_is_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1izqu52</id>
    <title>Its ARC-AGI | DeepSeek R1 is better than GPT 4.5</title>
    <updated>2025-02-27T21:20:59+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqu52/its_arcagi_deepseek_r1_is_better_than_gpt_45/"&gt; &lt;img alt="Its ARC-AGI | DeepSeek R1 is better than GPT 4.5" src="https://preview.redd.it/f2320u7s0rle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7d73253b157077c71953201a581eb62570026ee" title="Its ARC-AGI | DeepSeek R1 is better than GPT 4.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f2320u7s0rle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqu52/its_arcagi_deepseek_r1_is_better_than_gpt_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izqu52/its_arcagi_deepseek_r1_is_better_than_gpt_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T21:20:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1izfy2d</id>
    <title>LLaDA - Large Language Diffusion Model (weights + demo)</title>
    <updated>2025-02-27T13:36:28+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;HF Demo:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/multimodalart/LLaDA"&gt;https://huggingface.co/spaces/multimodalart/LLaDA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct"&gt;https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Base"&gt;https://huggingface.co/GSAI-ML/LLaDA-8B-Base&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.09992"&gt;https://arxiv.org/abs/2502.09992&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Diffusion LLMs are looking promising for alternative architecture. Some lab also recently announced a proprietary one (inception) which you could test, it can generate code quite well. &lt;/p&gt; &lt;p&gt;This stuff comes with the promise of parallelized token generation.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;LLaDA predicts all masked tokens simultaneously during each step of the reverse process.&amp;quot; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we wouldn't need super high bandwidth for fast t/s anymore. It's not memory bandwidth bottlenecked, it has a compute bottleneck. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izfy2d/llada_large_language_diffusion_model_weights_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T13:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1izqhfw</id>
    <title>Any theories on what's going on here for this coding benchmark?</title>
    <updated>2025-02-27T21:05:52+00:00</updated>
    <author>
      <name>/u/__eita__</name>
      <uri>https://old.reddit.com/user/__eita__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqhfw/any_theories_on_whats_going_on_here_for_this/"&gt; &lt;img alt="Any theories on what's going on here for this coding benchmark?" src="https://preview.redd.it/uc4k9x64yqle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e3fc22fdd5fdbf678a6a10b6d9392048ae32f70" title="Any theories on what's going on here for this coding benchmark?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why a reasoning model would perform way better for swe-bench verified while performing poorly for swe-lancer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__eita__"&gt; /u/__eita__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uc4k9x64yqle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izqhfw/any_theories_on_whats_going_on_here_for_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izqhfw/any_theories_on_whats_going_on_here_for_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T21:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ize4n0</id>
    <title>Dual 5090FE</title>
    <updated>2025-02-27T12:01:15+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"&gt; &lt;img alt="Dual 5090FE" src="https://preview.redd.it/defh49ux8ole1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=843767edca5506f54e0bdb3a8b57d7e022c97a89" title="Dual 5090FE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/defh49ux8ole1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ize4n0/dual_5090fe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1izwh49</id>
    <title>DeepSeek OpenSourceWeek Day 5</title>
    <updated>2025-02-28T01:45:14+00:00</updated>
    <author>
      <name>/u/EssayHealthy5075</name>
      <uri>https://old.reddit.com/user/EssayHealthy5075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fire-Flyer File System (3FS)&lt;/p&gt; &lt;p&gt;Fire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.&lt;/p&gt; &lt;p&gt;âš¡ 6.6 TiB/s aggregate read throughput in a 180-node cluster.&lt;/p&gt; &lt;p&gt;âš¡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster.&lt;/p&gt; &lt;p&gt;âš¡ 40+ GiB/s peak throughput per client node for KVCache lookup.&lt;/p&gt; &lt;p&gt;ðŸ§¬ Disaggregated architecture with strong consistency semantics.&lt;/p&gt; &lt;p&gt;âœ… Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search &amp;amp; KVCache lookups for inference in V3/R1.&lt;/p&gt; &lt;p&gt;ðŸ”— 3FS â†’ &lt;a href="https://github.com/deepseek-ai/3FS"&gt;https://github.com/deepseek-ai/3FS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Smallpond - data processing framework on 3FS â†’ &lt;a href="https://github.com/deepseek-ai/smallpond"&gt;https://github.com/deepseek-ai/smallpond&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EssayHealthy5075"&gt; /u/EssayHealthy5075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izwh49/deepseek_opensourceweek_day_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T01:45:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1izmsrl</id>
    <title>Building a robot that can see, hear, talk, and dance. Powered by on-device AI!</title>
    <updated>2025-02-27T18:31:16+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izmsrl/building_a_robot_that_can_see_hear_talk_and_dance/"&gt; &lt;img alt="Building a robot that can see, hear, talk, and dance. Powered by on-device AI!" src="https://external-preview.redd.it/Y3JjNzRwc2k2cWxlMe__omCO_n66cYU7Fe7wXFz05iYznG-U5sQ5kSodSfXF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3b1aca9ec3aa181f1b6214eda1a89059c04a5ab" title="Building a robot that can see, hear, talk, and dance. Powered by on-device AI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h13dgnsi6qle1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izmsrl/building_a_robot_that_can_see_hear_talk_and_dance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izmsrl/building_a_robot_that_can_see_hear_talk_and_dance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T18:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1izt03h</id>
    <title>I have to share this with you - Free-Form Chat for writing, 100% local</title>
    <updated>2025-02-27T22:55:55+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izt03h/i_have_to_share_this_with_you_freeform_chat_for/"&gt; &lt;img alt="I have to share this with you - Free-Form Chat for writing, 100% local" src="https://preview.redd.it/7781ripihrle1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=503141823b634142caf9d0103c96974965baa8ef" title="I have to share this with you - Free-Form Chat for writing, 100% local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7781ripihrle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izt03h/i_have_to_share_this_with_you_freeform_chat_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izt03h/i_have_to_share_this_with_you_freeform_chat_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T22:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1izvwck</id>
    <title>DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp; smallpond (A lightweight data processing framework)</title>
    <updated>2025-02-28T01:15:12+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt; &lt;img alt="DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp;amp; smallpond (A lightweight data processing framework)" src="https://external-preview.redd.it/HvC95tBfvHDGJxAbUH6W9PmwC54Tm2U3z7QQDPE9EaM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f064ebf3053086f57b27efe553869e937081d60d" title="DeepSeek Realse 5th Bomb! Cluster Bomb Again! 3FS (distributed file system) &amp;amp; smallpond (A lightweight data processing framework)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't believe DeepSeek has even revolutionized storage architecture... The last time I was amazed by a network file system was with HDFS and CEPH. But those are disk-oriented distributed file systems. Now, a truly modern SSD and RDMA network-oriented file system has been born!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3FS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/3FS"&gt;https://github.com/deepseek-ai/3FS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;smallpond&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A lightweight data processing framework built on &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt; and &lt;a href="https://github.com/deepseek-ai/3FS"&gt;3FS&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://github.com/deepseek-ai/smallpond"&gt;https://github.com/deepseek-ai/smallpond&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/inqemmkh6sle1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f451f20a22278229810505083e59b914b64fd82"&gt;https://preview.redd.it/inqemmkh6sle1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1f451f20a22278229810505083e59b914b64fd82&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izvwck/deepseek_realse_5th_bomb_cluster_bomb_again_3fs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T01:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1izf4zf</id>
    <title>Pythagoras : i should've guessed first hand ðŸ˜© !</title>
    <updated>2025-02-27T12:59:00+00:00</updated>
    <author>
      <name>/u/BidHot8598</name>
      <uri>https://old.reddit.com/user/BidHot8598</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt; &lt;img alt="Pythagoras : i should've guessed first hand ðŸ˜© !" src="https://preview.redd.it/m3vrfaz8jole1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e5f34f13dda8627c1b31cbceebdf6cfb503c19e" title="Pythagoras : i should've guessed first hand ðŸ˜© !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BidHot8598"&gt; /u/BidHot8598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m3vrfaz8jole1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izf4zf/pythagoras_i_shouldve_guessed_first_hand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T12:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1izoyxk</id>
    <title>A diffusion based 'small' coding LLM that is 10x faster in token generation than transformer based LLMs (apparently 1000 tok/s on H100)</title>
    <updated>2025-02-27T20:01:26+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Karpathy post: &lt;a href="https://xcancel.com/karpathy/status/1894923254864978091"&gt;https://xcancel.com/karpathy/status/1894923254864978091&lt;/a&gt; (covers some interesting nuance about transformer vs diffusion for image/video vs text)&lt;/p&gt; &lt;p&gt;Artificial analysis comparison: &lt;a href="https://pbs.twimg.com/media/GkvZinZbAAABLVq.jpg?name=orig"&gt;https://pbs.twimg.com/media/GkvZinZbAAABLVq.jpg?name=orig&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo video: &lt;a href="https://xcancel.com/InceptionAILabs/status/1894847919624462794"&gt;https://xcancel.com/InceptionAILabs/status/1894847919624462794&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The chat link (down rn, probably over capacity) &lt;a href="https://chat.inceptionlabs.ai/"&gt;https://chat.inceptionlabs.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's interesting here is that this thing generates all tokens at once and then goes through refinements as opposed to transformer based one token at a time. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izoyxk/a_diffusion_based_small_coding_llm_that_is_10x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-27T20:01:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1izw4q6</id>
    <title>Meme updated for 2025</title>
    <updated>2025-02-28T01:27:11+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izw4q6/meme_updated_for_2025/"&gt; &lt;img alt="Meme updated for 2025" src="https://preview.redd.it/8r9nhakq8sle1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31b5ddb71761bd275281c07aa8ad8c55def4a1af" title="Meme updated for 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8r9nhakq8sle1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1izw4q6/meme_updated_for_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1izw4q6/meme_updated_for_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-28T01:27:11+00:00</published>
  </entry>
</feed>
