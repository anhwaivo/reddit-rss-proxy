<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-04T15:49:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n83kzo</id>
    <title>Please help, anyone here has archived the Google Colab notebook of VibeVoice ?</title>
    <updated>2025-09-04T07:23:53+00:00</updated>
    <author>
      <name>/u/CesarOverlorde</name>
      <uri>https://old.reddit.com/user/CesarOverlorde</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n83kzo/please_help_anyone_here_has_archived_the_google/"&gt; &lt;img alt="Please help, anyone here has archived the Google Colab notebook of VibeVoice ?" src="https://preview.redd.it/g585evlpn3nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0ea9a57e461437341492da5152dbc3c10e88e47" title="Please help, anyone here has archived the Google Colab notebook of VibeVoice ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I only have a very weak laptop that can't run the model locally unfortunately. If anyone archived this notebook I would really appreciate if you can share it. Thank you in advance!&lt;/p&gt; &lt;p&gt;I tried accessing it using wayback machine but it's just white blank page.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CesarOverlorde"&gt; /u/CesarOverlorde &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g585evlpn3nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n83kzo/please_help_anyone_here_has_archived_the_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n83kzo/please_help_anyone_here_has_archived_the_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T07:23:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n81d1t</id>
    <title>[Level 0] Fine-tuned my first personal chatbot</title>
    <updated>2025-09-04T05:06:17+00:00</updated>
    <author>
      <name>/u/FastCommission2913</name>
      <uri>https://old.reddit.com/user/FastCommission2913</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"&gt; &lt;img alt="[Level 0] Fine-tuned my first personal chatbot" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="[Level 0] Fine-tuned my first personal chatbot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wrapped up my first LLM fine-tuning project and wanted to share the experience since I learned a ton. Used Unsloth + `cognitivecomputations/dolphin-2.9-llama3-8b` with around 1400 custom examples about myself, trained on Colab's free T4 GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How I learnt:&lt;/strong&gt; I knew the basics of LoRA and QLoRA since we were never taught the practical. I am a self taught with a medical condition. Rest I followed the steps of ChatGPT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Generated dataset using ChatGPT by providing it with my personal info (background, interests, projects, etc.). Formatted as simple question-answer pairs in JSONL. Used LoRA with r=16, trained for 300 steps (~20 minutes), ended with loss around 0.74.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/39hnvx6zl2nf1.png?width=2394&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0d0c1bcdd0ea06139760b8817ac64939070008c"&gt;This is what my current dataset looks like.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: Model went from generic &amp;quot;I'm an AI assistant created by...&amp;quot; to actually knowing I'm Sohaib Ahmed, ..... grad from ...., into anime (1794 watched according to my Anilist), gaming (Genshin Impact, ZZZ), and that I built InSightAI library with minimal PyPI downloads. Responses sound natural and match my personality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What worked&lt;/strong&gt;: Llama 3.1 8B base model was solid. Dataset quality mattered more than quantity. Unsloth made everything stupid fast and memory-efficient.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Issues hit&lt;/strong&gt;: Tried Mistral 7B first but got incomplete responses (&amp;quot;I am and I do&amp;quot;). Safety triggers still override on certain phrases - asking about &amp;quot;abusive language&amp;quot; makes it revert to generic safety mode instead of answering as me. Occasionally hallucinates experiences I never had when answering general knowledge questions.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Next steps&lt;/strong&gt;: &amp;quot;I don't know&amp;quot; boundary examples to fix the hallucination issue. How do I make it so that it says &amp;quot;I don't know&amp;quot; for other general purpose questions? How can I improve it further?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Level 1 (based on my idiotic knowledge): I want to learn how can I make the text summarization personalized. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Final model actually passes the &amp;quot;tell me about yourself&amp;quot; test convincingly. Pretty solid for a first attempt.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Colab notebook:&lt;/strong&gt; &lt;a href="https://colab.research.google.com/drive/1Az3gFYEKSzPouxrhvES7v5oafyhnm80v?usp=sharing"&gt;https://colab.research.google.com/drive/1Az3gFYEKSzPouxrhvES7v5oafyhnm80v?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Confusions:&lt;/strong&gt; I don't know much on hosting/ deploying a Local LLM. Following are my specs: &lt;strong&gt;MacBook Pro with Apple M4 chip, 16GB RAM, and an Apple M4 GPU with 10 cores&lt;/strong&gt;. I only know that I can run any LLM &amp;lt; 16GB but don't know any good yet to do the tool calling and all that stuff. I want to make something with it.&lt;/p&gt; &lt;p&gt;So, sorry in advance if my Colab Notebook's code is messy. Any advice would be a appreciated. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastCommission2913"&gt; /u/FastCommission2913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n81d1t/level_0_finetuned_my_first_personal_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T05:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8bgtr</id>
    <title>[2507.14799] Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree</title>
    <updated>2025-09-04T14:19:08+00:00</updated>
    <author>
      <name>/u/Salt_Comfort6099</name>
      <uri>https://old.reddit.com/user/Salt_Comfort6099</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt_Comfort6099"&gt; /u/Salt_Comfort6099 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.14799"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8bgtr/250714799_manipulating_llm_web_agents_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8bgtr/250714799_manipulating_llm_web_agents_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n88sqb</id>
    <title>Deploying 1.4KW GPUs (B300) what's the biggest bottleneck you've seen power delivery or cooling?</title>
    <updated>2025-09-04T12:29:53+00:00</updated>
    <author>
      <name>/u/DingoOutrageous7124</name>
      <uri>https://old.reddit.com/user/DingoOutrageous7124</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most people see a GPU cluster and think about FLOPS. What’s been killing us lately is the supporting infrastructure.&lt;/p&gt; &lt;p&gt;Each B300 pulls ~1,400W. That’s 40+ W/cm² of heat in a small footprint. Air cooling stops being viable past ~800W, so at this density you need DLC (direct liquid cooling).&lt;/p&gt; &lt;p&gt;Power isn’t easier a single rack can hit 25kW+. That means 240V circuits, smart PDUs, and hundreds of supercaps just to keep power stable.&lt;/p&gt; &lt;p&gt;And the dumbest failure mode? A $200 thermal sensor installed wrong can kill a $2M deployment.&lt;/p&gt; &lt;p&gt;It feels like the semiconductor roadmap has outpaced the “boring” stuff power and cooling engineering.&lt;/p&gt; &lt;p&gt;For those who’ve deployed or worked with high-density GPU clusters (1kW+ per device), what’s been the hardest to scale reliably:&lt;/p&gt; &lt;p&gt;Power distribution and transient handling?&lt;/p&gt; &lt;p&gt;Cooling (DLC loops, CDU redundancy, facility water integration)?&lt;/p&gt; &lt;p&gt;Or something else entirely (sensoring, monitoring, failure detection)?&lt;/p&gt; &lt;p&gt;Would love to hear real-world experiences especially what people overlooked on their first large-scale deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DingoOutrageous7124"&gt; /u/DingoOutrageous7124 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n88sqb/deploying_14kw_gpus_b300_whats_the_biggest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n88sqb/deploying_14kw_gpus_b300_whats_the_biggest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n88sqb/deploying_14kw_gpus_b300_whats_the_biggest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T12:29:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8b7ex</id>
    <title>yeah, intel b50 is bad. but is the b60 not amazing?</title>
    <updated>2025-09-04T14:09:22+00:00</updated>
    <author>
      <name>/u/No-Tiger3430</name>
      <uri>https://old.reddit.com/user/No-Tiger3430</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The intel b50 is $350 USD, not amazing when you can get a 5060 ti 16gb with double the memory bandwidth for $60 more, however is the b60 not amazing? its 24gb for the base model (you can get a 2 die version with 48gb of VRAM) and it actually has a decent memory bandwidth, even more than the 5060 ti. Pricing is still unknown but rumoured to be ~$600 USD (24gb) and ~$1100 USD for the 2 die (48gb)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Tiger3430"&gt; /u/No-Tiger3430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8b7ex/yeah_intel_b50_is_bad_but_is_the_b60_not_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8b7ex/yeah_intel_b50_is_bad_but_is_the_b60_not_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8b7ex/yeah_intel_b50_is_bad_but_is_the_b60_not_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:09:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7fdy4</id>
    <title>Introducing Kimi K2-0905</title>
    <updated>2025-09-03T13:51:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt; &lt;img alt="Introducing Kimi K2-0905" src="https://b.thumbs.redditmedia.com/lyzeYJ2XI6oIjcCbfXBgsYvdUpg2tM8OGWtELhu--Xc.jpg" title="Introducing Kimi K2-0905" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's new:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8"&gt;https://preview.redd.it/u8oxbcfyfymf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=87daf02d6f257631f0a0a8847de7180dc9d9eed8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7fdy4/introducing_kimi_k20905/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T13:51:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7l5kg</id>
    <title>Intel launches Arc Pro B50 graphics card at $349</title>
    <updated>2025-09-03T17:27:29+00:00</updated>
    <author>
      <name>/u/levian_</name>
      <uri>https://old.reddit.com/user/levian_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt; &lt;img alt="Intel launches Arc Pro B50 graphics card at $349" src="https://preview.redd.it/357rwwhaizmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=066c5073e108f00168fb16f32dcc905e00df9cae" title="Intel launches Arc Pro B50 graphics card at $349" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Initial review, source:&lt;a href="https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349"&gt;https://videocardz.com/newz/intel-launches-arc-pro-b50-graphics-card-at-349&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/levian_"&gt; /u/levian_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/357rwwhaizmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7l5kg/intel_launches_arc_pro_b50_graphics_card_at_349/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T17:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7mien</id>
    <title>Best current NSFW TTS model?</title>
    <updated>2025-09-03T18:17:17+00:00</updated>
    <author>
      <name>/u/Stock-Fault5734</name>
      <uri>https://old.reddit.com/user/Stock-Fault5734</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which one? And how to use it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Fault5734"&gt; /u/Stock-Fault5734 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7mien/best_current_nsfw_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T18:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7wh65</id>
    <title>VibeVoice Gone?</title>
    <updated>2025-09-04T01:00:53+00:00</updated>
    <author>
      <name>/u/atrfx</name>
      <uri>https://old.reddit.com/user/atrfx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like the GitHub page and the huggingface page are gone. The huggingface only has the 1.5B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Modelscope still has it (for now) &lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/microsoft/VibeVoice-Large/summary"&gt;https://modelscope.cn/models/microsoft/VibeVoice-Large/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atrfx"&gt; /u/atrfx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7wh65/vibevoice_gone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7wh65/vibevoice_gone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7wh65/vibevoice_gone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T01:00:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8broq</id>
    <title>Welcome to the Battleslop benchmark !</title>
    <updated>2025-09-04T14:30:27+00:00</updated>
    <author>
      <name>/u/Qual_</name>
      <uri>https://old.reddit.com/user/Qual_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8broq/welcome_to_the_battleslop_benchmark/"&gt; &lt;img alt="Welcome to the Battleslop benchmark !" src="https://b.thumbs.redditmedia.com/7LMTHtje613VV-pOlyfZ0U6iB6wm0SPEini6of6HDig.jpg" title="Welcome to the Battleslop benchmark !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/irn9nf4lp5nf1.png?width=2536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4d608d01cbea744b11caf4d3553921c5dddbf24"&gt;https://preview.redd.it/irn9nf4lp5nf1.png?width=2536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4d608d01cbea744b11caf4d3553921c5dddbf24&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to see if GPT-OSS 20B can handle tool calls + some spatial reasoning. Battleship alone was boring… so I added cards + mana.&lt;/p&gt; &lt;p&gt;Now it’s not just coordinates anymore. It’s attacks, defenses, tempo swings, fog, scans, mines, shields… and NUKES. 🚢🔥&lt;/p&gt; &lt;p&gt;I used Grok Code Fast as cheap baseline, here’s some matches:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPT-OSS 20B vs Grok Code Fast → 3–3&lt;/li&gt; &lt;li&gt;GPT-5 nano vs Grok Code Fast → 0–3&lt;/li&gt; &lt;li&gt;GPT-OSS 120B vs Grok Code Fast → 4–2&lt;/li&gt; &lt;li&gt;GPT-5 vs Grok Code Fast → 6–0&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;( I did way way more matches during dev but winrates were pretty similar ) &lt;/p&gt; &lt;p&gt;20B is way stronger than I thought, tool-calls are reliable (after some wrangling w/ Ollama/OpenRouter/vLLM/LM Studio). It's very fast !&lt;/p&gt; &lt;p&gt;I also tested vs a pretty strong heuristic bot: 20B usually loses but only by a small margin, while 120B does better (probably just better at chaining smart combos + tempo stuff).&lt;/p&gt; &lt;p&gt;So question: what matches do you want to see next? (models needs to support tool calls)&lt;/p&gt; &lt;p&gt;I'm using ai sdk, ollama and openrouter. &lt;/p&gt; &lt;p&gt;Fun fact: it started as just plain Battleship. Then I kept adding more stuff. At some point I wanted to play vs the LLM, so I added that. Then I was like, why not also make it so I can play with friends too? Long story short… we actually enjoy the game now lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qual_"&gt; /u/Qual_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8broq/welcome_to_the_battleslop_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8broq/welcome_to_the_battleslop_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8broq/welcome_to_the_battleslop_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8aqi8</id>
    <title>BenderNet - A demonstration app for using Qwen3 1.7b q4f16 with web-llm</title>
    <updated>2025-09-04T13:52:59+00:00</updated>
    <author>
      <name>/u/gajananpp</name>
      <uri>https://old.reddit.com/user/gajananpp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8aqi8/bendernet_a_demonstration_app_for_using_qwen3_17b/"&gt; &lt;img alt="BenderNet - A demonstration app for using Qwen3 1.7b q4f16 with web-llm" src="https://external-preview.redd.it/emFycnp1bDdqNW5mMcFeSbd7MPl-hlbSK9XDmWZGPdomW8w2E4v5E_699wws.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76699af374829dfe961bd28c0ea2d4ee12cd761a" title="BenderNet - A demonstration app for using Qwen3 1.7b q4f16 with web-llm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This app runs client-side thanks to an awesome tech stack:&lt;/p&gt; &lt;p&gt;𝐌𝐨𝐝𝐞𝐥: Qwen3-1.7b (q4f16)&lt;/p&gt; &lt;p&gt;𝐄𝐧𝐠𝐢𝐧𝐞: MLC's WebLLM engine for in-browser inference&lt;/p&gt; &lt;p&gt;𝐑𝐮𝐧𝐭𝐢𝐦𝐞: LangGraph Web &lt;/p&gt; &lt;p&gt;𝐀𝐫𝐜𝐡𝐢𝐭𝐞𝐜𝐭𝐮𝐫𝐞: Two separate web workers—one for the model and one for the Python-based Lark parser.&lt;/p&gt; &lt;p&gt;𝐔𝐈: assistant-ui&lt;/p&gt; &lt;p&gt;App Link: &lt;a href="https://bendernet.vercel.app"&gt;https://bendernet.vercel.app&lt;/a&gt;&lt;br /&gt; Github Link: &lt;a href="https://github.com/gajananpp/bendernet"&gt;https://github.com/gajananpp/bendernet&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7369358620875993088/"&gt;Original LinkedIn Post&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gajananpp"&gt; /u/gajananpp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u44geul7j5nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8aqi8/bendernet_a_demonstration_app_for_using_qwen3_17b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8aqi8/bendernet_a_demonstration_app_for_using_qwen3_17b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c37s</id>
    <title>Introducing FineVision: a huge open-source dataset for training SOTA Vision Language Models</title>
    <updated>2025-09-04T14:42:36+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c37s/introducing_finevision_a_huge_opensource_dataset/"&gt; &lt;img alt="Introducing FineVision: a huge open-source dataset for training SOTA Vision Language Models" src="https://external-preview.redd.it/Kk3FbZHykZyAJhqYa4Z4NYO9s55fzcUILVr6lnVjZ8c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=239545df9819cc604424b2eb0f34dd7990b2642f" title="Introducing FineVision: a huge open-source dataset for training SOTA Vision Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/bcwfesjot5nf1.png?width=2320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a30c3d1822dc2d3b3eed3ee37ad0237aed74a50"&gt;https://preview.redd.it/bcwfesjot5nf1.png?width=2320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a30c3d1822dc2d3b3eed3ee37ad0237aed74a50&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;gt; 17.3M images&lt;br /&gt; &amp;gt; 24.3M samples&lt;br /&gt; &amp;gt; 88.9M turns&lt;br /&gt; &amp;gt; 9.5B answer tokens&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/HuggingFaceM4/FineVision"&gt;Blog Post&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision"&gt;Dataset&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c37s/introducing_finevision_a_huge_opensource_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c37s/introducing_finevision_a_huge_opensource_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c37s/introducing_finevision_a_huge_opensource_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:42:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7zfj5</id>
    <title>built and trained this 103M MoE from scratch - went good</title>
    <updated>2025-09-04T03:21:57+00:00</updated>
    <author>
      <name>/u/External_Mushroom978</name>
      <uri>https://old.reddit.com/user/External_Mushroom978</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zfj5/built_and_trained_this_103m_moe_from_scratch_went/"&gt; &lt;img alt="built and trained this 103M MoE from scratch - went good" src="https://preview.redd.it/vqtopd08g2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b650bedc8fac15fe5bfe8adb9d475eb26ed8f5bb" title="built and trained this 103M MoE from scratch - went good" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i made this model a few weeks ago and experimented with SFT and LoRA. &lt;/p&gt; &lt;p&gt;technical report - &lt;a href="https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf"&gt;https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf&lt;/a&gt;&lt;br /&gt; you could find the full source code and weights here - &lt;a href="https://github.com/Abinesh-Mathivanan/beens-minimax"&gt;https://github.com/Abinesh-Mathivanan/beens-minimax&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mushroom978"&gt; /u/External_Mushroom978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vqtopd08g2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zfj5/built_and_trained_this_103m_moe_from_scratch_went/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zfj5/built_and_trained_this_103m_moe_from_scratch_went/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89ryn</id>
    <title>Most affordable AI computer with GPU (“GPUter”) you can build in 2025?</title>
    <updated>2025-09-04T13:13:12+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89ryn/most_affordable_ai_computer_with_gpu_gputer_you/"&gt; &lt;img alt="Most affordable AI computer with GPU (“GPUter”) you can build in 2025?" src="https://preview.redd.it/bk6tf5l2e5nf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da7afc16f4d8ff260c98ad24de5cc8adc50a222" title="Most affordable AI computer with GPU (“GPUter”) you can build in 2025?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After a bunch of testing and experiments, we landed on what looks like the best price-to-performance build you can do right now (using all new parts in the US, 2025). Total spend: $1,040.&lt;/p&gt; &lt;p&gt;That’s the actual GPUter in the photo — whisper-quiet but surprisingly powerful.&lt;/p&gt; &lt;p&gt;Parts list:&lt;/p&gt; &lt;p&gt;GPU: NVIDIA RTX 5060 Ti 16GB Blackwell (759 AI TOPS) – $429 &lt;a href="https://newegg.com/p/N82E16814932791"&gt;https://newegg.com/p/N82E16814932791&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Motherboard: B550M – $99 &lt;a href="https://amazon.com/dp/B0BDCZRBD6"&gt;https://amazon.com/dp/B0BDCZRBD6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CPU: AMD Ryzen 5 5500 – $60 &lt;a href="https://amazon.com/dp/B09VCJ171S"&gt;https://amazon.com/dp/B09VCJ171S&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RAM: 32GB DDR4 (2×16GB) – $52 &lt;a href="https://amazon.com/dp/B07RW6Z692"&gt;https://amazon.com/dp/B07RW6Z692&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Storage: M.2 SSD 4TB – $249 &lt;a href="https://amazon.com/dp/B0DHLBDSP7"&gt;https://amazon.com/dp/B0DHLBDSP7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Case: JONSBO/JONSPLUS Z20 mATX – $109 &lt;a href="https://amazon.com/dp/B0D1YKXXJD"&gt;https://amazon.com/dp/B0D1YKXXJD&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PSU: 600W – $42 &lt;a href="https://amazon.com/dp/B014W3EMAO"&gt;https://amazon.com/dp/B014W3EMAO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Grand total: $1,040&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Note: configs can vary, and you can go wild if you want (e.g. check out used AMD EPYC CPUs on eBay - 128 vCPUs for cheap 😉)&lt;/p&gt; &lt;p&gt;In terms of memory, here’s what this build gives you:&lt;/p&gt; &lt;p&gt;⚡ 16 GB of GDDR7 VRAM on the GPU with 448 GB/s bandwidth&lt;/p&gt; &lt;p&gt;🖥️ 32 GB of DDR4 RAM on the CPU side (dual channel) with ~51 GB/s bandwidth&lt;/p&gt; &lt;p&gt;On our workloads, GPU VRAM runs at about 86% utilization, while CPU RAM sits around 50% usage.&lt;/p&gt; &lt;p&gt;This machine also boots straight into AI workloads using the AI-optimized Linux distro Sbnb Linux: &lt;a href="https://github.com/sbnb-io/sbnb"&gt;https://github.com/sbnb-io/sbnb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💡 &lt;strong&gt;What can this thing actually do?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We used this exact setup in our Google Gemma3n Hackathon submission — it was able to process 16 live security camera feeds with real-time video understanding: &lt;a href="https://kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sixth-sense-for-security-guards-powered-by-googles"&gt;https://kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sixth-sense-for-security-guards-powered-by-googles&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy building if anyone wants to replicate! Feel free to share your configs and findings 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6tf5l2e5nf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89ryn/most_affordable_ai_computer_with_gpu_gputer_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89ryn/most_affordable_ai_computer_with_gpu_gputer_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:13:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n860bg</id>
    <title>DeepSeek Targets AI Agent Release by End of Year to Rival OpenAI</title>
    <updated>2025-09-04T10:01:10+00:00</updated>
    <author>
      <name>/u/alanwong</name>
      <uri>https://old.reddit.com/user/alanwong</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n860bg/deepseek_targets_ai_agent_release_by_end_of_year/"&gt; &lt;img alt="DeepSeek Targets AI Agent Release by End of Year to Rival OpenAI" src="https://external-preview.redd.it/zq3vtY7JidZAIOIb5HnnpP8-DLavzSbkRkKDWz39uG0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7f396c0eb56f8886bfd55dad93a9f1d3218e5a3" title="DeepSeek Targets AI Agent Release by End of Year to Rival OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alanwong"&gt; /u/alanwong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-09-04/deepseek-targets-ai-agent-release-by-end-of-year-to-rival-openai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n860bg/deepseek_targets_ai_agent_release_by_end_of_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n860bg/deepseek_targets_ai_agent_release_by_end_of_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T10:01:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7uocj</id>
    <title>PSA: Make sure your API ports aren't exposed to the open internet</title>
    <updated>2025-09-03T23:38:13+00:00</updated>
    <author>
      <name>/u/nooclear</name>
      <uri>https://old.reddit.com/user/nooclear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are about 1,100 exposed Ollama servers out there according to this blog post:&lt;/p&gt; &lt;p&gt;&lt;a href="https://blogs.cisco.com/security/detecting-exposed-llm-servers-shodan-case-study-on-ollama"&gt;https://blogs.cisco.com/security/detecting-exposed-llm-servers-shodan-case-study-on-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, if you see the prompt &amp;quot;What is 2+2?&amp;quot; in your logs, it was Cisco.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nooclear"&gt; /u/nooclear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7uocj/psa_make_sure_your_api_ports_arent_exposed_to_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7uocj/psa_make_sure_your_api_ports_arent_exposed_to_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7uocj/psa_make_sure_your_api_ports_arent_exposed_to_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T23:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n86rl2</id>
    <title>Which is the Best LLM you can run on your hardware? Discover it with llm-eval simple</title>
    <updated>2025-09-04T10:45:34+00:00</updated>
    <author>
      <name>/u/gnorrisan</name>
      <uri>https://old.reddit.com/user/gnorrisan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n86rl2/which_is_the_best_llm_you_can_run_on_your/"&gt; &lt;img alt="Which is the Best LLM you can run on your hardware? Discover it with llm-eval simple" src="https://preview.redd.it/nsuc0la2n4nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6a9d7274af0bf58b60061454cb27096d8dcb54d" title="Which is the Best LLM you can run on your hardware? Discover it with llm-eval simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can check your prompts and get an heatmap of the most correct and fast LLMs you can run on your computer for the use-cases you care. The most intense colors means a faster reply.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/grigio/llm-eval-simple"&gt;https://github.com/grigio/llm-eval-simple&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnorrisan"&gt; /u/gnorrisan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nsuc0la2n4nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n86rl2/which_is_the_best_llm_you_can_run_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n86rl2/which_is_the_best_llm_you_can_run_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T10:45:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8abe6</id>
    <title>Eigent – Open Source, Local-First Multi-Agent Workforce</title>
    <updated>2025-09-04T13:36:11+00:00</updated>
    <author>
      <name>/u/FitHeron1933</name>
      <uri>https://old.reddit.com/user/FitHeron1933</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8abe6/eigent_open_source_localfirst_multiagent_workforce/"&gt; &lt;img alt="Eigent – Open Source, Local-First Multi-Agent Workforce" src="https://b.thumbs.redditmedia.com/o7mm3i-ghjjH2wm6PPrdvzWbqLN-x3TC3x0fwhx5pMw.jpg" title="Eigent – Open Source, Local-First Multi-Agent Workforce" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A month ago we shared &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=chatgpt.com&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Eigent&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=chatgpt.com&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;here&lt;/a&gt;, our attempt at building a fully open-source, local-first multi-agent workforce you can run on your own machine.&lt;/p&gt; &lt;p&gt;The response was amazing, and so was the feedback. Two things came up the most:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Needing to sign up before trying it&lt;/li&gt; &lt;li&gt;Concerns about the license not feeling “truly open”&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we focused on those. Now Eigent is fully local, you’ll still see a signup pipeline in the UI, but everything is stored only on your own device in a private Postgres database. Nothing leaves your machine. On the licensing side, we’ve also made updates. Eigent is now free for individuals and small teams of up to 10 users, including commercial use.&lt;/p&gt; &lt;p&gt;We’d love for you to give Eigent another try and let us know what you think. Your input is what helps us shape it into something that’s genuinely useful for developers and teams who want privacy, flexibility, and full ownership of their AI workflows, while unlocking exceptional productivity.&lt;/p&gt; &lt;p&gt;Follow the guide for setting it up locally: &lt;a href="https://github.com/eigent-ai/eigent/blob/main/server/README_EN.md"&gt;https://github.com/eigent-ai/eigent/blob/main/server/README_EN.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;→ GitHub: &lt;a href="https://github.com/eigent-ai/eigent"&gt;https://github.com/eigent-ai/eigent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;→ Download: &lt;a href="https://eigent.ai"&gt;https://eigent.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And if you find it useful, please give the repo a ⭐ and spread the word!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FitHeron1933"&gt; /u/FitHeron1933 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n8abe6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8abe6/eigent_open_source_localfirst_multiagent_workforce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8abe6/eigent_open_source_localfirst_multiagent_workforce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7z5kl</id>
    <title>Did M$ take down VibeVoice repo??</title>
    <updated>2025-09-04T03:08:12+00:00</updated>
    <author>
      <name>/u/x0rchidia</name>
      <uri>https://old.reddit.com/user/x0rchidia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7z5kl/did_m_take_down_vibevoice_repo/"&gt; &lt;img alt="Did M$ take down VibeVoice repo??" src="https://preview.redd.it/vsnyimd3e2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=277cc5e5dbf4b6c8f03d5f05352e5f7de6a92598" title="Did M$ take down VibeVoice repo??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not sure if I missed something, but &lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt; is a 404 now &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/x0rchidia"&gt; /u/x0rchidia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vsnyimd3e2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7z5kl/did_m_take_down_vibevoice_repo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7z5kl/did_m_take_down_vibevoice_repo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7zk45</id>
    <title>VibeVoice RIP? What do you think?</title>
    <updated>2025-09-04T03:28:29+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"&gt; &lt;img alt="VibeVoice RIP? What do you think?" src="https://preview.redd.it/un6uilkoh2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39144e5e650c4ae66ef8205b6d09c62f6427edad" title="VibeVoice RIP? What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past two weeks, I had been working hard to try and contribute to OpenSource AI by creating the VibeVoice nodes for ComfyUI. I’m glad to see that my contribution has helped quite a few people:&lt;br /&gt; &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;https://github.com/Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A short while ago, Microsoft suddenly deleted its official VibeVoice repository on GitHub. As of the time I’m writing this, the reason is still unknown (or at least I don’t know it).&lt;/p&gt; &lt;p&gt;At the same time, Microsoft also removed the VibeVoice-Large and VibeVoice-Large-Preview models from HF. For now, they are still available here: &lt;a href="https://modelscope.cn/models/microsoft/VibeVoice-Large/files"&gt;https://modelscope.cn/models/microsoft/VibeVoice-Large/files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, for those who have already downloaded and installed my nodes and the models, they will continue to work. Technically, I could decide to embed a copy of VibeVoice directly into my repo, but first I need to understand why Microsoft chose to remove its official repository. My hope is that they are just fixing a few things and that it will be back online soon. I also hope there won’t be any changes to the usage license...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPDATE: I have released a new 1.0.9 version that embed VibeVoice. No longer requires external VibeVoice installation.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/un6uilkoh2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n84rp5</id>
    <title>Mistral Set for $14 Billion Valuation With New Funding Round</title>
    <updated>2025-09-04T08:43:07+00:00</updated>
    <author>
      <name>/u/robberviet</name>
      <uri>https://old.reddit.com/user/robberviet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n84rp5/mistral_set_for_14_billion_valuation_with_new/"&gt; &lt;img alt="Mistral Set for $14 Billion Valuation With New Funding Round" src="https://external-preview.redd.it/M4AwBB5q0ft-ep7S9kw_Y8TYtAOJMnISlkcxXVEEP40.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=741648af7070d19298c0fb93541f0b6cf31c20b1" title="Mistral Set for $14 Billion Valuation With New Funding Round" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral has secured new funding, ensuring continued independence. No more rumors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robberviet"&gt; /u/robberviet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-09-03/mistral-set-for-14-billion-valuation-with-new-funding-round"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n84rp5/mistral_set_for_14_billion_valuation_with_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n84rp5/mistral_set_for_14_billion_valuation_with_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T08:43:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89wi8</id>
    <title>power limit your GPU(s) to reduce electricity costs</title>
    <updated>2025-09-04T13:18:43+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"&gt; &lt;img alt="power limit your GPU(s) to reduce electricity costs" src="https://a.thumbs.redditmedia.com/uyJHr0LNNqlJuU5QeZvn8UmkeX2y2aeJN7dPnMhD4t0.jpg" title="power limit your GPU(s) to reduce electricity costs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;many people worry about high electricity costs, the solution is simply power limit the GPU to about 50% of its TDP (&lt;code&gt;nvidia-smi -i $GPU_ID --power-limit=$LIMIT_IN_WATTS&lt;/code&gt;) because token generation speed does not increase past some power limit amount so you just waste electricity with the full power. As an example here is a result of &lt;code&gt;llama-bench&lt;/code&gt; (pp1024, tg1024, model Qwen3-32B Q8_0 33 GB) running on RTX Pro 6000 Workstation (600W TDP) power limited from 150W to 600W in 30W increments. 350W is the best spot for that card which is obvious on the token generation speed chart, however the prompt processing speed rise is also not linear and starts to slow down at about 350W. And another example: the best power limit for 4090 (450W TDP) is 270W, tested with Qwen3 8B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n89wi8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c56m</id>
    <title>Hugging Face open-sources FineVision</title>
    <updated>2025-09-04T14:44:45+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm Andi, the multimodal research lead at Hugging Face. We just open-sourced FineVision, the largest curation of datasets for VLMs, with over 200 sources! &lt;/p&gt; &lt;p&gt;With Finevision we have:&lt;/p&gt; &lt;p&gt;&amp;gt; 20% improvement across 10 benchmarks&lt;br /&gt; &amp;gt; 17M unique images&lt;br /&gt; &amp;gt; 10B answer tokens&lt;br /&gt; &amp;gt; New capabilities: GUI navigation, pointing, counting&lt;/p&gt; &lt;p&gt;We wrote a blog full of interesting details for the dataset, go check it out and let me know what you think :)&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/HuggingFaceM4/FineVision"&gt;https://huggingface.co/spaces/HuggingFaceM4/FineVision&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c56m/hugging_face_opensources_finevision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c56m/hugging_face_opensources_finevision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c56m/hugging_face_opensources_finevision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n82ndz</id>
    <title>Finally: 3090 Successor: 5070 Ti super 24Gb 800$</title>
    <updated>2025-09-04T06:24:02+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt; &lt;img alt="Finally: 3090 Successor: 5070 Ti super 24Gb 800$" src="https://external-preview.redd.it/kT4ohg_saogl0QowFisFMgdjPOl3cV1Xjwbw3qji8TU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01c103360d1456c04311f988c3089b01de5157d0" title="Finally: 3090 Successor: 5070 Ti super 24Gb 800$" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/j9riehskc3nf1.jpg?width=1341&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd5386a95c701b1a750a20a2b4116c93df426306"&gt;https://preview.redd.it/j9riehskc3nf1.jpg?width=1341&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd5386a95c701b1a750a20a2b4116c93df426306&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9ii4qrzfV5w"&gt;https://www.youtube.com/watch?v=9ii4qrzfV5w&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If they are well compressed in terms of energy consumption, then now it will be possible to assemble a rig with 100 gigabytes of VRAM without kilowatts of energy consumption, and we shouldn’t forget about the new FP4 formats&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T06:24:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89dy9</id>
    <title>🤷‍♂️</title>
    <updated>2025-09-04T12:56:20+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89dy9/_/"&gt; &lt;img alt="🤷‍♂️" src="https://preview.redd.it/21ivxa12b5nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e7a2744c78f03b518a206253cd3c9e861ea71c9" title="🤷‍♂️" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/21ivxa12b5nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89dy9/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89dy9/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T12:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c3l2</id>
    <title>AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more.</title>
    <updated>2025-09-04T14:43:01+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt; &lt;img alt="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." src="https://external-preview.redd.it/y8IJElEOEd_2568MHNUZQsP7_aRTCAzyzXUKpDJwl1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e377887ea8d7eae841499cc497b90b82aa97816" title="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're super excited to do this AMA. Come ask your questions to the researchers behind &lt;strong&gt;SmolLM, SmolVLM, FineWeb&lt;/strong&gt;, and more. You can learn more about our work at &lt;a href="http://hf.co/science"&gt;hf.co/science&lt;/a&gt; 🤗&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we release a new &lt;strong&gt;FineVision&lt;/strong&gt; dataset, check it out! &lt;a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision"&gt;https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eliebak"&gt;Elie Bakouch&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/loubnabnl"&gt;Loubna Ben Allal&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/loubnabnl"&gt;u/loubnabnl&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nouamanetazi"&gt;Nouamane Tazi&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Norlax_42"&gt;u/Norlax_42&lt;/a&gt; (Nanotron/SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lvwerra"&gt;Leandro von Werra&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lvwerra"&gt;u/lvwerra&lt;/a&gt; (Head of Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/edbeeching"&gt;Edward Beeching&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/edbeeching"&gt;u/edbeeching&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cmpatino"&gt;Carlos Miguel Patiño&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/cmpatino_"&gt;u/cmpatino_&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kashif"&gt;Kashif Rasul&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/krasul"&gt;u/krasul&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lewtun"&gt;Lewis Tunstall&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lewtun"&gt;u/lewtun&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/qgallouedec"&gt;Quentin Gallouédec&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/qgallouedec"&gt;u/qgallouedec&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/clefourrier"&gt;Clémentine Fourrier&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/clefourrier"&gt;u/clefourrier&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/SaylorTwift"&gt;Nathan Habib&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/HauntingMoment"&gt;u/HauntingMoment&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lusxvr"&gt;Luis Wiedmann&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/luswd"&gt;u/luswd&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/andito"&gt;Andres Marafioti&lt;/a&gt;, &lt;a href="/u/futterneid"&gt;u/futterneid&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/guipenedo"&gt;Guilherme Penedo&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/PhilipsNostrum"&gt;u/PhilipsNostrum&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/hynky"&gt;Hynek Kydlíček&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Other_Housing8453"&gt;u/Other_Housing8453&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/reach-vb"&gt;Vaibhav Srivastav,&lt;/a&gt; &lt;a href="/u/vaibhavs10"&gt;u/vaibhavs10&lt;/a&gt; (Head of Developer Experience and Community)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BrigitteTousi"&gt;Brigitte Tousignant&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/BriggieSmalls1992"&gt;u/BriggieSmalls1992&lt;/a&gt; (Comms)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Xenova"&gt;Xenova&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/xenovatech"&gt;u/xenovatech&lt;/a&gt; (Transformers.js)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/craffel"&gt;Colin Raffel&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/craffel"&gt;u/craffel&lt;/a&gt; (Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ngxson"&gt;Xuan Son Nguyen&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/MediocreProgrammer99"&gt;u/MediocreProgrammer99&lt;/a&gt; (llama.cpp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM – 11 AM PST, with the Hugging Face team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135"&gt;https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:43:01+00:00</published>
  </entry>
</feed>
