<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-05-12T10:07:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kk43eo</id>
    <title>Hardware specs comparison to host Mistral small 24B</title>
    <updated>2025-05-11T15:49:07+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk43eo/hardware_specs_comparison_to_host_mistral_small/"&gt; &lt;img alt="Hardware specs comparison to host Mistral small 24B" src="https://external-preview.redd.it/T4sMn15QFsNG_0KdS2BwGdWLAF8Ie_ulw9XpnsXfqsE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37e0992d289e953d6af5186034ff41b5b4d77814" title="Hardware specs comparison to host Mistral small 24B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am comparing hardware specifications for a customer who wants to host Mistral small 24B locally for inference. He would like to know if it's worth buying a GPU server instead of consuming the MistralAI API, and if so, when the breakeven point occurs. Here are my assumptions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Model weights are FP16 and the 128k context window is fully utilized.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The formula to compute the required VRAM is the product of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context length&lt;/li&gt; &lt;li&gt;Number of layers&lt;/li&gt; &lt;li&gt;Number of key-value heads&lt;/li&gt; &lt;li&gt;Head dimension - 2 (2-bytes per float16) - 2 (one for keys, one for values)&lt;/li&gt; &lt;li&gt;Number of users&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;To calculate the upper bound, the number of users is the maximum number of concurrent users the hardware can handle with the full 128k token context window.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The use of an AI agent consumes approximately 25 times the number of tokens compared to a normal chat (Source: &lt;a href="https://www.businessinsider.com/ai-super-agents-enough-computing-power-openai-deepseek-2025-3"&gt;https://www.businessinsider.com/ai-super-agents-enough-computing-power-openai-deepseek-2025-3&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My comparison resulted in this table. The price of electricity for professionals here is about 0.20€/kWh all taxes included. Because of this, the breakeven point is at least 8.3 years for the Nvidia DGX A100. The Apple Mac Studio M3 Ultra reaches breakeven after 6 months, but it is significantly slower than the Nvidia and AMD products.&lt;/p&gt; &lt;p&gt;Given these data I think this is not worth investing in a GPU server, unless the customer absolutely requires privacy.&lt;/p&gt; &lt;p&gt;Do you think the numbers I found are reasonable? Were my assumptions too far off? I hope this helps the community.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c0140tgw960f1.png?width=2427&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fdf2d5f2b15d88ef4621a830436459baebbaf3e"&gt;https://preview.redd.it/c0140tgw960f1.png?width=2427&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fdf2d5f2b15d88ef4621a830436459baebbaf3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Below some graphs :&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ghlcd725b60f1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=804fe43c28dab4a4cde53a1df5d1ca6b67df3a67"&gt;https://preview.redd.it/ghlcd725b60f1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=804fe43c28dab4a4cde53a1df5d1ca6b67df3a67&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3f5x0dk5b60f1.png?width=1188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c799d2e711a84b1355cd3b4515560a4450a3e0e"&gt;https://preview.redd.it/3f5x0dk5b60f1.png?width=1188&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c799d2e711a84b1355cd3b4515560a4450a3e0e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7emca9v5b60f1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7295ff311460e0d45dfa3ddd671e188840394c6"&gt;https://preview.redd.it/7emca9v5b60f1.png?width=1187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7295ff311460e0d45dfa3ddd671e188840394c6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8bl4pcb6b60f1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ed692b1afc9caa440470f8779b44d46130de02f"&gt;https://preview.redd.it/8bl4pcb6b60f1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ed692b1afc9caa440470f8779b44d46130de02f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/94h5rso6b60f1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc7f3f07abc2f5c9f236e30ff20f300446f3f0c"&gt;https://preview.redd.it/94h5rso6b60f1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fc7f3f07abc2f5c9f236e30ff20f300446f3f0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wm0y3j37b60f1.png?width=1185&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7af8a86a7fbee60b5028349525fe2430ce2313d4"&gt;https://preview.redd.it/wm0y3j37b60f1.png?width=1185&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7af8a86a7fbee60b5028349525fe2430ce2313d4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk43eo/hardware_specs_comparison_to_host_mistral_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk43eo/hardware_specs_comparison_to_host_mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk43eo/hardware_specs_comparison_to_host_mistral_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T15:49:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkk0ye</id>
    <title>would fine-tuning improve the content creation output?</title>
    <updated>2025-05-12T04:37:43+00:00</updated>
    <author>
      <name>/u/jamesftf</name>
      <uri>https://old.reddit.com/user/jamesftf</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm new to fine-tuning and, due to limited hardware, can only use cloud-based solution.&lt;/p&gt; &lt;p&gt;I'm seeking advice on a problem: I'm testing content creation for the X industry. &lt;/p&gt; &lt;p&gt;I've tried multiple n8n AI agents in sequence, but with lengthy writing rules, they hallucinate or fail to meet requirements. &lt;/p&gt; &lt;p&gt;I have custom writing rules, industry-specific jargon, language guidelines, and a specific output template in the prompts. &lt;/p&gt; &lt;p&gt;Where should I start with fine-tuned Anthropic or Gemini models, as they seem to produce the best human-like outputs for my needs? &lt;/p&gt; &lt;p&gt;Can you suggest, based on your knowledge, which direction I should explore? &lt;/p&gt; &lt;p&gt;I'm overwhelmed by the information and YouTube tutorials available.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamesftf"&gt; /u/jamesftf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkk0ye/would_finetuning_improve_the_content_creation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkk0ye/would_finetuning_improve_the_content_creation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkk0ye/would_finetuning_improve_the_content_creation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T04:37:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk0ghi</id>
    <title>Speed Comparison with Qwen3-32B-q8_0, Ollama, Llama.cpp, 2x3090, M3Max</title>
    <updated>2025-05-11T12:59:11+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Requested by &lt;a href="/u/MLDataScientist"&gt;/u/MLDataScientist&lt;/a&gt;, here is a comparison test between Ollama and Llama.cpp on 2 x RTX-3090 and M3-Max with 64GB using Qwen3-32B-q8_0.&lt;/p&gt; &lt;p&gt;Just note, if you are interested in a comparison with most optimized setup, it would be &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ke26sl/another_attempt_to_measure_speed_for_qwen3_moe_on/"&gt;SGLang/VLLM for 4090 and MLX for M3Max with Qwen MoE architecture.&lt;/a&gt; This was primarily to compare Ollama and Llama.cpp under the same condition with Qwen3-32b model based on dense architecture. If interested, I also ran another &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kgxhdt/ollama_vs_llamacpp_on_2x3090_and_m3max_using/"&gt;similar benchmark using Qwen MoE architecture.&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Metrics&lt;/h3&gt; &lt;p&gt;To ensure consistency, I used a custom Python script that sends requests to the server via the OpenAI-compatible API. Metrics were calculated as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Time to First Token (TTFT): Measured from the start of the streaming request to the first streaming event received.&lt;/li&gt; &lt;li&gt;Prompt Processing Speed (PP): Number of prompt tokens divided by TTFT.&lt;/li&gt; &lt;li&gt;Token Generation Speed (TG): Number of generated tokens divided by (total duration - TTFT).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The displayed results were truncated to two decimal places, but the calculations used full precision. I made the script to prepend new material in the beginning of next longer prompt to avoid caching effect.&lt;/p&gt; &lt;p&gt;Here's my script for anyone interest. &lt;a href="https://github.com/chigkim/prompt-test"&gt;https://github.com/chigkim/prompt-test&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It uses OpenAI API, so it should work in variety setup. Also, this tests one request at a time, so multiple parallel requests could result in higher throughput in different tests.&lt;/p&gt; &lt;h3&gt;Setup&lt;/h3&gt; &lt;p&gt;Both use the same q8_0 model from Ollama library with flash attention. I'm sure you can further optimize Llama.cpp, but I copied the flags from Ollama log in order to keep it consistent, so both use the exactly same flags when loading the model.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./build/bin/llama-server --model ~/.ollama/models/blobs/sha256... --ctx-size 22000 --batch-size 512 --n-gpu-layers 65 --threads 32 --flash-attn --parallel 1 --tensor-split 33,32 --port 11434&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama.cpp: 5339 (3b24d26c)&lt;/li&gt; &lt;li&gt;Ollama: 0.6.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each row in the results represents a test (a specific combination of machine, engine, and prompt length). There are 4 tests per prompt length.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Setup 1: 2xRTX3090, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 2: 2xRTX3090, Ollama&lt;/li&gt; &lt;li&gt;Setup 3: M3Max, Llama.cpp&lt;/li&gt; &lt;li&gt;Setup 4: M3Max, Ollama&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Result&lt;/h3&gt; &lt;p&gt;Please zoom in to see the graph better.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img 26e05b1zd50f1...&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Machine&lt;/th&gt; &lt;th&gt;Engine&lt;/th&gt; &lt;th&gt;Prompt Tokens&lt;/th&gt; &lt;th&gt;PP/s&lt;/th&gt; &lt;th&gt;TTFT&lt;/th&gt; &lt;th&gt;Generated Tokens&lt;/th&gt; &lt;th&gt;TG/s&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;1033.18&lt;/td&gt; &lt;td&gt;0.26&lt;/td&gt; &lt;td&gt;968&lt;/td&gt; &lt;td&gt;21.71&lt;/td&gt; &lt;td&gt;44.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;853.87&lt;/td&gt; &lt;td&gt;0.31&lt;/td&gt; &lt;td&gt;1041&lt;/td&gt; &lt;td&gt;21.44&lt;/td&gt; &lt;td&gt;48.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;153.63&lt;/td&gt; &lt;td&gt;1.72&lt;/td&gt; &lt;td&gt;739&lt;/td&gt; &lt;td&gt;10.41&lt;/td&gt; &lt;td&gt;72.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;152.12&lt;/td&gt; &lt;td&gt;1.74&lt;/td&gt; &lt;td&gt;885&lt;/td&gt; &lt;td&gt;10.35&lt;/td&gt; &lt;td&gt;87.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;1184.75&lt;/td&gt; &lt;td&gt;0.38&lt;/td&gt; &lt;td&gt;1154&lt;/td&gt; &lt;td&gt;21.66&lt;/td&gt; &lt;td&gt;53.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;1013.60&lt;/td&gt; &lt;td&gt;0.44&lt;/td&gt; &lt;td&gt;1177&lt;/td&gt; &lt;td&gt;21.38&lt;/td&gt; &lt;td&gt;55.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;171.37&lt;/td&gt; &lt;td&gt;2.63&lt;/td&gt; &lt;td&gt;1273&lt;/td&gt; &lt;td&gt;10.28&lt;/td&gt; &lt;td&gt;126.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;169.53&lt;/td&gt; &lt;td&gt;2.65&lt;/td&gt; &lt;td&gt;1275&lt;/td&gt; &lt;td&gt;10.33&lt;/td&gt; &lt;td&gt;126.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;1405.67&lt;/td&gt; &lt;td&gt;0.51&lt;/td&gt; &lt;td&gt;1288&lt;/td&gt; &lt;td&gt;21.63&lt;/td&gt; &lt;td&gt;60.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;1292.38&lt;/td&gt; &lt;td&gt;0.56&lt;/td&gt; &lt;td&gt;1343&lt;/td&gt; &lt;td&gt;21.31&lt;/td&gt; &lt;td&gt;63.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;164.83&lt;/td&gt; &lt;td&gt;4.39&lt;/td&gt; &lt;td&gt;1274&lt;/td&gt; &lt;td&gt;10.29&lt;/td&gt; &lt;td&gt;128.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;163.79&lt;/td&gt; &lt;td&gt;4.41&lt;/td&gt; &lt;td&gt;1204&lt;/td&gt; &lt;td&gt;10.27&lt;/td&gt; &lt;td&gt;121.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;1602.61&lt;/td&gt; &lt;td&gt;0.76&lt;/td&gt; &lt;td&gt;1815&lt;/td&gt; &lt;td&gt;21.44&lt;/td&gt; &lt;td&gt;85.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;1498.43&lt;/td&gt; &lt;td&gt;0.81&lt;/td&gt; &lt;td&gt;1445&lt;/td&gt; &lt;td&gt;21.35&lt;/td&gt; &lt;td&gt;68.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;169.15&lt;/td&gt; &lt;td&gt;7.21&lt;/td&gt; &lt;td&gt;1302&lt;/td&gt; &lt;td&gt;10.19&lt;/td&gt; &lt;td&gt;134.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;168.32&lt;/td&gt; &lt;td&gt;7.24&lt;/td&gt; &lt;td&gt;1686&lt;/td&gt; &lt;td&gt;10.11&lt;/td&gt; &lt;td&gt;173.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;1734.46&lt;/td&gt; &lt;td&gt;1.07&lt;/td&gt; &lt;td&gt;1375&lt;/td&gt; &lt;td&gt;21.37&lt;/td&gt; &lt;td&gt;65.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;1635.95&lt;/td&gt; &lt;td&gt;1.14&lt;/td&gt; &lt;td&gt;1293&lt;/td&gt; &lt;td&gt;21.13&lt;/td&gt; &lt;td&gt;62.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;166.81&lt;/td&gt; &lt;td&gt;11.14&lt;/td&gt; &lt;td&gt;1411&lt;/td&gt; &lt;td&gt;10.09&lt;/td&gt; &lt;td&gt;151.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;166.96&lt;/td&gt; &lt;td&gt;11.13&lt;/td&gt; &lt;td&gt;1450&lt;/td&gt; &lt;td&gt;10.10&lt;/td&gt; &lt;td&gt;154.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;1789.89&lt;/td&gt; &lt;td&gt;1.66&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;21.09&lt;/td&gt; &lt;td&gt;96.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;1735.97&lt;/td&gt; &lt;td&gt;1.72&lt;/td&gt; &lt;td&gt;1628&lt;/td&gt; &lt;td&gt;20.83&lt;/td&gt; &lt;td&gt;79.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;162.22&lt;/td&gt; &lt;td&gt;18.36&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;9.89&lt;/td&gt; &lt;td&gt;220.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;161.46&lt;/td&gt; &lt;td&gt;18.45&lt;/td&gt; &lt;td&gt;1643&lt;/td&gt; &lt;td&gt;9.88&lt;/td&gt; &lt;td&gt;184.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;1791.05&lt;/td&gt; &lt;td&gt;2.61&lt;/td&gt; &lt;td&gt;1326&lt;/td&gt; &lt;td&gt;20.77&lt;/td&gt; &lt;td&gt;66.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;1746.71&lt;/td&gt; &lt;td&gt;2.67&lt;/td&gt; &lt;td&gt;1592&lt;/td&gt; &lt;td&gt;20.47&lt;/td&gt; &lt;td&gt;80.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;154.16&lt;/td&gt; &lt;td&gt;30.29&lt;/td&gt; &lt;td&gt;1593&lt;/td&gt; &lt;td&gt;9.67&lt;/td&gt; &lt;td&gt;194.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;153.03&lt;/td&gt; &lt;td&gt;30.51&lt;/td&gt; &lt;td&gt;1450&lt;/td&gt; &lt;td&gt;9.66&lt;/td&gt; &lt;td&gt;180.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;1756.76&lt;/td&gt; &lt;td&gt;4.52&lt;/td&gt; &lt;td&gt;1255&lt;/td&gt; &lt;td&gt;20.29&lt;/td&gt; &lt;td&gt;66.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;1706.41&lt;/td&gt; &lt;td&gt;4.66&lt;/td&gt; &lt;td&gt;1404&lt;/td&gt; &lt;td&gt;20.10&lt;/td&gt; &lt;td&gt;74.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;140.11&lt;/td&gt; &lt;td&gt;56.73&lt;/td&gt; &lt;td&gt;1748&lt;/td&gt; &lt;td&gt;9.20&lt;/td&gt; &lt;td&gt;246.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;138.99&lt;/td&gt; &lt;td&gt;57.18&lt;/td&gt; &lt;td&gt;1650&lt;/td&gt; &lt;td&gt;9.18&lt;/td&gt; &lt;td&gt;236.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;1648.97&lt;/td&gt; &lt;td&gt;7.53&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;19.59&lt;/td&gt; &lt;td&gt;109.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;1616.69&lt;/td&gt; &lt;td&gt;7.68&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;19.30&lt;/td&gt; &lt;td&gt;111.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;127.96&lt;/td&gt; &lt;td&gt;97.03&lt;/td&gt; &lt;td&gt;1395&lt;/td&gt; &lt;td&gt;8.60&lt;/td&gt; &lt;td&gt;259.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;127.08&lt;/td&gt; &lt;td&gt;97.70&lt;/td&gt; &lt;td&gt;1778&lt;/td&gt; &lt;td&gt;8.57&lt;/td&gt; &lt;td&gt;305.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;1481.92&lt;/td&gt; &lt;td&gt;13.61&lt;/td&gt; &lt;td&gt;598&lt;/td&gt; &lt;td&gt;18.72&lt;/td&gt; &lt;td&gt;45.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;1458.86&lt;/td&gt; &lt;td&gt;13.83&lt;/td&gt; &lt;td&gt;1627&lt;/td&gt; &lt;td&gt;18.30&lt;/td&gt; &lt;td&gt;102.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;111.18&lt;/td&gt; &lt;td&gt;181.44&lt;/td&gt; &lt;td&gt;1771&lt;/td&gt; &lt;td&gt;7.58&lt;/td&gt; &lt;td&gt;415.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;M3Max&lt;/td&gt; &lt;td&gt;Ollama&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;111.80&lt;/td&gt; &lt;td&gt;180.43&lt;/td&gt; &lt;td&gt;1372&lt;/td&gt; &lt;td&gt;7.53&lt;/td&gt; &lt;td&gt;362.54&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Updates&lt;/h3&gt; &lt;p&gt;People commented below how I'm not using &amp;quot;tensor parallelism&amp;quot; properly with llama.cpp. I specified &lt;code&gt;--n-gpu-layers 65&lt;/code&gt;, and split with &lt;code&gt;--tensor-split 33,32&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I also tried &lt;code&gt;-sm row --tensor-split 1,1&lt;/code&gt;, but it consistently dramatically decreased prompt processing to around 400tk/s. It also dropped token generation speed as well. The result is below.&lt;/p&gt; &lt;p&gt;Could someone tell me how and what flags do I need to use in order to take advantage of &amp;quot;tensor parallelism&amp;quot; that people are talking about?&lt;/p&gt; &lt;p&gt;&lt;code&gt;./build/bin/llama-server --model ... --ctx-size 22000 --n-gpu-layers 99 --threads 32 --flash-attn --parallel 1 -sm row --tensor-split 1,1&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Machine&lt;/th&gt; &lt;th&gt;Engine&lt;/th&gt; &lt;th&gt;Prompt Tokens&lt;/th&gt; &lt;th&gt;PP/s&lt;/th&gt; &lt;th&gt;TTFT&lt;/th&gt; &lt;th&gt;Generated Tokens&lt;/th&gt; &lt;th&gt;TG/s&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;381.86&lt;/td&gt; &lt;td&gt;0.69&lt;/td&gt; &lt;td&gt;1040&lt;/td&gt; &lt;td&gt;19.57&lt;/td&gt; &lt;td&gt;53.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;410.24&lt;/td&gt; &lt;td&gt;1.10&lt;/td&gt; &lt;td&gt;1409&lt;/td&gt; &lt;td&gt;19.57&lt;/td&gt; &lt;td&gt;73.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;440.61&lt;/td&gt; &lt;td&gt;1.64&lt;/td&gt; &lt;td&gt;1266&lt;/td&gt; &lt;td&gt;19.54&lt;/td&gt; &lt;td&gt;66.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;446.84&lt;/td&gt; &lt;td&gt;2.73&lt;/td&gt; &lt;td&gt;1692&lt;/td&gt; &lt;td&gt;19.37&lt;/td&gt; &lt;td&gt;90.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;445.79&lt;/td&gt; &lt;td&gt;4.17&lt;/td&gt; &lt;td&gt;1525&lt;/td&gt; &lt;td&gt;19.30&lt;/td&gt; &lt;td&gt;83.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;437.87&lt;/td&gt; &lt;td&gt;6.80&lt;/td&gt; &lt;td&gt;1840&lt;/td&gt; &lt;td&gt;19.17&lt;/td&gt; &lt;td&gt;102.78&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;433.98&lt;/td&gt; &lt;td&gt;10.76&lt;/td&gt; &lt;td&gt;1555&lt;/td&gt; &lt;td&gt;18.84&lt;/td&gt; &lt;td&gt;93.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;416.62&lt;/td&gt; &lt;td&gt;19.08&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;18.48&lt;/td&gt; &lt;td&gt;127.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;429.59&lt;/td&gt; &lt;td&gt;28.90&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;17.84&lt;/td&gt; &lt;td&gt;141.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;LCPP&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;402.50&lt;/td&gt; &lt;td&gt;50.12&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;17.10&lt;/td&gt; &lt;td&gt;167.09&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here's same test with SGLang with prompt caching disabled.&lt;/p&gt; &lt;p&gt;`python -m sglang.launch_server --model-path Qwen/Qwen3-32B-FP8 --context-length 22000 --tp-size 2 --disable-chunked-prefix-cache --disable-radix-cache&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Machine&lt;/th&gt; &lt;th&gt;Engine&lt;/th&gt; &lt;th&gt;Prompt Tokens&lt;/th&gt; &lt;th&gt;PP/s&lt;/th&gt; &lt;th&gt;TTFT&lt;/th&gt; &lt;th&gt;Generated Tokens&lt;/th&gt; &lt;th&gt;TG/s&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;264&lt;/td&gt; &lt;td&gt;843.54&lt;/td&gt; &lt;td&gt;0.31&lt;/td&gt; &lt;td&gt;777&lt;/td&gt; &lt;td&gt;35.03&lt;/td&gt; &lt;td&gt;22.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;450&lt;/td&gt; &lt;td&gt;852.32&lt;/td&gt; &lt;td&gt;0.53&lt;/td&gt; &lt;td&gt;1445&lt;/td&gt; &lt;td&gt;34.86&lt;/td&gt; &lt;td&gt;41.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;723&lt;/td&gt; &lt;td&gt;903.44&lt;/td&gt; &lt;td&gt;0.80&lt;/td&gt; &lt;td&gt;1250&lt;/td&gt; &lt;td&gt;34.79&lt;/td&gt; &lt;td&gt;36.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;1219&lt;/td&gt; &lt;td&gt;943.47&lt;/td&gt; &lt;td&gt;1.29&lt;/td&gt; &lt;td&gt;1809&lt;/td&gt; &lt;td&gt;34.66&lt;/td&gt; &lt;td&gt;53.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;1858&lt;/td&gt; &lt;td&gt;948.24&lt;/td&gt; &lt;td&gt;1.96&lt;/td&gt; &lt;td&gt;1640&lt;/td&gt; &lt;td&gt;34.54&lt;/td&gt; &lt;td&gt;49.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;2979&lt;/td&gt; &lt;td&gt;957.28&lt;/td&gt; &lt;td&gt;3.11&lt;/td&gt; &lt;td&gt;1898&lt;/td&gt; &lt;td&gt;34.23&lt;/td&gt; &lt;td&gt;58.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;4669&lt;/td&gt; &lt;td&gt;956.29&lt;/td&gt; &lt;td&gt;4.88&lt;/td&gt; &lt;td&gt;1692&lt;/td&gt; &lt;td&gt;33.89&lt;/td&gt; &lt;td&gt;54.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;7948&lt;/td&gt; &lt;td&gt;932.63&lt;/td&gt; &lt;td&gt;8.52&lt;/td&gt; &lt;td&gt;2000&lt;/td&gt; &lt;td&gt;33.34&lt;/td&gt; &lt;td&gt;68.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;12416&lt;/td&gt; &lt;td&gt;907.01&lt;/td&gt; &lt;td&gt;13.69&lt;/td&gt; &lt;td&gt;1967&lt;/td&gt; &lt;td&gt;32.60&lt;/td&gt; &lt;td&gt;74.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RTX3090&lt;/td&gt; &lt;td&gt;SGLang&lt;/td&gt; &lt;td&gt;20172&lt;/td&gt; &lt;td&gt;857.66&lt;/td&gt; &lt;td&gt;23.52&lt;/td&gt; &lt;td&gt;1786&lt;/td&gt; &lt;td&gt;31.51&lt;/td&gt; &lt;td&gt;80.20&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk0ghi/speed_comparison_with_qwen332bq8_0_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk0ghi/speed_comparison_with_qwen332bq8_0_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk0ghi/speed_comparison_with_qwen332bq8_0_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T12:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kju1y1</id>
    <title>Unsloth's Qwen3 GGUFs are updated with a new improved calibration dataset</title>
    <updated>2025-05-11T05:59:20+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"&gt; &lt;img alt="Unsloth's Qwen3 GGUFs are updated with a new improved calibration dataset" src="https://external-preview.redd.it/8ePyWxYJavtNkgThp-DI68bW9d5fj-oFIybzu4pnoUM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a7b2715032a28656454c9bee39e79aafee721d37" title="Unsloth's Qwen3 GGUFs are updated with a new improved calibration dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-128K-GGUF/discussions/3#681edd400153e42b1c7168e9"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-128K-GGUF/discussions/3#681edd400153e42b1c7168e9&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We've uploaded them all now&lt;/p&gt; &lt;p&gt;Also with a new improved calibration dataset :)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/51rr8j7qd30f1.png?width=362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e0b8891020518424f286d35814501b87cbd9cc0"&gt;https://preview.redd.it/51rr8j7qd30f1.png?width=362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e0b8891020518424f286d35814501b87cbd9cc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They updated All Qwen3 ggufs&lt;/p&gt; &lt;p&gt;Plus more gguf variants for Qwen3-30B-A3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ckx6zfn0e30f1.png?width=397&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dde922fd59d02d5223680a6d584758387bdc476"&gt;https://preview.redd.it/ckx6zfn0e30f1.png?width=397&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dde922fd59d02d5223680a6d584758387bdc476&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/models?sort=modified&amp;amp;search=unsloth+qwen3+gguf"&gt;https://huggingface.co/models?sort=modified&amp;amp;search=unsloth+qwen3+gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kju1y1/unsloths_qwen3_ggufs_are_updated_with_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T05:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kju0ty</id>
    <title>Why new models feel dumber?</title>
    <updated>2025-05-11T05:57:09+00:00</updated>
    <author>
      <name>/u/SrData</name>
      <uri>https://old.reddit.com/user/SrData</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it just me, or do the new models feel… dumber?&lt;/p&gt; &lt;p&gt;I’ve been testing Qwen 3 across different sizes, expecting a leap forward. Instead, I keep circling back to Qwen 2.5. It just feels sharper, more coherent, less… bloated. Same story with Llama. I’ve had long, surprisingly good conversations with 3.1. But 3.3? Or Llama 4? It’s like the lights are on but no one’s home.&lt;/p&gt; &lt;p&gt;Some flaws I have found: They lose thread persistence. They forget earlier parts of the convo. They repeat themselves more. Worse, they feel like they’re trying to sound smarter instead of being coherent.&lt;/p&gt; &lt;p&gt;So I’m curious: Are you seeing this too? Which models are you sticking with, despite the version bump? Any new ones that have genuinely impressed you, especially in longer sessions?&lt;/p&gt; &lt;p&gt;Because right now, it feels like we’re in this strange loop of releasing “smarter” models that somehow forget how to talk. And I’d love to know I’m not the only one noticing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrData"&gt; /u/SrData &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju0ty/why_new_models_feel_dumber/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kju0ty/why_new_models_feel_dumber/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kju0ty/why_new_models_feel_dumber/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T05:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkh3cw</id>
    <title>Qwen 3 30B-A3B on P40</title>
    <updated>2025-05-12T01:52:16+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has someone benched this model on the P40. Since you can fit the quantized model with 40k context on a single P40, I was wondering how fast this runs on the P40.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkh3cw/qwen_3_30ba3b_on_p40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkh3cw/qwen_3_30ba3b_on_p40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkh3cw/qwen_3_30ba3b_on_p40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T01:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkiae1</id>
    <title>Speculative Decoding + ktransformers</title>
    <updated>2025-05-12T02:57:44+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not very qualified to speak on this as I have no experience with either. Just been reading about both independently. Looking through reddit and elsewhere I haven't found much on this, and I don't trust ChatGPT's answer (it said it works).&lt;/p&gt; &lt;p&gt;For those with more experience, do you know if it does work? Or is there a reason that explains why it seems no one ever asked the question 😅&lt;/p&gt; &lt;p&gt;For those of us to which this is also unknown territory: Speculative decoding lets you run a small 'draft' model in parallel to your large (and much smarter) 'target' model. The draft model comes up with tokens very quickly, which the large one then &amp;quot;verifies&amp;quot;, making inference reportedly up to 3x-6x faster. At least that's what they say in the EAGLE 3 paper. Ktransformers is a library, which lets you run LLMs on CPU. This is especially interesting for RAM-rich systems where you can run very high parameter count models, albeit quite slowly compared to VRAM. Seemed like combining the two could be a smart idea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkiae1/speculative_decoding_ktransformers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkiae1/speculative_decoding_ktransformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkiae1/speculative_decoding_ktransformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T02:57:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kknasa</id>
    <title>Where I can find Gen AI images dataset with input text prompts?</title>
    <updated>2025-05-12T08:20:37+00:00</updated>
    <author>
      <name>/u/gpt-d13</name>
      <uri>https://old.reddit.com/user/gpt-d13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I am working on my research paper and a side project. I need a small dataset of images generated by LLMs along with the input prompts. &lt;/p&gt; &lt;p&gt;I am working on an enhancement project for images generated by AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gpt-d13"&gt; /u/gpt-d13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kknasa/where_i_can_find_gen_ai_images_dataset_with_input/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kknasa/where_i_can_find_gen_ai_images_dataset_with_input/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kknasa/where_i_can_find_gen_ai_images_dataset_with_input/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T08:20:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkg73u</id>
    <title>Framework for on-device inference on mobile phones.</title>
    <updated>2025-05-12T01:03:39+00:00</updated>
    <author>
      <name>/u/Henrie_the_dreamer</name>
      <uri>https://old.reddit.com/user/Henrie_the_dreamer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkg73u/framework_for_ondevice_inference_on_mobile_phones/"&gt; &lt;img alt="Framework for on-device inference on mobile phones." src="https://external-preview.redd.it/axO1lDn6CC2LrdsT3SbLMT45Vyzny3pcatpM2NKYpJg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=905ec86a17c05e43cc0018a3680500f2a395a8b5" title="Framework for on-device inference on mobile phones." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, just seeking feedback on a project we've been working on, to for running LLMs on mobile devices more seamless. Cactus has unified and consistent APIs across&lt;/p&gt; &lt;ul&gt; &lt;li&gt;React-Native&lt;/li&gt; &lt;li&gt;Android/Kotlin&lt;/li&gt; &lt;li&gt;Android/Java&lt;/li&gt; &lt;li&gt;iOS/Swift&lt;/li&gt; &lt;li&gt;iOS/Objective-C++&lt;/li&gt; &lt;li&gt;Flutter/Dart&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cactus currently leverages GGML backends to support any GGUF model already compatible with Llama.cpp, while we focus on broadly supporting every moblie app development platform, as well as upcoming features like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MCP&lt;/li&gt; &lt;li&gt;phone tool use&lt;/li&gt; &lt;li&gt;thinking&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please give us feedback if you have the time, and if feeling generous, please leave a star ⭐ to help us attract contributors :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Henrie_the_dreamer"&gt; /u/Henrie_the_dreamer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/cactus-compute/cactus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkg73u/framework_for_ondevice_inference_on_mobile_phones/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkg73u/framework_for_ondevice_inference_on_mobile_phones/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T01:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk6pjp</id>
    <title>Bielik v3 family of SOTA Polish open SLMs has been released</title>
    <updated>2025-05-11T17:43:41+00:00</updated>
    <author>
      <name>/u/niutech</name>
      <uri>https://old.reddit.com/user/niutech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk6pjp/bielik_v3_family_of_sota_polish_open_slms_has/"&gt; &lt;img alt="Bielik v3 family of SOTA Polish open SLMs has been released" src="https://external-preview.redd.it/tEBlMyeFm6RtilVklQB-UJC_6CHQzvAf1MLIxBngxSA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a27f61c9021f49e3a3ad4ca6a38d379c8fbd2cad" title="Bielik v3 family of SOTA Polish open SLMs has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/niutech"&gt; /u/niutech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/speakleash/bielik-v3-family-681a47f877f72cae528bdab1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk6pjp/bielik_v3_family_of_sota_polish_open_slms_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk6pjp/bielik_v3_family_of_sota_polish_open_slms_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T17:43:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk7oo8</id>
    <title>More fun with Qwen 3 8b! This time it created 2 Starfields and a playable Xylophone for me! Not at all bad for a model that can fit in an 8-12GB GPU!</title>
    <updated>2025-05-11T18:25:44+00:00</updated>
    <author>
      <name>/u/c64z86</name>
      <uri>https://old.reddit.com/user/c64z86</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk7oo8/more_fun_with_qwen_3_8b_this_time_it_created_2/"&gt; &lt;img alt="More fun with Qwen 3 8b! This time it created 2 Starfields and a playable Xylophone for me! Not at all bad for a model that can fit in an 8-12GB GPU!" src="https://external-preview.redd.it/iIvsvRqpQ2fJ2gASUwDJdJzk7Y-NRsJGfolwfmr4gyo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1e591211e94beaef8ea3976b3e70b2a58ac718e" title="More fun with Qwen 3 8b! This time it created 2 Starfields and a playable Xylophone for me! Not at all bad for a model that can fit in an 8-12GB GPU!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/c64z86"&gt; /u/c64z86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/fvsJezacCW4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk7oo8/more_fun_with_qwen_3_8b_this_time_it_created_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk7oo8/more_fun_with_qwen_3_8b_this_time_it_created_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T18:25:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kjvb8i</id>
    <title>I Built a Tool That Tells Me If a Side Project Will Ruin My Weekend</title>
    <updated>2025-05-11T07:24:15+00:00</updated>
    <author>
      <name>/u/IntelligentHope9866</name>
      <uri>https://old.reddit.com/user/IntelligentHope9866</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to lie to myself every weekend:&lt;br /&gt; “I’ll build this in an hour.”&lt;/p&gt; &lt;p&gt;Spoiler: I never did.&lt;/p&gt; &lt;p&gt;So I built a tool that tracks how long my features actually take — and uses a local LLM to estimate future ones.&lt;/p&gt; &lt;p&gt;It logs my coding sessions, summarizes them, and tells me:&lt;br /&gt; &amp;quot;Yeah, this’ll eat your whole weekend. Don’t even start.&amp;quot;&lt;/p&gt; &lt;p&gt;It lives in my terminal and keeps me honest.&lt;/p&gt; &lt;p&gt;Full writeup + code: &lt;a href="https://www.rafaelviana.io/posts/code-chrono"&gt;https://www.rafaelviana.io/posts/code-chrono&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntelligentHope9866"&gt; /u/IntelligentHope9866 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjvb8i/i_built_a_tool_that_tells_me_if_a_side_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kjvb8i/i_built_a_tool_that_tells_me_if_a_side_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kjvb8i/i_built_a_tool_that_tells_me_if_a_side_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T07:24:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkltpr</id>
    <title>Fp6 and Blackwell</title>
    <updated>2025-05-12T06:35:00+00:00</updated>
    <author>
      <name>/u/Green-Ad-3964</name>
      <uri>https://old.reddit.com/user/Green-Ad-3964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most news have been focusing on the Blackwell hardware acceleration for fp4. But as far as I understand it can also accelerate fp6. Is that correct? And if so, are there any quantized LLMs to benefit from this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Green-Ad-3964"&gt; /u/Green-Ad-3964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkltpr/fp6_and_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkltpr/fp6_and_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkltpr/fp6_and_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T06:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk66rj</id>
    <title>Jamba mini 1.6 actually outperformed GPT-40 for our RAG support bot</title>
    <updated>2025-05-11T17:21:08+00:00</updated>
    <author>
      <name>/u/NullPointerJack</name>
      <uri>https://old.reddit.com/user/NullPointerJack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These results surprised me. We were testing a few models for a support use case (chat summarization + QA over internal docs) and figured GPT-4o would easily win, but Jamba mini 1.6 (open weights) actually gave us more accurate grounded answers and ran much faster.&lt;/p&gt; &lt;p&gt;Some of the main takeaways -&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It beat Jamba 1.5 by a decent margin. About 21% more of our QA outputs were grounded correctly and it was basically tied with GPT-4o in how well it grounded information from our RAG setup&lt;/li&gt; &lt;li&gt;Much faster latency. We're running it quantized with vLLM in our own VPC and it was like 2x faster than GPT-4o for token generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We havent tested math/coding or multilingual yet, just text-heavy internal documents and customer chat logs.&lt;/p&gt; &lt;p&gt;GPT-4o is definitely better for ambiguous questions and slightly more natural in how it phrases answers. But for our exact use case, Jamba Mini handled it better and cheaper.&lt;/p&gt; &lt;p&gt;Is anyone else here running Jamba locally or on-premises?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NullPointerJack"&gt; /u/NullPointerJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk66rj/jamba_mini_16_actually_outperformed_gpt40_for_our/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk66rj/jamba_mini_16_actually_outperformed_gpt40_for_our/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk66rj/jamba_mini_16_actually_outperformed_gpt40_for_our/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T17:21:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk7dwb</id>
    <title>New Project: Llama ParamPal - A LLM (Sampling) Parameter Repository</title>
    <updated>2025-05-11T18:12:30+00:00</updated>
    <author>
      <name>/u/StrikeOner</name>
      <uri>https://old.reddit.com/user/StrikeOner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;After spending way too much time researching the correct sampling parameters to get local LLMs running with the optimal sampling parameters with llama.cpp, I tought that it might be smarter to built something that might save me and you the headache in the future:&lt;/p&gt; &lt;p&gt;🔧 &lt;a href="https://github.com/kseyhan/llama-param-pal"&gt;Llama ParamPal &lt;/a&gt;— a repository to serve as a database with the recommended sampling parameters for running local LLMs using &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp.&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;✅ Why This Exists&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Getting a new model running usually involves:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Digging through a lot of scattered docs to be lucky to find the recommended sampling parameters for this model i just downloaded documented somewhere which in some cases like QwQ for example can be as crazy as changing the order of samplers:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt; Trial and error (and more error...)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Llama ParamPal aims to fix that by:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Collecting sampling parameters and their successive documentations.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Offering a searchable frontend: &lt;a href="https://llama-parampal.codecut.de"&gt;https://llama-parampal.codecut.de&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;📦 What’s Inside?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/kseyhan/llama-param-pal/refs/heads/main/models.json"&gt;models.json&lt;/a&gt; — the core file where all recommended configs live &lt;/li&gt; &lt;li&gt;&lt;a href="https://llama-parampal.codecut.de"&gt;Simple web UI&lt;/a&gt; to browse/search the parameter sets ( thats currently under development and will be made available to be hosted localy in near future)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Validation scripts to keep everything clean and structured&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;✍️ Help me, you and your llama fellows and constribute!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The database constists of a whooping 4 entries at the moment, i'll try to add some models here and there but better would be if some of you guys would constribute and help to grow this database.&lt;/li&gt; &lt;li&gt;Add your favorite model with the sampling parameters + source of the documenation as a new profile into the models.json, validate the JSON, and open a PR. That’s it!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Instructions here 👉 &lt;a href="https://github.com/kseyhan/llama-param-pal"&gt;GitHub repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, contributions, or just a sanity check! Your knowledge can help others in the community.&lt;/p&gt; &lt;p&gt;Let me know what you think 🫡&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StrikeOner"&gt; /u/StrikeOner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk7dwb/new_project_llama_parampal_a_llm_sampling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk7dwb/new_project_llama_parampal_a_llm_sampling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk7dwb/new_project_llama_parampal_a_llm_sampling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T18:12:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkocfx</id>
    <title>llama.cpp not using kv cache effectively?</title>
    <updated>2025-05-12T09:36:43+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp not using kv cache effectively?&lt;/p&gt; &lt;p&gt;I'm running the unsloth UD q4 quanto of qwen3 30ba3b and noticed that when adding new responses in a chat, it seemed to re-process the whole conversation instead of using the kv cache.&lt;/p&gt; &lt;p&gt;any ideas?&lt;/p&gt; &lt;p&gt;``` May 12 09:33:13 llm llm[948025]: srv params&lt;em&gt;from&lt;/em&gt;: Chat format: Content-only May 12 09:33:13 llm llm[948025]: slot launch&lt;em&gt;slot&lt;/em&gt;: id 0 | task 105562 | processing task May 12 09:33:13 llm llm[948025]: slot update_slots: id 0 | task 105562 | new prompt, n_ctx_slot = 40960, n_keep = 0, n_prompt_tokens = 15411 May 12 09:33:13 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [3, end) May 12 09:33:13 llm llm[948025]: slot update_slots: id 0 | task 105562 | prompt processing progress, n_past = 2051, n_tokens = 2048, progress = &amp;gt; May 12 09:33:16 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [2051, end) May 12 09:33:16 llm llm[948025]: slot update_slots: id 0 | task 105562 | prompt processing progress, n_past = 4099, n_tokens = 2048, progress = &amp;gt; May 12 09:33:18 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [4099, end) May 12 09:33:18 llm llm[948025]: slot update_slots: id 0 | task 105562 | prompt processing progress, n_past = 6147, n_tokens = 2048, progress = &amp;gt; May 12 09:33:21 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [6147, end) May 12 09:33:21 llm llm[948025]: slot update_slots: id 0 | task 105562 | prompt processing progress, n_past = 8195, n_tokens = 2048, progress = &amp;gt; May 12 09:33:25 llm llm[948025]: slot update_slots: id 0 | task 105562 | kv cache rm [8195, end)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkocfx/llamacpp_not_using_kv_cache_effectively/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkocfx/llamacpp_not_using_kv_cache_effectively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkocfx/llamacpp_not_using_kv_cache_effectively/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T09:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkkhkf</id>
    <title>"How many days is it between 12/5/2025 and 20/7/2025? (dd/mm/yy)". Did some dishes, went out with trash. They really th0nk about it, innocent question; but sometimes I can feel a bit ambivalent about this. But it's better than between the one, and zero I guess, on the other hand, it's getting there.</title>
    <updated>2025-05-12T05:07:01+00:00</updated>
    <author>
      <name>/u/Ein-neiveh-blaw-bair</name>
      <uri>https://old.reddit.com/user/Ein-neiveh-blaw-bair</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkkhkf/how_many_days_is_it_between_1252025_and_2072025/"&gt; &lt;img alt="&amp;quot;How many days is it between 12/5/2025 and 20/7/2025? (dd/mm/yy)&amp;quot;. Did some dishes, went out with trash. They really th0nk about it, innocent question; but sometimes I can feel a bit ambivalent about this. But it's better than between the one, and zero I guess, on the other hand, it's getting there." src="https://preview.redd.it/dg50eahh8a0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca25176cc0131c24ad4b3dda3c0fc89336805285" title="&amp;quot;How many days is it between 12/5/2025 and 20/7/2025? (dd/mm/yy)&amp;quot;. Did some dishes, went out with trash. They really th0nk about it, innocent question; but sometimes I can feel a bit ambivalent about this. But it's better than between the one, and zero I guess, on the other hand, it's getting there." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ein-neiveh-blaw-bair"&gt; /u/Ein-neiveh-blaw-bair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dg50eahh8a0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkkhkf/how_many_days_is_it_between_1252025_and_2072025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkkhkf/how_many_days_is_it_between_1252025_and_2072025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T05:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkiif9</id>
    <title>Ktransformer VS Llama CPP</title>
    <updated>2025-05-12T03:10:02+00:00</updated>
    <author>
      <name>/u/Bluesnow8888</name>
      <uri>https://old.reddit.com/user/Bluesnow8888</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking into Ktransformer lately (&lt;a href="https://github.com/kvcache-ai/ktransformers"&gt;https://github.com/kvcache-ai/ktransformers&lt;/a&gt;), but I have not tried it myself yet.&lt;/p&gt; &lt;p&gt;Based on its readme, it can handle very large model , such as the Deepseek 671B or Qwen3 235B with only 1 or 2 GPUs.&lt;/p&gt; &lt;p&gt;However, I don't see it gets discussed a lot here. I wonder why everyone still uses Llama CPP? Will I gain more performance by switching to Ktransformer? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bluesnow8888"&gt; /u/Bluesnow8888 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkiif9/ktransformer_vs_llama_cpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkiif9/ktransformer_vs_llama_cpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkiif9/ktransformer_vs_llama_cpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T03:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkjyvk</id>
    <title>A collection of open source tools to summarize the news using Rust, Llama.cpp and Qwen 2.5 3B.</title>
    <updated>2025-05-12T04:34:06+00:00</updated>
    <author>
      <name>/u/sqli</name>
      <uri>https://old.reddit.com/user/sqli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkjyvk/a_collection_of_open_source_tools_to_summarize/"&gt; &lt;img alt="A collection of open source tools to summarize the news using Rust, Llama.cpp and Qwen 2.5 3B." src="https://preview.redd.it/u28jb5s74a0f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05bcf2f1fef0da32184133fff187d14aa925a27d" title="A collection of open source tools to summarize the news using Rust, Llama.cpp and Qwen 2.5 3B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm Thomas, I created &lt;a href="https://awfulsec.com/introducing_awful_security_news.html"&gt;Awful Security News&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I found that prompt engineering is quite difficult for those who don't like Python and prefer to use command line tools over comprehensive suites like Silly Tavern.&lt;/p&gt; &lt;p&gt;I also prefer being able to run inference without access to the internet, on my local machine. I saw that LM Studio now supports Open-AI tool calling and Response Formats and long wanted to learn how this works without wasting hundreds of dollars and hours using Open-AI's products.&lt;/p&gt; &lt;p&gt;I was pretty impressed with the capabilities of Qwen's models and needed a distraction free way to read the news of the day. Also, the speed of the news cycles and the firehouse of important details, say &lt;em&gt;Named Entities&lt;/em&gt; and &lt;em&gt;Dates&lt;/em&gt; makes recalling these facts when necessary for the conversation more of a workout than necessary.&lt;/p&gt; &lt;p&gt;I was interested in the fact that Qwen is a multilingual model made by the long renown Chinese company Alibaba. I know that when I'm reading foreign languages, written by native speakers in their country of origin, things like Named Entities might not always translate over in my brain. It's easy to confuse a title or name for an action or an event. For instance, the Securities Exchange Commission could mean that Investments are trading each other bonuses they made on sales or &amp;quot;Securities are exchanging commission.&amp;quot; Things like this can be easily disregarded as &amp;quot;bad translation.&amp;quot;&lt;/p&gt; &lt;p&gt;I thought it may be easier to parse news as a brief summary (crucially one that links to the original source), followed by a list and description of each named Entity, why they are important to the story and the broader context. Then a list of important dates and timeframes mentioned in the article.&lt;/p&gt; &lt;p&gt;mdBook provides a great, distraction-free reading experience in the style of a book. I hate databases and extra layers of complexity so this provides the basis for the web based version of the final product. The code also builds a JSON API that allows you to plumb the data for interesting trends or find a needle in a haystack.&lt;/p&gt; &lt;p&gt;For example we can collate all of the Named Entites listed, alongside a given Named Entity, for all of the articles in a publication.&lt;/p&gt; &lt;p&gt;&lt;code&gt;mdBook&lt;/code&gt; also provides for us a fantastic search feature that requires no external database as a dependency. The entire project website is made of static, flat-files.&lt;/p&gt; &lt;p&gt;The Rust library that calls Open-AI compatible API's for model inference, &lt;code&gt;aj&lt;/code&gt; is available on my Github: &lt;a href="https://github.com/graves/awful%5C_aj"&gt;https://github.com/graves/awful\_aj&lt;/a&gt;. The blog post linked to at the top of this post contains details on how the prompt engineering works. It uses &lt;code&gt;yaml&lt;/code&gt; files to specify everything necessary. Personally, I find it much easier to work with, when actually typing, than &lt;code&gt;json&lt;/code&gt; or in-line code. This library can also be used as a command line client to call Open-AI compatible APIs AND has a home-rolled custom Vector Database implementation that allows your conversation to recall memories that fall outside of the conversation context. There is an &lt;code&gt;interactive&lt;/code&gt; mode and an &lt;code&gt;ask&lt;/code&gt; mode that will just print the LLM inference response content to stdout.&lt;/p&gt; &lt;p&gt;The Rust command line client that uses &lt;code&gt;aj&lt;/code&gt; as dependency and actually organizes Qwen's responses into a daily news publication fit for &lt;code&gt;mdBook&lt;/code&gt; is also available on my Github: &lt;a href="https://github.com/graves/awful%5C_text%5C_news"&gt;https://github.com/graves/awful\_text\_news&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;mdBook&lt;/code&gt; project I used as a starting point for the first few runs is also available on my Github: &lt;a href="https://github.com/graves/awful_security_news"&gt;https://github.com/graves/awful_security_news&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are some interesting things I'd like to do like add the astrological moon phase to each edition (without using an external service). I'd also like to build parody site to act as a mirror to the world's events, and use the &lt;a href="https://huggingface.co/teknium/Mistral-Trismegistus-7B"&gt;Mistral Trismegistus model&lt;/a&gt; to rewrite the world's events from the &lt;strong&gt;perspective of angelic intervention being the initiating factor of each key event.&lt;/strong&gt; 😇🌙😇&lt;/p&gt; &lt;p&gt;Contributions to the code are welcome and both the site and API are free to use and will remain free to use as long as I am physically capable of keeping them running.&lt;/p&gt; &lt;p&gt;I would love any feedback, tips, or discussion on how to make the site or tools that build it more useful. ♥️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sqli"&gt; /u/sqli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u28jb5s74a0f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkjyvk/a_collection_of_open_source_tools_to_summarize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkjyvk/a_collection_of_open_source_tools_to_summarize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T04:34:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kko4xu</id>
    <title>Support for InternVL has been merged into llama.cpp</title>
    <updated>2025-05-12T09:21:46+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13422"&gt;https://github.com/ggml-org/llama.cpp/pull/13422&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13443"&gt;https://github.com/ggml-org/llama.cpp/pull/13443&lt;/a&gt;&lt;/p&gt; &lt;p&gt;when GGUF? ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kko4xu/support_for_internvl_has_been_merged_into_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kko4xu/support_for_internvl_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kko4xu/support_for_internvl_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T09:21:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkbhxr</id>
    <title>Wow! DeerFlow is OSS now: LLM + Langchain + tools (web search, crawler, code exec)</title>
    <updated>2025-05-11T21:11:32+00:00</updated>
    <author>
      <name>/u/behradkhodayar</name>
      <uri>https://old.reddit.com/user/behradkhodayar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bytedance (the company behind TikTok), opensourced DeerFlow (&lt;strong&gt;D&lt;/strong&gt;eep &lt;strong&gt;E&lt;/strong&gt;xploration and &lt;strong&gt;E&lt;/strong&gt;fficient &lt;strong&gt;R&lt;/strong&gt;esearch &lt;strong&gt;Flow&lt;/strong&gt;), such a great give-back.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bytedance/deer-flow"&gt;https://github.com/bytedance/deer-flow&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/behradkhodayar"&gt; /u/behradkhodayar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkbhxr/wow_deerflow_is_oss_now_llm_langchain_tools_web/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkbhxr/wow_deerflow_is_oss_now_llm_langchain_tools_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkbhxr/wow_deerflow_is_oss_now_llm_langchain_tools_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T21:11:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkl39r</id>
    <title>Findings from LoRA Finetuning for Qwen3</title>
    <updated>2025-05-12T05:46:44+00:00</updated>
    <author>
      <name>/u/Reader3123</name>
      <uri>https://old.reddit.com/user/Reader3123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Fine-tuned Qwen3-8B with a small LoRA setup to preserve its ability to switch behaviors using &lt;code&gt;/think&lt;/code&gt; (reasoning) and &lt;code&gt;/no_think&lt;/code&gt; (casual) prompts. Rank 8 gave the best results. Training took ~30 minutes for 8B using 4,000 examples. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;LoRA Rank Testing Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;Rank 8&lt;/strong&gt;: Best outcome—preserved both &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; behavior.&lt;/li&gt; &lt;li&gt;❌ &lt;strong&gt;Rank 32&lt;/strong&gt;: Model started ignoring the &lt;code&gt;/think&lt;/code&gt; prompt.&lt;/li&gt; &lt;li&gt;💀 &lt;strong&gt;Rank 64&lt;/strong&gt;: Completely broke—output became nonsensical.&lt;/li&gt; &lt;li&gt;🧠 &lt;strong&gt;Rank 128&lt;/strong&gt;: Overfit hard—model became overly STUPID&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Training Configuration:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Applied LoRA to: &lt;code&gt;q_proj&lt;/code&gt;, &lt;code&gt;k_proj&lt;/code&gt;, &lt;code&gt;v_proj&lt;/code&gt;, &lt;code&gt;o_proj&lt;/code&gt;, &lt;code&gt;gate_proj&lt;/code&gt;, &lt;code&gt;up_proj&lt;/code&gt;, &lt;code&gt;down_proj&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Rank: 8&lt;/li&gt; &lt;li&gt;Alpha: 16&lt;/li&gt; &lt;li&gt;Dropout: 0.05&lt;/li&gt; &lt;li&gt;Bias: Disabled&lt;/li&gt; &lt;li&gt;Gradient Checkpointing: Enabled to reduce memory usage&lt;/li&gt; &lt;li&gt;Batch Size: 2&lt;/li&gt; &lt;li&gt;Gradient Accumulation: 4 steps&lt;/li&gt; &lt;li&gt;Learning Rate: 2e-4&lt;/li&gt; &lt;li&gt;Epochs: 1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also tested whether full finetuning or using the model without 4-bit quantization would help. Neither approach gave better results. In fact, the model sometimes performed worse or became inconsistent in responding to &lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt;. This confirmed that lightweight LoRA with rank 8 was the ideal trade-off between performance and resource use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Collection:&lt;/strong&gt; 👉 &lt;a href="https://huggingface.co/collections/soob3123/grayline-collection-qwen3-6821009e843331c5a9c27da1"&gt;GrayLine-Qwen3 Collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future Plans:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-32B&lt;/li&gt; &lt;li&gt;Try fine-tuning Qwen3-30B-A3B (MoE version) to see if it handles behavior switching better at scale.&lt;/li&gt; &lt;li&gt;Run full benchmark evaluations using LM-Eval to better understand model performance across reasoning, safety, and general capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know if you want me to try any other configs!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reader3123"&gt; /u/Reader3123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkl39r/findings_from_lora_finetuning_for_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkl39r/findings_from_lora_finetuning_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkl39r/findings_from_lora_finetuning_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T05:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkea2w</id>
    <title>LPT: Got an old low VRAM GPU you're not using? Use it to increase your VRAM pool.</title>
    <updated>2025-05-11T23:23:26+00:00</updated>
    <author>
      <name>/u/pneuny</name>
      <uri>https://old.reddit.com/user/pneuny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got an RTX 5060 Ti 16GB, but 16GB is still not enough to fit something like Qwen 3 30b-a3b. That's where the old GTX 1060 I got in return for handing down a 3060 Ti comes in handy. In LMStudio, using the Vulkan backend, with full GPU offloading to both the RTX and GTX cards, I managed to get 43 t/s, which is way better than the ~13 t/s with partial CPU offloading when using CUDA 12.&lt;/p&gt; &lt;p&gt;So yeah, if you have a 16GB card, break out that old card and add it to your system if your motherboard has the PCIE slot to spare.&lt;/p&gt; &lt;p&gt;PS: This also gives you 32 bit physx support on your RTX 50 series if the old card is Nvidia.&lt;/p&gt; &lt;p&gt;TL;DR: RTX 5060 Ti 16GB + GTX 1060 6GB = 43t/s on Qwen3 30b-a3b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pneuny"&gt; /u/pneuny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkea2w/lpt_got_an_old_low_vram_gpu_youre_not_using_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkea2w/lpt_got_an_old_low_vram_gpu_youre_not_using_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkea2w/lpt_got_an_old_low_vram_gpu_youre_not_using_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T23:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1kk97m7</id>
    <title>We made an open source agent builder and framework designed to work with local llms!</title>
    <updated>2025-05-11T19:31:37+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk97m7/we_made_an_open_source_agent_builder_and/"&gt; &lt;img alt="We made an open source agent builder and framework designed to work with local llms!" src="https://preview.redd.it/ha9ptoygf70f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89d2d79a3d2e7586b294f58dfb84c68117b05a1a" title="We made an open source agent builder and framework designed to work with local llms!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ha9ptoygf70f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kk97m7/we_made_an_open_source_agent_builder_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kk97m7/we_made_an_open_source_agent_builder_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-11T19:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1kkgzip</id>
    <title>INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning</title>
    <updated>2025-05-12T01:46:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"&gt; &lt;img alt="INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning" src="https://external-preview.redd.it/C1X5HGKGzXyAtD9lvvvB3VxlaW_Pl5NuFtz4_fp414w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8977006bf732e56b214f916d46801909a0bb97fa" title="INTELLECT-2 Released: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kkgzip/intellect2_released_the_first_32b_parameter_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-05-12T01:46:22+00:00</published>
  </entry>
</feed>
