<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-04-30T17:06:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1kb8yyw</id>
    <title>DFloat11: Lossless LLM Compression for Efficient GPU Inference</title>
    <updated>2025-04-30T05:31:23+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/LeanModels/DFloat11"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb8yyw/dfloat11_lossless_llm_compression_for_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb8yyw/dfloat11_lossless_llm_compression_for_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T05:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbdi9l</id>
    <title>Raspberry Pi 5: a small comparison between Qwen3 0.6B and Microsoft's new BitNet model</title>
    <updated>2025-04-30T10:53:05+00:00</updated>
    <author>
      <name>/u/privacyparachute</name>
      <uri>https://old.reddit.com/user/privacyparachute</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdi9l/raspberry_pi_5_a_small_comparison_between_qwen3/"&gt; &lt;img alt="Raspberry Pi 5: a small comparison between Qwen3 0.6B and Microsoft's new BitNet model" src="https://b.thumbs.redditmedia.com/6lnRiaUi2Hx1wJ6i_Smnb5wkjtY8gFBP5W7g2zDSZzI.jpg" title="Raspberry Pi 5: a small comparison between Qwen3 0.6B and Microsoft's new BitNet model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been doing some quick tests today, and wanted to share my results. I was testing this for a local voice assistant feature. The Raspberry Pi has 4Gb of memory, and is running a &lt;a href="https://www.candlesmarthome.com"&gt;smart home controller&lt;/a&gt; at the same time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen 3 0.6B, Q4 gguf&lt;/strong&gt; &lt;em&gt;using llama.cpp&lt;/em&gt;&lt;br /&gt; - 0.6GB in size&lt;br /&gt; - Uses 600MB of memory&lt;br /&gt; - About 20 tokens per second&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;`./llama-cli -m qwen3_06B_Q4.gguf -c 4096 -cnv -t 4`&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0k8pgez1cyxe1.png?width=2644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=374a7543adcf213a1835a5b8cd39c4c25bf4a0f4"&gt;https://preview.redd.it/0k8pgez1cyxe1.png?width=2644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=374a7543adcf213a1835a5b8cd39c4c25bf4a0f4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;BitNet-b1.58-2B-4T&lt;/strong&gt; &lt;em&gt;using&lt;/em&gt; &lt;a href="https://www.bijanbowen.com/bitnet-b1-58-on-raspberry-pi-4b/"&gt;BitNet&lt;/a&gt; &lt;em&gt;(Microsoft's fork of llama.cpp)&lt;/em&gt;&lt;br /&gt; - 1.2GB in size&lt;br /&gt; - Uses 300MB of memory (!)&lt;br /&gt; - About 7 tokens per second&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mzftb1x4cyxe1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a739aae47625710b9378a37b0ffac5cc030ab11f"&gt;https://preview.redd.it/mzftb1x4cyxe1.png?width=1784&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a739aae47625710b9378a37b0ffac5cc030ab11f&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;`python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p &amp;quot;Hello from BitNet on Pi5!&amp;quot; -cnv -t 4 -c 4096`&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The low memory use of the BitNet model seems pretty impressive? But what I don't understand is why the BitNet model is relatively slow. Is there a way to improve performance of the BitNet model? Or is Qwen 3 just that fast?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/privacyparachute"&gt; /u/privacyparachute &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdi9l/raspberry_pi_5_a_small_comparison_between_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdi9l/raspberry_pi_5_a_small_comparison_between_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdi9l/raspberry_pi_5_a_small_comparison_between_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T10:53:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1kay93z</id>
    <title>You can run Qwen3-30B-A3B on a 16GB RAM CPU-only PC!</title>
    <updated>2025-04-29T20:36:39+00:00</updated>
    <author>
      <name>/u/Foxiya</name>
      <uri>https://old.reddit.com/user/Foxiya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got the Qwen3-30B-A3B model in q4 running on my CPU-only PC using llama.cpp, and honestly, I’m blown away by how well it's performing. I'm running the q4 quantized version of the model, and despite having just 16GB of RAM and no GPU, I’m consistently getting more than 10 tokens per second.&lt;/p&gt; &lt;p&gt;I wasnt expecting much given the size of the model and my relatively modest hardware setup. I figured it would crawl or maybe not even load at all, but to my surprise, it's actually snappy and responsive for many tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foxiya"&gt; /u/Foxiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kay93z/you_can_run_qwen330ba3b_on_a_16gb_ram_cpuonly_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kay93z/you_can_run_qwen330ba3b_on_a_16gb_ram_cpuonly_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kay93z/you_can_run_qwen330ba3b_on_a_16gb_ram_cpuonly_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-29T20:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbj97u</id>
    <title>Local / Private voice agent via Ollama, Kokoro, Whisper, LiveKit</title>
    <updated>2025-04-30T15:26:00+00:00</updated>
    <author>
      <name>/u/Shayps</name>
      <uri>https://old.reddit.com/user/Shayps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a totally local Speech-to-Speech agent that runs completely on CPU (mostly because I'm a mac user) with a combo of the following: &lt;/p&gt; &lt;p&gt;- Whisper via Vox-box for STT: &lt;a href="https://github.com/gpustack/vox-box"&gt;https://github.com/gpustack/vox-box&lt;/a&gt;&lt;br /&gt; - Ollama w/ Gemma3:4b for LLM: &lt;a href="https://ollama.com"&gt;https://ollama.com&lt;/a&gt;&lt;br /&gt; - Kokoro via FastAPI by remsky for TTS: &lt;a href="https://github.com/remsky/Kokoro-FastAPI"&gt;https://github.com/remsky/Kokoro-FastAPI&lt;/a&gt;&lt;br /&gt; - LiveKit Server for agent orchestration and transport: &lt;a href="https://github.com/livekit/livekit"&gt;https://github.com/livekit/livekit&lt;/a&gt;&lt;br /&gt; - LiveKit Agents for all of the agent logic and gluing together the STT / LLM / TTS pipeline: &lt;a href="https://github.com/livekit/agents"&gt;https://github.com/livekit/agents&lt;/a&gt;&lt;br /&gt; - The Web Voice Assistant template in Next.js: &lt;a href="https://github.com/livekit-examples/voice-assistant-frontend"&gt;https://github.com/livekit-examples/voice-assistant-frontend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I used `all-MiniLM-L6-v2` as the embedding model and FAISS for efficient similarity search, both to optimize performance and minimize RAM usage. &lt;/p&gt; &lt;p&gt;Ollama tends to reload the model when switching between embedding and completion endpoints, so this approach avoids that issue. If anyone hows how to fix this, I might switch back to Ollama for embeddings, but I legit could not find the answer anywhere.&lt;/p&gt; &lt;p&gt;If you want, you could modify the project to use GPU as well—which would dramatically improve response speed, but then it will only run on Linux machines. Will probably ship some changes soon to make it easier.&lt;/p&gt; &lt;p&gt;There's some issues with WSL audio and network connections via Docker, so it doesn't work on Windows yet, but I'm hoping to get it working at some point (or I'm always happy to see PRs &amp;lt;3) &lt;/p&gt; &lt;p&gt;The repo: &lt;a href="https://github.com/ShayneP/local-voice-ai"&gt;https://github.com/ShayneP/local-voice-ai&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Run the project with `./test.sh`&lt;/p&gt; &lt;p&gt;If you run into any issues either drop a note on the repo or let me know here and I'll try to fix it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shayps"&gt; /u/Shayps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbj97u/local_private_voice_agent_via_ollama_kokoro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbj97u/local_private_voice_agent_via_ollama_kokoro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbj97u/local_private_voice_agent_via_ollama_kokoro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T15:26:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbiokq</id>
    <title>deepseek-ai/DeepSeek-Prover-V2-7B · Hugging Face</title>
    <updated>2025-04-30T15:02:12+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbiokq/deepseekaideepseekproverv27b_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-Prover-V2-7B · Hugging Face" src="https://external-preview.redd.it/3dM4nULZ8IZaICRt-gDS2MMGpKEYwP5nklHZaBtAHkY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=525cd6be303a752c1086be826c8df28f01e94af1" title="deepseek-ai/DeepSeek-Prover-V2-7B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbiokq/deepseekaideepseekproverv27b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbiokq/deepseekaideepseekproverv27b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T15:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb97ys</id>
    <title>ubergarm/Qwen3-235B-A22B-GGUF over 140 tok/s PP and 10 tok/s TG quant for gaming rigs!</title>
    <updated>2025-04-30T05:47:24+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb97ys/ubergarmqwen3235ba22bgguf_over_140_toks_pp_and_10/"&gt; &lt;img alt="ubergarm/Qwen3-235B-A22B-GGUF over 140 tok/s PP and 10 tok/s TG quant for gaming rigs!" src="https://external-preview.redd.it/cYA9q4-I6muEuEtTYSLVR3CPWT7qR_y4KkbLTSvSOXY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b11d509cf500d20e1a15b7a569f5bf8fcc448a5f" title="ubergarm/Qwen3-235B-A22B-GGUF over 140 tok/s PP and 10 tok/s TG quant for gaming rigs!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just cooked up an experimental ik_llama.cpp exclusive 3.903 BPW quant blend for Qwen3-235B-A22B that delivers good quality and speed on a high end gaming rig fitting full 32k context in under 120 GB (V)RAM e.g. 24GB VRAM + 2x48GB DDR5 RAM.&lt;/p&gt; &lt;p&gt;Just benchmarked over 140 tok/s prompt processing and 10 tok/s generation on my 3090TI FE + AMD 9950X 96GB RAM DDR5-6400 gaming rig (see comment for graph).&lt;/p&gt; &lt;p&gt;Keep in mind this quant is *not* supported by mainline llama.cpp, ollama, koboldcpp, lm studio etc. I'm not releasing those as mainstream quality quants are available from bartowski, unsloth, mradermacher, et al.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb97ys/ubergarmqwen3235ba22bgguf_over_140_toks_pp_and_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb97ys/ubergarmqwen3235ba22bgguf_over_140_toks_pp_and_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T05:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbdk08</id>
    <title>GitHub - XiaomiMiMo/MiMo: MiMo: Unlocking the Reasoning Potential of Language Model – From Pretraining to Posttraining</title>
    <updated>2025-04-30T10:56:08+00:00</updated>
    <author>
      <name>/u/marcocastignoli</name>
      <uri>https://old.reddit.com/user/marcocastignoli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdk08/github_xiaomimimomimo_mimo_unlocking_the/"&gt; &lt;img alt="GitHub - XiaomiMiMo/MiMo: MiMo: Unlocking the Reasoning Potential of Language Model – From Pretraining to Posttraining" src="https://external-preview.redd.it/U_NJ00xovJr4fKj6AJwQ35Odzc6w6jc272-TXHXNUC4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=471cbfb82ac86afc15e99dceb63f180024f5a0b3" title="GitHub - XiaomiMiMo/MiMo: MiMo: Unlocking the Reasoning Potential of Language Model – From Pretraining to Posttraining" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marcocastignoli"&gt; /u/marcocastignoli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/XiaomiMiMo/MiMo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdk08/github_xiaomimimomimo_mimo_unlocking_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdk08/github_xiaomimimomimo_mimo_unlocking_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T10:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbkv2d</id>
    <title>Qwen3-30B-A3B is on another level (Appreciation Post)</title>
    <updated>2025-04-30T16:32:27+00:00</updated>
    <author>
      <name>/u/Prestigious-Use5483</name>
      <uri>https://old.reddit.com/user/Prestigious-Use5483</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: Qwen3-30B-A3B-UD-Q4_K_XL.gguf | 32K Context (Max Output 8K) | 95 Tokens/sec&lt;br /&gt; PC: Ryzen 7 7700 | 32GB DDR5 6000Mhz | RTX 3090 24GB VRAM | Win11 Pro x64 | KoboldCPP&lt;/p&gt; &lt;p&gt;Okay, I just wanted to share my extreme satisfaction for this model. It is lightning fast and I can keep it on 24/7 (while using my PC normally - aside from gaming of course). There's no need for me to bring up ChatGPT or Gemini anymore for general inquiries, since it's always running and I don't need to load it up every time I want to use it. I have deleted all other LLMs from my PC as well. This is now the standard for me and I won't settle for anything less.&lt;/p&gt; &lt;p&gt;For anyone just starting to use it, it took a few variants of the model to find the right one. The 4K_M one was bugged and would stay in an infinite loop. Now the UD-Q4_K_XL variant didn't have that issue and works as intended.&lt;/p&gt; &lt;p&gt;There isn't any point to this post other than to give credit and voice my satisfaction to all the people involved that made this model and variant. Kudos to you. I no longer feel FOMO either of wanting to upgrade my PC (GPU, RAM, architecture, etc.). This model is fantastic and I can't wait to see how it is improved upon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prestigious-Use5483"&gt; /u/Prestigious-Use5483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbkv2d/qwen330ba3b_is_on_another_level_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbkv2d/qwen330ba3b_is_on_another_level_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbkv2d/qwen330ba3b_is_on_another_level_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T16:32:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbl3vv</id>
    <title>Qwen just dropped an omnimodal model</title>
    <updated>2025-04-30T16:42:50+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Qwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaAneously generating text and natural speech responses in a streaming manner.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;There are &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-3B"&gt;3B&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-7B"&gt;7B&lt;/a&gt; variants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbl3vv/qwen_just_dropped_an_omnimodal_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbl3vv/qwen_just_dropped_an_omnimodal_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbl3vv/qwen_just_dropped_an_omnimodal_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T16:42:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbgj16</id>
    <title>Mellum Goes Open Source: A Purpose-Built LLM for Developers, Now on Hugging Face</title>
    <updated>2025-04-30T13:29:29+00:00</updated>
    <author>
      <name>/u/BarracudaPff</name>
      <uri>https://old.reddit.com/user/BarracudaPff</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbgj16/mellum_goes_open_source_a_purposebuilt_llm_for/"&gt; &lt;img alt="Mellum Goes Open Source: A Purpose-Built LLM for Developers, Now on Hugging Face" src="https://external-preview.redd.it/cO_ycPzjF9y-xz9dkp2pJWilKCTUD8M8MWZVSX3SYlQ.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09644e50897d70591f5620c1e80209e63a1d7658" title="Mellum Goes Open Source: A Purpose-Built LLM for Developers, Now on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BarracudaPff"&gt; /u/BarracudaPff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.jetbrains.com/ai/2025/04/mellum-goes-open-source-a-purpose-built-llm-for-developers-now-on-hugging-face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbgj16/mellum_goes_open_source_a_purposebuilt_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbgj16/mellum_goes_open_source_a_purposebuilt_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T13:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbazrd</id>
    <title>Qwen3 on LiveBench</title>
    <updated>2025-04-30T07:52:20+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbazrd/qwen3_on_livebench/"&gt; &lt;img alt="Qwen3 on LiveBench" src="https://b.thumbs.redditmedia.com/IlWChU1967WBI_0P25pC4bLTg5smTk6gYbPMDSLkzTM.jpg" title="Qwen3 on LiveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://livebench.ai/#/"&gt;https://livebench.ai/#/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9wg6nkargxxe1.png?width=925&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d9016c13d45318a17731b376fb4e39f640251aa"&gt;https://preview.redd.it/9wg6nkargxxe1.png?width=925&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d9016c13d45318a17731b376fb4e39f640251aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n6nx96prgxxe1.png?width=947&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b4d9a9ac2f50f9bd95667bc088b3d388536d09b"&gt;https://preview.redd.it/n6nx96prgxxe1.png?width=947&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b4d9a9ac2f50f9bd95667bc088b3d388536d09b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bqf2671sgxxe1.png?width=940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e465c0584d3a19b3dd372b86397f737ac8d04e5c"&gt;https://preview.redd.it/bqf2671sgxxe1.png?width=940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e465c0584d3a19b3dd372b86397f737ac8d04e5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3mi1zmhxgxxe1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=069841c9dada3aaf5c340977f6e0db382e868c53"&gt;https://preview.redd.it/3mi1zmhxgxxe1.png?width=943&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=069841c9dada3aaf5c340977f6e0db382e868c53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/28rqjpuxgxxe1.png?width=1048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dacdc258ff44e6eabcb2bc94c555a7e84d36662b"&gt;https://preview.redd.it/28rqjpuxgxxe1.png?width=1048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dacdc258ff44e6eabcb2bc94c555a7e84d36662b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbazrd/qwen3_on_livebench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbazrd/qwen3_on_livebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbazrd/qwen3_on_livebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T07:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbf1m1</id>
    <title>Qwen3-30B-A3B solves the o1-preview Cipher problem!</title>
    <updated>2025-04-30T12:19:43+00:00</updated>
    <author>
      <name>/u/sunpazed</name>
      <uri>https://old.reddit.com/user/sunpazed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-30B-A3B (4_0 quant) solves the Cipher problem first showcased in the &lt;a href="https://openai.com/index/learning-to-reason-with-llms/"&gt;OpenAI o1-preview Technical Paper&lt;/a&gt;. Only 2 months ago &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j4s0o4/qwq32b_solves_the_o1preview_cipher_problem/"&gt;QwQ solved it in 32 minutes&lt;/a&gt;, while now Qwen3 solves it in 5 minutes! Obviously the MoE greatly improves performance, but it is interesting to note Qwen3 uses 20% less tokens. I'm impressed that I can run a o1-class model on a MacBook.&lt;/p&gt; &lt;p&gt;Here's the full output from llama.cpp;&lt;br /&gt; &lt;a href="https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4"&gt;https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunpazed"&gt; /u/sunpazed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbf1m1/qwen330ba3b_solves_the_o1preview_cipher_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbf1m1/qwen330ba3b_solves_the_o1preview_cipher_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbf1m1/qwen330ba3b_solves_the_o1preview_cipher_problem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T12:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbfkad</id>
    <title>Granite 4 Pull requests submitted to vllm and transformers</title>
    <updated>2025-04-30T12:45:11+00:00</updated>
    <author>
      <name>/u/a_slay_nub</name>
      <uri>https://old.reddit.com/user/a_slay_nub</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbfkad/granite_4_pull_requests_submitted_to_vllm_and/"&gt; &lt;img alt="Granite 4 Pull requests submitted to vllm and transformers" src="https://external-preview.redd.it/lmY6SZWhi1CjBygy9eRmfiGrXYfX6mezsuQz9ssU3Sk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c22013b44e73f2300cd86dcf4a2c506feb28337" title="Granite 4 Pull requests submitted to vllm and transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_slay_nub"&gt; /u/a_slay_nub &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/17461"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbfkad/granite_4_pull_requests_submitted_to_vllm_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbfkad/granite_4_pull_requests_submitted_to_vllm_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T12:45:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbbz1n</id>
    <title>New model DeepSeek-Prover-V2-671B</title>
    <updated>2025-04-30T09:06:09+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbz1n/new_model_deepseekproverv2671b/"&gt; &lt;img alt="New model DeepSeek-Prover-V2-671B" src="https://preview.redd.it/v83oaiaztxxe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73be969d15c5e7a4e0f77d22ddd89e84ed0cccd6" title="New model DeepSeek-Prover-V2-671B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;link: &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B/tree/main"&gt;https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B/tree/main&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v83oaiaztxxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbz1n/new_model_deepseekproverv2671b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbz1n/new_model_deepseekproverv2671b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T09:06:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbj6q3</id>
    <title>A new DeepSeek just released [ deepseek-ai/DeepSeek-Prover-V2-671B ]</title>
    <updated>2025-04-30T15:23:00+00:00</updated>
    <author>
      <name>/u/Rare-Programmer-1747</name>
      <uri>https://old.reddit.com/user/Rare-Programmer-1747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbj6q3/a_new_deepseek_just_released/"&gt; &lt;img alt="A new DeepSeek just released [ deepseek-ai/DeepSeek-Prover-V2-671B ]" src="https://a.thumbs.redditmedia.com/sFXiY4ugTYxgzlP6ZnhuK0ye_7LyfpwMgctzbJXbm_0.jpg" title="A new DeepSeek just released [ deepseek-ai/DeepSeek-Prover-V2-671B ]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new DeepSeek model has recently been released. You can find information about it on Hugging Face.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g1m3lns4pzxe1.png?width=3572&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=412b93e8ccd03433e05c114ad27200f484ab3ec1"&gt;https://preview.redd.it/g1m3lns4pzxe1.png?width=3572&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=412b93e8ccd03433e05c114ad27200f484ab3ec1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A new language model has been released: DeepSeek-Prover-V2.&lt;/p&gt; &lt;p&gt;This model is designed specifically for formal theorem proving in Lean 4. It uses advanced techniques involving recursive proof search and learning from both informal and formal mathematical reasoning.&lt;/p&gt; &lt;p&gt;The model, DeepSeek-Prover-V2-671B, shows strong performance on theorem proving benchmarks like MiniF2F-test and PutnamBench. A new benchmark called ProverBench, featuring problems from AIME and textbooks, was also introduced alongside the model.&lt;/p&gt; &lt;p&gt;This represents a significant step in using AI for mathematical theorem proving.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare-Programmer-1747"&gt; /u/Rare-Programmer-1747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbj6q3/a_new_deepseek_just_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbj6q3/a_new_deepseek_just_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbj6q3/a_new_deepseek_just_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T15:23:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbdzo2</id>
    <title>Qwen3 32B leading LiveBench / IF / story_generation</title>
    <updated>2025-04-30T11:22:01+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdzo2/qwen3_32b_leading_livebench_if_story_generation/"&gt; &lt;img alt="Qwen3 32B leading LiveBench / IF / story_generation" src="https://preview.redd.it/u3wxgjiaiyxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db301bb03849c31d1716dcac3b89a7a8e6e05f4a" title="Qwen3 32B leading LiveBench / IF / story_generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://livebench.ai/#/?IF=as"&gt;https://livebench.ai/#/?IF=as&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u3wxgjiaiyxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdzo2/qwen3_32b_leading_livebench_if_story_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbdzo2/qwen3_32b_leading_livebench_if_story_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T11:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbaecl</id>
    <title>Honestly, THUDM might be the new star on the horizon (creators of GLM-4)</title>
    <updated>2025-04-30T07:07:51+00:00</updated>
    <author>
      <name>/u/dampflokfreund</name>
      <uri>https://old.reddit.com/user/dampflokfreund</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've read many comments here saying that THUDM/GLM-4-32B-0414 is better than the latest Qwen 3 models and I have to agree. The 9B is also very good and fits in just 6 GB VRAM at IQ4_XS. These GLM-4 models have crazy efficient attention (less VRAM usage for context than any other model I've tried.)&lt;/p&gt; &lt;p&gt;It does better in my tests, I like its personality and writing style more and imo it also codes better. &lt;/p&gt; &lt;p&gt;I didn't expect these pretty unknown model creators to beat Qwen 3 to be honest, so if they keep it up they might have a chance to become the next DeepSeek. &lt;/p&gt; &lt;p&gt;There's nice room for improvement, like native multimodality, hybrid reasoning and better multilingual support (it leaks chinese characters sometimes, sadly)&lt;/p&gt; &lt;p&gt;What are your experiences with these models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dampflokfreund"&gt; /u/dampflokfreund &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbaecl/honestly_thudm_might_be_the_new_star_on_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbaecl/honestly_thudm_might_be_the_new_star_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbaecl/honestly_thudm_might_be_the_new_star_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T07:07:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbbt74</id>
    <title>DeepSeek-Prover-V2-671B is released</title>
    <updated>2025-04-30T08:54:11+00:00</updated>
    <author>
      <name>/u/Thin_Ad7360</name>
      <uri>https://old.reddit.com/user/Thin_Ad7360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B"&gt;https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thin_Ad7360"&gt; /u/Thin_Ad7360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbt74/deepseekproverv2671b_is_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbt74/deepseekproverv2671b_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbt74/deepseekproverv2671b_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T08:54:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb6bbl</id>
    <title>New study from Cohere shows Lmarena (formerly known as Lmsys Chatbot Arena) is heavily rigged against smaller open source model providers and favors big companies like Google, OpenAI and Meta</title>
    <updated>2025-04-30T02:54:14+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb6bbl/new_study_from_cohere_shows_lmarena_formerly/"&gt; &lt;img alt="New study from Cohere shows Lmarena (formerly known as Lmsys Chatbot Arena) is heavily rigged against smaller open source model providers and favors big companies like Google, OpenAI and Meta" src="https://b.thumbs.redditmedia.com/b4ReiD50vW0GnRbxWwbT4dVjH2h_fZBzjjqsVwguaHU.jpg" title="New study from Cohere shows Lmarena (formerly known as Lmsys Chatbot Arena) is heavily rigged against smaller open source model providers and favors big companies like Google, OpenAI and Meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Meta tested over 27 private variants, Google 10 to select the best performing one. \&lt;/li&gt; &lt;li&gt;OpenAI and Google get the majority of data from the arena (~40%). &lt;/li&gt; &lt;li&gt;All closed source providers get more frequently featured in the battles.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2504.20879"&gt;https://arxiv.org/abs/2504.20879&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1kb6bbl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb6bbl/new_study_from_cohere_shows_lmarena_formerly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb6bbl/new_study_from_cohere_shows_lmarena_formerly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T02:54:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbgug8</id>
    <title>Qwen/Qwen2.5-Omni-3B · Hugging Face</title>
    <updated>2025-04-30T13:43:06+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbgug8/qwenqwen25omni3b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen2.5-Omni-3B · Hugging Face" src="https://external-preview.redd.it/VrKoYH0rSUXnh7ZB3s06wxaBwwhqJf3tLxecGQXrrRk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b355cd4af64e469bec4d086fb697e376fa807fe9" title="Qwen/Qwen2.5-Omni-3B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Omni-3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbgug8/qwenqwen25omni3b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbgug8/qwenqwen25omni3b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T13:43:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1kb3gox</id>
    <title>Technically Correct, Qwen 3 working hard</title>
    <updated>2025-04-30T00:29:20+00:00</updated>
    <author>
      <name>/u/poli-cya</name>
      <uri>https://old.reddit.com/user/poli-cya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb3gox/technically_correct_qwen_3_working_hard/"&gt; &lt;img alt="Technically Correct, Qwen 3 working hard" src="https://preview.redd.it/dudbg02v9vxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40a85bf16e35037679b06a0406d1230e4e6050e5" title="Technically Correct, Qwen 3 working hard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poli-cya"&gt; /u/poli-cya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dudbg02v9vxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kb3gox/technically_correct_qwen_3_working_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kb3gox/technically_correct_qwen_3_working_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T00:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbfhxx</id>
    <title>Jetbrains opensourced their Mellum model</title>
    <updated>2025-04-30T12:42:04+00:00</updated>
    <author>
      <name>/u/stark-light</name>
      <uri>https://old.reddit.com/user/stark-light</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's now on Hugging Face: &lt;a href="https://huggingface.co/JetBrains/Mellum-4b-base"&gt;https://huggingface.co/JetBrains/Mellum-4b-base&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Their announcement: &lt;a href="https://blog.jetbrains.com/ai/2025/04/mellum-goes-open-source-a-purpose-built-llm-for-developers-now-on-hugging-face/"&gt;https://blog.jetbrains.com/ai/2025/04/mellum-goes-open-source-a-purpose-built-llm-for-developers-now-on-hugging-face/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stark-light"&gt; /u/stark-light &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbfhxx/jetbrains_opensourced_their_mellum_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbfhxx/jetbrains_opensourced_their_mellum_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbfhxx/jetbrains_opensourced_their_mellum_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T12:42:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbbcp8</id>
    <title>deepseek-ai/DeepSeek-Prover-V2-671B · Hugging Face</title>
    <updated>2025-04-30T08:18:47+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbcp8/deepseekaideepseekproverv2671b_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-Prover-V2-671B · Hugging Face" src="https://external-preview.redd.it/MfS0kbAv6ZSxumHFhRuKL1EVBrJ457E-QoycmpgMTBk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff61d01135c5b0a1b9141534d8a5e2c46dbfa952" title="deepseek-ai/DeepSeek-Prover-V2-671B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbcp8/deepseekaideepseekproverv2671b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbbcp8/deepseekaideepseekproverv2671b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T08:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbeoqw</id>
    <title>7B UI Model that does charts and interactive elements</title>
    <updated>2025-04-30T12:00:51+00:00</updated>
    <author>
      <name>/u/United-Rush4073</name>
      <uri>https://old.reddit.com/user/United-Rush4073</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbeoqw/7b_ui_model_that_does_charts_and_interactive/"&gt; &lt;img alt="7B UI Model that does charts and interactive elements" src="https://preview.redd.it/p1jwcst8pyxe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f2457b3e00c6d0f8ce792b7638ea74e15edea7f" title="7B UI Model that does charts and interactive elements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-T2-7B-Q8_0-GGUF"&gt;https://huggingface.co/Tesslate/UIGEN-T2-7B-Q8_0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Rush4073"&gt; /u/United-Rush4073 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p1jwcst8pyxe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbeoqw/7b_ui_model_that_does_charts_and_interactive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbeoqw/7b_ui_model_that_does_charts_and_interactive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T12:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1kbi47j</id>
    <title>Qwen3:4b runs on my 3.5 years old Pixel 6 phone</title>
    <updated>2025-04-30T14:38:27+00:00</updated>
    <author>
      <name>/u/osherz5</name>
      <uri>https://old.reddit.com/user/osherz5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbi47j/qwen34b_runs_on_my_35_years_old_pixel_6_phone/"&gt; &lt;img alt="Qwen3:4b runs on my 3.5 years old Pixel 6 phone" src="https://preview.redd.it/zbzcq79ihzxe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd015487d2280215d98a5300b8f7e6e99b586939" title="Qwen3:4b runs on my 3.5 years old Pixel 6 phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is a bit slow, but still I'm surprised that this is even possible.&lt;/p&gt; &lt;p&gt;Imagine being stuck somewhere with no network connectivity, running a model like this allows you to have a compressed knowledge base that can help you survive in whatever crazy situation you might find yourself in.&lt;/p&gt; &lt;p&gt;Managed to run 8b too, but it was even slower to the point of being impractical.&lt;/p&gt; &lt;p&gt;Truly exciting time to be alive!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/osherz5"&gt; /u/osherz5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zbzcq79ihzxe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1kbi47j/qwen34b_runs_on_my_35_years_old_pixel_6_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1kbi47j/qwen34b_runs_on_my_35_years_old_pixel_6_phone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-04-30T14:38:27+00:00</published>
  </entry>
</feed>
