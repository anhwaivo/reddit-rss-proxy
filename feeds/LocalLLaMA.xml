<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-16T23:48:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ms6iyu</id>
    <title>gpt‚Äëoss Is actually cooked? Well, the abliterated fine tunes at least.</title>
    <updated>2025-08-16T20:20:02+00:00</updated>
    <author>
      <name>/u/Claxvii</name>
      <uri>https://old.reddit.com/user/Claxvii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really liked these last agentic models that have been rolling out in the open‚Äësources scene, but gpt‚Äëoss, when I tried to run it, I ended up naming my tests ‚Äúgpt‚Äëass‚Äù and my conclusion was that the model was too censored and too ideologically inclined to get useful results out of anything other than a coding copilot. For coding tasks though, I preferred the qwen3‚Äëcoder models which are hell to make fit in a single GPU, so I usually suffer in speed quite a lot when using them.&lt;/p&gt; &lt;p&gt;The abliterated version, on the other hand, fits on my GPU with a hefty context length, and now that they work outside the constraints of a r/ singularity poster ideology, I really started to vibe with the model. It is really because of the capabilities threshold and the speed. I say it has become &lt;em&gt;go to&lt;/em&gt; agentic model I am using on my machine.&lt;/p&gt; &lt;p&gt;Have you guys tried it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Claxvii"&gt; /u/Claxvii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms6iyu/gptoss_is_actually_cooked_well_the_abliterated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms6iyu/gptoss_is_actually_cooked_well_the_abliterated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms6iyu/gptoss_is_actually_cooked_well_the_abliterated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T20:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrurtu</id>
    <title>How big a dataset do you need to finetune a model? Gemma3 270M, Qwen30B A3B, Gpt-OSS20B, etc.?</title>
    <updated>2025-08-16T13:07:37+00:00</updated>
    <author>
      <name>/u/zekuden</name>
      <uri>https://old.reddit.com/user/zekuden</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How big a dataset do you need to finetune a model? Gemma3 270M, Qwen30B A3B, Gpt-OSS20B, etc.?&lt;/p&gt; &lt;p&gt;other model information are welcome, these are just some examples of models to finetune.&lt;/p&gt; &lt;p&gt;and get consistent results. And as i understand it, for finetuning a dataset should look like:&lt;br /&gt; prompt:&lt;br /&gt; &amp;lt;dataset here&amp;gt;&lt;/p&gt; &lt;p&gt;output:&lt;br /&gt; &amp;lt;dataset here&amp;gt;&lt;/p&gt; &lt;p&gt;is that correct?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zekuden"&gt; /u/zekuden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrurtu/how_big_a_dataset_do_you_need_to_finetune_a_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrurtu/how_big_a_dataset_do_you_need_to_finetune_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrurtu/how_big_a_dataset_do_you_need_to_finetune_a_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T13:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms56t4</id>
    <title>A Guide to GRPO Fine-Tuning on Windows Using the TRL Library</title>
    <updated>2025-08-16T19:30:15+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms56t4/a_guide_to_grpo_finetuning_on_windows_using_the/"&gt; &lt;img alt="A Guide to GRPO Fine-Tuning on Windows Using the TRL Library" src="https://preview.redd.it/vc86avyknfjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f068ddf771afa38b821ea61669c858b8be8aa14e" title="A Guide to GRPO Fine-Tuning on Windows Using the TRL Library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wrote a hands-on guide for fine-tuning LLMs with GRPO (Group-Relative PPO) locally on Windows, using Hugging Face's TRL library. My goal was to create a practical workflow that doesn't require Colab or Linux.&lt;/p&gt; &lt;p&gt;The guide and the accompanying script focus on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A TRL-based implementation&lt;/strong&gt; that runs on consumer GPUs (with LoRA and optional 4-bit quantization).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A verifiable reward system&lt;/strong&gt; that uses numeric, format, and boilerplate checks to create a more reliable training signal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic data mapping&lt;/strong&gt; for most Hugging Face datasets to simplify preprocessing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical troubleshooting&lt;/strong&gt; and configuration notes for local setups.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is for anyone looking to experiment with reinforcement learning techniques on their own machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the blog post:&lt;/strong&gt; &lt;a href="https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323"&gt;&lt;code&gt;https://pavankunchalapk.medium.com/windows-friendly-grpo-fine-tuning-with-trl-from-zero-to-verifiable-rewards-f28008c89323&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get the code:&lt;/strong&gt; &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/trl-ppo-fine-tuning"&gt;Reinforcement-learning-with-verifable-rewards-Learnings/projects/trl-ppo-fine-tuning at main ¬∑ Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm open to any feedback. Thanks!&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Portfolio:&lt;/em&gt; &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;Pavan Kunchala - AI Engineer &amp;amp; Full-Stack Developer&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vc86avyknfjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms56t4/a_guide_to_grpo_finetuning_on_windows_using_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms56t4/a_guide_to_grpo_finetuning_on_windows_using_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:30:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrvuab</id>
    <title>I Want Everything Local ‚Äî Building My Offline AI Workspace</title>
    <updated>2025-08-16T13:50:50+00:00</updated>
    <author>
      <name>/u/badhiyahai</name>
      <uri>https://old.reddit.com/user/badhiyahai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrvuab/i_want_everything_local_building_my_offline_ai/"&gt; &lt;img alt="I Want Everything Local ‚Äî Building My Offline AI Workspace" src="https://external-preview.redd.it/QmTTXNVh4bThHR_hqXouTK6BMQOfpcZ731LR1bUqCFY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c04a6c25796a326d0d7b9bc93e22700684157322" title="I Want Everything Local ‚Äî Building My Offline AI Workspace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;I want everything local ‚Äî no cloud, no remote code execution.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;That‚Äôs what a friend said. That one-line requirement, albeit simple, would need multiple things to work in tandem to make it happen.&lt;/p&gt; &lt;p&gt;What does a mainstream LLM (Large Language Model) chat app like ChatGPT or Claude provide at a high level?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ability to use chat with a cloud hosted LLM,&lt;/li&gt; &lt;li&gt;Ability to run code generated by them mostly on their cloud infra, sometimes locally via shell,&lt;/li&gt; &lt;li&gt;Ability to access the internet for new content or services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With so many LLMs being open source / open weights, shouldn't it be possible to do all that locally? But just local LLM is not enough, we need a truely isolated environment to run code as well.&lt;/p&gt; &lt;p&gt;So, LLM for chat, Docker to containerize code execution, and finally a browser access of some sort for content.&lt;/p&gt; &lt;h1&gt;üß† The Idea&lt;/h1&gt; &lt;p&gt;We wanted a system where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLMs run completely &lt;strong&gt;locally&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code executes inside a lightweight VM&lt;/strong&gt;, not on the host machine&lt;/li&gt; &lt;li&gt;Bonus: &lt;strong&gt;headless browser&lt;/strong&gt; for automation and internet access&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ktxl2zdsydjf1.png?width=2668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c0db9f76936f89c242c0368fdff070deb6960af4"&gt;https://preview.redd.it/ktxl2zdsydjf1.png?width=2668&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c0db9f76936f89c242c0368fdff070deb6960af4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The idea was to perform tasks which require privacy to be executed completely locally, starting from planning via LLM to code execution inside a container. For instance, if you wanted to edit your photos or videos, how could you do it without giving your data to OpenAI/Google/Anthropic? Though they take security seriously (more than many), it's just a matter of one slip leading to your private data being compromised, a case in point being the early days of ChatGPT when user chats were &lt;a href="https://www.bloomberg.com/news/articles/2023-03-21/openai-shut-down-chatgpt-to-fix-bug-exposing-user-chat-titles"&gt;accessible&lt;/a&gt; from another's account!&lt;/p&gt; &lt;h1&gt;The Stack We Used&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLMs&lt;/strong&gt;: &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; for local models (also private models for now)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend UI&lt;/strong&gt;: &lt;a href="https://github.com/assistant-ui/assistant-ui"&gt;&lt;code&gt;assistant-ui&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sandboxed VM Runtime&lt;/strong&gt;: &lt;a href="https://github.com/apple/container"&gt;&lt;code&gt;container&lt;/code&gt;&lt;/a&gt; by Apple&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Orchestration&lt;/strong&gt;: &lt;a href="https://github.com/instavm/coderunner"&gt;&lt;code&gt;coderunner&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Browser Automation&lt;/strong&gt;: &lt;a href="https://playwright.dev"&gt;Playwright&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;üí° We ran this entirely on Apple Silicon, using &lt;code&gt;container&lt;/code&gt; for isolation.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;üõ†Ô∏è Our Attempt at a Mac App&lt;/h1&gt; &lt;p&gt;We started with zealous ambition: make it feel native. We tried using &lt;a href="http://a0.dev"&gt;&lt;code&gt;a0.dev&lt;/code&gt;&lt;/a&gt;, hoping it could help generate a Mac app. But it appears to be meant more for iOS app development ‚Äî and getting it to work for MacOS was painful, to say the least.&lt;/p&gt; &lt;p&gt;Even with help from the &amp;quot;world's best&amp;quot; LLMs, things didn't go quite as smoothly as we had expected. They hallucinated steps, missed platform-specific quirks, and often left us worse off.&lt;/p&gt; &lt;p&gt;Then we tried wrapping a &lt;code&gt;NextJS&lt;/code&gt; app inside Electron. It took us longer than we'd like to admit. As of this writing, it looks like there's just no (clean) way to do it.&lt;/p&gt; &lt;p&gt;So, we gave up on the Mac app. The local web version of &lt;code&gt;assistant-ui&lt;/code&gt; was good enough ‚Äî simple, configurable, and didn't fight back.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qxtnf0dwydjf1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65bd766164eaf8219879cce826ea5fcb84a2e190"&gt;https://preview.redd.it/qxtnf0dwydjf1.png?width=1580&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65bd766164eaf8219879cce826ea5fcb84a2e190&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Assistant UI&lt;/h1&gt; &lt;p&gt;We thought &lt;code&gt;Assistant-UI&lt;/code&gt; provided multiple LLM support out-of-the-box, as their landing page shows a drop-down of models. But, no. So, we had to look for examples on how to go about it, and &lt;code&gt;ai-sdk&lt;/code&gt; appeared to be the popular choice. Finally we had a dropdown for model selection. We decided not to restrict the set to just local models, as smaller local models are not quite there just yet. Users can get familiar with the tool and its capabilities, and later as small local models become better, they can just switch to being completely local.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m8bdeb00zdjf1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1eca9d8df54641293ff17b98ade701fa544baa8d"&gt;https://preview.redd.it/m8bdeb00zdjf1.png?width=1096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1eca9d8df54641293ff17b98ade701fa544baa8d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Tool-calling&lt;/h1&gt; &lt;p&gt;Our use-case also required us to have models that support tool-calling. While some models do, Ollama has not implemented the tool support for them. For instance:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;responseBody: '{&amp;quot;error&amp;quot;:&amp;quot;registry.ollama.ai/library/deepseek-r1:8b does not support tools&amp;quot;}', &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And to add to the confusion, Ollama has decided to put this model under tool calling category on their site. Understandably, with the fast-moving AI landscape, it can be difficult for community driven projects to keep up.&lt;/p&gt; &lt;p&gt;At the moment, essential information like whether a model has tool-support or not, pricing per token, for various models are so fickle. A model's official page mentions tool-support but then tools like Ollama take a while to implement them. Anyway, we shouldn't complain - it's open source, we could've contributed.&lt;/p&gt; &lt;h1&gt;Containerized execution&lt;/h1&gt; &lt;p&gt;After the UI was MVP-level sorted, we moved on to the isolated VM part. Recently Apple released a tool called 'Container'. Yes, that's right. So, we checked it out and it seemed better than Docker as it provided one isolated VM per container - a perfect fit for running AI generated code. So, we deployed a Jupyter server in the VM, exposed it as MCP (Model Context Protocol) tool, and made it available at &lt;code&gt;http://coderunner.local:8222/mcp&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The advantage of MCPing vs a exposing an API is that existing tools that work with MCPs can use this right away. For instance, Claude Desktop and Gemini CLI can start executing AI-generated code with a simple config.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;mcpServers&amp;quot;: { &amp;quot;coderunner&amp;quot;: { &amp;quot;httpUrl&amp;quot;: &amp;quot;http://coderunner.local:8222/mcp&amp;quot; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see below, Claude figured out it should use the tool &lt;code&gt;execute_python_code&lt;/code&gt; exposed from our isolated VM via the MCP endpoint. Aside - if you want to just use the &lt;code&gt;coderunner&lt;/code&gt; bit as an MCP to execute code with your existing tools, the code for &lt;code&gt;coderunner&lt;/code&gt; is &lt;a href="https://github.com/instavm/coderunner"&gt;public&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l1xlqa83zdjf1.png?width=1224&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb7be9c8899eb659a67274ffec83b809ba8f9452"&gt;https://preview.redd.it/l1xlqa83zdjf1.png?width=1224&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb7be9c8899eb659a67274ffec83b809ba8f9452&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A tangent - if you're planning to work with Apple &lt;code&gt;container&lt;/code&gt; and building VM images using it, have an abundance of patience. The build keeps failing with &lt;code&gt;Trap&lt;/code&gt; error or just hangs without any output. To continue, you should &lt;code&gt;pkill&lt;/code&gt; all container processes and restart the &lt;code&gt;container&lt;/code&gt; tool. Then remove the &lt;code&gt;buildkit&lt;/code&gt; image so that the next &lt;code&gt;build&lt;/code&gt; process fetches a fresh one. And repeat the three steps till it successfully works; this can take hours. We are excited to see Apple container mature as it moves beyond its early stages.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Back to our app, we tested the &lt;code&gt;UI + LLMs + CodeRunner&lt;/code&gt; on a task to &lt;code&gt;edit a video&lt;/code&gt; and it worked!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b14cq1b7zdjf1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9131c4007e1d8ff6c1a60d8b4f2362902ce9c2ec"&gt;https://preview.redd.it/b14cq1b7zdjf1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9131c4007e1d8ff6c1a60d8b4f2362902ce9c2ec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I asked it to address me as Lord Voldemort as a sanity check for system instructions&lt;/em&gt;&lt;/p&gt; &lt;p&gt;After the coderunner was verified to be working, we decided to add the support of a headless browser. The main reason was to allow the app to look for new/updated tools/information online, for example, browsing github to find installation instruction for some tool it doesn't yet know about. Another reason was laying the foundation for &lt;code&gt;research&lt;/code&gt;. We chose Playwright for the task. We deployed it in the same container and exposed it as an MCP tool. Here is one task we asked it to do -&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ow78fju9zdjf1.jpg?width=1428&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=363cf853179b4c6bd1173dca1753414dd573fb61"&gt;https://preview.redd.it/ow78fju9zdjf1.jpg?width=1428&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=363cf853179b4c6bd1173dca1753414dd573fb61&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With this our basic set up was ready: &lt;strong&gt;Local LLM + Sandboxed arbitrary code execution + Headless browser&lt;/strong&gt; for latest information.&lt;/p&gt; &lt;h1&gt;What It Can Do (Examples)&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Do research on a topic&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generate and render charts&lt;/strong&gt; from CSV using plain English&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edit videos&lt;/strong&gt; (via &lt;code&gt;ffmpeg&lt;/code&gt;) ‚Äî e.g., ‚Äúcut between 0:10 and 1:00‚Äù&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edit images&lt;/strong&gt; ‚Äî resize, crop, convert formats&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Install tools from GitHub&lt;/strong&gt; in a containerized space&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use a headless browser&lt;/strong&gt; to fetch pages and summarize content etc.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Volumes and Isolation&lt;/h1&gt; &lt;p&gt;We mapped a volume from &lt;code&gt;~/.coderunner/assets&lt;/code&gt; (host) to &lt;code&gt;/app/uploads&lt;/code&gt; (container)&lt;/p&gt; &lt;p&gt;So files edited/generated stay in a safe shared space, &lt;strong&gt;but code never touches the host system&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Limitations &amp;amp; Next Steps&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Currently &lt;strong&gt;only works on Apple Silicon&lt;/strong&gt; (macOS 26 is optional)&lt;/li&gt; &lt;li&gt;Needs better UI for managing tools and output streaming&lt;/li&gt; &lt;li&gt;Headless browser is classified as bot by various sites&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;This is more than a just an experiment. It's a philosophy shift &lt;strong&gt;bringing compute and agency back to your machine&lt;/strong&gt;. No cloud dependency. No privacy tradeoffs. While the best models will probably be always with the giants, we hope that we will still have local tools which can get our day-to-day work done with the privacy we deserve.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We didn't just imagine it. We built it. And now, you can use it too.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;üîó Resources&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/assistant-ui/assistant-ui"&gt;assistant-ui&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/instavm/coderunner"&gt;instavm/coderunner&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/apple/container"&gt;Apple/container&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/instavm/coderunner-ui"&gt;instavm/coderunner-ui&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badhiyahai"&gt; /u/badhiyahai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrvuab/i_want_everything_local_building_my_offline_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrvuab/i_want_everything_local_building_my_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrvuab/i_want_everything_local_building_my_offline_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T13:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mryhac</id>
    <title>Bringing Computer Use to the Web</title>
    <updated>2025-08-16T15:29:24+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mryhac/bringing_computer_use_to_the_web/"&gt; &lt;img alt="Bringing Computer Use to the Web" src="https://external-preview.redd.it/OWwyYmp4YjFoZWpmMRcxEnlpDBBJVNjXlCDC4HUtgXjfB5ufLszRpp9PEi0H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bdb2cc2e27aa842ee354eb0dee4dd5ee4b0cd840" title="Bringing Computer Use to the Web" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are bringing Computer Use to the web, you can now control cloud desktops from JavaScript right in the browser.&lt;/p&gt; &lt;p&gt;Until today computer use was Python only shutting out web devs. Now you can automate real UIs without servers, VMs, or any weird work arounds.&lt;/p&gt; &lt;p&gt;What you can now build : Pixel-perfect UI tests,Live AI demos,In app assistants that actually move the cursor, or parallel automation streams for heavy workloads.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Read more here : &lt;a href="https://www.trycua.com/blog/bringing-computer-use-to-the-web"&gt;https://www.trycua.com/blog/bringing-computer-use-to-the-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x4psh9j1hejf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mryhac/bringing_computer_use_to_the_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mryhac/bringing_computer_use_to_the_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T15:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrsfcc</id>
    <title>OpenAI Cookbook - Verifying gpt-oss implementations</title>
    <updated>2025-08-16T11:21:32+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsfcc/openai_cookbook_verifying_gptoss_implementations/"&gt; &lt;img alt="OpenAI Cookbook - Verifying gpt-oss implementations" src="https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11ab391878f109e16178aaa55bd6d3f3b344fed6" title="OpenAI Cookbook - Verifying gpt-oss implementations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cookbook.openai.com/articles/gpt-oss/verifying-implementations"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsfcc/openai_cookbook_verifying_gptoss_implementations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsfcc/openai_cookbook_verifying_gptoss_implementations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T11:21:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms0au0</id>
    <title>Open-source Space Exploration Companion</title>
    <updated>2025-08-16T16:34:45+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms0au0/opensource_space_exploration_companion/"&gt; &lt;img alt="Open-source Space Exploration Companion" src="https://external-preview.redd.it/lBImVBFuGg-C2JDLzGlY2MBioNw4CbsYYCfCO_AqUVs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1ece6a1c35cf300733ce60d6d85df4bbbe21d78" title="Open-source Space Exploration Companion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/tarun7r/antrikshGPT"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms0au0/opensource_space_exploration_companion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms0au0/opensource_space_exploration_companion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T16:34:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1msa1n4</id>
    <title>So Steam finally got back to me</title>
    <updated>2025-08-16T22:34:49+00:00</updated>
    <author>
      <name>/u/ChrisZavadil</name>
      <uri>https://old.reddit.com/user/ChrisZavadil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 5 weeks of waiting for steam to approve my application that would allow users to input their own llms locally and communicate with them they told me that my app failed testing because it lacked the proper guardrails. They want me to block input and output for the LLM. Anybody put an unguarded LLM on Steam before?&lt;/p&gt; &lt;p&gt;I added a walledguard and re-uploaded, but for now I just made the full unrestricted version available on Itch if anyone wants to give it a try:&lt;br /&gt; &lt;a href="https://zavgaming.itch.io/megan-ai"&gt;https://zavgaming.itch.io/megan-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChrisZavadil"&gt; /u/ChrisZavadil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:34:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms7auo</id>
    <title>Docker Model Runner is really neat</title>
    <updated>2025-08-16T20:49:00+00:00</updated>
    <author>
      <name>/u/blue_marker_</name>
      <uri>https://old.reddit.com/user/blue_marker_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been exploring a variety of options for managing inference on my local setup. My needs involve bouncing back and forth between a handful of SOTA local models, running embeddings, things like that.&lt;/p&gt; &lt;p&gt;I just came across Docker's Model Runner: &lt;a href="https://docs.docker.com/ai/model-runner/"&gt;https://docs.docker.com/ai/model-runner/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More detailed explanation of how it runs here: &lt;a href="https://www.docker.com/blog/how-we-designed-model-runner-and-whats-next/"&gt;https://www.docker.com/blog/how-we-designed-model-runner-and-whats-next/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can easily download and manage models and there are some nice networking features, but it really shines in two areas:&lt;br /&gt; - When running in Docker Desktop on Mac, it runs the inference processes on the host, not in containers. This gives you full access to Metal GPU engine. When running on docker CE (e.g. on linux), it runs inside containers using optimized images to give you full Nvidia CUDA acceleration&lt;br /&gt; - It queues requests and loads / unloads models based on need. In my use case, I have times where I programmatically swap between multiple SOTA opensource models that do not fit into my system resources at the same time. This means that after using Model 1, if I make a request to Model 2, it will queue that request. As soon as Model 1 is not actively serving a request or have a queue of requests, it will unload it and then load in Model 2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blue_marker_"&gt; /u/blue_marker_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms7auo/docker_model_runner_is_really_neat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms7auo/docker_model_runner_is_really_neat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms7auo/docker_model_runner_is_really_neat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T20:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms1vrq</id>
    <title>UwU ‚Äì Generate CLI commands without leaving your terminal</title>
    <updated>2025-08-16T17:31:44+00:00</updated>
    <author>
      <name>/u/sonic_op</name>
      <uri>https://old.reddit.com/user/sonic_op</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms1vrq/uwu_generate_cli_commands_without_leaving_your/"&gt; &lt;img alt="UwU ‚Äì Generate CLI commands without leaving your terminal" src="https://external-preview.redd.it/qVKoWeU1jsYB4_TrvI8_kno2eiL6Oh3Ry4U7yQ0ZzSw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c58d3824533fb17c4d7447a25e3156a29ccadef" title="UwU ‚Äì Generate CLI commands without leaving your terminal" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted a dead-simple way to quickly generate CLI commands without the overhead of Claude Code or Cursor, so I built it in an afternoon.&lt;/p&gt; &lt;p&gt;Supports Ollama and other local model servers. Qwen3 is quite good at generating commands, still not quite a good as GPT-5. &lt;/p&gt; &lt;p&gt;The project uses some zsh/bash magic to allow for quick editing of the model's response before running the command. Commands show up in your shell history, just like any other command.&lt;/p&gt; &lt;p&gt;UwU is intended to be very simple and hackable -- the whole project is one TypeScript file.&lt;/p&gt; &lt;p&gt;Feedback (and pull requests) welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sonic_op"&gt; /u/sonic_op &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/context-labs/uwu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms1vrq/uwu_generate_cli_commands_without_leaving_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms1vrq/uwu_generate_cli_commands_without_leaving_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T17:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mroal8</id>
    <title>Huihui-gpt-oss-120b-BF16-abliterated</title>
    <updated>2025-08-16T07:36:52+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mroal8/huihuigptoss120bbf16abliterated/"&gt; &lt;img alt="Huihui-gpt-oss-120b-BF16-abliterated" src="https://external-preview.redd.it/Whbl3EQ8tzvwyKl63iWfJrIBTWW6XBRLW7AQQgHk37I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a5c4cc8d89ef3ca3df1e29ec46225752e44231a" title="Huihui-gpt-oss-120b-BF16-abliterated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mroal8/huihuigptoss120bbf16abliterated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mroal8/huihuigptoss120bbf16abliterated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T07:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms5dih</id>
    <title>MiniPC Intel N150 CPU benchmark with Vulkan</title>
    <updated>2025-08-16T19:37:16+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kubuntu 25.04 running on miniPC with Intel N150 cpu, and 16Gb of DDR4 RAM using &lt;a href="https://huggingface.co/tinybiggames/Dolphin3.0-Llama3.1-8B-Q4_K_M-GGUF"&gt;Dolphin3.0-Llama3.1-8B-Q4_K_M&lt;/a&gt; model from &lt;a href="https://huggingface.co/"&gt;Huggingface&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Regular llama.cpp file &lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b6182/llama-b6182-bin-ubuntu-x64.zip"&gt;llama-b6182-bin-ubuntu-x64&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time ./llama-bench --model ~/Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf load_backend: loaded RPC backend from /home/user33/build/bin/libggml-rpc.so load_backend: loaded CPU backend from /home/user33/build/bin/libggml-cpu-alderlake.so | model | size | params | backend| ngl| test | t/s | | --------------------- | -------: | -----: | -------| --:| -----:| ---------: | | llama 8B Q4_K - Medium| 4.58 GiB | 8.03 B | RPC | 99 | pp512 | 7.14 ¬± 0.15| | llama 8B Q4_K - Medium| 4.58 GiB | 8.03 B | RPC | 99 | tg128 | 4.03 ¬± 0.02| build: 1fe00296 (6182) real 9m48.044s user 38m46.892s sys 0m2.007s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With VULKAN file &lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b6182/llama-b6182-bin-ubuntu-vulkan-x64.zip"&gt;llama-b6182-bin-ubuntu-vulkan-x64&lt;/a&gt; (same size and params)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;time ./llama-bench --model ~/Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf load_backend: loaded RPC backend from /home/user33/build/bin/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Intel(R) Graphics (ADL-N) (Intel open-source Mesa driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from /home/user33/build/bin/libggml-vulkan.so load_backend: loaded CPU backend from /home/user33/build/bin/libggml-cpu-alderlake.so | model | backend | ngl | test | t/s | | ---------------------- | ---------- | --: | ----: | -----------: | | llama 8B Q4_K - Medium | RPC,Vulkan | 99 | pp512 | 25.57 ¬± 0.01 | | llama 8B Q4_K - Medium | RPC,Vulkan | 99 | tg128 | 2.66 ¬± 0.00 | build: 1fe00296 (6182) real 6m5.129s user 1m5.952s sys 0m4.007s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Benchmark time dropped from 9m48s to 6m5s thanks to VULKAN&lt;/p&gt; &lt;p&gt;pp512 with VULKAN token per second with &lt;strong&gt;up&lt;/strong&gt; to 25.57 vs 8.03 t/s.&lt;/p&gt; &lt;p&gt;tg128 with VULKAN token per second went &lt;strong&gt;down&lt;/strong&gt; to 2.66 vs 4.03 t/s.&lt;/p&gt; &lt;p&gt;To Vulkan or not to Vulkan? Need to read lots of input data? Use Vulkan&lt;/p&gt; &lt;p&gt;Looking for quick answer like a chatbot Q/A then don't use Vulkan for now.&lt;/p&gt; &lt;p&gt;Having both downloaded and ready to use based usage pattern would be best bet for now with a miniPC.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms5dih/minipc_intel_n150_cpu_benchmark_with_vulkan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms5dih/minipc_intel_n150_cpu_benchmark_with_vulkan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms5dih/minipc_intel_n150_cpu_benchmark_with_vulkan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:37:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrbtqt</id>
    <title>DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM</title>
    <updated>2025-08-15T22:00:47+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt; &lt;img alt="DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM" src="https://external-preview.redd.it/dm1scXBiZnU4OWpmMbd7l6YK9EDz0b8q8nzrd_PHLYbyTzK6nb4d-_lrl57d.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a680cf7593e65adcab4110d0090bab480e862303" title="DINOv3 visualization tool running 100% locally in your browser on WebGPU/WASM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DINOv3 released yesterday, a new state-of-the-art vision backbone trained to produce rich, dense image features. I loved their demo video so much that I decided to re-create their visualization tool. &lt;/p&gt; &lt;p&gt;Everything runs locally in your browser with Transformers.js, using WebGPU if available and falling back to WASM if not. Hope you like it! &lt;/p&gt; &lt;p&gt;Link to demo + source code: &lt;a href="https://huggingface.co/spaces/webml-community/dinov3-web"&gt;https://huggingface.co/spaces/webml-community/dinov3-web&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yhe3jbfu89jf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-15T22:00:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms56fo</id>
    <title>Two V100s beat 2 Modded 3080 20GBs at Deepseek-R1:70B in Ollama</title>
    <updated>2025-08-16T19:29:52+00:00</updated>
    <author>
      <name>/u/AssociationAdept4052</name>
      <uri>https://old.reddit.com/user/AssociationAdept4052</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm not sure if I'm doing anything wrong, but Deepseek R1 70B ran *MUCH* faster on my dual Tesla V100 setup (16gb SXM2 and 32gb SXM2 in a 300G NVlink board), than my dual 3080 20GB. Yes it is 8GB less VRAM, but would that influence such a drastic difference in inference? I'm assuming its because the teslas have much better memory bandwidth, but is that the case, since everyone keeps telling me they are worse and not worth it?&lt;/p&gt; &lt;p&gt;I made sure it was both running on vram and all cards are being utilized.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ms56fo/video/0wl99bc8nfjf1/player"&gt;https://reddit.com/link/1ms56fo/video/0wl99bc8nfjf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ms56fo/video/se2lb1rwnfjf1/player"&gt;https://reddit.com/link/1ms56fo/video/se2lb1rwnfjf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AssociationAdept4052"&gt; /u/AssociationAdept4052 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms56fo/two_v100s_beat_2_modded_3080_20gbs_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms56fo/two_v100s_beat_2_modded_3080_20gbs_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms56fo/two_v100s_beat_2_modded_3080_20gbs_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:29:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1msb0mq</id>
    <title>For those who run large models locally.. HOW DO YOU AFFORD THOSE GPUS</title>
    <updated>2025-08-16T23:14:00+00:00</updated>
    <author>
      <name>/u/abaris243</name>
      <uri>https://old.reddit.com/user/abaris243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;okay I'm just being nosy.. I mostly run models and fine tune as a hobby so I typically only run models under the 10b parameter range, is everyone that is running larger models just paying for cloud services to run them? and for those of you who do have stacks of A100/H100s is this what you do for a living, how do you afford it??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abaris243"&gt; /u/abaris243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T23:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms9djc</id>
    <title>Wan2.2 i2v Censors Chinese-looking women in nsfw workflows</title>
    <updated>2025-08-16T22:08:13+00:00</updated>
    <author>
      <name>/u/dennisitnet</name>
      <uri>https://old.reddit.com/user/dennisitnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using wan2.2 i2v for generating over 100 nsfw videos so far. Noticed something curious. Lol When input image is chinese-looking, it never outputs nsfw videos. But when I use non-chinese input images, it outputs nsfw.&lt;/p&gt; &lt;p&gt;Anybody else experienced this? Lol really curious shiz&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dennisitnet"&gt; /u/dennisitnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrlpxd</id>
    <title>My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)</title>
    <updated>2025-08-16T05:21:33+00:00</updated>
    <author>
      <name>/u/FunConversation7257</name>
      <uri>https://old.reddit.com/user/FunConversation7257</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt; &lt;img alt="My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)" src="https://external-preview.redd.it/Os4oYZsYLVlsXnga3hPOUAlxvPVzcyCPA6N9lZAIVyQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7ea802471412bf40b6e93f29c186991e9a7c4e2" title="My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7qr019kqgbjf1.png?width=671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34985293292691f5bd4067ed3297e5fdaf6f0174"&gt;https://preview.redd.it/7qr019kqgbjf1.png?width=671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34985293292691f5bd4067ed3297e5fdaf6f0174&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recently, Codex, OpenAI's coding CLI released a way to authenticate with your ChatGPT account, and use that for usage instead of api keys. I dug through the code and saw that by using Codex CLI, you can login with your account and send requests right to OpenAI, albeit restricted by slightly tougher rate limits than on the ChatGPT app.&lt;/p&gt; &lt;p&gt;However, still was decent enough for my use case, so I made a python script which allows one to login with their ChatGPT account, and then serve a OpenAI compatible endpoint you can use programmatically or via a chat app of your choice.&lt;br /&gt; Might be useful for you too for data analysis, or just chatting in a better app than the ChatGPT desktop app. It's also customisable with thinking effort, and even sends back thinking summaries, and can use tools.&lt;/p&gt; &lt;p&gt;Not strictly &amp;quot;local&amp;quot;, but brought that 2023 vibe back, and thought it was kinda cool.&lt;/p&gt; &lt;p&gt;Will try to make it a better package soon than just python files.&lt;br /&gt; Github link: &lt;a href="https://github.com/RayBytes/ChatMock"&gt;https://github.com/RayBytes/ChatMock&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: have now also released a macos gui version, should be easier to use than simply running the flask server&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FunConversation7257"&gt; /u/FunConversation7257 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T05:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrsoug</id>
    <title>GPT-OSS-20B is in the sweet spot for building Agents</title>
    <updated>2025-08-16T11:34:29+00:00</updated>
    <author>
      <name>/u/sunpazed</name>
      <uri>https://old.reddit.com/user/sunpazed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The latest updates to llama.cpp greatly improve tool calling and stability with the OSS models. I have found that they are now quite reliable for my Agent Network, which runs a number of tools, ie; MCPs, RAG, and SQL answering, etc. The MoE and Quant enables me to run this quite easily on a 32Gb developer MacBook at ~40tks without breaking a sweat, I t‚Äôs almost game-changing! How has everyone else faired with these models??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunpazed"&gt; /u/sunpazed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsoug/gptoss20b_is_in_the_sweet_spot_for_building_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsoug/gptoss20b_is_in_the_sweet_spot_for_building_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsoug/gptoss20b_is_in_the_sweet_spot_for_building_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T11:34:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrz5gd</id>
    <title>Running LLM and VLM exclusively on AMD Ryzen AI NPU</title>
    <updated>2025-08-16T15:53:38+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre a small team working on &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; ‚Äî a lightweight runtime for running &lt;strong&gt;LLaMA, Qwen, DeepSeek, and now Gemma (Vision)&lt;/strong&gt; exclusively on the AMD Ryzen‚Ñ¢ AI NPU.&lt;/p&gt; &lt;p&gt;‚ö° Runs &lt;strong&gt;entirely on the NPU&lt;/strong&gt; ‚Äî no CPU or iGPU fallback.&lt;br /&gt; üëâ Think Ollama, but &lt;strong&gt;purpose-built for AMD NPUs&lt;/strong&gt;, with both CLI and REST API modes.&lt;/p&gt; &lt;p&gt;üîë Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports: &lt;strong&gt;LLaMA3.1/3.2, Qwen3, DeepSeek-R1, Gemma3:4B (Vision)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;First &lt;strong&gt;NPU-only VLM&lt;/strong&gt; shipped&lt;/li&gt; &lt;li&gt;Up to &lt;strong&gt;128K context&lt;/strong&gt; (LLaMA3.1/3.2, Gemma3:4B)&lt;/li&gt; &lt;li&gt;~11√ó power efficiency vs CPU/iGPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üëâ Repo here: &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;GitHub ‚Äì FastFlowLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôd love to hear your feedback if you give it a spin ‚Äî what works, what breaks, and what you‚Äôd like to see next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrz5gd/running_llm_and_vlm_exclusively_on_amd_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrz5gd/running_llm_and_vlm_exclusively_on_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrz5gd/running_llm_and_vlm_exclusively_on_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T15:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrfqsd</id>
    <title>Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months</title>
    <updated>2025-08-16T00:40:29+00:00</updated>
    <author>
      <name>/u/timfduffy</name>
      <uri>https://old.reddit.com/user/timfduffy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt; &lt;img alt="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" src="https://preview.redd.it/kbdu3pyq1ajf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6766455308e18a9b20204df7a38e2406f44eff0" title="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timfduffy"&gt; /u/timfduffy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kbdu3pyq1ajf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T00:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms0wov</id>
    <title>NSF and NVIDIA partnership enables Ai2 to develop fully open AI models to fuel U.S. scientific innovation</title>
    <updated>2025-08-16T16:56:48+00:00</updated>
    <author>
      <name>/u/skinnyjoints</name>
      <uri>https://old.reddit.com/user/skinnyjoints</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms0wov/nsf_and_nvidia_partnership_enables_ai2_to_develop/"&gt; &lt;img alt="NSF and NVIDIA partnership enables Ai2 to develop fully open AI models to fuel U.S. scientific innovation" src="https://external-preview.redd.it/oNkrlzevV17OdlTwW_iSTWkMB7fWbsRHpsJQwnpOvuY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddbfd29b8ef53d4cf463c7d6bd12a001c72ed4d8" title="NSF and NVIDIA partnership enables Ai2 to develop fully open AI models to fuel U.S. scientific innovation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm surprised this hasn‚Äôt been shared in this community yet. To me, this feels like a big deal.&lt;/p&gt; &lt;p&gt;Ai2 made some great models already (OLMo) and shared the training data and their methodology. With 152 million in support I‚Äôm excited to see what they build. The language from the NSF and Nvidia focuses on the creation of a larger open ecosystem for America. &lt;/p&gt; &lt;p&gt;Ai2‚Äôs statement ends with the discussion of creating new models and tools for the public:&lt;/p&gt; &lt;p&gt;‚ÄúWith this support, we‚Äôll produce leading open multimodal models, resources, and tools that help ensure America‚Äôs leadership in AI, building on the strong foundation we set with OLMo and Molmo.‚Äù&lt;/p&gt; &lt;p&gt;I think this could be the missing piece for enterprise level adoption of local builds (a fully transparent open-source model developed in America by a non-profit with government funding). &lt;/p&gt; &lt;p&gt;Ultimately, I think we are going to end up with a suite of models and tools with unprecedented documentation and support specifically designed for the local community to build and test new ideas. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skinnyjoints"&gt; /u/skinnyjoints &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nsf.gov/news/nsf-nvidia-partnership-enables-ai2-develop-fully-open-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms0wov/nsf_and_nvidia_partnership_enables_ai2_to_develop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms0wov/nsf_and_nvidia_partnership_enables_ai2_to_develop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T16:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrxuwd</id>
    <title>Best Opensource LM Studio alternative</title>
    <updated>2025-08-16T15:06:22+00:00</updated>
    <author>
      <name>/u/haterloco</name>
      <uri>https://old.reddit.com/user/haterloco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best app to use llama.cpp or Ollama with a GUI on Linux.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/haterloco"&gt; /u/haterloco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T15:06:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrqj6y</id>
    <title>Moxie goes local</title>
    <updated>2025-08-16T09:42:55+00:00</updated>
    <author>
      <name>/u/Over-Mix7071</name>
      <uri>https://old.reddit.com/user/Over-Mix7071</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt; &lt;img alt="Moxie goes local" src="https://external-preview.redd.it/NjRrNWZhaTZyY2pmMSz-4GeMjZaaPuK_BtqJdauJLy8SeG31djvp2OceGUPi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f56d4e2d6d85d38d0a6fee04a3f5cd06f2d2d7df" title="Moxie goes local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished a localllama version of the OpenMoxie&lt;/p&gt; &lt;p&gt;It uses faster-whisper on the local for STT or the OpenAi whisper api (when selected in setup)&lt;/p&gt; &lt;p&gt;Supports LocalLLaMA, or OpenAi for conversations.&lt;/p&gt; &lt;p&gt;I also added support for XAI (Grok3 et al ) using the XAI API.&lt;/p&gt; &lt;p&gt;allows you to select what AI model you want to run for the local service.. right now 3:2b &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Mix7071"&gt; /u/Over-Mix7071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eiwf36o6rcjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T09:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms222w</id>
    <title>"AGI" is equivalent to "BTC is going to take over the financial world"</title>
    <updated>2025-08-16T17:37:53+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;AGI&amp;quot; is really just another hypetrain. Sure AI is going to disrupt industries, displace jobs and cause mayhem in the social fabric - but the omnipotent &amp;quot;AGI&amp;quot; that governs all aspects of life and society and most importantly, ushers in &amp;quot;post labor economics&amp;quot;? Wonder how long it takes until tech bros and fanboys realize this. GPT5, Opus 4 and all others are only incremental improvements, if at all. Where's the path to &amp;quot;AGI&amp;quot; in this reality? People who believe this are going to build a bubble for themselves, detached from reality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T17:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms4n55</id>
    <title>What does it feel like: Cloud LLM vs Local LLM.</title>
    <updated>2025-08-16T19:10:29+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt; &lt;img alt="What does it feel like: Cloud LLM vs Local LLM." src="https://preview.redd.it/8qtcdau4kfjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64c4609d4440c5a870f624682bb7bead5dece104" title="What does it feel like: Cloud LLM vs Local LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't get me wrong, I love local models, but they give me this anxiety. We need to fix this... üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qtcdau4kfjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
