<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-21T17:28:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mw2xci</id>
    <title>monkeSearch's first prototype is now public, And it works! Offline natural language query for local files using a VERY small LLM (Qwen3-0.6b) and it works amazingly right away. With temporal awareness.</title>
    <updated>2025-08-21T06:03:45+00:00</updated>
    <author>
      <name>/u/fuckAIbruhIhateCorps</name>
      <uri>https://old.reddit.com/user/fuckAIbruhIhateCorps</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, this is a follow up post of my old post, which was about building a local natural language file search engine using qwen0.6b and LangExtract, and today I am very excited to release a very bare bones and working prototype for this!&lt;br /&gt; &lt;a href="https://github.com/monkesearch/monkeSearch"&gt;https://github.com/monkesearch/monkeSearch&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I'd love to get reviews and suggestions for this, and I've used macOS's inbuilt spotlight indexing for the query. There are a lot of modifications and feature additions to be done now but I want you guys to try it out locally. Current file search is only limited to a few file types because I am associating the macOS specific uniform type identifiers with file types, and that has been done manually just for the prototype right now. But I'd love to get ideas on how can I improve this. &lt;/p&gt; &lt;p&gt;No data leaves your pc and it is aimed at being able to run on potato pcs. And I'm currently aiming at a smaller and smarter model (Gemma 3 270M finetune) to increase the accuracy of the tool (even though it's pretty accurate right away with base Qwen3) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuckAIbruhIhateCorps"&gt; /u/fuckAIbruhIhateCorps &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2xci/monkesearchs_first_prototype_is_now_public_and_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2xci/monkesearchs_first_prototype_is_now_public_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2xci/monkesearchs_first_prototype_is_now_public_and_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwa9aa</id>
    <title>Weaponizing image scaling against production AI systems</title>
    <updated>2025-08-21T12:59:38+00:00</updated>
    <author>
      <name>/u/_QWUKE</name>
      <uri>https://old.reddit.com/user/_QWUKE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwa9aa/weaponizing_image_scaling_against_production_ai/"&gt; &lt;img alt="Weaponizing image scaling against production AI systems" src="https://external-preview.redd.it/hJirNYdbhQ_WgMVYs7Y4bwQFjFjmkaHKNpaeYLJaF-Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb5877f5b4fc041ce9c1154a7e0ce429457d0ef5" title="Weaponizing image scaling against production AI systems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_QWUKE"&gt; /u/_QWUKE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwa9aa/weaponizing_image_scaling_against_production_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwa9aa/weaponizing_image_scaling_against_production_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T12:59:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw9qgw</id>
    <title>[Project Release] Running TinyLlama on Intel NPU with OpenVINO (my first GitHub repo 🎉)</title>
    <updated>2025-08-21T12:36:46+00:00</updated>
    <author>
      <name>/u/Spiritual-Ad-5916</name>
      <uri>https://old.reddit.com/user/Spiritual-Ad-5916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw9qgw/project_release_running_tinyllama_on_intel_npu/"&gt; &lt;img alt="[Project Release] Running TinyLlama on Intel NPU with OpenVINO (my first GitHub repo 🎉)" src="https://external-preview.redd.it/cm53eHhtdXE5ZGtmMXwz12yAXJpbZNSu1z7afGTTOzAq2FgoU-VUYNvwwCr8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7fc045d3b329ff66341fe7f545076c5301346775" title="[Project Release] Running TinyLlama on Intel NPU with OpenVINO (my first GitHub repo 🎉)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just finished my very first open-source project and wanted to share it here. I managed to get &lt;strong&gt;TinyLlama 1.1B Chat&lt;/strong&gt; running &lt;strong&gt;locally&lt;/strong&gt; on my Intel Core Ultra laptop’s &lt;strong&gt;NPU&lt;/strong&gt; using &lt;strong&gt;OpenVINO GenAI&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Exported the HuggingFace model with &lt;code&gt;optimum-cli&lt;/code&gt; → OpenVINO IR format&lt;/li&gt; &lt;li&gt;Quantized it to &lt;strong&gt;INT4/FP16&lt;/strong&gt; for NPU acceleration&lt;/li&gt; &lt;li&gt;&lt;p&gt;Packaged everything neatly into a GitHub repo for others to try&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it’s interesting:&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No GPU required — just the &lt;strong&gt;Intel NPU&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;100% &lt;strong&gt;offline&lt;/strong&gt; inference&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;TinyLlama runs surprisingly well when optimized&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A good demo of OpenVINO GenAI for students/newcomers&lt;/p&gt; &lt;p&gt;Repo link: [&lt;a href="https://github.com/balaragavan2007/tinyllama-on-intel-npu%5C"&gt;https://github.com/balaragavan2007/tinyllama-on-intel-npu\&lt;/a&gt;]&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is my &lt;strong&gt;first GitHub project&lt;/strong&gt;, so feedback is very welcome! If you have suggestions for improving performance, UI, or deployment (like .exe packaging), I’d love to hear them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual-Ad-5916"&gt; /u/Spiritual-Ad-5916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w0twnkuq9dkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw9qgw/project_release_running_tinyllama_on_intel_npu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw9qgw/project_release_running_tinyllama_on_intel_npu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T12:36:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mweeod</id>
    <title>New "Sonic" Stealth Model (Grok-4-Code/4.5) + Cursor Makes 300 Tool Calls for a Single Prompt</title>
    <updated>2025-08-21T15:37:13+00:00</updated>
    <author>
      <name>/u/Longjumping-Solid563</name>
      <uri>https://old.reddit.com/user/Longjumping-Solid563</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mweeod/new_sonic_stealth_model_grok4code45_cursor_makes/"&gt; &lt;img alt="New &amp;quot;Sonic&amp;quot; Stealth Model (Grok-4-Code/4.5) + Cursor Makes 300 Tool Calls for a Single Prompt" src="https://external-preview.redd.it/djRubHNsZjIyZWtmMbBsYRXpfNN00cVdnGBuEYoU8Hrh8a7-LUpnqkB8QZys.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7312cd4f6513e885e2c9cda8fcc0192e5920311d" title="New &amp;quot;Sonic&amp;quot; Stealth Model (Grok-4-Code/4.5) + Cursor Makes 300 Tool Calls for a Single Prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to test out a new stealth model, Sonic, last night after Claude/Qwen-3 struggled to solve a problem. &lt;a href="https://x.com/mark_k/status/1958437678933844028"&gt;Sonic is rumored to be Grok&lt;/a&gt; (It's obviously Grok). The prompt was about integrating GLSL into Manim, ManimCE's OpenGL logic is a mess so it's a really solid coding question. In my first try, it made over 50 tools calls (cut-off by cursor) and &lt;a href="https://pastebin.com/NEFQ4X0N"&gt;second over 300&lt;/a&gt;, in the end getting the question wrong. It would grep the same file over and over again. Is it being served at 0.0001 temp or just stupid? This is extra funny because &lt;a href="https://x.com/elonmusk/status/1958499441469739329"&gt;Elon is saying on twitter that Grok-5 will have a shot at &amp;quot;true AGI&amp;quot;&lt;/a&gt;. 200,000 H100s for this!!! Guess their just too dedicated making gooners happy lol. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping-Solid563"&gt; /u/Longjumping-Solid563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cxo5slf22ekf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mweeod/new_sonic_stealth_model_grok4code45_cursor_makes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mweeod/new_sonic_stealth_model_grok4code45_cursor_makes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T15:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvy6ai</id>
    <title>Qwen2.5 0.5B vs Qwen3 0.6B answering the same question. Definitely a big improvement.</title>
    <updated>2025-08-21T01:57:14+00:00</updated>
    <author>
      <name>/u/airbus_a360_when</name>
      <uri>https://old.reddit.com/user/airbus_a360_when</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvy6ai/qwen25_05b_vs_qwen3_06b_answering_the_same/"&gt; &lt;img alt="Qwen2.5 0.5B vs Qwen3 0.6B answering the same question. Definitely a big improvement." src="https://a.thumbs.redditmedia.com/QpddyvC1oI2y7Ei80TMeqG06SbkjYBp8kDygMRpJtF8.jpg" title="Qwen2.5 0.5B vs Qwen3 0.6B answering the same question. Definitely a big improvement." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/airbus_a360_when"&gt; /u/airbus_a360_when &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mvy6ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvy6ai/qwen25_05b_vs_qwen3_06b_answering_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvy6ai/qwen25_05b_vs_qwen3_06b_answering_the_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T01:57:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwgs6w</id>
    <title>PACT: a new head-to-head negotiation benchmark for LLMs</title>
    <updated>2025-08-21T17:03:05+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwgs6w/pact_a_new_headtohead_negotiation_benchmark_for/"&gt; &lt;img alt="PACT: a new head-to-head negotiation benchmark for LLMs" src="https://external-preview.redd.it/kiN9MzMfAK5bpfItm0YrlZ4AidBiuSPXUaXIrrpbCs8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c89fedc8c090e254f7c40aeaea3ecf283728886" title="PACT: a new head-to-head negotiation benchmark for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;span class="md-spoiler-text"&gt;GPT-5 leads. GPT-OSS-120B is the top open weights model.&lt;/span&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/lechmazur/pact/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwgs6w/pact_a_new_headtohead_negotiation_benchmark_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwgs6w/pact_a_new_headtohead_negotiation_benchmark_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T17:03:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvw3hz</id>
    <title>NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model</title>
    <updated>2025-08-21T00:20:54+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"&gt; &lt;img alt="NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model" src="https://b.thumbs.redditmedia.com/e1iw4XluNhKx2_uQFrUBfjQ-KXChA-T80tHk2Ay_3VI.jpg" title="NVIDIA Achieves 35% Performance Boost for OpenAI’s GPT-OSS-120B Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mvw3hz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvw3hz/nvidia_achieves_35_performance_boost_for_openais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T00:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw5due</id>
    <title>LiteRP – lightweight open-source frontend for local LLM roleplay</title>
    <updated>2025-08-21T08:38:38+00:00</updated>
    <author>
      <name>/u/sumrix</name>
      <uri>https://old.reddit.com/user/sumrix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw5due/literp_lightweight_opensource_frontend_for_local/"&gt; &lt;img alt="LiteRP – lightweight open-source frontend for local LLM roleplay" src="https://preview.redd.it/sfwt2l733ckf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e50f5d1c5a6adbcb3044ec3d9dfddeef2047756f" title="LiteRP – lightweight open-source frontend for local LLM roleplay" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a minimal frontend for chatting and roleplay with AI characters, and I’d like to share the first early beta release &lt;strong&gt;LiteRP v0.3&lt;/strong&gt;: &lt;a href="https://github.com/Sumrix/LiteRP"&gt;https://github.com/Sumrix/LiteRP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most roleplay frontends (like SillyTavern) are powerful but heavy and complex to set up. LiteRP takes a different approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single compact executable (~17 MB) for Windows, Linux, macOS&lt;/li&gt; &lt;li&gt;No Python, npm, or extra dependencies&lt;/li&gt; &lt;li&gt;Launch the binary → browser opens at http://localhost:5000/&lt;/li&gt; &lt;li&gt;Supports TavernAI v2 character cards (&lt;code&gt;.png&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Interface similar to ChatGPT/character.ai, simple and familiar&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Right now LiteRP connects through &lt;strong&gt;Ollama&lt;/strong&gt;. That’s the only supported backend for the moment, but the design allows for additional APIs/backends in the future.&lt;/p&gt; &lt;p&gt;Downloads: &lt;a href="https://github.com/Sumrix/LiteRP/releases/latest"&gt;GitHub Releases&lt;/a&gt;&lt;br /&gt; Screenshots: &lt;a href="https://github.com/Sumrix/LiteRP/blob/main/SCREENSHOTS.md"&gt;Gallery&lt;/a&gt;&lt;br /&gt; Roadmap: &lt;a href="https://github.com/Sumrix/LiteRP/blob/main/ROADMAP.md"&gt;ROADMAP&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’re just looking for a model to try, I’ve had good results with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama pull nchapman/mn-12b-mag-mell-r1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Current version is early beta (v0.3). Basic roleplay already works, but features like message editing and other polish are still coming. Feedback is very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sumrix"&gt; /u/sumrix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sfwt2l733ckf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw5due/literp_lightweight_opensource_frontend_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw5due/literp_lightweight_opensource_frontend_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T08:38:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw724q</id>
    <title>Introducing Intern-S1-mini, a lightweight version of Intern-S1, which contains an 8B language model and a 0.3B vision encoder.</title>
    <updated>2025-08-21T10:22:21+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw724q/introducing_interns1mini_a_lightweight_version_of/"&gt; &lt;img alt="Introducing Intern-S1-mini, a lightweight version of Intern-S1, which contains an 8B language model and a 0.3B vision encoder." src="https://external-preview.redd.it/iJbqIXj4e8d90OQ705I7po4CzD6K5SM0Vr9TFSrm88U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1b47ebb9c988da9c5a585463d2f79deee9f0407" title="Introducing Intern-S1-mini, a lightweight version of Intern-S1, which contains an 8B language model and a 0.3B vision encoder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/InternLM/Intern-S1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw724q/introducing_interns1mini_a_lightweight_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw724q/introducing_interns1mini_a_lightweight_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T10:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwb6j3</id>
    <title>Where is AMD NPU driver for Linux?</title>
    <updated>2025-08-21T13:38:01+00:00</updated>
    <author>
      <name>/u/gnorrisan</name>
      <uri>https://old.reddit.com/user/gnorrisan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb6j3/where_is_amd_npu_driver_for_linux/"&gt; &lt;img alt="Where is AMD NPU driver for Linux?" src="https://preview.redd.it/l6rwjjqlldkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e11c98d62ab452c2e0f2fdbcd2919fc858d77ee6" title="Where is AMD NPU driver for Linux?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnorrisan"&gt; /u/gnorrisan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l6rwjjqlldkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb6j3/where_is_amd_npu_driver_for_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb6j3/where_is_amd_npu_driver_for_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T13:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw0tc4</id>
    <title>Finally Kimi-VL-A3B-Thinking-2506-GGUF is available</title>
    <updated>2025-08-21T04:06:14+00:00</updated>
    <author>
      <name>/u/kironlau</name>
      <uri>https://old.reddit.com/user/kironlau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0tc4/finally_kimivla3bthinking2506gguf_is_available/"&gt; &lt;img alt="Finally Kimi-VL-A3B-Thinking-2506-GGUF is available" src="https://external-preview.redd.it/m2lF_KqN7wgwWcFm1a3lN4x_joA1P4xA8L66Y-aVPXM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e8aa6a2602eb7ba21a6af8e92b4d296f3365548" title="Finally Kimi-VL-A3B-Thinking-2506-GGUF is available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original model: &lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506"&gt;https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Supported added in this PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15458"&gt;https://github.com/ggml-org/llama.cpp/pull/15458&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kironlau"&gt; /u/kironlau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0tc4/finally_kimivla3bthinking2506gguf_is_available/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw0tc4/finally_kimivla3bthinking2506gguf_is_available/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T04:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwexgd</id>
    <title>DeepSeek V3.1 (Thinking) aggregated benchmarks (vs. gpt-oss-120b)</title>
    <updated>2025-08-21T15:56:14+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwexgd/deepseek_v31_thinking_aggregated_benchmarks_vs/"&gt; &lt;img alt="DeepSeek V3.1 (Thinking) aggregated benchmarks (vs. gpt-oss-120b)" src="https://b.thumbs.redditmedia.com/LSriMSIBiKRIhiPtaWtN1ed8AonIRYXPSVg0ORTkNXA.jpg" title="DeepSeek V3.1 (Thinking) aggregated benchmarks (vs. gpt-oss-120b)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was personally interested in comparing with gpt-oss-120b on intelligence vs. speed, tabulating those numbers below for reference:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;DeepSeek 3.1 (Thinking)&lt;/th&gt; &lt;th align="left"&gt;gpt-oss-120b (High)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Total parameters&lt;/td&gt; &lt;td align="left"&gt;671B&lt;/td&gt; &lt;td align="left"&gt;120B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Active parameters&lt;/td&gt; &lt;td align="left"&gt;37B&lt;/td&gt; &lt;td align="left"&gt;5.1B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Context&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;131K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Intelligence Index&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Coding Index&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Math Index&lt;/td&gt; &lt;td align="left"&gt;?&lt;/td&gt; &lt;td align="left"&gt;?&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Response Time (500 tokens + thinking)&lt;/td&gt; &lt;td align="left"&gt;127.8 s&lt;/td&gt; &lt;td align="left"&gt;11.5 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Output Speed (tokens / s)&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;228&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Cheapest Openrouter Provider Pricing (input / output)&lt;/td&gt; &lt;td align="left"&gt;$0.32 / $1.15&lt;/td&gt; &lt;td align="left"&gt;$0.072 / $0.28&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mwexgd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwexgd/deepseek_v31_thinking_aggregated_benchmarks_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwexgd/deepseek_v31_thinking_aggregated_benchmarks_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T15:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3kmd</id>
    <title>DeepSeek-V3.1 (Thinking and Non Thinking)</title>
    <updated>2025-08-21T06:43:19+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3kmd/deepseekv31_thinking_and_non_thinking/"&gt; &lt;img alt="DeepSeek-V3.1 (Thinking and Non Thinking)" src="https://preview.redd.it/131ngchkjbkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c84b80fbda47f246b1ba5d8d04285fb722a69588" title="DeepSeek-V3.1 (Thinking and Non Thinking)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid thinking mode&lt;/strong&gt;: One model supports both thinking mode and non-thinking mode by changing the chat template.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smarter tool calling&lt;/strong&gt;: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Higher thinking efficiency&lt;/strong&gt;: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Category&lt;/th&gt; &lt;th align="left"&gt;Benchmark (Metric)&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3.1-NonThinking&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3 0324&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3.1-Thinking&lt;/th&gt; &lt;th align="left"&gt;DeepSeek R1 0528&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;General&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;MMLU-Redux (EM)&lt;/td&gt; &lt;td align="left"&gt;91.8&lt;/td&gt; &lt;td align="left"&gt;90.5&lt;/td&gt; &lt;td align="left"&gt;93.7&lt;/td&gt; &lt;td align="left"&gt;93.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;MMLU-Pro (EM)&lt;/td&gt; &lt;td align="left"&gt;83.7&lt;/td&gt; &lt;td align="left"&gt;81.2&lt;/td&gt; &lt;td align="left"&gt;84.8&lt;/td&gt; &lt;td align="left"&gt;85.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;GPQA-Diamond (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;74.9&lt;/td&gt; &lt;td align="left"&gt;68.4&lt;/td&gt; &lt;td align="left"&gt;80.1&lt;/td&gt; &lt;td align="left"&gt;81.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Humanity's Last Exam (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;15.9&lt;/td&gt; &lt;td align="left"&gt;17.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Search Agent&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;BrowseComp&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;30.0&lt;/td&gt; &lt;td align="left"&gt;8.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;BrowseComp_zh&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;49.2&lt;/td&gt; &lt;td align="left"&gt;35.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Humanity's Last Exam (Python + Search)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;29.8&lt;/td&gt; &lt;td align="left"&gt;24.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;SimpleQA&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;93.4&lt;/td&gt; &lt;td align="left"&gt;92.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;LiveCodeBench (2408-2505) (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;56.4&lt;/td&gt; &lt;td align="left"&gt;43.0&lt;/td&gt; &lt;td align="left"&gt;74.8&lt;/td&gt; &lt;td align="left"&gt;73.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Codeforces-Div1 (Rating)&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;2091&lt;/td&gt; &lt;td align="left"&gt;1930&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Aider-Polyglot (Acc.)&lt;/td&gt; &lt;td align="left"&gt;68.4&lt;/td&gt; &lt;td align="left"&gt;55.1&lt;/td&gt; &lt;td align="left"&gt;76.3&lt;/td&gt; &lt;td align="left"&gt;71.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code Agent&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;SWE Verified (Agent mode)&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;45.4&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;44.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;SWE-bench Multilingual (Agent mode)&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;29.3&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;30.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Terminal-bench (Terminus 1 framework)&lt;/td&gt; &lt;td align="left"&gt;31.3&lt;/td&gt; &lt;td align="left"&gt;13.3&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;td align="left"&gt;5.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Math&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;AIME 2024 (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;66.3&lt;/td&gt; &lt;td align="left"&gt;59.4&lt;/td&gt; &lt;td align="left"&gt;93.1&lt;/td&gt; &lt;td align="left"&gt;91.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;AIME 2025 (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;49.8&lt;/td&gt; &lt;td align="left"&gt;51.3&lt;/td&gt; &lt;td align="left"&gt;88.4&lt;/td&gt; &lt;td align="left"&gt;87.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;HMMT 2025 (Pass@1)&lt;/td&gt; &lt;td align="left"&gt;33.5&lt;/td&gt; &lt;td align="left"&gt;29.2&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;td align="left"&gt;79.4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/131ngchkjbkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3kmd/deepseekv31_thinking_and_non_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3kmd/deepseekv31_thinking_and_non_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mvnmjo</id>
    <title>My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834</title>
    <updated>2025-08-20T18:49:36+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt; &lt;img alt="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" src="https://external-preview.redd.it/bruJaed8mpWclO3rYYnLL_4tpIRSDSNQT1lxjc08864.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de2bdb3583ed7a714be2ea7450e90270d3b3e83" title="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’ve posted on here a couple times sharing my project. I'm training LLM’s from scratch on 1800’s London texts (no fine tune/modern data). I built a dataset using 7,000 texts published between 1800 to 1875 in the city of London, and also trained a custom tokenizer on the dataset itself to get rid of modern vocab. &lt;/p&gt; &lt;p&gt;So far I’ve trained 3 models, 2 with nanoGPT and the latest using Phi 1.5. After training, I messed around with some prompts and used this one:&lt;/p&gt; &lt;p&gt;&amp;quot;It was the year of our Lord 1834&amp;quot; &lt;/p&gt; &lt;p&gt;Here’s the output:&lt;/p&gt; &lt;p&gt; &amp;quot;It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity&amp;quot; (The last sentence is weird but stuff like that shows up a lot probably due to heavy biblical influence)&lt;/p&gt; &lt;p&gt;I was interested to see if a protest had actually occurred in 1834 London and it really did happen but I thought it was maybe just a coincidence. The output also brought up “Lord Palmerston” and after a google search I learned that his actions resulted in the 1834 protests. So this idea is past just mimicking 1800s text and can now actually recall real historical events. &lt;/p&gt; &lt;p&gt;This is all from just 5-6GB of data, imagine the results with 30GB or more. I’m not sure if just scaling the data up will ever result in reasoning but even now it kinda feels like digital time travel. I want to eventually try different cities also, maybe a Chinese, Russian or Indian or even just another English city model. I’m just doing this for fun so if anyone would like to collaborate let me know, I’m open to anything really. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e"&gt;https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwb5ix</id>
    <title>Intern-S1-mini 8B multimodal is out!</title>
    <updated>2025-08-21T13:36:55+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Intern-S1-mini is a lightweight multimodal reasoning large language model 🤖.&lt;/p&gt; &lt;p&gt;Base: Built on Qwen3-8B 🧠 + InternViT-0.3B 👁️.&lt;/p&gt; &lt;p&gt;Training: Pretrained on 5 trillion tokens 📚, more than half from scientific domains (chemistry, physics, biology, materials science 🧪).&lt;/p&gt; &lt;p&gt;Strengths: Can handle text, images, and video 💬🖼️🎥, excelling at scientific reasoning tasks like interpreting chemical structures, proteins, and materials data, while still performing well in general-purpose benchmarks.&lt;/p&gt; &lt;p&gt;Deployment: Small enough to run on a single GPU ⚡, and designed for compatibility with OpenAI-style APIs 🔌, tool calling, and local inference frameworks like vLLM, LMDeploy, and Ollama.&lt;/p&gt; &lt;p&gt;Use case: A research assistant for real-world scientific applications, but still capable of general multimodal chat and reasoning.&lt;/p&gt; &lt;p&gt;⚡ In short: it’s a science-focused, multimodal LLM optimized to be lightweight and high-performing.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini"&gt;https://huggingface.co/internlm/Intern-S1-mini&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb5ix/interns1mini_8b_multimodal_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb5ix/interns1mini_8b_multimodal_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb5ix/interns1mini_8b_multimodal_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T13:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3j7l</id>
    <title>Deepseek V3.1 is not so bad after all..</title>
    <updated>2025-08-21T06:40:50+00:00</updated>
    <author>
      <name>/u/Trevor050</name>
      <uri>https://old.reddit.com/user/Trevor050</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"&gt; &lt;img alt="Deepseek V3.1 is not so bad after all.." src="https://b.thumbs.redditmedia.com/T9qzPo7Bvut8hGvtV6Xg52JupsxwiKXPdU34MXCHojI.jpg" title="Deepseek V3.1 is not so bad after all.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like it just was a different purpose, speed and agency. Its pretty good at what its meant for&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trevor050"&gt; /u/Trevor050 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mw3j7l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwevt4</id>
    <title>Why low-bit models aren't totally braindead: A guide from 1-bit meme to FP16 research</title>
    <updated>2025-08-21T15:54:35+00:00</updated>
    <author>
      <name>/u/Small-Fall-6500</name>
      <uri>https://old.reddit.com/user/Small-Fall-6500</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwevt4/why_lowbit_models_arent_totally_braindead_a_guide/"&gt; &lt;img alt="Why low-bit models aren't totally braindead: A guide from 1-bit meme to FP16 research" src="https://preview.redd.it/5t58iz5u9ekf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=100a692755cfd42ca5aa55904085997b9a5c2344" title="Why low-bit models aren't totally braindead: A guide from 1-bit meme to FP16 research" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright, it's not exactly the same picture, but the core idea is quite similar. This post will explain how, by breaking down LLM quantization into varying levels of precision, starting from a 1-bit meme, then a 2-bit TL;DR, 4-bit overview, 8-bit further reading, and lastly the highest precision FP16 research itself.&lt;/p&gt; &lt;h1&gt;Q1 Version (The Meme Above)&lt;/h1&gt; &lt;p&gt;That's it. A high-compression, low-nuance, instant-takeaway version of the entire concept.&lt;/p&gt; &lt;h1&gt;Q2 Version (The TL;DR)&lt;/h1&gt; &lt;p&gt;LLM quantization is JPEG compression for an AI brain.&lt;/p&gt; &lt;p&gt;It’s all about smart sacrifices, throwing away the least important information to make the model massively smaller, while keeping the core of its intelligence intact. JPEG keeps the general shapes and colors of an image while simplifying the details you won't miss. Quantization does the same to a model's &amp;quot;weights&amp;quot; (its learned knowledge), keeping the most critical parts at high precision while squashing the rest to low precision.&lt;/p&gt; &lt;h1&gt;Q4 Version (Deeper Dive)&lt;/h1&gt; &lt;p&gt;Like a JPEG, the more you compress, the more detail you lose. But if the original model is big enough (like a 70B parameter model), you can compress it a lot before quality drops noticeably.&lt;/p&gt; &lt;p&gt;So, can only big models be highly quantized? Not quite. There are a few key tricks that make even small models maintain their usefulness at low-precision:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trick #1: Mixed Precision (Not All Knowledge is Equal)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The parts of the model that handle grammar are probably more important than the part that remembers 14th-century basket-weaving history. Modern quantization schemes understand this. They intelligently assign more bits to the &amp;quot;important&amp;quot; parts of the model and fewer bits to the &amp;quot;less important&amp;quot; parts. It’s not a uniform 2-bit model; it's an average of 2-bits, preserving performance where it matters most.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trick #2: Calibration (Smart Rounding)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of just blindly rounding numbers, quantization uses a &amp;quot;calibration dataset.&amp;quot; It runs a small amount of data through the model to figure out the best way to group and round the weights to minimize information loss. It tunes the compression algorithm specifically for that one model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trick #3: New Architectures (Building for Compression)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Why worry about quantization after training a model when you can just start with the model already quantized? It turns out, it’s possible to design models from the ground up to run at super low precision. Microsoft's BitNet is the most well-known example, which started with a true 1-bit precision model, for both training and inference. They expanded this to a more efficient ~1.58 bit precision (using only -1, 0, or 1 for each of its weights).&lt;/p&gt; &lt;h1&gt;Q8 Resources (Visuals &amp;amp; Docs)&lt;/h1&gt; &lt;p&gt;A higher-precision look at the concepts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual Overview (Article):&lt;/strong&gt; &lt;a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization"&gt;A Visual Guide to Quantization&lt;/a&gt; - An intuitive breakdown of these ideas.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specific Implementations (Docs):&lt;/strong&gt; &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;Unsloth Dynamic 2.0 GGUFs&lt;/a&gt; - See how a recent quantization method uses these tricks to maximize performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Great Overview (Video):&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=WBm0nyDkVYM"&gt;The myth of 1-bit LLMs&lt;/a&gt; - A fantastic video explaining Quantization-Aware Training.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;FP16 Resources (Foundational Research)&lt;/h1&gt; &lt;p&gt;The full precision source material:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Original BitNet Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers&lt;/a&gt; - The paper that started the 1-bit hype.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Updated Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs (1.58-bit)&lt;/a&gt; - Microsoft's follow-up showing incredible results with ternary weights.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bitnet Model Weights:&lt;/strong&gt; &lt;a href="https://huggingface.co/microsoft/bitnet-b1.58-2B-4T"&gt;microsoft/bitnet-b1.58-2B-4T&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Small-Fall-6500"&gt; /u/Small-Fall-6500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5t58iz5u9ekf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwevt4/why_lowbit_models_arent_totally_braindead_a_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwevt4/why_lowbit_models_arent_totally_braindead_a_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T15:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw86zw</id>
    <title>I’m gonna say it:</title>
    <updated>2025-08-21T11:24:07+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw86zw/im_gonna_say_it/"&gt; &lt;img alt="I’m gonna say it:" src="https://preview.redd.it/gxbn2ofuxckf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d114958447f9d74be0e026a7a772ce1a19fab790" title="I’m gonna say it:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gxbn2ofuxckf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw86zw/im_gonna_say_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw86zw/im_gonna_say_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T11:24:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw7x6f</id>
    <title>New DeepSeek API pricing: -chat prices increasing, -reasoner prices decreasing</title>
    <updated>2025-08-21T11:09:58+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7x6f/new_deepseek_api_pricing_chat_prices_increasing/"&gt; &lt;img alt="New DeepSeek API pricing: -chat prices increasing, -reasoner prices decreasing" src="https://preview.redd.it/d2xgmwobvckf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d9ce209cc69d0fc5c15c3d82253b16c89c0b2ac" title="New DeepSeek API pricing: -chat prices increasing, -reasoner prices decreasing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New API pricing scheme goes into effect on September 5, 2025: &lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;https://api-docs.deepseek.com/quick_start/pricing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2xgmwobvckf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7x6f/new_deepseek_api_pricing_chat_prices_increasing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7x6f/new_deepseek_api_pricing_chat_prices_increasing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T11:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw73uz</id>
    <title>DeepSeek has revealed that the next generation of China-made chips is about to be released</title>
    <updated>2025-08-21T10:25:14+00:00</updated>
    <author>
      <name>/u/Dry-Ad8947</name>
      <uri>https://old.reddit.com/user/Dry-Ad8947</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"&gt; &lt;img alt="DeepSeek has revealed that the next generation of China-made chips is about to be released" src="https://b.thumbs.redditmedia.com/Ior1GVKeeGS67aHoSqPUGnAsxjCS7RxbgR-JhoE9meg.jpg" title="DeepSeek has revealed that the next generation of China-made chips is about to be released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5j7osgkanckf1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bad0b7a62ad023889c86de7320fda3c7f4871f03"&gt;https://preview.redd.it/5j7osgkanckf1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bad0b7a62ad023889c86de7320fda3c7f4871f03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In an official post on DeepSeek's official WeChat account, DeepSeek further explained that UE8M0 FP8 is designed for the upcoming next-generation domestic chip.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry-Ad8947"&gt; /u/Dry-Ad8947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T10:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwdgdw</id>
    <title>Command A Reasoning: Enterprise-grade control for AI agents</title>
    <updated>2025-08-21T15:02:58+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwdgdw/command_a_reasoning_enterprisegrade_control_for/"&gt; &lt;img alt="Command A Reasoning: Enterprise-grade control for AI agents" src="https://b.thumbs.redditmedia.com/F-wu3xdsFTQXIWp9SHh00FXzZZM-hiWCdK7h7OwtNkI.jpg" title="Command A Reasoning: Enterprise-grade control for AI agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://cohere.com/blog/command-a-reasoning"&gt;https://cohere.com/blog/command-a-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF Link: &lt;a href="https://huggingface.co/CohereLabs/command-a-reasoning-08-2025"&gt;https://huggingface.co/CohereLabs/command-a-reasoning-08-2025&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mwdgdw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwdgdw/command_a_reasoning_enterprisegrade_control_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwdgdw/command_a_reasoning_enterprisegrade_control_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T15:02:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw2lme</id>
    <title>Frontier AI labs’ publicized 100k-H100 training runs under-deliver because software and systems don’t scale efficiently, wasting massive GPU fleets</title>
    <updated>2025-08-21T05:44:41+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"&gt; &lt;img alt="Frontier AI labs’ publicized 100k-H100 training runs under-deliver because software and systems don’t scale efficiently, wasting massive GPU fleets" src="https://b.thumbs.redditmedia.com/qEAZdKzAv0zTNBneqhh9EDw7ENWMaMbCmlZSPzIj5Hw.jpg" title="Frontier AI labs’ publicized 100k-H100 training runs under-deliver because software and systems don’t scale efficiently, wasting massive GPU fleets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mw2lme"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T05:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3nat</id>
    <title>DeepSeek-V3.1 implements Anthropic API compatibility</title>
    <updated>2025-08-21T06:47:55+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"&gt; &lt;img alt="DeepSeek-V3.1 implements Anthropic API compatibility" src="https://preview.redd.it/0pp8mwjkkbkf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e7a0529d636accee5763e0a807e41d636629b2" title="DeepSeek-V3.1 implements Anthropic API compatibility" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/guides/anthropic_api"&gt;https://api-docs.deepseek.com/guides/anthropic_api&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0pp8mwjkkbkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3c7s</id>
    <title>deepseek-ai/DeepSeek-V3.1 · Hugging Face</title>
    <updated>2025-08-21T06:28:56+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1 · Hugging Face" src="https://external-preview.redd.it/RJXEgvNDm4zhSkGlks1Mt4ppnLOAENNDWYNaVwpLE9k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d409de545b1a58fad7e22e741370f1a55018f432" title="deepseek-ai/DeepSeek-V3.1 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwbsww</id>
    <title>Love small but mighty team of DeepSeek</title>
    <updated>2025-08-21T14:02:32+00:00</updated>
    <author>
      <name>/u/dbhalla4</name>
      <uri>https://old.reddit.com/user/dbhalla4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwbsww/love_small_but_mighty_team_of_deepseek/"&gt; &lt;img alt="Love small but mighty team of DeepSeek" src="https://preview.redd.it/38d427vmpdkf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54c26ac5a1090a8e75470e0b3b24a163e5402cee" title="Love small but mighty team of DeepSeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They are working so hard they are even inventing new spellings!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbhalla4"&gt; /u/dbhalla4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/38d427vmpdkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwbsww/love_small_but_mighty_team_of_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwbsww/love_small_but_mighty_team_of_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T14:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
