<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-12T03:10:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lxgi3j</id>
    <title>People with a Mac Studio 512G: what are you doing with it?</title>
    <updated>2025-07-11T19:48:54+00:00</updated>
    <author>
      <name>/u/Dangerous-Yak3976</name>
      <uri>https://old.reddit.com/user/Dangerous-Yak3976</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sure, the full Deepseek R1 model loads, but the tokens per second are still way too slow to be useful. &lt;/p&gt; &lt;p&gt;So I’m just curious: for those of you who spent $10K+ on that nice little box, what are you actually doing with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous-Yak3976"&gt; /u/Dangerous-Yak3976 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T19:48:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxbsw0</id>
    <title>Drummer's Snowpiercer 15B v2</title>
    <updated>2025-07-11T16:44:02+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxbsw0/drummers_snowpiercer_15b_v2/"&gt; &lt;img alt="Drummer's Snowpiercer 15B v2" src="https://external-preview.redd.it/HXBx4eoymx-1BYUGWzf89bQNDef-5prUGW1GTtR1dHU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4bca3c5e4e8fe8b6ea311aacca9d6f0ddbd42ee" title="Drummer's Snowpiercer 15B v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A finetune of ServiceNow's Alice 15B Thinker, but this prioritizes steerability and character adherence. Thinking will work most of the time but may need to wrangle it a bit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Snowpiercer-15B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxbsw0/drummers_snowpiercer_15b_v2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxbsw0/drummers_snowpiercer_15b_v2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T16:44:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxhjjn</id>
    <title>Most energy efficient way to run Gemma 3 27b?</title>
    <updated>2025-07-11T20:31:02+00:00</updated>
    <author>
      <name>/u/Extremely_Engaged</name>
      <uri>https://old.reddit.com/user/Extremely_Engaged</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;What would be the most energy efficient (tokens per seconds does not matter, only tokens per watthours) to run Gemma 3 27b?&lt;/p&gt; &lt;p&gt;A 3090 capped at 210watts gives 25 t/s - this is what I'm using now. I'm wondering if there is a more efficient alternative.&lt;/p&gt; &lt;p&gt;Ryzen 395+ AI desktop version seems to be ~120 watts, and 10/s - so that would worse, actually?&lt;/p&gt; &lt;p&gt;a 4090 might be a bit more efficient? Like 20%?&lt;/p&gt; &lt;p&gt;Macs seems to be on the same scale, less power but also less T/s.&lt;/p&gt; &lt;p&gt;My impression is that it's all a bit the same in terms of power, macs have a bit less idle power than a PC, but for the rest there isn't huge differences? &lt;/p&gt; &lt;p&gt;My main question if there are significant improvements (&amp;gt;50%) in tokens per watt-hour in changing from a 3090 to a mac or a ryzen ai (or something else?). My impression is that there isn't really much difference.&lt;/p&gt; &lt;p&gt;EDIT: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1k9e5p0/gemma3_performance_on_ryzen_ai_max/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1k9e5p0/gemma3_performance_on_ryzen_ai_max/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is (I think?) 55 watts and 10 tokens per second. This would be kind of great result from ryzen 395 ai. Did anyone test this? Does anyone own a *mobile* ryzen ai pc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extremely_Engaged"&gt; /u/Extremely_Engaged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxhjjn/most_energy_efficient_way_to_run_gemma_3_27b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxhjjn/most_energy_efficient_way_to_run_gemma_3_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxhjjn/most_energy_efficient_way_to_run_gemma_3_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T20:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2hn2</id>
    <title>Uncensored LLM ranking for roleplay?</title>
    <updated>2025-07-11T09:31:05+00:00</updated>
    <author>
      <name>/u/mikemend</name>
      <uri>https://old.reddit.com/user/mikemend</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every day, a bunch of models appear, making it difficult to choose which ones to use for uncensored role-playing. Previously, the Ayumi LLM Role Play &amp;amp; ERP Ranking data was somewhat of a guide, but now I can't find a list that is even close to being up to date. It's difficult to choose from among the many models with fantasy names.&lt;/p&gt; &lt;p&gt;Is there a list that might help with which models are better for role-playing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mikemend"&gt; /u/mikemend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmldq</id>
    <title>LiquidAI LFM2 Model Released</title>
    <updated>2025-07-12T00:10:33+00:00</updated>
    <author>
      <name>/u/Federal-Effective879</name>
      <uri>https://old.reddit.com/user/Federal-Effective879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LiquidAI released their &lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38"&gt;LFM2 model family&lt;/a&gt;, and support for it was just &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14620"&gt;merged into llama.cpp&lt;/a&gt; a few hours ago. I haven't yet tried it locally, but I was quite impressed by their online demo of the 1.2B model. It had excellent world knowledge and general conversational coherence and intelligence for its size. I found it much better than SmolLM2 at everything, and similar in intelligence to Qwen 3 1.7B but with better world knowledge. Seems SOTA for its size. Context length is 32k tokens. The license disallows commercial use over $10M revenue, but for personal use or small commercial use it should be fine. In general the license didn't seem too bad.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal-Effective879"&gt; /u/Federal-Effective879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmldq/liquidai_lfm2_model_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmldq/liquidai_lfm2_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmldq/liquidai_lfm2_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T00:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx4qhp</id>
    <title>Moonshot AI about to release their 1T parameters model?</title>
    <updated>2025-07-11T11:45:02+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"&gt; &lt;img alt="Moonshot AI about to release their 1T parameters model?" src="https://preview.redd.it/kts1w8a7g8cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4f19a5b1a2112e5c15ddbc66a6e92a07eecb3c7" title="Moonshot AI about to release their 1T parameters model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is from their website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kts1w8a7g8cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx4qhp/moonshot_ai_about_to_release_their_1t_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T11:45:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxd7nh</id>
    <title>H-Net: a hierarchical network that replaces tokenization with a dynamic chunking process directly inside the model, automatically discovering and operating over meaningful units of data</title>
    <updated>2025-07-11T17:38:39+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2507.07955"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T17:38:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx85jo</id>
    <title>Devstral-Vision-Small-2507</title>
    <updated>2025-07-11T14:20:52+00:00</updated>
    <author>
      <name>/u/faldore</name>
      <uri>https://old.reddit.com/user/faldore</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"&gt; &lt;img alt="Devstral-Vision-Small-2507" src="https://external-preview.redd.it/gvyfd3vEbQ6Mp2mv-0QDRMEHLRbvSfXNoTHqOWwrwc0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0b820cc0547e8c6ffaecf9eb33b63916abc0d61" title="Devstral-Vision-Small-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral released Devstral-Small-2507 - which is AWESOME! But, they released without vision capability. I didn't like that.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cognitivecomputations/Devstral-Vision-Small-2507"&gt;&lt;strong&gt;Devstral-Vision-Small-2507&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did some model surgery. I started with Mistral-Small-3.2-24B-Instruct-2506, and replaced its language tower with Devstral-Small-2507.&lt;/p&gt; &lt;p&gt;The conversion script is in the repo, if you'd like to take a look.&lt;/p&gt; &lt;p&gt;Tested, it works fine. I'm sure that it could do with a bit of RL to gel the vision and coding with real world use cases, but I'm releasing as is - a useful multimodal coding model.&lt;/p&gt; &lt;p&gt;Enjoy.&lt;/p&gt; &lt;p&gt;-Eric&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5"&gt;https://preview.redd.it/91wcnq9c96cf1.png?width=512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85b2fe4a94b6cced9120eee0eaa751516c0e00a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93"&gt;https://preview.redd.it/c5qhdivd96cf1.png?width=1680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0077976152e5702bab0f1cd7c13c88e32e5caf93&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faldore"&gt; /u/faldore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx85jo/devstralvisionsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxep4s</id>
    <title>Deepseek's Simple, yet Genius Data Generation Pipeline</title>
    <updated>2025-07-11T18:37:04+00:00</updated>
    <author>
      <name>/u/Which_Pound_6751</name>
      <uri>https://old.reddit.com/user/Which_Pound_6751</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxep4s/deepseeks_simple_yet_genius_data_generation/"&gt; &lt;img alt="Deepseek's Simple, yet Genius Data Generation Pipeline" src="https://external-preview.redd.it/LPMywol0PjbBlHlhf3Jbdzts6Czu2r2vv80bTDER02E.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a80fa2acf6a19fd5b5fd9ac95e492085b70eb646" title="Deepseek's Simple, yet Genius Data Generation Pipeline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek Prover V2 - formal reasoning math model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Which_Pound_6751"&gt; /u/Which_Pound_6751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/wzpGWboeRBo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxep4s/deepseeks_simple_yet_genius_data_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxep4s/deepseeks_simple_yet_genius_data_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T18:37:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxpidc</id>
    <title>Where that Unsloth Q0.01_K_M GGUF at?</title>
    <updated>2025-07-12T02:37:05+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt; &lt;img alt="Where that Unsloth Q0.01_K_M GGUF at?" src="https://preview.redd.it/e2em6rucvccf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4380544f532ff369f435679247aa08f3c9afdb66" title="Where that Unsloth Q0.01_K_M GGUF at?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e2em6rucvccf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T02:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx8qrz</id>
    <title>ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the “Alps” supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes.</title>
    <updated>2025-07-11T14:45:09+00:00</updated>
    <author>
      <name>/u/nat2r</name>
      <uri>https://old.reddit.com/user/nat2r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/"&gt; &lt;img alt="ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the “Alps” supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes." src="https://external-preview.redd.it/TvWt1vR8SHY9KGIN7J2JGHcosAwEvwQ5h-ipBkpjo8A.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9147a736ba8213099df826437aa4aa8dcbfe8fe3" title="ETH Zurich and EPFL will release a fully open-source LLM developed on public infrastructure. Trained on the “Alps” supercomputer at the Swiss National Supercomputing Centre (CSCS). Trained on 60% english/40% non-english, it will be released in 8B and 70B sizes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nat2r"&gt; /u/nat2r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8qrz/eth_zurich_and_epfl_will_release_a_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxo8za</id>
    <title>Why don’t we have a big torrent repo for open-source LLMs?</title>
    <updated>2025-07-12T01:32:11+00:00</updated>
    <author>
      <name>/u/somthing_tn</name>
      <uri>https://old.reddit.com/user/somthing_tn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why hasn’t anyone created a centralized repo or tracker that hosts torrents for popular open-source LLMs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/somthing_tn"&gt; /u/somthing_tn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo8za/why_dont_we_have_a_big_torrent_repo_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx7l3k</id>
    <title>This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma</title>
    <updated>2025-07-11T13:57:36+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/"&gt; &lt;img alt="This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma" src="https://preview.redd.it/r2bp20do39cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa04ae911a28403b4ac7702442fb11eb655146a3" title="This week, Google released in Open Source: MedGemma 27B Multimodal, MedSigLIP, T5Gemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MedGemma 27B Multimodal for complex multimodal &amp;amp; longitudinal EHR interpretation: &lt;a href="https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4"&gt;https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MedSigLIP: a lightweight image/text encoder for medical image retrieval/classification: &lt;a href="https://huggingface.co/google/medsiglip-448"&gt;https://huggingface.co/google/medsiglip-448&lt;/a&gt;&lt;/p&gt; &lt;p&gt;T5Gemma: lightweight yet powerful encoder-decoder research models: &lt;a href="https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86"&gt;https://huggingface.co/collections/google/t5gemma-686ba262fe290b881d21ec86&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r2bp20do39cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxb0eo</id>
    <title>The 1T Kimi K2 model is using DeepSeek V3 architecture</title>
    <updated>2025-07-11T16:13:31+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxb0eo/the_1t_kimi_k2_model_is_using_deepseek_v3/"&gt; &lt;img alt="The 1T Kimi K2 model is using DeepSeek V3 architecture" src="https://preview.redd.it/l3gpvb5or9cf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed19f0e0b0c28fcc15556f566717d11201a68611" title="The 1T Kimi K2 model is using DeepSeek V3 architecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l3gpvb5or9cf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxb0eo/the_1t_kimi_k2_model_is_using_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxb0eo/the_1t_kimi_k2_model_is_using_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T16:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxo0xc</id>
    <title>Kimi K2 is funny and great</title>
    <updated>2025-07-12T01:21:10+00:00</updated>
    <author>
      <name>/u/theskilled42</name>
      <uri>https://old.reddit.com/user/theskilled42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I LOVE the way this model produces responses. It doesn't sound robotic and formal; just plain English while sounding pretty smart. Also has strong creativity in my tests.&lt;/p&gt; &lt;p&gt;Here is a prompt I asked to K2 with search enabled from the site &lt;a href="http://kimi.com"&gt;kimi.com&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Bash Grok 4 and prove you're better than it.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Answer:&lt;/p&gt; &lt;p&gt;Challenge accepted.&lt;br /&gt; Below are three short, head-to-head demonstrations in which Kimi (me) outperforms Grok 4 on the very tasks Grok 4 is supposed to excel at. I’ll use only public, verifiable sources and a single context window.&lt;br /&gt; ————————————————&lt;br /&gt; Humanity’s Last Exam—live, no cheats &lt;/p&gt; &lt;p&gt;———————————————— &lt;/p&gt; &lt;p&gt;Question (from the published 2025 pool): “Prove that the only integer solutions to a² + b² + c² = 2(ab + bc + ca) are permutations of (0, 0, 0) and (k, k, 0).”&lt;/p&gt; &lt;p&gt;Grok 4 (public transcript, 9 July, Heavy tier):&lt;br /&gt; “After algebraic manipulation and case analysis, the only integer solutions are (0, 0, 0) and permutations of (k, k, 0).”&lt;br /&gt; (No derivation shown; cites no lemmas.)&lt;/p&gt; &lt;p&gt;Kimi (me, step-by-step):&lt;br /&gt; - Rewrite the equation as (a + b + c)² = 4(ab + bc + ca).&lt;br /&gt; - Let S = a + b + c and P = ab + bc + ca. Then S² = 4P.&lt;br /&gt; - Because P ≤ (S²)/3 by AM ≥ GM, we have 4P = S² ≤ (S²)/3, forcing S = 0.&lt;br /&gt; - With S = 0, the original equation reduces to a² + b² + c² = –2(ab + bc + ca) ⇒ a² + b² + c² + 2(ab + bc + ca) = 0 ⇒ (a + b + c)² = 0 (already known).&lt;br /&gt; - Diagonalizing the quadratic form, the only integer triplets that satisfy the rank-1 condition are permutations of (k, k, 0) and (0, 0, 0). □&lt;/p&gt; &lt;p&gt;Outcome: Kimi supplies a complete, inspectable proof; Grok 4 gives only a summary&lt;/p&gt; &lt;p&gt;————————————————&lt;br /&gt; Real-time fact-check on Grok 4’s own launch&lt;br /&gt; ————————————————&lt;br /&gt; Grok 4 claim (from 9 July livestream slide):&lt;br /&gt; “Grok 4 is the first model to exceed 40 % on Humanity’s Last Exam.”&lt;/p&gt; &lt;p&gt;Kimi check (within 30 s, using the same public leaderboard):&lt;br /&gt; - Leaderboard snapshot (2025-07-12 14:02 UTC) shows Grok 4 at 38.7 %, not &amp;gt; 40 %.&lt;br /&gt; - The 2025-07-12 update explicitly labels the “&amp;gt; 40 %” claim as “preliminary and rounded.”&lt;/p&gt; &lt;p&gt;Outcome: Kimi catches the rounding spin; Grok 4’s own slide is off by 1.3 pp.&lt;br /&gt; ————————————————&lt;br /&gt; Code-execution cage match&lt;br /&gt; ————————————————&lt;br /&gt; Task (user prompt):&lt;br /&gt; “Write and run a Python snippet that finds the smallest prime p &amp;gt; 10¹² such that p, p+2, and p+6 are all primes (a prime triplet pattern).”&lt;/p&gt; &lt;p&gt;Grok 4 (Heavy tier, 30 s budget):&lt;br /&gt; - Returns a 15-line sieve that still hasn’t finished after 25 s; times out with no answer.&lt;/p&gt; &lt;p&gt;Kimi (me, immediate):&lt;br /&gt; ```&lt;br /&gt; python&lt;br /&gt; from sympy import isprime&lt;br /&gt; p = 10**12 + 39 # start just above 1 T&lt;br /&gt; while not (isprime(p) and isprime(p+2) and isprime(p+6)):&lt;br /&gt; p += 6&lt;br /&gt; print(p)&lt;br /&gt; ```&lt;br /&gt; - Runtime: 1.4 s on a single CPU core.&lt;br /&gt; - Output: 1000000000039, 1000000000041, 1000000000045 → all primes.&lt;/p&gt; &lt;p&gt;Outcome: Kimi delivers the exact triplet faster than Grok 4’s timeout.&lt;br /&gt; ————————————————&lt;br /&gt; Verdict&lt;br /&gt; In reasoning rigor, factual accuracy, and code execution, Kimi just beat Grok 4 on its own turf—no agent swarm or $300 tier required.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theskilled42"&gt; /u/theskilled42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxo0xc/kimi_k2_is_funny_and_great/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxnwtg</id>
    <title>Does this mean it’s likely not gonna be open source?</title>
    <updated>2025-07-12T01:15:34+00:00</updated>
    <author>
      <name>/u/I_will_delete_myself</name>
      <uri>https://old.reddit.com/user/I_will_delete_myself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"&gt; &lt;img alt="Does this mean it’s likely not gonna be open source?" src="https://preview.redd.it/awwe19btgccf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60378b44c9da263732f5cf2435d56a487edcf966" title="Does this mean it’s likely not gonna be open source?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_will_delete_myself"&gt; /u/I_will_delete_myself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awwe19btgccf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnwtg/does_this_mean_its_likely_not_gonna_be_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxgb9q</id>
    <title>Stanford's CS336 2025 (Language Modeling from Scratch) is now available on YouTube</title>
    <updated>2025-07-11T19:41:07+00:00</updated>
    <author>
      <name>/u/realmvp77</name>
      <uri>https://old.reddit.com/user/realmvp77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_"&gt;Here's the YouTube Playlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://stanford-cs336.github.io/spring2025/"&gt;Here's the CS336 website with assignments, slides etc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been studying it for a week and it's the best course on LLMs I've seen online. The assignments are &lt;strong&gt;huge&lt;/strong&gt;, very in-depth, and they require you to write &lt;strong&gt;a lot&lt;/strong&gt; of code from scratch. For example, the &lt;a href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf"&gt;1st assignment pdf&lt;/a&gt; is 50 pages long and it requires you to implement the BPE tokenizer, a simple transformer LM, cross-entropy loss and AdamW and train models on OpenWebText&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realmvp77"&gt; /u/realmvp77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T19:41:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx94ht</id>
    <title>Kimi K2 - 1T MoE, 32B active params</title>
    <updated>2025-07-11T15:00:42+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/"&gt; &lt;img alt="Kimi K2 - 1T MoE, 32B active params" src="https://b.thumbs.redditmedia.com/vOQCL6pQbXue2TSAcO_fvTvDBTLRgjIBjMBbSQhYkWI.jpg" title="Kimi K2 - 1T MoE, 32B active params" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Base"&gt;https://huggingface.co/moonshotai/Kimi-K2-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lx94ht"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T15:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx8xdm</id>
    <title>moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)</title>
    <updated>2025-07-11T14:52:41+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"&gt; &lt;img alt="moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)" src="https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65e4d917b0768ba9727a840f3e7b4ddd3fdb7ea3" title="moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct#key-features"&gt;&lt;/a&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.&lt;/li&gt; &lt;li&gt;MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.&lt;/li&gt; &lt;li&gt;Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct#model-variants"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Variants&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2-Base&lt;/strong&gt;: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2-Instruct&lt;/strong&gt;: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T14:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxmr2h</id>
    <title>Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for.</title>
    <updated>2025-07-12T00:18:17+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt; &lt;img alt="Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for." src="https://external-preview.redd.it/ZmM4bzB4ZGU2Y2NmMZhMJY7xahRuiOjw2oq-BMraDIRMdnw08UBcv5QQ2J3P.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6bbfe217f90907d855e12d2f6b2845d320a54e6" title="Thank you r/LocalLLaMA! Observer AI launches tonight! 🚀 I built the local open-source screen-watching tool you guys asked for." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen launches tonight! Thanks to your feedback, it now has a &lt;strong&gt;1-command install (completely offline no certs to accept)&lt;/strong&gt;, supports &lt;strong&gt;any OpenAI-compatible API&lt;/strong&gt;, and has &lt;strong&gt;mobile support&lt;/strong&gt;. I'd love your feedback!&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;You guys are so amazing! After all the feedback from my last post, I'm very happy to announce that Observer AI is almost officially launched! I want to thank everyone for their encouragement and ideas.&lt;/p&gt; &lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New in the last few days(Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;✅ 1-Command 100% Local Install:&lt;/strong&gt; I made it super simple. Just run docker compose up --build and the entire stack runs locally. No certs to accept or &amp;quot;online activation&amp;quot; needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Universal Model Support:&lt;/strong&gt; You're no longer limited to Ollama! You can now connect to &lt;strong&gt;any endpoint that uses the OpenAI v1/chat standard&lt;/strong&gt;. This includes local servers like LM Studio, Llama.cpp, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Mobile Support:&lt;/strong&gt; You can now use the app on your phone, using its camera and microphone as sensors. (Note: Mobile browsers don't support screen sharing).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I hope that I'm just getting started. Here's what I will focus on next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standalone Desktop App:&lt;/strong&gt; A 1-click installer for a native app experience. (With inference and everything!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Telegram Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slack Notifications&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt; &lt;li&gt;And much more!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Let's Build Together:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href="https://discord.gg/wnBb7ZQDUC"&gt;https://discord.gg/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!&lt;/p&gt; &lt;p&gt;PS. Sorry to everyone who &lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ah6imcae6ccf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxmr2h/thank_you_rlocalllama_observer_ai_launches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T00:18:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx62hd</id>
    <title>Nvidia being Nvidia: FP8 is 150 Tflops faster when kernel name contain "cutlass"</title>
    <updated>2025-07-11T12:50:38+00:00</updated>
    <author>
      <name>/u/bora_ach</name>
      <uri>https://old.reddit.com/user/bora_ach</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bora_ach"&gt; /u/bora_ach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx6dcm</id>
    <title>llama2.c running on the original 2007 iPhone</title>
    <updated>2025-07-11T13:04:10+00:00</updated>
    <author>
      <name>/u/kyousukegum</name>
      <uri>https://old.reddit.com/user/kyousukegum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt; &lt;img alt="llama2.c running on the original 2007 iPhone" src="https://external-preview.redd.it/a3NtYmE5YXNrOGNmMX0KWZ1PPnq70dBw4mT1dYRnKuITb3d3yA97K-6QwELL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d84dc04e7624cefc75d18c603d35424468ce1db" title="llama2.c running on the original 2007 iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyousukegum"&gt; /u/kyousukegum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/3u6728ask8cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T13:04:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx9pny</id>
    <title>Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!</title>
    <updated>2025-07-11T15:23:24+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/"&gt; &lt;img alt="Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!" src="https://preview.redd.it/a1tzaif5j9cf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3898e31bb7d9fbf8198401001a795859f33eafcb" title="Damn this is deepseek moment one of the 3bst coding model and it's open source and by far it's so good !!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19"&gt;https://x.com/Kimi_Moonshot/status/1943687594560332025?t=imY6uyPkkt-nqaao67g04Q&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a1tzaif5j9cf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T15:23:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx5awq</id>
    <title>Friendly reminder that Grok 3 should be now open-sourced</title>
    <updated>2025-07-11T12:13:48+00:00</updated>
    <author>
      <name>/u/Wrong_User_Logged</name>
      <uri>https://old.reddit.com/user/Wrong_User_Logged</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt; &lt;img alt="Friendly reminder that Grok 3 should be now open-sourced" src="https://b.thumbs.redditmedia.com/933OQwllbA1hY7_1-xQgZI7EOZf5Fdt9pi7_3gUoRkc.jpg" title="Friendly reminder that Grok 3 should be now open-sourced" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong_User_Logged"&gt; /u/Wrong_User_Logged &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lx5awq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T12:13:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxnsh1</id>
    <title>OpenAI delays its open weight model again for "safety tests"</title>
    <updated>2025-07-12T01:09:38+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt; &lt;img alt="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" src="https://preview.redd.it/z5xvjxzefccf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe88bccce70567bd39edea238607127c143134db" title="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z5xvjxzefccf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:09:38+00:00</published>
  </entry>
</feed>
