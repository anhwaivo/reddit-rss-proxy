<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-01T08:35:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1idz487</id>
    <title>'we're in this bizarre world where the best way to learn about llms... is to read papers by chinese companies. i do not think this is a good state of the world' - us labs keeping their architectures and algorithms secret is ultimately hurting ai development in the us.' - Dr Chris Manning</title>
    <updated>2025-01-30T22:13:22+00:00</updated>
    <author>
      <name>/u/Research2Vec</name>
      <uri>https://old.reddit.com/user/Research2Vec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/atroyn/status/1884700560500416881"&gt;https://x.com/atroyn/status/1884700560500416881&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Research2Vec"&gt; /u/Research2Vec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1idz487/were_in_this_bizarre_world_where_the_best_way_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-30T22:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieh01f</id>
    <title>Fully open source codebase to train SOTA VLMs</title>
    <updated>2025-01-31T15:15:11+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm Andi from multimodal team at Hugging Face.&lt;/p&gt; &lt;p&gt;Today we're open-sourcing the codebase used to train SmolVLM from scratch on 256 H100s&lt;br /&gt; Inspired by our team's effort to open-source DeepSeek's R1 training, we are releasing the training and evaluation code on top of the weights&lt;br /&gt; Now you can train any of our SmolVLMs—or create your own custom VLMs!&lt;/p&gt; &lt;p&gt;Go check it out:&lt;br /&gt; &lt;a href="https://github.com/huggingface/smollm/tree/main/vision"&gt;https://github.com/huggingface/smollm/tree/main/vision&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieh01f/fully_open_source_codebase_to_train_sota_vlms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieh01f/fully_open_source_codebase_to_train_sota_vlms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieh01f/fully_open_source_codebase_to_train_sota_vlms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T15:15:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ierlm9</id>
    <title>OpenAI becoming open Ai?</title>
    <updated>2025-01-31T22:43:45+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ierlm9/openai_becoming_open_ai/"&gt; &lt;img alt="OpenAI becoming open Ai?" src="https://b.thumbs.redditmedia.com/o-NfzbYjZnhokQE8QORzMwIs_iOL5NcrSkzjsRHW-ho.jpg" title="OpenAI becoming open Ai?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mq1udl0rqege1.png?width=755&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec3a57ca0b33095d0e9c8550abb788d27017b5b3"&gt;https://preview.redd.it/mq1udl0rqege1.png?width=755&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ec3a57ca0b33095d0e9c8550abb788d27017b5b3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;this was talked in the AMA: &lt;a href="https://www.reddit.com/r/OpenAI/comments/1ieonxv/comment/maa0dcx/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;sama answer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ierlm9/openai_becoming_open_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ierlm9/openai_becoming_open_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ierlm9/openai_becoming_open_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T22:43:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ielupk</id>
    <title>I added Live Web Search on top of DeepSeek-R1-LLama-70b and made it API</title>
    <updated>2025-01-31T18:39:31+00:00</updated>
    <author>
      <name>/u/sickleRunner</name>
      <uri>https://old.reddit.com/user/sickleRunner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielupk/i_added_live_web_search_on_top_of/"&gt; &lt;img alt="I added Live Web Search on top of DeepSeek-R1-LLama-70b and made it API" src="https://preview.redd.it/0pguliybjdge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9348eced5b7103939d1b6beb8b2ee81e5d8c3c83" title="I added Live Web Search on top of DeepSeek-R1-LLama-70b and made it API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sickleRunner"&gt; /u/sickleRunner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0pguliybjdge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielupk/i_added_live_web_search_on_top_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ielupk/i_added_live_web_search_on_top_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T18:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieaiq4</id>
    <title>Hey, some of you asked for a multilingual fine-tune of the R1 distills, so here they are! Trained on over 35 languages, this should quite reliably output CoT in your language. As always, the code, weights, and data are all open source.</title>
    <updated>2025-01-31T08:44:00+00:00</updated>
    <author>
      <name>/u/Peter_Lightblue</name>
      <uri>https://old.reddit.com/user/Peter_Lightblue</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieaiq4/hey_some_of_you_asked_for_a_multilingual_finetune/"&gt; &lt;img alt="Hey, some of you asked for a multilingual fine-tune of the R1 distills, so here they are! Trained on over 35 languages, this should quite reliably output CoT in your language. As always, the code, weights, and data are all open source." src="https://external-preview.redd.it/eMItb2dkatZwZPt-N-o-ODncWuvwGgn8w91JWJRsEcg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8507bb5e6a8a03ef08478289b8275d62d0245cc" title="Hey, some of you asked for a multilingual fine-tune of the R1 distills, so here they are! Trained on over 35 languages, this should quite reliably output CoT in your language. As always, the code, weights, and data are all open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter_Lightblue"&gt; /u/Peter_Lightblue &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/lightblue/r1-multilingual-679c890166ac0a84e83e38fa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieaiq4/hey_some_of_you_asked_for_a_multilingual_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieaiq4/hey_some_of_you_asked_for_a_multilingual_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T08:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1if1ahn</id>
    <title>Created a gist how to setup Ollama with Open WebUI in Docker on Ubuntu Server VM with Nvidia GPU on Proxmox, perhaps someone here finds it useful.</title>
    <updated>2025-02-01T07:36:23+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1ahn/created_a_gist_how_to_setup_ollama_with_open/"&gt; &lt;img alt="Created a gist how to setup Ollama with Open WebUI in Docker on Ubuntu Server VM with Nvidia GPU on Proxmox, perhaps someone here finds it useful." src="https://external-preview.redd.it/DaucjXMGsNHM-CtmdilC9-Be6MC8V2z4ykjVCgOkTFc.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62ca4cb88917f17e7200a6f1c665b5d959713745" title="Created a gist how to setup Ollama with Open WebUI in Docker on Ubuntu Server VM with Nvidia GPU on Proxmox, perhaps someone here finds it useful." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gist.github.com/cepa/94d81ea83b0c2e97340c7c512767da18"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1ahn/created_a_gist_how_to_setup_ollama_with_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if1ahn/created_a_gist_how_to_setup_ollama_with_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T07:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iexh50</id>
    <title>Soft Prompt Tuning Modern LLMs</title>
    <updated>2025-02-01T03:34:44+00:00</updated>
    <author>
      <name>/u/xnick77x</name>
      <uri>https://old.reddit.com/user/xnick77x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iexh50/soft_prompt_tuning_modern_llms/"&gt; &lt;img alt="Soft Prompt Tuning Modern LLMs" src="https://external-preview.redd.it/6_3ctAqxbdvXiaHQYdKYaOoJmjsKTUr9kcigziRYZMg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4894ec1ef80e49f99a7de1c890f9b439263adb94" title="Soft Prompt Tuning Modern LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xnick77x"&gt; /u/xnick77x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://frugalgpu.substack.com/p/soft-prompt-tuning-modern-llms-442"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iexh50/soft_prompt_tuning_modern_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iexh50/soft_prompt_tuning_modern_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T03:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ie6gv0</id>
    <title>It’s time to lead guys</title>
    <updated>2025-01-31T04:10:56+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"&gt; &lt;img alt="It’s time to lead guys" src="https://preview.redd.it/4r69mh9f89ge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f3c997deb132531af541fbe7a279f1544512cbb" title="It’s time to lead guys" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4r69mh9f89ge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ie6gv0/its_time_to_lead_guys/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T04:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iefhfj</id>
    <title>Mistral Small 3 24B GGUF quantization Evaluation results</title>
    <updated>2025-01-31T14:03:45+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"&gt; &lt;img alt="Mistral Small 3 24B GGUF quantization Evaluation results" src="https://external-preview.redd.it/6gL76ZMrrBOOoaB0ogD-JcHTREOFmGNz2SY3hi5tJtE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=506e8acadec458fdbc5263aab62e7fe23bff3e73" title="Mistral Small 3 24B GGUF quantization Evaluation results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ontcp7qk5cge1.png?width=790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=867fa635acedb4047fe1b1a0a77f20d5eaa3534c"&gt;https://preview.redd.it/ontcp7qk5cge1.png?width=790&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=867fa635acedb4047fe1b1a0a77f20d5eaa3534c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h92f0kol5cge1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc5d097366612440247bc260fd5c4bf2f4c10ce1"&gt;https://preview.redd.it/h92f0kol5cge1.png?width=1605&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bc5d097366612440247bc260fd5c4bf2f4c10ce1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hzo2smfm5cge1.png?width=2321&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df8455553ec547e9c17cd69022d1a6f86be766ab"&gt;https://preview.redd.it/hzo2smfm5cge1.png?width=2321&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=df8455553ec547e9c17cd69022d1a6f86be766ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please note that the purpose of this test is to check if the model's intelligence will be significantly affected at low quantization levels, rather than evaluating which gguf is the best.&lt;/p&gt; &lt;p&gt;Regarding Q6_K-lmstudio: This model was downloaded from the lmstudio hf repo and uploaded by bartowski. However, this one is a static quantization model, while others are dynamic quantization models from bartowski's own repo.&lt;/p&gt; &lt;p&gt;gguf: &lt;a href="https://huggingface.co/bartowski/Mistral-Small-24B-Instruct-2501-GGUF"&gt;https://huggingface.co/bartowski/Mistral-Small-24B-Instruct-2501-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Backend: &lt;a href="https://www.ollama.com/"&gt;https://www.ollama.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;evaluation tool: &lt;a href="https://github.com/chigkim/Ollama-MMLU-Pro"&gt;https://github.com/chigkim/Ollama-MMLU-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;evaluation config: &lt;a href="https://pastebin.com/mqWZzxaH"&gt;https://pastebin.com/mqWZzxaH&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iefhfj/mistral_small_3_24b_gguf_quantization_evaluation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T14:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieosbx</id>
    <title>Relatively budget 671B R1 CPU inference workstation setup, 2-3T/s</title>
    <updated>2025-01-31T20:43:05+00:00</updated>
    <author>
      <name>/u/xinranli</name>
      <uri>https://old.reddit.com/user/xinranli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw a post going over how to do Q2 R1 inference with a gaming rig by reading the weights directly from SSDs. It's a very neat technique and I would also like to share my experiences with CPU inference with a regular EPYC workstation setup. This setup has good memory capacity and relatively decent CPU inference performance, while also providing a great backbone for GPU or SSD expansions. Being a workstation rather than a server means this rig should be rather easily worked with and integrated into your bedroom. &lt;/p&gt; &lt;p&gt;I am using a Q4KM GGUF and still experimenting with turning cores/CCDs/SMT on and off on my 7773X and trying different context lengths to better understand where the limit is at, but 3T/s seems to be the limit as everything is still extremely memory bandwidth starved. &lt;/p&gt; &lt;p&gt;CPU: Any Milan EPYC over 32 cores should be okay. The price of these things varies greatly depending on the part number and if they are ES/QS/OEM/Production chips. I recommend buying an ES or OEM 64-core variant, some of them go for $500-$600. Some cheapest 32-core OEM models can go as low as $200-$300. Make sure you ask the seller CPU/board/BIOSver compatibility before purchasing. &lt;strong&gt;Never buy Lenovo or DELL locked EPYC chips unless you know what you are doing!&lt;/strong&gt; They are never going to work on consumer motherboards. Rome EPYCs can also work since they also support DDR4 3200, but they aren't too much cheaper and have quite a bit lower CPU performance compared to Milan. There are several overclockable ES/OEM Rome chips out here such as 32 core ZS1711E3VIVG5 and 100-000000054-04. 64 core ZS1406E2VJUG5 and 100-000000053-04. I had both ZS1711 and 54-04 and it was super fun to tweak around and OC them to 3.7GHz all core, if you can find one at a reasonable price, they are also great options. &lt;/p&gt; &lt;p&gt;Motherboard: H12SSL goes for around $500-600, and ROMED8-2T goes for $600-700. I recommend ROMED8-2T over H12SSL for the total 7x16 PCIe connectors rather than H12SSL's 5x16 + 2x8. &lt;/p&gt; &lt;p&gt;DRAM: This is where most money should be spent. You will want to get 8 sticks of 64GB DDR4 3200MT/s RDIMM. &lt;strong&gt;It has to be RDIMM (Registered DIMM), and it also has to be the same model of memory.&lt;/strong&gt; Each stick costs around $100-125, so in total you should spend $800-1000 on memory. This will give you 512GB capacity and 200GB/s bandwidth. The stick I got is HMAA8GR7AJR4N-XN, which works well with my ROMED8-2T. You don't have to pick from the QVL list of the motherboard vendor, just use it as a reference. 3200MT/s is not a strict requirement, if your budget is tight, you can go down to 2933 or 2666. Also, I would avoid 64GB LRDIMMs (Load Reduced DIMM). They are earlier DIMMs in DDR4 era when per DRAM chip density was still low, so each DRAM package has 2 or 4 chips packed inside (DDP or 3DS), the buffers on them are also additional points of failure. 128GB and 256GB LRDIMMs are the cutting edge for DDR4, but they are outrageously expensive and hard to find. 8x64GB is enough for Q4 inference.&lt;/p&gt; &lt;p&gt;CPU cooler: I would limit the spending here to around $50. Any SP3 heatsink should be OK. If you bought 280W TDP CPUs, consider maybe getting better ones but there is no need to go above $100.&lt;/p&gt; &lt;p&gt;PSU: This system should be a backbone for more GPUs to one day be installed. I would start with a pretty beefy one, maybe around 1200W ish. I think around $200 is a good spot to shop for. &lt;/p&gt; &lt;p&gt;Storage: Any 2TB+ NVME SSD should be fairly flexible, they are fairly cheap these days. $100&lt;/p&gt; &lt;p&gt;Case: I recommend a full-tower with dual PSU support. I highly recommend Lianli's o11 and o11 XL family. They are quite pricy but done really well. $200&lt;/p&gt; &lt;p&gt;In conclusion, this whole setup should cost around $2000-2500 from scratch, not too much more expensive than a single 4090 nowadays. It can do Q4 R1 inference with usable context length and it's going to be a good starting point for future local inference. The 7 x16 PCIe gen 4 expansion provided is really handy and can do so much more once you can afford more GPUs. &lt;/p&gt; &lt;p&gt;I am also looking into testing some old Xeons such as running dual E5v4s, they are dirt cheap right now. Will post some results once I have them running!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xinranli"&gt; /u/xinranli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively_budget_671b_r1_cpu_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively_budget_671b_r1_cpu_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively_budget_671b_r1_cpu_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ielhyu</id>
    <title>Tutorial: How to Run DeepSeek-R1 (671B) 1.58bit on Open WebUI</title>
    <updated>2025-01-31T18:24:36+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"&gt; &lt;img alt="Tutorial: How to Run DeepSeek-R1 (671B) 1.58bit on Open WebUI" src="https://external-preview.redd.it/GToYANeKQHKhFdKJjLK03Emv1ylZ0l8jeD1iuQJ8-dE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fbf9c89972d5c31e3bd2d3354696be4e8d5b9d" title="Tutorial: How to Run DeepSeek-R1 (671B) 1.58bit on Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! Daniel &amp;amp; I (Mike) at &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; collabed with Tim from &lt;a href="https://github.com/open-webui/open-webui"&gt;Open WebUI&lt;/a&gt; to bring you this step-by-step on how to run the non-distilled DeepSeek-R1 Dynamic 1.58-bit model locally!&lt;/p&gt; &lt;p&gt;This guide is summarized so I highly recommend you read the full guide (with pics) here: &lt;a href="https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/"&gt;https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Expect 2 tokens/s with 96GB RAM (without GPU).&lt;/p&gt; &lt;h1&gt;To Run DeepSeek-R1:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Install Llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download prebuilt binaries or build from source following &lt;a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md"&gt;this guide&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Download the Model (1.58-bit, 131GB) from&lt;/strong&gt; &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Get the model from &lt;a href="https://huggingface.co/unsloth/DeepSeek-R1-GGUF"&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Use Python to download it programmatically:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from huggingface_hub import snapshot_download snapshot_download( repo_id=&amp;quot;unsloth/DeepSeek-R1-GGUF&amp;quot;, local_dir=&amp;quot;DeepSeek-R1-GGUF&amp;quot;, allow_patterns=[&amp;quot;*UD-IQ1_S*&amp;quot;] ) &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Once the download completes, you’ll find the model files in a directory structure like this:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DeepSeek-R1-GGUF/ ├── DeepSeek-R1-UD-IQ1_S/ │ ├── DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf │ ├── DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf │ ├── DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Ensure you know the path where the files are stored.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Install and Run Open WebUI&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you don’t already have it installed, no worries! It’s a simple setup. Just follow the Open WebUI docs here: &lt;a href="https://docs.openwebui.com/"&gt;https://docs.openwebui.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Once installed, start the application - we’ll connect it in a later step to interact with the DeepSeek-R1 model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Start the Model Server with Llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now that the model is downloaded, the next step is to run it using Llama.cpp’s server mode.&lt;/p&gt; &lt;h1&gt;🛠️Before You Begin:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Locate the llama-server Binary&lt;/li&gt; &lt;li&gt;If you built Llama.cpp from source, the llama-server executable is located in:llama.cpp/build/bin Navigate to this directory using:cd [path-to-llama-cpp]/llama.cpp/build/bin Replace [path-to-llama-cpp] with your actual Llama.cpp directory. For example:cd ~/Documents/workspace/llama.cpp/build/bin&lt;/li&gt; &lt;li&gt;Point to Your Model Folder&lt;/li&gt; &lt;li&gt;Use the full path to the downloaded GGUF files.When starting the server, specify the first part of the split GGUF files (e.g., DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf).&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;🚀Start the Server&lt;/h1&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ --model /[your-directory]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40 &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;Example (If Your Model is in /Users/tim/Documents/workspace):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;✅ Once running, the server will be available at:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://127.0.0.1:10000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;🖥️ Llama.cpp Server Running&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/erjbg5v5cbge1.png?width=3428&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fff4de133562bb6f67076db17285860b7294f2ad"&gt;After running the command, you should see a message confirming the server is active and listening on port 10000.&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Step 5: Connect Llama.cpp to Open WebUI&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open Admin Settings in Open WebUI.&lt;/li&gt; &lt;li&gt;Go to Connections &amp;gt; OpenAI Connections.&lt;/li&gt; &lt;li&gt;Add the following details:&lt;/li&gt; &lt;li&gt;URL → &lt;a href="http://127.0.0.1:10000/v1API"&gt;http://127.0.0.1:10000/v1API&lt;/a&gt; Key → none&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Adding Connection in Open WebUI&lt;/h1&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8eja3yugcbge1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d890d2ed9c7bb20f6b2293a84c9c294a16de0a2"&gt;https://preview.redd.it/8eja3yugcbge1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d890d2ed9c7bb20f6b2293a84c9c294a16de0a2&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;You don't need a GPU to run this model but it will make it faster especially when you have at least 24GB of VRAM.&lt;/li&gt; &lt;li&gt;Try to have a sum of RAM + VRAM = 120GB+ to get decent tokens/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you have any questions please let us know and also - any suggestions are also welcome! Happy running folks! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ielhyu/tutorial_how_to_run_deepseekr1_671b_158bit_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T18:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1if0f0q</id>
    <title>kokoro-on-browser:fully offline, local web-based text-to-speech application built using React and Vite. Powered by Kokoro.js</title>
    <updated>2025-02-01T06:34:15+00:00</updated>
    <author>
      <name>/u/fajrmn</name>
      <uri>https://old.reddit.com/user/fajrmn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if0f0q/kokoroonbrowserfully_offline_local_webbased/"&gt; &lt;img alt="kokoro-on-browser:fully offline, local web-based text-to-speech application built using React and Vite. Powered by Kokoro.js" src="https://external-preview.redd.it/0HyIJ1l1CEno8j1KoR6iI-87ktiGCajBTpMb_ydGwDk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bee35d7add852122ecb3630117e2b2782c49c0af" title="kokoro-on-browser:fully offline, local web-based text-to-speech application built using React and Vite. Powered by Kokoro.js" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1if0f0q/video/xm1kznq22hge1/player"&gt;https://reddit.com/link/1if0f0q/video/xm1kznq22hge1/player&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Browser-based text-to-speech conversion&lt;/li&gt; &lt;li&gt;Powered by Kokoro.js library by &lt;a href="https://www.reddit.com/user/xenovatech/"&gt;xenovatech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Responsive React application / dev with Vite&lt;/li&gt; &lt;li&gt;One click netlify deployment. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks to all the teams who have contributed to Kokoro.js&lt;/p&gt; &lt;p&gt;Repo : &lt;a href="https://github.com/fajrmn/kokoro-on-browser"&gt;https://github.com/fajrmn/kokoro-on-browser&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fajrmn"&gt; /u/fajrmn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if0f0q/kokoroonbrowserfully_offline_local_webbased/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if0f0q/kokoroonbrowserfully_offline_local_webbased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if0f0q/kokoroonbrowserfully_offline_local_webbased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T06:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1iesirf</id>
    <title>The new Mistral Small model is disappointing</title>
    <updated>2025-01-31T23:24:55+00:00</updated>
    <author>
      <name>/u/Master-Meal-77</name>
      <uri>https://old.reddit.com/user/Master-Meal-77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was super excited to see a brand new 24B model from Mistral but after actually using it for more than single-turn interaction... I just find it to be disappointing&lt;/p&gt; &lt;p&gt;In my experience with the model it has a really hard time taking into account any information that is not crammed down its throat. It easily gets off track or confused&lt;/p&gt; &lt;p&gt;For single-turn question -&amp;gt; response it's good. For conversation, or anything that requires paying attention to context, it shits the bed. I've quadruple-checked and I'm using the right prompt format and system prompt...&lt;/p&gt; &lt;p&gt;Bonus question: Why is the rope theta value 100M? The model is not long context. I think this was a misstep in choosing the architecture &lt;/p&gt; &lt;p&gt;Am I alone on this? Have any of you gotten it to work properly on tasks that require intelligence and instruction following?&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Master-Meal-77"&gt; /u/Master-Meal-77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iesirf/the_new_mistral_small_model_is_disappointing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iesirf/the_new_mistral_small_model_is_disappointing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iesirf/the_new_mistral_small_model_is_disappointing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T23:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1iettv1</id>
    <title>Deepseek bitnet</title>
    <updated>2025-02-01T00:25:36+00:00</updated>
    <author>
      <name>/u/Thistleknot</name>
      <uri>https://old.reddit.com/user/Thistleknot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iettv1/deepseek_bitnet/"&gt; &lt;img alt="Deepseek bitnet" src="https://preview.redd.it/cm74ybjy8fge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf896e28b8d16473f9f4db98d30877291e849edd" title="Deepseek bitnet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thistleknot"&gt; /u/Thistleknot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cm74ybjy8fge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iettv1/deepseek_bitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iettv1/deepseek_bitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T00:25:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieihjr</id>
    <title>What the hell do people expect?</title>
    <updated>2025-01-31T16:20:03+00:00</updated>
    <author>
      <name>/u/Suitable-Name</name>
      <uri>https://old.reddit.com/user/Suitable-Name</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After the release of R1 I saw so many &amp;quot;But it can't talk about tank man!&amp;quot;, &amp;quot;But it's censored!&amp;quot;, &amp;quot;But it's from the chinese!&amp;quot; posts. &lt;/p&gt; &lt;ol&gt; &lt;li&gt;They are all censored. And for R1 in particular... I don't want to discuss chinese politics (or politics at all) with my LLM. That's not my use-case and I don't think I'm in a minority here.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What would happen if it was not censored the way it is? The guy behind it would probably have disappeared by now.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;They all give a fuck about data privacy as much as they can. Else we wouldn't have ever read about samsung engineers not being allowed to use GPT for processor development anymore.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The model itself is much less censored than the web chat&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;IMHO it's not worse or better than the rest (non self-hosted) and the negative media reports are 1:1 the same like back in the days when Zen was released by AMD and all Intel could do was cry like &amp;quot;But it's just cores they glued together!&amp;quot;&lt;/p&gt; &lt;p&gt;Edit: Added clarification that the web chat is more censored than the model itself (self-hosted)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Name"&gt; /u/Suitable-Name &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieihjr/what_the_hell_do_people_expect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T16:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1iewnwe</id>
    <title>I was able to 1-shot prompt the unlsoth "python flappy bird game" test with Deepseek R1 distilled 70B. The distilled models deserve more credit.</title>
    <updated>2025-02-01T02:50:40+00:00</updated>
    <author>
      <name>/u/Hisma</name>
      <uri>https://old.reddit.com/user/Hisma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the unlsoth models and how they were getting excellent performance using dynamic quantization of the main model down to 2 bits and shrinking the model 80% in size. See article here - &lt;a href="https://unsloth.ai/blog/deepseekr1-dynamic"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still, even at 80% the size, when you consider kv cache &amp;amp; context, you still need a minimum of around 200GB of high bandwidth memory to run those models at any sort of respectable speed. And even then, you're getting 5-6 t/s tops, unless you can splurge on a 24-channel dual EPYC system. &lt;/p&gt; &lt;p&gt;I am lucky enough to have a 4x 3090 system, so I have 96GB of VRAM to play with. To me 96GB of VRAM is the sweet spot for running 70B models where you can run bigger quants (6bit+) with plenty left over for context window. And with all that still get above 15 t/s (and much much faster using parallelism &amp;amp; draft models/etc).&lt;/p&gt; &lt;p&gt;Anyway, I played around with 70B distilled model. I am running it with ollama, but I had to customize it to run at it's intended temperature of .6, and increased the context window to 16k (ollama stupidly defaults every model to 2k context). &lt;/p&gt; &lt;p&gt;Then I ran the unsloth flappy bird test prompt to see how close I could come to replicate it's performance - &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Prompt: Create a Flappy Bird game in Python. You must include these things: You must use pygame. The background color should be randomly chosen and is a light shade. Start with a light blue color. Pressing SPACE multiple times will accelerate the bird. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color. Place on the bottom some land colored as dark brown or yellow chosen randomly. Make a score shown on the top right side. Increment if you pass pipes and don't hit them. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again. The final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Here's the full response the model produced - &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Okay, I need to create a Flappy Bird game using Pygame based on the user's requirements. Let me go through each requirement step by step.&lt;/p&gt; &lt;p&gt;First, the background color must be light and randomly chosen, starting with light blue. So I'll set a default light blue but have it change randomly each time the game restarts.&lt;/p&gt; &lt;p&gt;Next, pressing SPACE should make the bird accelerate. That means each press will add to the bird's velocity, allowing it to jump higher. I'll need to handle multiple key presses, so maybe track if the spacebar is pressed and apply force each frame until released.&lt;/p&gt; &lt;p&gt;The bird's shape should be random—square, circle, or triangle. Its color must be dark. So I'll create functions to generate a random dark color and another to choose the shape type randomly each game.&lt;/p&gt; &lt;p&gt;There's land at the bottom, colored either dark brown or yellow, chosen randomly. I'll add a ground rectangle with a randomly selected color from those options.&lt;/p&gt; &lt;p&gt;The score is displayed on the top right and increases when passing pipes without hitting them. So I'll need to track when the bird passes a pipe and increment the score accordingly. The best score should be saved and shown when the game ends.&lt;/p&gt; &lt;p&gt;Pipes must be randomly spaced with enough space between them for the bird to pass. They can be dark green, light brown, or dark gray. I'll create a list of pipes, each with random heights and gaps, moving from right to left.&lt;/p&gt; &lt;p&gt;When the bird hits a pipe or the ground, the game ends. Then, display the best score on the screen. The user can press Q or Esc to quit and SPACE to restart.&lt;/p&gt; &lt;p&gt;I'll structure the code with functions for generating colors, creating pipes, handling collisions, drawing elements, and updating the game state. I'll also set up event loops for handling key presses and quitting.&lt;/p&gt; &lt;p&gt;Testing each part as I go is important. For example, ensuring the bird's velocity increases correctly when pressing SPACE multiple times, checking that pipes generate without overlapping, and verifying collision detection works as expected.&lt;/p&gt; &lt;p&gt;Finally, I'll make sure all elements are styled according to the color requirements and that the game restarts properly with new randomizations. &lt;/p&gt; &lt;p&gt;Here's a complete implementation of Flappy Bird with all the specified requirements:&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt;&lt;code&gt;import pygame import sys import random pygame.init() # Constants WIDTH, HEIGHT = 800, 600 GRAVITY = 0.5 JUMP_FORCE = -8 PIPE_WIDTH = 80 PIPE_GAP = 150 GROUND_HEIGHT = 50 SCORE_FONT = pygame.font.SysFont('Arial', 30) BEST_SCORE_FONT = pygame.font.SysFont('Arial', 40) class Bird: def __init__(self): self.shape = random.choice(['square', 'circle', 'triangle']) self.color = (random.randint(0,100), random.randint(0,100), random.randint(0,100)) self.x = WIDTH // 4 self.y = HEIGHT // 2 self.velocity = 0 self.size = 20 def draw(self, screen): if self.shape == 'square': pygame.draw.rect(screen, self.color, (self.x, self.y, self.size, self.size)) elif self.shape == 'circle': pygame.draw.circle(screen, self.color, (int(self.x + self.size//2), int(self.y + self.size//2)), self.size//2) elif self.shape == 'triangle': points = [ (self.x, self.y), (self.x + self.size, self.y), (self.x + self.size//2, self.y + self.size) ] pygame.draw.polygon(screen, self.color, points) class Pipe: def __init__(self): self.height = random.randint(50, HEIGHT - PIPE_GAP - 100) self.x = WIDTH self.color = (random.choice([0, 128, 64]), random.choice([128, 64, 0]), random.choice([0, 64, 32])) def draw(self, screen): pygame.draw.rect(screen, self.color, (self.x, 0, PIPE_WIDTH, self.height)) pygame.draw.rect(screen, self.color, (self.x, self.height + PIPE_GAP, PIPE_WIDTH, HEIGHT - (self.height + PIPE_GAP))) def get_light_color(): return (random.randint(128, 255), random.randint(128, 255), random.randint(128, 255)) def get_dark_color(): return (random.randint(0, 128), random.randint(0, 128), random.randint(0, 128)) def main(): screen = pygame.display.set_mode((WIDTH, HEIGHT)) clock = pygame.time.Clock() background_color = (173, 216, 230) # Light blue ground_color = (101, 67, 33) # Dark brown bird = Bird() pipes = [Pipe()] score = 0 best_score = 0 game_active = True jump_force = JUMP_FORCE while True: for event in pygame.event.get(): if event.type == pygame.QUIT: pygame.quit() sys.exit() elif event.type == pygame.KEYDOWN: if event.key == pygame.K_SPACE and game_active: jump_force = JUMP_FORCE bird.velocity = jump_force elif (event.key == pygame.K_q or event.key == pygame.K_ESCAPE): pygame.quit() sys.exit() keys = pygame.key.get_pressed() if keys[pygame.K_SPACE] and game_active: bird.velocity += 0.5 screen.fill(background_color) # Draw ground ground_rect = pygame.Rect(0, HEIGHT - GROUND_HEIGHT, WIDTH, GROUND_HEIGHT) pygame.draw.rect(screen, ground_color, ground_rect) if game_active: # Bird physics bird.velocity += GRAVITY bird.y += bird.velocity # Update pipes for pipe in pipes: pipe.x -= 2 if pipe.x &amp;lt; -PIPE_WIDTH: pipes.remove(pipe) score += 1 # Add new pipes if len(pipes) == 0 or pipes[-1].x &amp;lt; WIDTH // 2: pipes.append(Pipe()) # Check collisions for pipe in pipes: if (bird.x + bird.size &amp;gt; pipe.x and bird.x &amp;lt; pipe.x + PIPE_WIDTH): if bird.y &amp;lt; pipe.height or bird.y + bird.size &amp;gt; pipe.height + PIPE_GAP: game_active = False if bird.y + bird.size &amp;gt; HEIGHT - GROUND_HEIGHT: game_active = False else: # Game over screen text = SCORE_FONT.render(f'Score: {score}', True, (0, 0, 0)) best_text = BEST_SCORE_FONT.render(f'Best Score: {best_score}', True, (0, 0, 0)) restart_text = SCORE_FONT.render('Press SPACE to restart', True, (0, 0, 0)) screen.blit(text, (WIDTH // 2 - text.get_width() // 2, HEIGHT // 2)) screen.blit(best_text, (WIDTH // 2 - best_text.get_width() // 2, HEIGHT // 2 + 40)) screen.blit(restart_text, (WIDTH // 2 - restart_text.get_width() // 2, HEIGHT // 2 + 80)) # Draw pipes for pipe in pipes: pipe.draw(screen) bird.draw(screen) score_text = SCORE_FONT.render(f'Score: {score}', True, (0, 0, 0)) screen.blit(score_text, (WIDTH - 100, 10)) if not game_active: best_score = max(best_score, score) if keys[pygame.K_SPACE]: # Reset game background_color = get_light_color() bird = Bird() pipes.clear() score = 0 game_active = True pygame.display.flip() clock.tick(60) if __name__ == &amp;quot;__main__&amp;quot;: main() &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;This implementation includes all the requested features:&lt;/p&gt; &lt;p&gt;Random light background color (starting with light blue) Bird shape can be square, circle, or triangle with random dark colors Ground with random dark brown or yellow color Randomly spaced pipes with different colors Score tracking and best score display Space to jump, Q/Esc to quit, SPACE to restart Physics-based bird movement Collision detection The game runs smoothly at 60 FPS and includes all the requested gameplay mechanics. You can run it by saving it as a Python file and running it with Pygame Installed (pip install pygame).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The results - &lt;a href="https://imgur.com/Sv0micW"&gt;Imgur&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This response nailed every requirement. And I know for a fact this model wasn't pre-trained on this prompt. Because I tried it w/ the default 2k context and it did not produce a fully working game first try. &lt;/p&gt; &lt;p&gt;Deepseek R1 is a reasoning model, so it needs enough context to go through it's thought process + produce a output. So bumping the context window to 16k is what allowed the model to run at it's potential. The base R1 model was actually designed to run at 128k context, so I'll play with how high I can go before I run out of GPU memory. But going from 2k to 16k already yielded much better results.&lt;/p&gt; &lt;p&gt;So, color me impressed. I wonder if the people that have had poor results w/ the distilled models are because they didn't give the model enough context, or ran 4 bpw or below quants. I think with these models you're better off trying to get 6 bpw or higher as they're already distilled and unoptimized out of the gate. And to repeat, reasoning models need enough context to perform their CoT and produce an output.&lt;/p&gt; &lt;p&gt;All I know is even the distilled models are incredible with just a little tuning. And I believe in the near future someone will actually optimize these distilled models and we'll have near frontier model performance at reasonable sizes. &lt;/p&gt; &lt;p&gt;This is such an exciting time to be a local llm enthusiast!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hisma"&gt; /u/Hisma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iewnwe/i_was_able_to_1shot_prompt_the_unlsoth_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iewnwe/i_was_able_to_1shot_prompt_the_unlsoth_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iewnwe/i_was_able_to_1shot_prompt_the_unlsoth_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T02:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1iejazu</id>
    <title>DeepSeek 8B gets surprised by the 3 R's in strawberry, but manages to do it</title>
    <updated>2025-01-31T16:54:33+00:00</updated>
    <author>
      <name>/u/Fusseldieb</name>
      <uri>https://old.reddit.com/user/Fusseldieb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iejazu/deepseek_8b_gets_surprised_by_the_3_rs_in/"&gt; &lt;img alt="DeepSeek 8B gets surprised by the 3 R's in strawberry, but manages to do it" src="https://preview.redd.it/oemawg4i0dge1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c75d540a6d15cd68cdeabc673be92b5e657f0e0" title="DeepSeek 8B gets surprised by the 3 R's in strawberry, but manages to do it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fusseldieb"&gt; /u/Fusseldieb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oemawg4i0dge1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iejazu/deepseek_8b_gets_surprised_by_the_3_rs_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iejazu/deepseek_8b_gets_surprised_by_the_3_rs_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T16:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iefan2</id>
    <title>Idea: "Can I Run This LLM?" Website</title>
    <updated>2025-01-31T13:55:03+00:00</updated>
    <author>
      <name>/u/Dangerous_Bunch_3669</name>
      <uri>https://old.reddit.com/user/Dangerous_Bunch_3669</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"&gt; &lt;img alt="Idea: &amp;quot;Can I Run This LLM?&amp;quot; Website" src="https://preview.redd.it/l344q42n4cge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d54b955e3b1b2cf6f6d117e19782d25f8f4603c8" title="Idea: &amp;quot;Can I Run This LLM?&amp;quot; Website" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have and idea. You know how websites like Can You Run It let you check if a game can run on your PC, showing FPS estimates and hardware requirements?&lt;/p&gt; &lt;p&gt;What if there was a similar website for LLMs? A place where you could enter your hardware specs and see:&lt;/p&gt; &lt;p&gt;Tokens per second, VRAM &amp;amp; RAM requirements etc.&lt;/p&gt; &lt;p&gt;It would save so much time instead of digging through forums or testing models manually. &lt;/p&gt; &lt;p&gt;Does something like this exist already? 🤔&lt;/p&gt; &lt;p&gt;I would pay for that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Bunch_3669"&gt; /u/Dangerous_Bunch_3669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l344q42n4cge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iefan2/idea_can_i_run_this_llm_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T13:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieooqe</id>
    <title>DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark</title>
    <updated>2025-01-31T20:38:45+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"&gt; &lt;img alt="DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark" src="https://preview.redd.it/i2p0m8em4ege1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4859ed3af650610750eb873e1231f2d526388ec" title="DeepSeek R1 takes #1 overall on a Creative Short Story Writing Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i2p0m8em4ege1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieooqe/deepseek_r1_takes_1_overall_on_a_creative_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1if1rls</id>
    <title>We've been incredibly fortunate with how things have developed over the past year</title>
    <updated>2025-02-01T08:11:39+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I still remember how in late 2023, people were speculating that Mixtral-8x7b was the best open-weights model that the community would get &amp;quot;for a long time&amp;quot;, and possibly ever. Shortly afterwards, Mistral published a controversial blog post that appeared to indicate that they were moving away from open weights – an ominous sign at a time when there were very few open-weights models available, and Anthropic and OpenAI seemed as far out of reach as the stars.&lt;/p&gt; &lt;p&gt;But since then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Meta released the excellent Llama 3 series as open weights (though not entirely free software).&lt;/li&gt; &lt;li&gt;Contrary to what many had feared, Mistral continued to publish open-weights models, even releasing the weights for Mistral Large, which was previously API-only, and now publishing their latest Mistral Small under the Apache License, when the previous version was still under their proprietary MRL.&lt;/li&gt; &lt;li&gt;Yi-34b transitioned from a proprietary license to Apache.&lt;/li&gt; &lt;li&gt;Microsoft has been publishing a number of excellent small models under permissive licenses.&lt;/li&gt; &lt;li&gt;Qwen came out of nowhere, and released the best models that can be run on consumer hardware, almost all of them under permissive licenses.&lt;/li&gt; &lt;li&gt;DeepSeek upended the entire industry, and &lt;strong&gt;an MIT-licensed model is now ranked joint #1 on style-controlled LMSYS,&lt;/strong&gt; on par with cutting-edge, proprietary, API-only models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was completely unforeseeable a year ago. Reality has outpaced the wildest dreams of the most naive optimists. Some doomsayers even predicted that open-weights models would soon be outlawed. The exact opposite has happened, and continues to happen.&lt;/p&gt; &lt;p&gt;To get an idea for what could easily have been, just look at the world of image generation models. In 15 months, there have only been two significant open-weights releases: SD3, and Flux.1D. SD3 was mired in controversy due to Stability's behavior and has been all but ignored by the community, and Flux is crippled by distillation. Both models are censored to a degree that has become the stuff of memes, and their licenses essentially make them unusable for anything except horsing around.&lt;/p&gt; &lt;p&gt;That is how the LLM world could have turned out. Instead, we have a world where I don't even download every new model anymore, because there are multiple exciting releases every week and I simply lack the time to take all of them for a spin. I now regularly delete models from my hard drive that I would have given my right hand for not too long ago. It's just incredible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1if1rls/weve_been_incredibly_fortunate_with_how_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T08:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iep1i4</id>
    <title>DeepSeek AI blocked by Italian authorities</title>
    <updated>2025-01-31T20:54:04+00:00</updated>
    <author>
      <name>/u/ApprehensiveCook2236</name>
      <uri>https://old.reddit.com/user/ApprehensiveCook2236</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"&gt; &lt;img alt="DeepSeek AI blocked by Italian authorities" src="https://external-preview.redd.it/cwVbdtxL_MOCraBvILhveGZjsoXBHPHOS4Ik8eBEAT4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c74e97f5b3d89f2b950da588950bdaa1d7f71e9d" title="DeepSeek AI blocked by Italian authorities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveCook2236"&gt; /u/ApprehensiveCook2236 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.euronews.com/next/2025/01/31/deepseek-ai-blocked-by-italian-authorities-as-others-member-states-open-probes"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iep1i4/deepseek_ai_blocked_by_italian_authorities/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ienetu</id>
    <title>Deepseek R1 is now hosted by Nvidia</title>
    <updated>2025-01-31T19:44:44+00:00</updated>
    <author>
      <name>/u/Outrageous-Win-3244</name>
      <uri>https://old.reddit.com/user/Outrageous-Win-3244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"&gt; &lt;img alt="Deepseek R1 is now hosted by Nvidia" src="https://preview.redd.it/1zufl131vdge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c70d8c80da395577b63301493ed66fac0dc6c408" title="Deepseek R1 is now hosted by Nvidia" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA just brought DeepSeek-R1 671-bn param model to NVIDIA NIM microservice on build.nvidia .com&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The DeepSeek-R1 NIM microservice can deliver up to 3,872 tokens per second on a single NVIDIA HGX H200 system.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Using NVIDIA Hopper architecture, DeepSeek-R1 can deliver high-speed inference by leveraging FP8 Transformer Engines and 900 GB/s NVLink bandwidth for expert communication.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;As usual with NVIDIA's NIM, its a enterprise-scale setu to securely experiment, and deploy AI agents with industry-standard APIs. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Win-3244"&gt; /u/Outrageous-Win-3244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1zufl131vdge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T19:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1iehstw</id>
    <title>GPU pricing is spiking as people rush to self-host deepseek</title>
    <updated>2025-01-31T15:50:54+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt; &lt;img alt="GPU pricing is spiking as people rush to self-host deepseek" src="https://preview.redd.it/599a10y9pcge1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a60ac26bd7a2d3395eefcaee8fbf07a28102792f" title="GPU pricing is spiking as people rush to self-host deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/599a10y9pcge1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iehstw/gpu_pricing_is_spiking_as_people_rush_to_selfhost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T15:50:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ies630</id>
    <title>openai can be opening again</title>
    <updated>2025-01-31T23:09:00+00:00</updated>
    <author>
      <name>/u/tensorsgo</name>
      <uri>https://old.reddit.com/user/tensorsgo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"&gt; &lt;img alt="openai can be opening again" src="https://preview.redd.it/1oovs3vgvege1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acc945b69ba442d4e66865f5e83ab96ac9b83b7b" title="openai can be opening again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensorsgo"&gt; /u/tensorsgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1oovs3vgvege1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ies630/openai_can_be_opening_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-31T23:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ieurv8</id>
    <title>My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:</title>
    <updated>2025-02-01T01:11:25+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt; &lt;img alt="My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:" src="https://preview.redd.it/jixqkaabhfge1.gif?width=216&amp;amp;crop=smart&amp;amp;s=c67a878b6f732544b4693cf47d6dc14a8220e551" title="My PC 10 seconds after I typed “ollama run deepseek-r1:671b”:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jixqkaabhfge1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ieurv8/my_pc_10_seconds_after_i_typed_ollama_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-01T01:11:25+00:00</published>
  </entry>
</feed>
