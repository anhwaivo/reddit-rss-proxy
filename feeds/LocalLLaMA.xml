<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-21T09:48:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1itv9ia</id>
    <title>Agent using Canva. Things are getting wild now...</title>
    <updated>2025-02-20T10:13:03+00:00</updated>
    <author>
      <name>/u/ljhskyso</name>
      <uri>https://old.reddit.com/user/ljhskyso</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itv9ia/agent_using_canva_things_are_getting_wild_now/"&gt; &lt;img alt="Agent using Canva. Things are getting wild now..." src="https://external-preview.redd.it/NTlhcjg4czRyOWtlMY88yKM0XPFK9vDNwHuU8bb82IoeEzVPUXcqILOpddQA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d10dc91d97d69b43afa2b66abd3152b04b563263" title="Agent using Canva. Things are getting wild now..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ljhskyso"&gt; /u/ljhskyso &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hjbttwq4r9ke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itv9ia/agent_using_canva_things_are_getting_wild_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itv9ia/agent_using_canva_things_are_getting_wild_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T10:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1itytpy</id>
    <title>Reasoning model based on Qwen2.5-Max will soon be released</title>
    <updated>2025-02-20T13:46:26+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess new &amp;amp; larger QwQ models are also coming soon?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;On February 20th, during Alibaba's earnings call, Alibaba Group CEO Wu Yongming stated that looking ahead, Alibaba will continue to focus on three main business types: domestic and international e-commerce, AI + cloud computing technology, and internet platform products. Over the next three years, Alibaba will increase investment in three areas around the strategic core of AI: AI infrastructure, basic model platforms and AI native applications, and the AI transformation of existing businesses.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;At the same time, Wu Yongming revealed that Alibaba will also release a deep reasoning model based on Qwen2.5-Max in the near future.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itytpy/reasoning_model_based_on_qwen25max_will_soon_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itytpy/reasoning_model_based_on_qwen25max_will_soon_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itytpy/reasoning_model_based_on_qwen25max_will_soon_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T13:46:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iul655</id>
    <title>Correct Deepseek model for 48gb vram</title>
    <updated>2025-02-21T07:16:31+00:00</updated>
    <author>
      <name>/u/RDofFF</name>
      <uri>https://old.reddit.com/user/RDofFF</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which deepseek model will run okay-ish with 48gb vram and 64gb ram?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RDofFF"&gt; /u/RDofFF &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iul655/correct_deepseek_model_for_48gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iul655/correct_deepseek_model_for_48gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iul655/correct_deepseek_model_for_48gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T07:16:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1itq30t</id>
    <title>Qwen/Qwen2.5-VL-3B/7B/72B-Instruct are out!!</title>
    <updated>2025-02-20T04:28:52+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ"&gt;https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The key enhancements of Qwen2.5-VL are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Visual Understanding: Improved ability to recognize and analyze objects, text, charts, and layouts within images.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Agentic Capabilities: Acts as a visual agent capable of reasoning and dynamically interacting with tools (e.g., using a computer or phone).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Long Video Comprehension: Can understand videos longer than 1 hour and pinpoint relevant segments for event detection.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Visual Localization: Accurately identifies and localizes objects in images with bounding boxes or points, providing stable JSON outputs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Structured Output Generation: Can generate structured outputs for complex data like invoices, forms, and tables, useful in domains like finance and commerce.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itq30t/qwenqwen25vl3b7b72binstruct_are_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1itq30t/qwenqwen25vl3b7b72binstruct_are_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1itq30t/qwenqwen25vl3b7b72binstruct_are_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T04:28:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu7jnw</id>
    <title>Were successful hobbyist finetunes just a part of the Llama2 era?</title>
    <updated>2025-02-20T19:50:42+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A year ago when Llama2 was the star of the show, it seems like the best models for all purposes were community fine-tunes. Wizard was a way better general-purpose model than Llama2, there were writing models of all different flavors, hermes was a big power boost, dolphin made instruct better, etc.. etc.. I could go on. There were fine tunes from smaller groups of people that kicked ass and became community favorites.&lt;/p&gt; &lt;p&gt;You don't see those nowadays though. Is Llama3 just better? Has increased context size taken the fun out of fine-tuning? Are modern foundational models just &lt;em&gt;harder&lt;/em&gt; to fine-tune?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7jnw/were_successful_hobbyist_finetunes_just_a_part_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7jnw/were_successful_hobbyist_finetunes_just_a_part_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7jnw/were_successful_hobbyist_finetunes_just_a_part_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ityftd</id>
    <title>Samsung is working on its own on-device LLM.</title>
    <updated>2025-02-20T13:26:39+00:00</updated>
    <author>
      <name>/u/WordyBug</name>
      <uri>https://old.reddit.com/user/WordyBug</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ityftd/samsung_is_working_on_its_own_ondevice_llm/"&gt; &lt;img alt="Samsung is working on its own on-device LLM." src="https://preview.redd.it/cgbfpkphpake1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5bd286726a3a9cd81d352de72126809656fd7e96" title="Samsung is working on its own on-device LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WordyBug"&gt; /u/WordyBug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cgbfpkphpake1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ityftd/samsung_is_working_on_its_own_ondevice_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ityftd/samsung_is_working_on_its_own_ondevice_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T13:26:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1iubaht</id>
    <title>Introduction to CUDA Programming for Python Developers</title>
    <updated>2025-02-20T22:26:22+00:00</updated>
    <author>
      <name>/u/Brilliant-Day2748</name>
      <uri>https://old.reddit.com/user/Brilliant-Day2748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iubaht/introduction_to_cuda_programming_for_python/"&gt; &lt;img alt="Introduction to CUDA Programming for Python Developers" src="https://b.thumbs.redditmedia.com/tT09OGcBeXzrs9R40vsqDk7IMZMfNfQzUvuG54nKVaY.jpg" title="Introduction to CUDA Programming for Python Developers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We wrote a blog post on introducing CUDA programming to Python developers, hope it's useful! 👋&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ja8lmxziedke1.png?width=2301&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9dfd969968f725c9ca23a0d28dad831578a2596"&gt;https://preview.redd.it/ja8lmxziedke1.png?width=2301&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d9dfd969968f725c9ca23a0d28dad831578a2596&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant-Day2748"&gt; /u/Brilliant-Day2748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iubaht/introduction_to_cuda_programming_for_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iubaht/introduction_to_cuda_programming_for_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iubaht/introduction_to_cuda_programming_for_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T22:26:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iuicqb</id>
    <title>S*: Test Time Scaling for Code Generation</title>
    <updated>2025-02-21T04:23:14+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.14382"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iuicqb/s_test_time_scaling_for_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iuicqb/s_test_time_scaling_for_code_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T04:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iufbmy</id>
    <title>Deepseek R1 671b minimum hardware to get 20TPS running only in RAM</title>
    <updated>2025-02-21T01:47:06+00:00</updated>
    <author>
      <name>/u/therebrith</name>
      <uri>https://old.reddit.com/user/therebrith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking into full chatgpt replacement and shopping for hardware. I've seen the digital spaceport's $2k &lt;a href="https://digitalspaceport.com/how-to-run-deepseek-r1-671b-fully-locally-on-2000-epyc-rig/"&gt;build&lt;/a&gt; that gives 5ish TPS using an 7002/7003 EPYC and 512GB of DDR4 2400. It's a good experiment, but 5 token/s is not gonna replace chatgpt from day to day use. So I wonder what would be the minimum hardwares like to get minimum 20 token/s with 3~4s or less first token wait time, running only on RAM? &lt;/p&gt; &lt;p&gt;I'm sure not a lot of folks have tried this, but just throwing out there, that a setup with 1TB DDR5 at 4800 with dual EPYC 9005(192c/384t), would that be enough for the 20TPS ask?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/therebrith"&gt; /u/therebrith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iufbmy/deepseek_r1_671b_minimum_hardware_to_get_20tps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iufbmy/deepseek_r1_671b_minimum_hardware_to_get_20tps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iufbmy/deepseek_r1_671b_minimum_hardware_to_get_20tps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T01:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu7e0n</id>
    <title>arcee-ai/Arcee-Maestro-7B-Preview, DeepSeek-R1-Distill-Qwen-7B with further GPRO training</title>
    <updated>2025-02-20T19:44:09+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7e0n/arceeaiarceemaestro7bpreview/"&gt; &lt;img alt="arcee-ai/Arcee-Maestro-7B-Preview, DeepSeek-R1-Distill-Qwen-7B with further GPRO training" src="https://external-preview.redd.it/kXmp-4xU9RU4uIBtaGUmIjjXZHg6PBZeRqkSIrz5cEE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=989184e6cf8fabf2f16b5eb65f889cafbaec4454" title="arcee-ai/Arcee-Maestro-7B-Preview, DeepSeek-R1-Distill-Qwen-7B with further GPRO training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/arcee-ai/Arcee-Maestro-7B-Preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7e0n/arceeaiarceemaestro7bpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7e0n/arceeaiarceemaestro7bpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:44:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1iude4d</id>
    <title>I built reddacted - a local LLM-powered reddit privacy suite to analyze &amp; secure your reddit history 🔒</title>
    <updated>2025-02-21T00:00:43+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iude4d/i_built_reddacted_a_local_llmpowered_reddit/"&gt; &lt;img alt="I built reddacted - a local LLM-powered reddit privacy suite to analyze &amp;amp; secure your reddit history 🔒" src="https://external-preview.redd.it/GoQi34apR2K06qGMbN8S5cW2_F_4TC3T_n3LZrOovQM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb14b7c2243ac740d4572db4c0b6d9eded572420" title="I built reddacted - a local LLM-powered reddit privacy suite to analyze &amp;amp; secure your reddit history 🔒" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/taylorwilsdon/reddacted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iude4d/i_built_reddacted_a_local_llmpowered_reddit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iude4d/i_built_reddacted_a_local_llmpowered_reddit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T00:00:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1iujafd</id>
    <title>Best LLMs!? (Focus: Best &amp; 7B-32B) 02/21/2025</title>
    <updated>2025-02-21T05:16:02+00:00</updated>
    <author>
      <name>/u/DeadlyHydra8630</name>
      <uri>https://old.reddit.com/user/DeadlyHydra8630</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I am fairly new to this space and this is my first post here so go easy on me 😅&lt;/p&gt; &lt;pre&gt;&lt;code&gt;For those who are also new! What does this 7B, 14B, 32B parameters even mean? - It represents the number of trainable weights in the model, which determine how much data it can learn and process. - Larger models can capture more complex patterns but require more compute, memory, and data, while smaller models can be faster and more efficient. What do I need to run Local Models? - Ideally you'd want the most VRAM GPU possible allowing you to run bigger models - Though if you have a laptop with a NPU that's also great! - If you do not have a GPU focus on trying to use smaller models 7B and lower! - (Reference the Chart below) How do I run a Local Model? - Theres various guides online - I personally like using LMStudio it has a nice interface - I also use Ollama &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Quick Guide!&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;If this is too confusing, just get LM Studio; it will find a good fit for your hardware!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: This chart could have issues, please correct me!&lt;/p&gt; &lt;p&gt;Note: For Android, Smolchat and Pocketpal are great apps to download models from Huggingface&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Device Type&lt;/th&gt; &lt;th align="left"&gt;VRAM/RAM&lt;/th&gt; &lt;th align="left"&gt;Recommended Bit Precision&lt;/th&gt; &lt;th align="left"&gt;Max LLM Parameters (Approx.)&lt;/th&gt; &lt;th align="left"&gt;Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Smartphones&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Low-end phones&lt;/td&gt; &lt;td align="left"&gt;4 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;~1-2 billion&lt;/td&gt; &lt;td align="left"&gt;For basic tasks.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mid-range phones&lt;/td&gt; &lt;td align="left"&gt;6-8 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~2-4 billion&lt;/td&gt; &lt;td align="left"&gt;Good balance of performance and model size.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;High-end phones&lt;/td&gt; &lt;td align="left"&gt;12 GB RAM&lt;/td&gt; &lt;td align="left"&gt;8-bit&lt;/td&gt; &lt;td align="left"&gt;~6 billion&lt;/td&gt; &lt;td align="left"&gt;Can handle larger models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;x86 Laptops&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Integrated GPU (e.g., Intel Iris)&lt;/td&gt; &lt;td align="left"&gt;8 GB RAM&lt;/td&gt; &lt;td align="left"&gt;8-bit&lt;/td&gt; &lt;td align="left"&gt;~4 billion&lt;/td&gt; &lt;td align="left"&gt;Suitable for smaller to medium-sized models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gaming Laptops (e.g., RTX 3050)&lt;/td&gt; &lt;td align="left"&gt;4-6 GB VRAM + RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 8-bit&lt;/td&gt; &lt;td align="left"&gt;~2-6 billion&lt;/td&gt; &lt;td align="left"&gt;Seems crazy ik but we aim for model size that runs smoothly and responsively&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;High-end Laptops (e.g., RTX 3060)&lt;/td&gt; &lt;td align="left"&gt;8-12 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;8-bit to 16-bit&lt;/td&gt; &lt;td align="left"&gt;~4-6 billion&lt;/td&gt; &lt;td align="left"&gt;Can handle larger models, especially with 16-bit for higher quality.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;ARM Devices&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Raspberry Pi 4&lt;/td&gt; &lt;td align="left"&gt;4-8 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit&lt;/td&gt; &lt;td align="left"&gt;~2-4 billion&lt;/td&gt; &lt;td align="left"&gt;Best for experimentation and smaller models due to memory constraints.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Apple M1/M2 (Unified Memory)&lt;/td&gt; &lt;td align="left"&gt;8-24 GB RAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 16-bit&lt;/td&gt; &lt;td align="left"&gt;~4-12 billion&lt;/td&gt; &lt;td align="left"&gt;Unified memory allows for larger models.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GPU Computers&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mid-range GPU (e.g., RTX 4070)&lt;/td&gt; &lt;td align="left"&gt;12 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;4-bit to 16-bit&lt;/td&gt; &lt;td align="left"&gt;~6-14 billion&lt;/td&gt; &lt;td align="left"&gt;Good for general LLM tasks and development.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;High-end GPU (e.g., RTX 3090)&lt;/td&gt; &lt;td align="left"&gt;24 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;16-bit&lt;/td&gt; &lt;td align="left"&gt;~12 billion&lt;/td&gt; &lt;td align="left"&gt;Big boi territory!&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Server GPU (e.g., A100)&lt;/td&gt; &lt;td align="left"&gt;40-80 GB VRAM&lt;/td&gt; &lt;td align="left"&gt;16-bit to 32-bit&lt;/td&gt; &lt;td align="left"&gt;~20-40 billion&lt;/td&gt; &lt;td align="left"&gt;For the largest models and research.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;If this is too confusing, just get LM Studio; it will find a good fit for your hardware!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The point of this post is to essentially find and keep updating this post with the best new models most people can actually use.&lt;/p&gt; &lt;p&gt;While sure the 70B, 405B, 671B and Closed sources models are incredible, some of us don't have the facilities for those huge models and don't want to give away our data 🙃&lt;/p&gt; &lt;p&gt;I will put up what &lt;strong&gt;I believe&lt;/strong&gt; are the best models for each of these categories &lt;strong&gt;CURRENTLY&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Please, please, please, those who are much much more knowledgeable, let me know what models I should put if I am missing any great models or categories I should include!)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: I cannot find RRD2.5 for the life of me on HuggingFace.&lt;/p&gt; &lt;p&gt;I will have benchmarks, so those are more definitive. some other stuff will be subjective I will also have links to the repo (I'm also including links; I am no evil man but don't trust strangers on the world wide web)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; {Parameter}: {Model} - {Score}&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;MMLU-Pro (language comprehension and reasoning across diverse domains):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best:&lt;/em&gt; &lt;em&gt;DeepSeek-R1 - 0.84&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/bartowski/QwQ-32B-Preview-GGUF"&gt;QwQ-32B-Preview&lt;/a&gt; - 0.7097&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/unsloth/phi-4-GGUF"&gt;Phi-4&lt;/a&gt; - 0.704&lt;/p&gt; &lt;p&gt;7B: &lt;a href="https://huggingface.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF"&gt;Qwen2.5-7B-Instruct&lt;/a&gt; - 0.4724&lt;br /&gt; ------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Math:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Gemini-2.0-Flash-exp - 0.8638&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B&lt;/a&gt; - 0.8053&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-1M-GGUF"&gt;Qwen2.5-14B&lt;/a&gt; - 0.6788&lt;/p&gt; &lt;p&gt;7B: &lt;a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF"&gt;Qwen2-7B-Instruct&lt;/a&gt; - 0.5803&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Coding (conceptual, debugging, implementation, optimization):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: OpenAI O1 - 0.981 (148/148)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-32B Coder&lt;/a&gt; - 0.817&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;Mistral Small 3&lt;/a&gt; - 0.692&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF"&gt;Qwen2.5-Coder-14B-Instruct&lt;/a&gt; - 0.6707&lt;/p&gt; &lt;p&gt;8B: &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"&gt;Llama3.1-8B Instruct&lt;/a&gt; - 0.385&lt;/p&gt; &lt;p&gt;HM:&lt;br /&gt; 32B: &lt;a href="https://huggingface.co/waldie/DeepSeek-R1-Distill-Qwen-32B-4bpw-h6-exl2"&gt;DeepSeek-R1-Distill&lt;/a&gt; - (148/148)&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/THUDM/codegeex4-all-9b"&gt;CodeGeeX4-All&lt;/a&gt; - (146/148)&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Creative Writing:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LM Arena Creative Writing:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Grok-3 - 1422, OpenAI 4o - 1420&lt;/em&gt;&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO"&gt;Gemma-2-9B-it-SimPO&lt;/a&gt; &lt;strong&gt;-&lt;/strong&gt; 1244&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;Mistral-Small-24B-Instruct-2501&lt;/a&gt; - 1199&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-Coder-32B-Instruct&lt;/a&gt; - 1178&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EQ Bench (Emotional Intelligence Benchmarks for LLMs):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: DeepSeek-R1 - 87.11&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;9B:&lt;/em&gt; &lt;a href="https://huggingface.co/ifable/gemma-2-Ifable-9B"&gt;gemma-2-Ifable-9B&lt;/a&gt; &lt;em&gt;- 84.59&lt;/em&gt;&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Longer Query (&amp;gt;= 500 tokens)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Grok-3 - 1425, Gemini-2.0-Pro/Flash-Thinking-Exp - 1399/1395&lt;/em&gt;&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"&gt;Mistral-Small-24B-Instruct-2501&lt;/a&gt; - 1264&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-Coder-32B-Instruct&lt;/a&gt; - 1261&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO"&gt;Gemma-2-9B-it-SimPO&lt;/a&gt; - 1239&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/microsoft/phi-4"&gt;Phi-4&lt;/a&gt; - 1233&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Heathcare/Medical (USMLE, AIIMS &amp;amp; NEET PG, College/Profession level quesions)&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;(8B) Best Avg.&lt;/em&gt;: &lt;a href="https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20"&gt;ProbeMedicalYonseiMAILab/medllama3-v20&lt;/a&gt; - 90.01&lt;/p&gt; &lt;p&gt;(8B) Best USMLE, AIIMS &amp;amp; NEET PG: &lt;a href="https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20"&gt;ProbeMedicalYonseiMAILab/medllama3-v20&lt;/a&gt; - 81.07&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Business&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Claude-3.5-Sonnet - 0.8137&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B&lt;/a&gt; - 0.7567&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-1M-GGUF"&gt;Qwen2.5-14B&lt;/a&gt; - 0.7085&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/google/gemma-2-9b-it"&gt;Gemma-2-9B-it&lt;/a&gt; - 0.5539&lt;/p&gt; &lt;p&gt;7B: &lt;a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF"&gt;Qwen2-7B-Instruct&lt;/a&gt; - 0.5412&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Economics&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Best: Claude-3.5-Sonnet - 0.859&lt;/em&gt;&lt;/p&gt; &lt;p&gt;32B: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B&lt;/a&gt; - 0.7725&lt;/p&gt; &lt;p&gt;14B: &lt;a href="https://huggingface.co/bartowski/Qwen2.5-14B-Instruct-1M-GGUF"&gt;Qwen2.5-14B&lt;/a&gt; - 0.7310&lt;/p&gt; &lt;p&gt;9B: &lt;a href="https://huggingface.co/google/gemma-2-9b-it"&gt;Gemma-2-9B-it&lt;/a&gt; - 0.6552&lt;/p&gt; &lt;p&gt;------------------------------------------------------------------------------------------&lt;/p&gt; &lt;p&gt;Sincerely, I do not trust myself yet to be benchmarking, so I used the web:&lt;/p&gt; &lt;p&gt;Sources:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro"&gt;https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/finosfoundation/Open-Financial-LLM-Leaderboard"&gt;https://huggingface.co/spaces/finosfoundation/Open-Financial-LLM-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard"&gt;https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/?leaderboard"&gt;https://lmarena.ai/?leaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://paperswithcode.com/sota/math-word-problem-solving-on-math"&gt;https://paperswithcode.com/sota/math-word-problem-solving-on-math&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://paperswithcode.com/sota/code-generation-on-humaneval"&gt;https://paperswithcode.com/sota/code-generation-on-humaneval&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeadlyHydra8630"&gt; /u/DeadlyHydra8630 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujafd/best_llms_focus_best_7b32b_02212025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujafd/best_llms_focus_best_7b32b_02212025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iujafd/best_llms_focus_best_7b32b_02212025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T05:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu7c24</id>
    <title>arcee-ai/Arcee-Blitz, Mistral-Small-24B-Instruct-2501 Finetune</title>
    <updated>2025-02-20T19:41:51+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7c24/arceeaiarceeblitz_mistralsmall24binstruct2501/"&gt; &lt;img alt="arcee-ai/Arcee-Blitz, Mistral-Small-24B-Instruct-2501 Finetune" src="https://external-preview.redd.it/nAJEyVNIP8SWOhtVuMgvqEBinfu5P4u1oNYU06SZdto.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e08074fa2ad405c1ee359322e0fc01d2fb5148f8" title="arcee-ai/Arcee-Blitz, Mistral-Small-24B-Instruct-2501 Finetune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/arcee-ai/Arcee-Blitz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7c24/arceeaiarceeblitz_mistralsmall24binstruct2501/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu7c24/arceeaiarceeblitz_mistralsmall24binstruct2501/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu4gvf</id>
    <title>I changed my mind about DeepSeek-R1-Distill-Llama-70B</title>
    <updated>2025-02-20T17:46:52+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4gvf/i_changed_my_mind_about_deepseekr1distillllama70b/"&gt; &lt;img alt="I changed my mind about DeepSeek-R1-Distill-Llama-70B" src="https://preview.redd.it/zknh3vk6xbke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7defd62a3749692bb67bb3c597046e8dd3da633" title="I changed my mind about DeepSeek-R1-Distill-Llama-70B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zknh3vk6xbke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4gvf/i_changed_my_mind_about_deepseekr1distillllama70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4gvf/i_changed_my_mind_about_deepseekr1distillllama70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T17:46:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu2sdk</id>
    <title>SmolVLM2: New open-source video models running on your toaster</title>
    <updated>2025-02-20T16:39:27+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! It's Merve from Hugging Face, working on zero-shot vision/multimodality 👋🏻&lt;/p&gt; &lt;p&gt;Today we released SmolVLM2, new vision LMs in three sizes: 256M, 500M, 2.2B. This release comes with zero-day support for transformers and MLX, and we built applications based on these, along with video captioning fine-tuning tutorial. &lt;/p&gt; &lt;p&gt;We release the following:&lt;br /&gt; &amp;gt; an iPhone app (runs on 500M model in MLX)&lt;br /&gt; &amp;gt; integration with VLC for segmentation of descriptions (based on 2.2B)&lt;br /&gt; &amp;gt; a video highlights extractor (based on 2.2B)&lt;/p&gt; &lt;p&gt;Here's a video from the iPhone app ⤵️ you can read and learn more from our blog and check everything in our collection 🤗&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1iu2sdk/video/fzmniv61obke1/player"&gt;https://reddit.com/link/1iu2sdk/video/fzmniv61obke1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu2sdk/smolvlm2_new_opensource_video_models_running_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu2sdk/smolvlm2_new_opensource_video_models_running_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu2sdk/smolvlm2_new_opensource_video_models_running_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T16:39:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iue6n1</id>
    <title>OpenThinker is a decensored 32B reasoning deepseek distilled model</title>
    <updated>2025-02-21T00:53:25+00:00</updated>
    <author>
      <name>/u/NousJaccuzi</name>
      <uri>https://old.reddit.com/user/NousJaccuzi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://bespokelabs.ai/blog/openthinker-is-a-decensored-reasoning-model"&gt;https://bespokelabs.ai/blog/openthinker-is-a-decensored-reasoning-model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/openthinker"&gt;https://ollama.com/library/openthinker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-7B"&gt;https://huggingface.co/open-thoughts/OpenThinker-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/open-thoughts/OpenThinker-32B"&gt;https://huggingface.co/open-thoughts/OpenThinker-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NousJaccuzi"&gt; /u/NousJaccuzi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iue6n1/openthinker_is_a_decensored_32b_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iue6n1/openthinker_is_a_decensored_32b_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iue6n1/openthinker_is_a_decensored_32b_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T00:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu4bc0</id>
    <title>New QwQ Confirmed to be in the works “no hurries”</title>
    <updated>2025-02-20T17:40:30+00:00</updated>
    <author>
      <name>/u/YTLupo</name>
      <uri>https://old.reddit.com/user/YTLupo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4bc0/new_qwq_confirmed_to_be_in_the_works_no_hurries/"&gt; &lt;img alt="New QwQ Confirmed to be in the works “no hurries”" src="https://preview.redd.it/7e0x8lh3zbke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88979b41d573ee37d2cb08acc56da58b982176c2" title="New QwQ Confirmed to be in the works “no hurries”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of interesting replies &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/justinlin610/status/1892625351664099613?s=46&amp;amp;t=4SUD3tHKISm8olRn08tH1A"&gt;https://x.com/justinlin610/status/1892625351664099613?s=46&amp;amp;t=4SUD3tHKISm8olRn08tH1A&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As someone who uses QWEN2.5 and the existing QwQ model I’m pretty hype to see what happens. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YTLupo"&gt; /u/YTLupo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7e0x8lh3zbke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4bc0/new_qwq_confirmed_to_be_in_the_works_no_hurries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu4bc0/new_qwq_confirmed_to_be_in_the_works_no_hurries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T17:40:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu56o1</id>
    <title>10x longer contexts for reasoning training - 90% less memory GRPO in Unsloth</title>
    <updated>2025-02-20T18:15:26+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! Thanks so much for the support on our GRPO release 2 weeks ago! Today, we're excited to announce that you can now train your own reasoning model with just &lt;strong&gt;5GB VRAM&lt;/strong&gt; for Qwen2.5 (1.5B) - down from 7GB in the previous &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; release!&lt;/p&gt; &lt;ol&gt; &lt;li&gt;This is thanks to our newly derived Efficient GRPO algorithm which enables &lt;strong&gt;&lt;em&gt;10x longer context&lt;/em&gt;&lt;/strong&gt; lengths while using &lt;strong&gt;&lt;em&gt;90% less VRAM&lt;/em&gt;&lt;/strong&gt; vs. all other GRPO LoRA/QLoRA implementations, even those utilizing Flash Attention 2 (FA2).&lt;/li&gt; &lt;li&gt;With a GRPO setup using TRL + FA2, Llama 3.1 (8B) training at 20K context length demands &lt;strong&gt;510.8G&lt;/strong&gt; of VRAM. However, Unsloth’s 90% VRAM reduction brings the requirement down to &lt;strong&gt;just 54.3GB&lt;/strong&gt; in the same setup.&lt;/li&gt; &lt;li&gt;We leverage our &lt;a href="https://unsloth.ai/blog/long-context"&gt;gradient checkpointing&lt;/a&gt; algorithm which we released a while ago. It smartly offloads intermediate activations to system RAM asynchronously whilst being only 1% slower. &lt;strong&gt;&lt;em&gt;This shaves a whopping 372GB VRAM&lt;/em&gt;&lt;/strong&gt; since we need num_generations = 8. We can reduce this memory usage even further through intermediate gradient accumulation.&lt;/li&gt; &lt;li&gt;We also implemented a highly memory efficient GRPO loss, which saves memory usage by 8x. Before 78GB was needed for 20K context length - now only 10GB!&lt;/li&gt; &lt;li&gt;Try our free GRPO notebook with 10x longer context: Llama 3.1 (8B) on Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Blog for more details on the algorithm, the Maths behind GRPO, issues we found and more: &lt;a href="https://unsloth.ai/blog/grpo"&gt;https://unsloth.ai/blog/grpo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GRPO VRAM Breakdown:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Unsloth&lt;/th&gt; &lt;th align="left"&gt;TRL + FA2&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Training Memory Cost (GB)&lt;/td&gt; &lt;td align="left"&gt;42GB&lt;/td&gt; &lt;td align="left"&gt;414GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GRPO Memory Cost (GB)&lt;/td&gt; &lt;td align="left"&gt;9.8GB&lt;/td&gt; &lt;td align="left"&gt;78.3GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Inference Cost (GB)&lt;/td&gt; &lt;td align="left"&gt;0GB&lt;/td&gt; &lt;td align="left"&gt;16GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Inference KV Cache for 20K context (GB)&lt;/td&gt; &lt;td align="left"&gt;2.5GB&lt;/td&gt; &lt;td align="left"&gt;2.5GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Total Memory Usage&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;54.3GB (90% less)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;510.8GB&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;ul&gt; &lt;li&gt;We also now provide full logging details for all reward functions now! Previously we only showed the total aggregated reward function itself.&lt;/li&gt; &lt;li&gt;You can now run and do inference with our &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;4-bit dynamic quants&lt;/a&gt; directly in vLLM.&lt;/li&gt; &lt;li&gt;Also we spent a lot of time on our Guide for everything on GRPO + reward functions/verifiers so would highly recommend you guys to read it: &lt;a href="https://docs.unsloth.ai/basics/reasoning-grpo-and-rl"&gt;docs.unsloth.ai/basics/reasoning&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thank you guys once again for all the support it truly means so much to us! We also have a major release coming within the next few weeks which I know you guys have been waiting for - and we're also excited for it!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu56o1/10x_longer_contexts_for_reasoning_training_90/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu56o1/10x_longer_contexts_for_reasoning_training_90/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu56o1/10x_longer_contexts_for_reasoning_training_90/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T18:15:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu6egg</id>
    <title>Even AI has some personality :)</title>
    <updated>2025-02-20T19:04:20+00:00</updated>
    <author>
      <name>/u/_idkwhattowritehere_</name>
      <uri>https://old.reddit.com/user/_idkwhattowritehere_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu6egg/even_ai_has_some_personality/"&gt; &lt;img alt="Even AI has some personality :)" src="https://preview.redd.it/3dlpqfoydcke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dfb9019fc88f0d5f525fe174f20b501b99bf0c66" title="Even AI has some personality :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_idkwhattowritehere_"&gt; /u/_idkwhattowritehere_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dlpqfoydcke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu6egg/even_ai_has_some_personality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu6egg/even_ai_has_some_personality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T19:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1iudao8</id>
    <title>langchain is still a rabbit hole in 2025</title>
    <updated>2025-02-20T23:56:22+00:00</updated>
    <author>
      <name>/u/henryclw</name>
      <uri>https://old.reddit.com/user/henryclw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;langchain is still a rabbit hole in 2025 And the langgraph framework as well&lt;/p&gt; &lt;p&gt;Is it just me or other people think this is the case as well?&lt;/p&gt; &lt;p&gt;Instead of spending hours going through the rabbit holes in these frameworks , I found out an ugly hard coded way is faster to implement. Yeah I know hard coed things are hard to maintain. But consider the break changes in langchain through 0.1, 0.2, 0.3. Things are hard to maintain in either way.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Edit&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Sorry my language might not be very friendly when I posted this, but I had a bad day. So here is what happened: I tried to build a automatic workflow to do something for me. Like everyone said, agent x LLM is the future blah blah blah... &lt;/p&gt; &lt;p&gt;Anyway, I start looking, for a workflow framework. There are dify, langflow, flowise, pyspur, Laminar, comfyui_LLM_party... But I picked langgraph since they are more or less codebased, doesn't require to setup things like clickhouse for a simple demo, and I could write custom nodes. &lt;/p&gt; &lt;p&gt;So I run in, into the rabbit holes. Like everyone in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; , I don't like OpenAI or other LLM provider, I like to host my own instance and make sure my data is mine. So I go with llama.cpp (which I've played with for a while) Then my bad day came:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama.cpp: The OpenAI compatible API doesn't work well with the tool calling &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/11988"&gt;https://github.com/ggml-org/llama.cpp/issues/11988&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/11847"&gt;https://github.com/ggml-org/llama.cpp/issues/11847&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: the jinja template is still buggy &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/11938"&gt;https://github.com/ggml-org/llama.cpp/issues/11938&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;llama.cpp: the tool calls doesn't return tool call id &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/11992"&gt;https://github.com/ggml-org/llama.cpp/issues/11992&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I just want to build a custom workflow that has tool calling with my llama.cpp, with custom node / function that could intergate with my current projects, why is it so hard...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryclw"&gt; /u/henryclw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iudao8/langchain_is_still_a_rabbit_hole_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iudao8/langchain_is_still_a_rabbit_hole_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iudao8/langchain_is_still_a_rabbit_hole_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T23:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu8f7s</id>
    <title>Speculative decoding can identify broken quants?</title>
    <updated>2025-02-20T20:26:14+00:00</updated>
    <author>
      <name>/u/NickNau</name>
      <uri>https://old.reddit.com/user/NickNau</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu8f7s/speculative_decoding_can_identify_broken_quants/"&gt; &lt;img alt="Speculative decoding can identify broken quants?" src="https://a.thumbs.redditmedia.com/XqCzop9kq738DVsg8GJQXPOa46DC9kly1ZUdEC5f5l4.jpg" title="Speculative decoding can identify broken quants?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickNau"&gt; /u/NickNau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1iu8f7s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu8f7s/speculative_decoding_can_identify_broken_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu8f7s/speculative_decoding_can_identify_broken_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T20:26:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1iu19zy</id>
    <title>2025 is an AI madhouse</title>
    <updated>2025-02-20T15:36:23+00:00</updated>
    <author>
      <name>/u/iamnotdeadnuts</name>
      <uri>https://old.reddit.com/user/iamnotdeadnuts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"&gt; &lt;img alt="2025 is an AI madhouse" src="https://preview.redd.it/ferhsryxcbke1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc1632f508c8f4f33f22d5753531a2d6bc7a1ca3" title="2025 is an AI madhouse" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2025 is straight-up wild for AI development. Just last year, it was mostly ChatGPT, Claude, and Gemini running the show. &lt;/p&gt; &lt;p&gt;Now? We’ve got an AI battle royale with everyone jumping in Deepseek, Kimi, Meta, Perplexity, Elon’s Grok&lt;/p&gt; &lt;p&gt;With all these options, the real question is: which one are you actually using daily?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamnotdeadnuts"&gt; /u/iamnotdeadnuts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ferhsryxcbke1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iu19zy/2025_is_an_ai_madhouse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-20T15:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1iulq4o</id>
    <title>We GRPO-ed a 1.5B model to test LLM Spatial Reasoning by solving MAZE</title>
    <updated>2025-02-21T07:56:29+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iulq4o/we_grpoed_a_15b_model_to_test_llm_spatial/"&gt; &lt;img alt="We GRPO-ed a 1.5B model to test LLM Spatial Reasoning by solving MAZE" src="https://external-preview.redd.it/dzQ4N25sbTM1Z2tlMVF8vLuY1I7D-30miO4pvAdRk1TFvpSr9DfFmva9zHJp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b308a1599c976871d72ed516a3f26b46303ad50a" title="We GRPO-ed a 1.5B model to test LLM Spatial Reasoning by solving MAZE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vkth2pm35gke1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iulq4o/we_grpoed_a_15b_model_to_test_llm_spatial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iulq4o/we_grpoed_a_15b_model_to_test_llm_spatial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T07:56:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iujig7</id>
    <title>Deepseek will publish 5 open source repos next week.</title>
    <updated>2025-02-21T05:29:06+00:00</updated>
    <author>
      <name>/u/WashWarm8360</name>
      <uri>https://old.reddit.com/user/WashWarm8360</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujig7/deepseek_will_publish_5_open_source_repos_next/"&gt; &lt;img alt="Deepseek will publish 5 open source repos next week." src="https://preview.redd.it/rdzshzfihfke1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e7b8ca74fce95adc4de966cdb0f09467f784c54" title="Deepseek will publish 5 open source repos next week." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WashWarm8360"&gt; /u/WashWarm8360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rdzshzfihfke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iujig7/deepseek_will_publish_5_open_source_repos_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iujig7/deepseek_will_publish_5_open_source_repos_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T05:29:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1iui6nk</id>
    <title>Starting next week, DeepSeek will open-source 5 repos</title>
    <updated>2025-02-21T04:13:54+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt; &lt;img alt="Starting next week, DeepSeek will open-source 5 repos" src="https://preview.redd.it/syeh0rmm3fke1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d3667b8c2ba5c4d6506f21080ba3334e6724119" title="Starting next week, DeepSeek will open-source 5 repos" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syeh0rmm3fke1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-21T04:13:54+00:00</published>
  </entry>
</feed>
