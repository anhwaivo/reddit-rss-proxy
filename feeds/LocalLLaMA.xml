<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-08T18:07:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ml229q</id>
    <title>GLM 4.5 Air - Optimizing - Vulkan vs. CUDA?</title>
    <updated>2025-08-08T17:51:08+00:00</updated>
    <author>
      <name>/u/naxan6</name>
      <uri>https://old.reddit.com/user/naxan6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I do want to run GLM 4.5 Air in Q4_K_M - as fast as possible for code generation with e.g. RooCode.&lt;br /&gt; My spec: 5060 TI 16 GB VRAM, Ryzen 9 9900X with 128GB 5600MHz DDR5 RAM, Windows 11&lt;/p&gt; &lt;p&gt;GLM 4.5 Air has been the best model by far to run locally on my machine for coding. I tried some warning fixing and unit testing on a small project. Qwen3 Coder 30B A3B Instruct in IQ3_XSS with kv cache q4_0 gives ~30tps with the same prompt size - but generates way more coding errors.&lt;/p&gt; &lt;p&gt;If I run the llama.cpp Vulkan-Version, i get around ~140+-10 tps for prompt processing and around 8.4 tps generation.&lt;br /&gt; If I run the llama.cpp CUDA12-Version, i get around ~320+-10 tps for prompt processing and around 5.7 tps generation.&lt;br /&gt; I measured this with a 60k token context and a 28k token prompt containing a question on a lorem ipsum text.&lt;br /&gt; Please find the commands below, each is the fastest settings I could find trying for a few hours.&lt;/p&gt; &lt;p&gt;Is there any chance to get both CUDAs ~330 tps for prompt processing &lt;strong&gt;and&lt;/strong&gt; Vulkans 8.4 tps for token generation?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any recommendations?&lt;/strong&gt;&lt;br /&gt; Thanks in advance!&lt;/p&gt; &lt;p&gt;This is what I run:&lt;br /&gt; &lt;strong&gt;command for vulkan:&lt;/strong&gt;&lt;br /&gt; llama-b6118-bin-win-vulkan-x64&amp;gt;.\llama-server.exe -c 60000 -fa -ctk q8_0 -ctv q8_0 -m &amp;quot;C:\Users\XXXXX\.lmstudio\models\unsloth\GLM-4.5-Air-GGUF\GLM-4.5-Air-Q4_K_M-00001-of-00002.gguf&amp;quot; -b 4096 -ub 4096 --temp 0.5 --top-p 1.0 --top-k 0 --min-p 0.1 --repeat-penalty 1.0 --no-mmap -sm none -ot &amp;quot;blk.*.ffn_.*_shexp.=Vulkan0,blk.[0-1].ffn_.*_exps.=Vulkan0,ffn_.*_exps.=CPU&amp;quot; -ngl 999 -t 24&lt;/p&gt; &lt;p&gt;&lt;strong&gt;command for CUDA:&lt;/strong&gt;&lt;br /&gt; llama-b6118-bin-win-cuda-12.4-x64&amp;gt;.\llama-server.exe -c 60000 -fa -ctk q8_0 -ctv q8_0 -m &amp;quot;C:\Users\XXXXX\.lmstudio\models\unsloth\GLM-4.5-Air-GGUF\GLM-4.5-Air-Q4_K_M-00001-of-00002.gguf&amp;quot; -b 4096 -ub 4096 --temp 0.5 --top-p 1.0 --top-k 0 --min-p 0.1 --repeat-penalty 1.0 --no-mmap -sm none --n-cpu-moe 46 -ngl 999 -t 24&lt;/p&gt; &lt;p&gt;PS1. there is a chance that the vulkan version isn't running fully correct, because atm I get &amp;quot;ggml_vulkan: Failed to allocate pinned memory (vk::Device::allocateMemory: ErrorOutOfDeviceMemory)&amp;quot; - but it still runs and works without crashing .. (?)&lt;br /&gt; PS2. I'm using LMStudio for inital tests and running smaller models and as a &amp;quot;model-downloader&amp;quot; - at least as long as -ot and --cpu-moe are missing - thats causing the pathes above ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/naxan6"&gt; /u/naxan6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml229q/glm_45_air_optimizing_vulkan_vs_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml229q/glm_45_air_optimizing_vulkan_vs_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml229q/glm_45_air_optimizing_vulkan_vs_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T17:51:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkxswi</id>
    <title>AI First Science Fantasy RPG with Open Lore and Machine Readable Dataset</title>
    <updated>2025-08-08T15:10:33+00:00</updated>
    <author>
      <name>/u/3RiversAINexus</name>
      <uri>https://old.reddit.com/user/3RiversAINexus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been building Aeonisk, a science fantasy tabletop RPG that is designed from the ground up for AI integration. It is not just playable at the table, it is fully usable as training data for local LLMs, narrative generators, and AI assisted game masters.&lt;/p&gt; &lt;p&gt;Aeonisk is AI first. Every rule, lore fragment, and encounter is structured for easy parsing by models in YAML datasets, Markdown documents, and PDFs that all align. Both the game system and the dataset are released under GPLv2 which means they are free to use, modify, and redistribute even commercially. All setting text is open licensed so you can build and sell your own works set in Aeonisk without worrying about copyright takedowns. The lore itself is under a separate commercial license I’ve made, which permits commercial use as long as you include reference back to the original game, Aeonisk. &lt;/p&gt; &lt;p&gt;The rules are built on the open source YAGS engine, expanded with metaphysics like Will, Bond, Void, and Soulcredit for consequence driven storytelling. The structured mechanics make it easy for AI systems to track values and respond dynamically.&lt;/p&gt; &lt;p&gt;You can use Aeonisk to fine tune a local model as a lore accurate GM, procedurally generate factions, quests, or Void events with full rules support, create NPCs who remember Bonds and track Soulcredit, or as a world kernel to seed your own AI driven game.&lt;/p&gt; &lt;p&gt;&lt;a href="https://aeonisk.itch.io/game"&gt;Here is the itch.io link for the game to download everything&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3RiversAINexus"&gt; /u/3RiversAINexus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://chatgpt.com/g/g-680299b1a5f08191b869fe352f33cc1a-aeonisk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxswi/ai_first_science_fantasy_rpg_with_open_lore_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxswi/ai_first_science_fantasy_rpg_with_open_lore_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml08oc</id>
    <title>Web Search MCP using Jina ai - open source</title>
    <updated>2025-08-08T16:42:36+00:00</updated>
    <author>
      <name>/u/Delicious-Farmer-234</name>
      <uri>https://old.reddit.com/user/Delicious-Farmer-234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I created an easily deployable streamable HTTP MCP server that anyone can use locally on Docker or Python. It was a quick project I put together, so I can leverage those free api tokens. I am using Qwen3 Coder together with LM Studio and the MCP server. Just wanted to share the project with anyone interested. Thanks &lt;a href="https://github.com/hypersniper05/JinaWebSearchMCP"&gt;https://github.com/hypersniper05/JinaWebSearchMCP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious-Farmer-234"&gt; /u/Delicious-Farmer-234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml08oc/web_search_mcp_using_jina_ai_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml08oc/web_search_mcp_using_jina_ai_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml08oc/web_search_mcp_using_jina_ai_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T16:42:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml1aef</id>
    <title>AMD MI50 32GB/Vega20 GPU Passthrough Guide for Proxmox</title>
    <updated>2025-08-08T17:21:55+00:00</updated>
    <author>
      <name>/u/Panda24z</name>
      <uri>https://old.reddit.com/user/Panda24z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What This Guide Solves&lt;/p&gt; &lt;p&gt;If you're trying to pass through an AMD Vega20 GPU (like the MI50 or Radeon Pro VII) to a VM in Proxmox and getting stuck with the dreaded &amp;quot;atombios stuck in loop&amp;quot; error, this guide is for you. The solution involves installing the vendor-reset kernel module on your Proxmox host.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important note:&lt;/strong&gt; This solution was developed after trying the standard PCIe passthrough setup first, which failed. While I'm not entirely sure if all the standard passthrough steps are required when using vendor-reset, I'm including them since they were part of my working configuration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; This involves kernel module compilation and hardware-level GPU reset procedures. Test this at your own risk.&lt;/p&gt; &lt;h1&gt;My Setup&lt;/h1&gt; &lt;p&gt;Here's what I was working with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Server Hardware:&lt;/strong&gt; 56-core Intel Xeon E5-2680 v4 @ 2.40GHz (2 sockets), 110GB RAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; Supermicro X10DRU-i+&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; Proxmox VE 8.4.8 running kernel 6.8.12-13-pve (EFI boot mode)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; AMD Radeon MI50 (bought from Alibaba, came pre-flashed with Radeon Pro VII BIOS - Device ID: 66a3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU Location:&lt;/strong&gt; PCI address 08:00.0&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Guest VM:&lt;/strong&gt; Ubuntu 22.04.5 LTS, Kernel 5.15&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Previous attempts:&lt;/strong&gt; Standard PCIe passthrough (failed with &amp;quot;atombios stuck in loop&amp;quot;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Part 1: Standard PCIe Passthrough Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Heads up:&lt;/strong&gt; These steps might not all be necessary with vendor-reset, but I did them first and they're part of my working setup.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Helpful video reference:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=_hOBAGKLQkI"&gt;Proxmox PCIe Passthrough Guide&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Enable IOMMU Support&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;If you're using a legacy boot system:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nano /etc/default/grub &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add this line:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet intel_iommu=on&amp;quot; # Or for AMD systems: GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet amd_iommu=on&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then save and run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;update-grub &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;If you're using EFI boot:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nano /etc/kernel/cmdline &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;intel_iommu=on # Or for AMD systems: amd_iommu=on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then save and run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;proxmox-boot-tool refresh &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Load VFIO Modules&lt;/h1&gt; &lt;p&gt;Edit the modules file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nano /etc/modules &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add these lines:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vfio vfio_iommu_type1 vfio_pci vfio_virqfd &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Find Your GPU and Current Driver&lt;/h1&gt; &lt;p&gt;First, let's see what we're working with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Find your AMD GPU lspci | grep -i amd # Get detailed info (replace 08:00 with your actual PCI address) lspci -n -s 08:00 -v &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's what I saw on my system:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;08:00.0 0300: 1002:66a3 (prog-if 00 [VGA controller]) Subsystem: 106b:0201 Flags: bus master, fast devsel, latency 0, IRQ 44, NUMA node 0, IOMMU group 111 Memory at b0000000 (64-bit, prefetchable) [size=256M] Memory at c0000000 (64-bit, prefetchable) [size=2M] I/O ports at 3000 [size=256] Memory at c7100000 (32-bit, non-prefetchable) [size=512K] Expansion ROM at c7180000 [disabled] [size=128K] Capabilities: [48] Vendor Specific Information: Len=08 &amp;lt;?&amp;gt; Capabilities: [50] Power Management version 3 Capabilities: [64] Express Legacy Endpoint, MSI 00 Capabilities: [a0] MSI: Enable+ Count=1/1 Maskable- 64bit+ Capabilities: [100] Vendor Specific Information: ID=0001 Rev=1 Len=010 &amp;lt;?&amp;gt; Capabilities: [150] Advanced Error Reporting Capabilities: [200] Physical Resizable BAR Capabilities: [270] Secondary PCI Express Capabilities: [2a0] Access Control Services Capabilities: [2b0] Address Translation Service (ATS) Capabilities: [2c0] Page Request Interface (PRI) Capabilities: [2d0] Process Address Space ID (PASID) Capabilities: [320] Latency Tolerance Reporting Kernel driver in use: vfio-pci Kernel modules: amdgpu &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice it shows &amp;quot;Kernel modules: amdgpu&amp;quot; - that's what we need to blacklist.&lt;/p&gt; &lt;h1&gt;Configure VFIO and Blacklist the AMD Driver&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;echo &amp;quot;options vfio_iommu_type1 allow_unsafe_interrupts=1&amp;quot; &amp;gt; /etc/modprobe.d/iommu_unsafe_interrupts.conf echo &amp;quot;options kvm ignore_msrs=1&amp;quot; &amp;gt; /etc/modprobe.d/kvm.conf # Blacklist the AMD GPU driver echo &amp;quot;blacklist amdgpu&amp;quot; &amp;gt;&amp;gt; /etc/modprobe.d/blacklist.conf &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Bind Your GPU to VFIO&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Use the vendor:device ID from your lspci output (mine was 1002:66a3) echo &amp;quot;options vfio-pci ids=1002:66a3 disable_vga=1&amp;quot; &amp;gt; /etc/modprobe.d/vfio.conf &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Apply Changes and Reboot&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;update-initramfs -u -k all reboot &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Check That VFIO Binding Worked&lt;/h1&gt; &lt;p&gt;After the reboot, verify your GPU is now using the vfio-pci driver:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Use your actual PCI address lspci -n -s 08:00 -v &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Kernel driver in use: vfio-pci Kernel modules: amdgpu &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you see &lt;code&gt;Kernel driver in use: vfio-pci&lt;/code&gt;, the standard passthrough setup is working correctly.&lt;/p&gt; &lt;h1&gt;Part 2: The vendor-reset Solution&lt;/h1&gt; &lt;p&gt;This is where the magic happens for AMD Vega20 GPUs.&lt;/p&gt; &lt;h1&gt;Check Your System is Ready&lt;/h1&gt; &lt;p&gt;Make sure your Proxmox host has the required kernel features:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Check your kernel version uname -r # Verify required features (all should show 'y') grep -E &amp;quot;CONFIG_FTRACE=|CONFIG_KPROBES=|CONFIG_PCI_QUIRKS=|CONFIG_KALLSYMS=|CONFIG_KALLSYMS_ALL=|CONFIG_FUNCTION_TRACER=&amp;quot; /boot/config-$(uname -r) # Find your GPU info again lspci -nn | grep -i amd &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;6.8.12-13-pve CONFIG_KALLSYMS=y CONFIG_KALLSYMS_ALL=y CONFIG_KPROBES=y CONFIG_PCI_QUIRKS=y CONFIG_FTRACE=y CONFIG_FUNCTION_TRACER=y 08:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Vega 20 [Radeon Pro Vega II/Radeon Pro Vega II Duo] [1002:66a3] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Make note of your GPU's PCI address (mine is &lt;code&gt;08:00.0&lt;/code&gt;) - you'll need this later.&lt;/p&gt; &lt;h1&gt;Install Build Dependencies&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Update and install what we need apt update apt install -y git dkms build-essential # Install Proxmox kernel headers apt install -y pve-headers-$(uname -r) # Double-check the headers are there ls -la /lib/modules/$(uname -r)/build &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see a symlink pointing to something like &lt;code&gt;/usr/src/linux-headers-X.X.X-X-pve&lt;/code&gt;.&lt;/p&gt; &lt;h1&gt;Build and Install vendor-reset&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Download the source cd /tmp git clone https://github.com/gnif/vendor-reset.git cd vendor-reset # Clean up any previous attempts sudo dkms remove vendor-reset/0.1.1 --all 2&amp;gt;/dev/null || true sudo rm -rf /usr/src/vendor-reset-0.1.1 sudo rm -rf /var/lib/dkms/vendor-reset # Build and install the module sudo dkms install . &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If everything goes well, you'll see output like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Sign command: /lib/modules/6.8.12-13-pve/build/scripts/sign-file Signing key: /var/lib/dkms/mok.key Public certificate (MOK): /var/lib/dkms/mok.pub Creating symlink /var/lib/dkms/vendor-reset/0.1.1/source -&amp;gt; /usr/src/vendor-reset-0.1.1 Building module: Cleaning build area... make -j56 KERNELRELEASE=6.8.12-13-pve KDIR=/lib/modules/6.8.12-13-pve/build... Signing module /var/lib/dkms/vendor-reset/0.1.1/build/vendor-reset.ko Cleaning build area... vendor-reset.ko: Running module version sanity check. - Original module - No original module exists within this kernel - Installation - Installing to /lib/modules/6.8.12-13-pve/updates/dkms/ depmod... &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Configure vendor-reset to Load at Boot&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Tell the system to load vendor-reset at boot echo &amp;quot;vendor-reset&amp;quot; | sudo tee -a /etc/modules # Copy the udev rules that automatically set the reset method sudo cp udev/99-vendor-reset.rules /etc/udev/rules.d/ # Update initramfs sudo update-initramfs -u -k all # Make sure the module file is where it should be ls -la /lib/modules/$(uname -r)/updates/dkms/vendor-reset.ko &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Reboot and Verify Everything Works&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;reboot &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After the reboot, check that everything is working:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Make sure vendor-reset is loaded lsmod | grep vendor_reset # Check the reset method for your GPU (use your actual PCI address) cat /sys/bus/pci/devices/0000:08:00.0/reset_method # Confirm your GPU is still detected lspci -nn | grep -i amd &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What you want to see:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vendor_reset 16384 0 device_specific 08:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Vega 20 [Radeon Pro Vega II/Radeon Pro Vega II Duo] [1002:66a3] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;The reset method MUST&lt;/strong&gt; &lt;code&gt;device_specific&lt;/code&gt;. If it shows &lt;code&gt;bus&lt;/code&gt;, the udev rules didn't work properly.&lt;/p&gt; &lt;h1&gt;Part 3: VM Configuration&lt;/h1&gt; &lt;h1&gt;Add the GPU to Your VM&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Through the Proxmox web interface:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to your VM → Hardware → Add → PCI Device&lt;/li&gt; &lt;li&gt;Select your GPU (like &lt;code&gt;0000:08:00&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Check &amp;quot;All Functions&amp;quot;&lt;/li&gt; &lt;li&gt;Apply the changes&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Machine Type:&lt;/strong&gt; I used q35 for my VM, I did not try the other options.&lt;/p&gt; &lt;h1&gt;Handle Large VRAM&lt;/h1&gt; &lt;p&gt;Since GPUs like the MI50 have tons of VRAM (32GB), you need to increase the PCI BAR size.&lt;br /&gt; Edit your VM config file (&lt;code&gt;/etc/pve/qemu-server/VMID.conf&lt;/code&gt;) and add this line:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;args: -cpu host,host-phys-bits=on -fw_cfg opt/ovmf/X-PciMmio64Mb,string=65536 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I opted to use this larger sized based on a recommendation from another reddit post.&lt;/p&gt; &lt;p&gt;Here's my complete working VM configuration for reference:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;args: -cpu host,host-phys-bits=on -fw_cfg opt/ovmf/X-PciMmio64Mb,string=65536 bios: seabios boot: order=scsi0;hostpci0;net0 cores: 8 cpu: host hostpci0: 0000:08:00 machine: q35 memory: 32768 name: AI-Node net0: virtio=XX:XX:XX:XX:XX:XX,bridge=vmbr0,tag=40 numa: 1 ostype: l26 scsi0: local-lvm:vm-106-disk-0,cache=writeback,iothread=1,size=300G,ssd=1 scsihw: virtio-scsi-single sockets: 2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Key points:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;hostpci0: 0000:08:00&lt;/code&gt; - This is the GPU passthrough (use your actual PCI address)&lt;/li&gt; &lt;li&gt;&lt;code&gt;machine: q35&lt;/code&gt; - Required chipset for modern PCIe passthrough&lt;/li&gt; &lt;li&gt;&lt;code&gt;args: -fw_cfg opt/ovmf/X-PciMmio64Mb,string=65536&lt;/code&gt; - Increased PCI BAR size for large VRAM&lt;/li&gt; &lt;li&gt;&lt;code&gt;bios: seabios&lt;/code&gt; - SeaBIOS works fine with these settings&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Test Your VM&lt;/h1&gt; &lt;p&gt;Start up your VM and check if the GPU initialized properly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Inside the Ubuntu VM, check the logs sudo dmesg | grep -Ei &amp;quot;bios|gpu|amd|drm&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If everything worked, you should see something like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[ 28.319860] [drm] initializing kernel modesetting (VEGA20 0x1002:0x66A1 0x1002:0x0834 0x02). [ 28.354277] amdgpu 0000:05:00.0: amdgpu: Fetched VBIOS from ROM BAR [ 28.354283] amdgpu: ATOM BIOS: 113-D1631700-111 [ 28.361352] amdgpu 0000:05:00.0: amdgpu: MEM ECC is active. [ 28.361354] amdgpu 0000:05:00.0: amdgpu: SRAM ECC is active. [ 29.376346] [drm] Initialized amdgpu 3.57.0 20150101 for 0000:05:00.0 on minor 0 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Part 4: Getting ROCm Working&lt;/h1&gt; &lt;p&gt;After I got Ubuntu 22.04.5 running in the VM, I followed AMD's standard ROCm installation guide to get everything working for Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; &lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html"&gt;ROCm Quick Start Installation Guide&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Install ROCm&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Download and install the amdgpu-install package wget https://repo.radeon.com/amdgpu-install/6.4.3/ubuntu/jammy/amdgpu-install_6.4.60403-1_all.deb sudo apt install ./amdgpu-install_6.4.60403-1_all.deb sudo apt update # Install some required Python packages sudo apt install python3-setuptools python3-wheel # Add your user to the right groups sudo usermod -a -G render,video $LOGNAME # Install ROCm sudo apt install rocm &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Install AMDGPU Kernel Module&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# If you haven't already downloaded the installer wget https://repo.radeon.com/amdgpu-install/6.4.3/ubuntu/jammy/amdgpu-install_6.4.60403-1_all.deb sudo apt install ./amdgpu-install_6.4.60403-1_all.deb sudo apt update # Install kernel headers and the AMDGPU driver sudo apt install &amp;quot;linux-headers-$(uname -r)&amp;quot; &amp;quot;linux-modules-extra-$(uname -r)&amp;quot; sudo apt install amdgpu-dkms &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Post-Installation Setup&lt;/h1&gt; &lt;p&gt;Following the &lt;a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/post-install.html"&gt;ROCm Post-Install Guide&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Set up library paths sudo tee --append /etc/ld.so.conf.d/rocm.conf &amp;lt;&amp;lt;EOF /opt/rocm/lib /opt/rocm/lib64 EOF sudo ldconfig # Check ROCm installation sudo update-alternatives --display rocm # Set up environment variable export LD_LIBRARY_PATH=/opt/rocm-6.4.3/lib &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You want to reboot the VM after installing ROCm and the AMDGPU drivers.&lt;/p&gt; &lt;h1&gt;Need to Remove Everything?&lt;/h1&gt; &lt;p&gt;If you want to completely remove vendor-reset:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Remove the DKMS module sudo dkms remove vendor-reset/0.1.1 --all sudo rm -rf /usr/src/vendor-reset-0.1.1 sudo rm -rf /var/lib/dkms/vendor-reset # Remove configuration files sudo sed -i '/vendor-reset/d' /etc/modules sudo rm -f /etc/udev/rules.d/99-vendor-reset.rules # Update initramfs and reboot sudo update-initramfs -u -k all reboot &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Credits and References&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Original solution by gnif: &lt;a href="https://github.com/gnif/vendor-reset"&gt;https://github.com/gnif/vendor-reset&lt;/a&gt;&lt;/li&gt; &lt;li&gt;PCI BAR size configuration and vendor-reset insights: &lt;a href="https://www.reddit.com/r/VFIO/comments/oxsku7/vfio_amd_vega20_gpu_passthrough_issues/"&gt;https://www.reddit.com/r/VFIO/comments/oxsku7/vfio_amd_vega20_gpu_passthrough_issues/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AMD GPU passthrough discussion: &lt;a href="https://github.com/ROCm/amdgpu/issues/157"&gt;https://github.com/ROCm/amdgpu/issues/157&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Proxmox-specific AMD GPU issues: &lt;a href="https://www.reddit.com/r/Proxmox/comments/1g4d5mf/amd_gpu_passthrough_issues_with_amd_mi60/"&gt;https://www.reddit.com/r/Proxmox/comments/1g4d5mf/amd_gpu_passthrough_issues_with_amd_mi60/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;This setup took me way longer to figure out than it should have. If this guide saves you some time and frustration, awesome! Feel free to contribute back with any improvements or issues you run into.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Panda24z"&gt; /u/Panda24z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml1aef/amd_mi50_32gbvega20_gpu_passthrough_guide_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml1aef/amd_mi50_32gbvega20_gpu_passthrough_guide_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml1aef/amd_mi50_32gbvega20_gpu_passthrough_guide_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T17:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml14kw</id>
    <title>Visualization - How LLMs Just Predict The Next Word</title>
    <updated>2025-08-08T17:15:45+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml14kw/visualization_how_llms_just_predict_the_next_word/"&gt; &lt;img alt="Visualization - How LLMs Just Predict The Next Word" src="https://external-preview.redd.it/NvAI6Yum9O40l3qZlOeyOssVIs2oLgJwnoMTWT8Xzzg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64d63adc6a8fdef03b738bcffef859cc8986b0e8" title="Visualization - How LLMs Just Predict The Next Word" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/6dn1kUwTFcc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml14kw/visualization_how_llms_just_predict_the_next_word/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml14kw/visualization_how_llms_just_predict_the_next_word/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T17:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ml0ltl</id>
    <title>MemU: Let AI Truly Memorize You</title>
    <updated>2025-08-08T16:56:15+00:00</updated>
    <author>
      <name>/u/EducationalSound5687</name>
      <uri>https://old.reddit.com/user/EducationalSound5687</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml0ltl/memu_let_ai_truly_memorize_you/"&gt; &lt;img alt="MemU: Let AI Truly Memorize You" src="https://preview.redd.it/n5rl0ud1tthf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2fa37a8392438ea09f5835bdc42f57c27b2fb43" title="MemU: Let AI Truly Memorize You" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Github: &lt;a href="https://github.com/NevaMind-AI/memU"&gt;https://github.com/NevaMind-AI/memU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MemU provides an intelligent memory layer for AI agents. It treats memory as a hierarchical file system: one where entries can be written, connected, revised, and prioritized automatically over time. At the core of MemU is a dedicated memory agent. It receives conversational input, documents, user behaviors, and multimodal context, converts structured memory files and updates existing memory files.&lt;/p&gt; &lt;p&gt;With memU, you can build AI companions that truly remember you. They learn who you are, what you care about, and grow alongside you through every interaction.&lt;/p&gt; &lt;p&gt;92.9% Accuracy - 90% Cost Reduction - AI Companion Specialized&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AI Companion Specialization - Adapt to AI companions application&lt;/li&gt; &lt;li&gt;92.9% Accuracy - State-of-the-art score in Locomo benchmark&lt;/li&gt; &lt;li&gt;Up to 90% Cost Reduction - Through optimized online platform&lt;/li&gt; &lt;li&gt;Advanced Retrieval Strategies - Multiple methods including semantic search, hybrid search, contextual retrieval&lt;/li&gt; &lt;li&gt;24/7 Support - For enterprise customers&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EducationalSound5687"&gt; /u/EducationalSound5687 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n5rl0ud1tthf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ml0ltl/memu_let_ai_truly_memorize_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ml0ltl/memu_let_ai_truly_memorize_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T16:56:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkx5zo</id>
    <title>a lightweight voice clone tool, not dependent on ffmpeg, Python, PyTorch, ONNX, just a single executable file</title>
    <updated>2025-08-08T14:46:11+00:00</updated>
    <author>
      <name>/u/Suitable-Patience916</name>
      <uri>https://old.reddit.com/user/Suitable-Patience916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hello everyone,&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I built an &lt;a href="https://github.com/myshell-ai/OpenVoice"&gt;OpenVoice&lt;/a&gt;-based voice cloning tool that requires no installation, just a single executable file (~14M), supporting multiple formats without dependencies on ffmpeg, Python, PyTorch, ONNX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Single-file executable - no installation required&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Independent of FFmpeg, Python, PyTorch, and ONNX&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Support multiple formats (e.g. mp4, mp3, wav)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Offer multiple built-in base speakers: en-au, en-br, en-default, en-india, en-newest, en-us, es, fr, jp, kr, zh&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Support CPU &amp;amp; GPU&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/jingangdidi/voice_clone"&gt;https://github.com/jingangdidi/voice_clone&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Patience916"&gt; /u/Suitable-Patience916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkx5zo/a_lightweight_voice_clone_tool_not_dependent_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkx5zo/a_lightweight_voice_clone_tool_not_dependent_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkx5zo/a_lightweight_voice_clone_tool_not_dependent_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T14:46:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkhbs9</id>
    <title>OpenAI new open-source model is basically Phi-5</title>
    <updated>2025-08-08T00:50:55+00:00</updated>
    <author>
      <name>/u/ik-when-that-hotline</name>
      <uri>https://old.reddit.com/user/ik-when-that-hotline</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ik-when-that-hotline"&gt; /u/ik-when-that-hotline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.ycombinator.com/item?id=44828884"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkhbs9/openai_new_opensource_model_is_basically_phi5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkhbs9/openai_new_opensource_model_is_basically_phi5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T00:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkxzdg</id>
    <title>Hugging Face AI Sheets, open-source tool to do data work with open and local models</title>
    <updated>2025-08-08T15:17:30+00:00</updated>
    <author>
      <name>/u/dvilasuero</name>
      <uri>https://old.reddit.com/user/dvilasuero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxzdg/hugging_face_ai_sheets_opensource_tool_to_do_data/"&gt; &lt;img alt="Hugging Face AI Sheets, open-source tool to do data work with open and local models" src="https://external-preview.redd.it/Z2tyN2NsODBidGhmMYRX1ulV9J5i3yzICa2szL8XEqVGjY7m7LWkytk1RKM3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbd101011d77eb093980aa91356b59b188b5e63c" title="Hugging Face AI Sheets, open-source tool to do data work with open and local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;I'm one of the authors of the tool. &lt;/p&gt; &lt;p&gt;We've just open sourced it and think it would be cool for this community. You can vibe test 1000s of models on Hugging Face via Inference Providers, and more importantly deploy it and run local models.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/huggingface/aisheets"&gt;https://github.com/huggingface/aisheets&lt;/a&gt;&lt;br /&gt; How to run it with local models: &lt;a href="https://github.com/huggingface/aisheets?tab=readme-ov-file#running-ai-sheets-with-custom-and-local-llms"&gt;https://github.com/huggingface/aisheets?tab=readme-ov-file#running-ai-sheets-with-custom-and-local-llms&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dvilasuero"&gt; /u/dvilasuero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/st1tql80bthf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxzdg/hugging_face_ai_sheets_opensource_tool_to_do_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxzdg/hugging_face_ai_sheets_opensource_tool_to_do_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:17:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkcwiv</id>
    <title>OpenAI open washing</title>
    <updated>2025-08-07T21:38:35+00:00</updated>
    <author>
      <name>/u/gwyngwynsituation</name>
      <uri>https://old.reddit.com/user/gwyngwynsituation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think OpenAI released GPT-OSS, a barely usable model, fully aware it would generate backlash once freely tested. But they also had in mind that releasing GPT-5 immediately afterward would divert all attention away from their low-effort model. In this way, they can defend themselves against criticism that they’re not committed to the open-source space, without having to face the consequences of releasing a joke of a model. Classic corporate behavior. And that concludes my rant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gwyngwynsituation"&gt; /u/gwyngwynsituation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkcwiv/openai_open_washing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T21:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkavhy</id>
    <title>random bar chart made by Qwen3-235B-A22B-2507</title>
    <updated>2025-08-07T20:19:55+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt; &lt;img alt="random bar chart made by Qwen3-235B-A22B-2507" src="https://preview.redd.it/rka3lhpnonhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd853635222d78299767b459957da8a9ae9f30b5" title="random bar chart made by Qwen3-235B-A22B-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;had it render the chart on HTML canvas&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rka3lhpnonhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkavhy/random_bar_chart_made_by_qwen3235ba22b2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T20:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mke7ef</id>
    <title>120B runs awesome on just 8GB VRAM!</title>
    <updated>2025-08-07T22:32:04+00:00</updated>
    <author>
      <name>/u/Wrong-Historian</name>
      <uri>https://old.reddit.com/user/Wrong-Historian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the thing, the expert layers run amazing on CPU (&lt;del&gt;~17T/s&lt;/del&gt; 25T/s on a 14900K) and you can force that with this new llama-cpp option: --cpu-moe .&lt;/p&gt; &lt;p&gt;You can offload just the attention layers to GPU (requiring about 5 to 8GB of VRAM) for fast prefill.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache for the sequence&lt;/li&gt; &lt;li&gt;Attention weights &amp;amp; activations&lt;/li&gt; &lt;li&gt;Routing tables&lt;/li&gt; &lt;li&gt;LayerNorms and other “non-expert” parameters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No giant MLP weights are resident on the GPU, so memory use stays low.&lt;/p&gt; &lt;p&gt;This yields an amazing snappy system for a 120B model! Even something like a 3060Ti would be amazing! GPU with BF16 support would be best (RTX3000+) because all layers except the MOE layers (which are mxfp4) are BF16.&lt;/p&gt; &lt;p&gt;64GB of system ram would be minimum, and 96GB would be ideal. (linux uses mmap so will keep the 'hot' experts in memory even if the whole model doesn't fit in memory)&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;prompt eval time = 28044.75 ms / 3440 tokens ( 8.15 ms per token, 122.66 tokens per second)&lt;/p&gt; &lt;p&gt;eval time = 5433.28 ms / 98 tokens ( 55.44 ms per token, 18.04 tokens per second)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;with 5GB of vram usage!&lt;/p&gt; &lt;p&gt;Honestly, I think this is the biggest win of this 120B model. This seems an amazing model to run fast for GPU-poor people. You can do this on a 3060Ti and 64GB of system ram is cheap.&lt;/p&gt; &lt;p&gt;edit: with this latest PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15157"&gt;https://github.com/ggml-org/llama.cpp/pull/15157&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \ -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \ --n-cpu-moe 36 \ #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster. --n-gpu-layers 999 \ #everything else on the GPU, about 8GB -c 0 -fa \ #max context (128k), flash attention --jinja --reasoning-format none \ --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \ prompt eval time = 94593.62 ms / 12717 tokens ( 7.44 ms per token, 134.44 tokens per second) eval time = 76741.17 ms / 1966 tokens ( 39.03 ms per token, 25.62 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hitting above 25T/s with only 8GB VRAM use!&lt;/p&gt; &lt;p&gt;Compared to running 8 MOE layers also on the GPU (about 22GB VRAM used total) :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \ -m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \ --n-cpu-moe 28 \ --n-gpu-layers 999 \ -c 0 -fa \ --jinja --reasoning-format none \ --host 0.0.0.0 --port 8502 --api-key &amp;quot;dummy&amp;quot; \ prompt eval time = 78003.66 ms / 12715 tokens ( 6.13 ms per token, 163.01 tokens per second) eval time = 70376.61 ms / 2169 tokens ( 32.45 ms per token, 30.82 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Honestly, this 120B is the perfect architecture for running at home on consumer hardware. Somebody did some smart thinking when designing all of this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Historian"&gt; /u/Wrong-Historian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T22:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkp0am</id>
    <title>Granite 3 8B is seriously underrated - still outperforming newer models</title>
    <updated>2025-08-08T07:42:17+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building AI pipelines using the 12 factor agent approach (shoutout to Dex - check out his YouTube talk and GitHub), and I have to say IBM's Granite 3 8B continues to impress me nearly a year after release.This model consistently outperforms newer closed-source options (yes, I talked about GPT-5 mini/nano) on specific tasks. Where it really shines:&lt;/p&gt; &lt;p&gt;Task classification with structured outputs - It's incredibly reliable at categorizing user requests into the right buckets&lt;/p&gt; &lt;p&gt;Keyword generation for search/RAG - Produces solid results for information retrieval&lt;/p&gt; &lt;p&gt;If you haven't tried Granite 3 8B yet, it's worth adding to your toolkit. I still use larger models for the final aggregation and presentation layer, but for these specialized tasks, Granite punches well above its weight class.&lt;/p&gt; &lt;p&gt;Anyone else having similar experiences with this model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkp0am/granite_3_8b_is_seriously_underrated_still/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkp0am/granite_3_8b_is_seriously_underrated_still/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkp0am/granite_3_8b_is_seriously_underrated_still/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:42:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkvks4</id>
    <title>How Attention Sinks Keep Language Models Stable</title>
    <updated>2025-08-08T13:43:01+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hanlab.mit.edu/blog/streamingllm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkvks4/how_attention_sinks_keep_language_models_stable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkvks4/how_attention_sinks_keep_language_models_stable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T13:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkon92</id>
    <title>Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China</title>
    <updated>2025-08-08T07:18:22+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"&gt; &lt;img alt="Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China" src="https://preview.redd.it/u7fdqw6zwqhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3114187ca53c4b97153190460034166fc59eaccc" title="Half of the models in the top 10 on Design Arena are OW/OS, and they're all from China" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since I started &lt;a href="https://www.designarena.ai/"&gt;my benchmark&lt;/a&gt; just about a month and a half ago, it has been interesting to see just how well the open weight / open source models are competing with their proprietary counterparts when evaluated on how user comparisons of different generations from each model. &lt;/p&gt; &lt;p&gt;Based on the benchmark, Qwen3 Coder, DeepSeek R1-0528, DeepSeek V3-2024, Qwen3 Instruct 2507, and GLM 4.5 could all be considered to be SOTA. I do think this ranking will change slightly though with one of the OS models being pushed out for GPT-5 (which was recently added, so sample size is too small). &lt;/p&gt; &lt;p&gt;That said, it really feels like we're in a golden age of open source models right now. We're also see a good amount of stagnation right now in the improvements being made by the proprietary models. Do you think OS will continue to keep pace? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u7fdqw6zwqhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkon92/half_of_the_models_in_the_top_10_on_design_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkw4ug</id>
    <title>GLM45 vs GPT-5, Claude Sonnet 4, Gemini 2.5 Pro — live coding test, same prompt</title>
    <updated>2025-08-08T14:05:13+00:00</updated>
    <author>
      <name>/u/darkageofme</name>
      <uri>https://old.reddit.com/user/darkageofme</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re running a live benchmark today with &lt;strong&gt;GLM45&lt;/strong&gt; in the mix against three major proprietary LLMs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every model gets the same prompt for each task&lt;/li&gt; &lt;li&gt;Multiple attempts: simple builds, bug fixes, complex projects, and possibly planning tasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’ll record:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How GLM45 performs on speed and accuracy&lt;/li&gt; &lt;li&gt;Where it matches or beats closed models&lt;/li&gt; &lt;li&gt;Debug handling in a live environment&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;16:00 UTC / 19:00 EEST&lt;/p&gt; &lt;p&gt;You'll find us here: &lt;a href="https://live.biela.dev"&gt;https://live.biela.dev&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkageofme"&gt; /u/darkageofme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkw4ug/glm45_vs_gpt5_claude_sonnet_4_gemini_25_pro_live/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkw4ug/glm45_vs_gpt5_claude_sonnet_4_gemini_25_pro_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkw4ug/glm45_vs_gpt5_claude_sonnet_4_gemini_25_pro_live/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T14:05:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkq4i4</id>
    <title>Qwen added 1M support for Qwen3-30B-A3B-Instruct-2507 and Qwen3-235B-A22B-Instruct-2507</title>
    <updated>2025-08-08T08:55:44+00:00</updated>
    <author>
      <name>/u/acec</name>
      <uri>https://old.reddit.com/user/acec</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkq4i4/qwen_added_1m_support_for_qwen330ba3binstruct2507/"&gt; &lt;img alt="Qwen added 1M support for Qwen3-30B-A3B-Instruct-2507 and Qwen3-235B-A22B-Instruct-2507" src="https://external-preview.redd.it/4L2FXW9Fym-Ol4pha2Ze5zHkeeMTtxPBl8ihz-UFknI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c994da656f69e4f6e8089e52864a4ba31055fa1f" title="Qwen added 1M support for Qwen3-30B-A3B-Instruct-2507 and Qwen3-235B-A22B-Instruct-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They claim that &amp;quot;On sequences approaching 1M tokens, the system achieves up to a &lt;strong&gt;3× speedup&lt;/strong&gt; compared to standard attention implementations.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/acec"&gt; /u/acec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507/commit/3ffd1f50b179e643d839c86df9ffbbefcb0d5018"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkq4i4/qwen_added_1m_support_for_qwen330ba3binstruct2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkq4i4/qwen_added_1m_support_for_qwen330ba3binstruct2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T08:55:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkwkcd</id>
    <title>What do you think it will be?</title>
    <updated>2025-08-08T14:22:21+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkwkcd/what_do_you_think_it_will_be/"&gt; &lt;img alt="What do you think it will be?" src="https://preview.redd.it/it28f5ns1thf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3e5ae37fbf6696b49a3a1409b98db6c6507247a" title="What do you think it will be?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/it28f5ns1thf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkwkcd/what_do_you_think_it_will_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkwkcd/what_do_you_think_it_will_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T14:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkowrw</id>
    <title>Llama.cpp just added a major 3x performance boost.</title>
    <updated>2025-08-08T07:35:50+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama cpp just merged the final piece to fully support attention sinks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15157"&gt;https://github.com/ggml-org/llama.cpp/pull/15157&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My prompt processing speed went from 300 to 1300 with a 3090 for the new oss model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkowrw/llamacpp_just_added_a_major_3x_performance_boost/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkowrw/llamacpp_just_added_a_major_3x_performance_boost/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkowrw/llamacpp_just_added_a_major_3x_performance_boost/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T07:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkf543</id>
    <title>To all GPT-5 posts</title>
    <updated>2025-08-07T23:11:59+00:00</updated>
    <author>
      <name>/u/Danny_Davitoe</name>
      <uri>https://old.reddit.com/user/Danny_Davitoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt; &lt;img alt="To all GPT-5 posts" src="https://preview.redd.it/8v08gwidjohf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a549b4a6f64e891d2fe2035565f6d9915347c9d1" title="To all GPT-5 posts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please. I don’t care about pricing. The only API teir I care about is which model gets port 8000 or 8080. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danny_Davitoe"&gt; /u/Danny_Davitoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v08gwidjohf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkf543/to_all_gpt5_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-07T23:11:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mky4jd</id>
    <title>Why Open Source is Needed</title>
    <updated>2025-08-08T15:22:58+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mky4jd/why_open_source_is_needed/"&gt; &lt;img alt="Why Open Source is Needed" src="https://preview.redd.it/k8n9e70mcthf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=756f312a40164e6bd66cc44530c38aea4caaa12b" title="Why Open Source is Needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With this new launch, OpenAI cut total weekly reasoning model requests from 2900 to 200(!!!!) also with a huge reduction in context window length. &lt;/p&gt; &lt;p&gt;Yes, $200 a month for a measly 128k context window length. Just goes to show why open source models and more companies being able to host these models protects the consumer from this disingenuous behavior.&lt;/p&gt; &lt;p&gt;(Also, GPT5 isn’t even that impressive…)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k8n9e70mcthf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mky4jd/why_open_source_is_needed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mky4jd/why_open_source_is_needed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:22:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkngs6</id>
    <title>I had to try the “blueberry” thing myself with GPT5. I merely report the results.</title>
    <updated>2025-08-08T06:06:41+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkngs6/i_had_to_try_the_blueberry_thing_myself_with_gpt5/"&gt; &lt;img alt="I had to try the “blueberry” thing myself with GPT5. I merely report the results." src="https://preview.redd.it/n3tapryqkqhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef40b2d7394bb222878d6008796d540bdf41673e" title="I had to try the “blueberry” thing myself with GPT5. I merely report the results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT5 keep saying it is the real deal lol. Is working but still far from the real deal in my opinion. &lt;/p&gt; &lt;p&gt;Credit: Kieran Healy‪@kjhealy.co‬&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n3tapryqkqhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkngs6/i_had_to_try_the_blueberry_thing_myself_with_gpt5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkngs6/i_had_to_try_the_blueberry_thing_myself_with_gpt5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T06:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkxmoa</id>
    <title>GLM-4.5 series new models will be open source soon</title>
    <updated>2025-08-08T15:03:54+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxmoa/glm45_series_new_models_will_be_open_source_soon/"&gt; &lt;img alt="GLM-4.5 series new models will be open source soon" src="https://preview.redd.it/mmvy25c79thf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff0519caa8ad5551e7e553775eb89d38a4aeb2c0" title="GLM-4.5 series new models will be open source soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mmvy25c79thf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxmoa/glm45_series_new_models_will_be_open_source_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkxmoa/glm45_series_new_models_will_be_open_source_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mkrb18</id>
    <title>🚀 Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context—up to 1 million tokens!</title>
    <updated>2025-08-08T10:11:45+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkrb18/qwen330ba3b2507_and_qwen3235ba22b2507_now_support/"&gt; &lt;img alt="🚀 Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context—up to 1 million tokens!" src="https://preview.redd.it/ud233u23trhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9666b41b192bef19c1d95e2dc31745f398def8d7" title="🚀 Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context—up to 1 million tokens!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507 now support ultra-long context—up to 1 million tokens!&lt;/p&gt; &lt;p&gt;🔧 Powered by:&lt;/p&gt; &lt;p&gt;• Dual Chunk Attention (DCA) – A length extrapolation method that splits long sequences into manageable chunks while preserving global coherence. &lt;/p&gt; &lt;p&gt;• MInference – Sparse attention that cuts overhead by focusing on key token interactions&lt;/p&gt; &lt;p&gt;💡 These innovations boost both generation quality and inference speed, delivering up to 3× faster performance on near-1M token sequences.&lt;/p&gt; &lt;p&gt;✅ Fully compatible with vLLM and SGLang for efficient deployment.&lt;/p&gt; &lt;p&gt;📄 See the update model cards for how to enable this feature.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-235B-A22B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ud233u23trhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mkrb18/qwen330ba3b2507_and_qwen3235ba22b2507_now_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mkrb18/qwen330ba3b2507_and_qwen3235ba22b2507_now_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T10:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mky54y</id>
    <title>Qwen Code Now Offering 2000 free Qwen Code runs daily</title>
    <updated>2025-08-08T15:23:35+00:00</updated>
    <author>
      <name>/u/z1xto</name>
      <uri>https://old.reddit.com/user/z1xto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mky54y/qwen_code_now_offering_2000_free_qwen_code_runs/"&gt; &lt;img alt="Qwen Code Now Offering 2000 free Qwen Code runs daily" src="https://preview.redd.it/0qdg1xmncthf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9082f0ec66dc87ddcf7e2f275893ea58d944c27" title="Qwen Code Now Offering 2000 free Qwen Code runs daily" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tweet link: &lt;a href="https://x.com/Alibaba_Qwen/status/1953835877555151134"&gt;https://x.com/Alibaba_Qwen/status/1953835877555151134&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/z1xto"&gt; /u/z1xto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0qdg1xmncthf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mky54y/qwen_code_now_offering_2000_free_qwen_code_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mky54y/qwen_code_now_offering_2000_free_qwen_code_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-08T15:23:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
</feed>
