<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-31T17:05:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n4v2qk</id>
    <title>QuEST/Quartet authors discuss their work on SOTA 4-bit training optimizations</title>
    <updated>2025-08-31T13:43:38+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v2qk/questquartet_authors_discuss_their_work_on_sota/"&gt; &lt;img alt="QuEST/Quartet authors discuss their work on SOTA 4-bit training optimizations" src="https://external-preview.redd.it/qfx9oCXJ4SXfBT_yraiZ7I_v9CRRUKfBMEKec171XZc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d411c7c2659170c8b8d034377340bfb2ad49a40" title="QuEST/Quartet authors discuss their work on SOTA 4-bit training optimizations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quartet: Native FP4 Training Can Be Optimal for Large Language Models - &lt;a href="https://arxiv.org/abs/2505.14669"&gt;https://arxiv.org/abs/2505.14669&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This looks like the best speedup you can get for full 4 bit pre-training at the moment. (MXFP4)&lt;/p&gt; &lt;p&gt;Both the forward and backward pass in training are done with low precision, resulting in nearly 2X FP8. &lt;/p&gt; &lt;p&gt;Good questions were asked: (paraphrased) &lt;/p&gt; &lt;ul&gt; &lt;li&gt;When Llama-3 came out, people thought we would eventually see the death of quantization with full model saturation. Does this still hold true?&lt;/li&gt; &lt;li&gt;Would these scaling laws change when certain parts of the model are left unquantized? &lt;/li&gt; &lt;li&gt;What other viable data types would you like to see for low bit training?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I can highly recommend watching if you have time!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=XVo17Q7YapA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v2qk/questquartet_authors_discuss_their_work_on_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v2qk/questquartet_authors_discuss_their_work_on_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T13:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4tsak</id>
    <title>Best use of 6 x RTX6000</title>
    <updated>2025-08-31T12:46:24+00:00</updated>
    <author>
      <name>/u/TaiMaiShu-71</name>
      <uri>https://old.reddit.com/user/TaiMaiShu-71</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've got a couple of H100s at work that I've been using for various production and dev environments for the past year or so. Enough has gone into production that the H100s are pretty much maxed out. I can't justify getting into the dgx ecosystem just yet but I was able to get a pretty well speced server with 6 x RTX6000 Pro Blackwells . I want to set this server up to serve API endpoints for inference, transcription, and embeddings and eventually expand it to support some image generation, real time STS , video etc . I want to run several models, we currently are using qwen3-30b, 4b, and a handful of other small models for embeddings, vision etc. With the new capacity I would like to offer bigger models too 100b+(gpt-oss-120) and maybe even a 200b+ (qwen3-235b) just depends on our use case. And if able maybe try out some 400b+ models. The goal is scalability and speed on the new platform as we automate more tasks and provide basic inference and chat to some users on domain specific subjects. I am thinking of this new box as the swiss army knife local host provider. If I can show an ROI and efficiency gains then I can more than likely justify going to into a dgx node next. I'm looking for opinions, What would your ideal stack be for setting this up? Engine? Proxy? Resource management?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TaiMaiShu-71"&gt; /u/TaiMaiShu-71 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4tsak/best_use_of_6_x_rtx6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4tsak/best_use_of_6_x_rtx6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4tsak/best_use_of_6_x_rtx6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T12:46:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4tshu</id>
    <title>Best local LLMs to run on a 5090 (32 GB VRAM)?</title>
    <updated>2025-08-31T12:46:38+00:00</updated>
    <author>
      <name>/u/PromotionTypical7824</name>
      <uri>https://old.reddit.com/user/PromotionTypical7824</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just picked up a 5090 for Stable Diffusion image generation. But I’d also like to experiment with running a local LLM and I’m curious what models or setups make the most sense with this GPU. Any recommendations or tips?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PromotionTypical7824"&gt; /u/PromotionTypical7824 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4tshu/best_local_llms_to_run_on_a_5090_32_gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4tshu/best_local_llms_to_run_on_a_5090_32_gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4tshu/best_local_llms_to_run_on_a_5090_32_gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T12:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4hkar</id>
    <title>I tried almost every tts model on my ryzen 7 5000 series 16gb ram rtx 3060 laptop 6-8GB Vram</title>
    <updated>2025-08-31T00:58:19+00:00</updated>
    <author>
      <name>/u/This_is_difficult_0</name>
      <uri>https://old.reddit.com/user/This_is_difficult_0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I needed a decent voiceover for my projects and I am using minimax audio currently. It's really good. But I wanted to try some local tts and i tested around 5 of them and here are my results.&lt;/p&gt; &lt;h1&gt;1. Kokoro - 0.4-0.5 x realtime .Fast and decent quality&lt;/h1&gt; &lt;p&gt;Fast koko api one from github has lower quality than the hexgrad. &lt;/p&gt; &lt;h1&gt;2. Xtts v2 - 0.8-1x realtime tried it from pinokio ai : speed is little slower than kokoro but decent quality with cloning capabilities.&lt;/h1&gt; &lt;h1&gt;3. Chatterbox - if you have lower end gpu and cpu like me don't try ! It's slow (1.5-2x realtime but high quality , expressive. I found out when i wanted it to be neutral or soothing, it still exaggerated or sounded too robotic but not in the middle.&lt;/h1&gt; &lt;p&gt;Installation was painful for me. Took me whole day figuring out to improve the speed. I tried different forks from github and messed up file paths😂 Extremely slow , so not useful for me.&lt;/p&gt; &lt;h1&gt;4. Zonos : 2-3x realtime Slow... Uninstalled it pretty soon&lt;/h1&gt; &lt;h1&gt;5. Orpheus : yet to try. Looks expressive but mismatch the rythm frequently in the huggin space. Maybe it will be slow on my laptop too&lt;/h1&gt; &lt;h1&gt;6. F5tts : around 1x realtime tried it from pinokio ai. Looks decent but somewhere in the middle, it hallucinates , so not useful&lt;/h1&gt; &lt;p&gt;I am thinking of trying Piper next. I want a lightweight better than kokoro. For now , i would rank kokoro as the best option for my pc. Do you have any good suggestions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/This_is_difficult_0"&gt; /u/This_is_difficult_0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4hkar/i_tried_almost_every_tts_model_on_my_ryzen_7_5000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T00:58:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4xfv8</id>
    <title>Axolotl offers 6x context length on single H100 how???</title>
    <updated>2025-08-31T15:19:49+00:00</updated>
    <author>
      <name>/u/bluewhale6674</name>
      <uri>https://old.reddit.com/user/bluewhale6674</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4xfv8/axolotl_offers_6x_context_length_on_single_h100/"&gt; &lt;img alt="Axolotl offers 6x context length on single H100 how???" src="https://preview.redd.it/s7zqdye0hdmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b8ef1a84c2d06fa0d9e9a9dca31dcb94997b05f" title="Axolotl offers 6x context length on single H100 how???" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bluewhale6674"&gt; /u/bluewhale6674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s7zqdye0hdmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4xfv8/axolotl_offers_6x_context_length_on_single_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4xfv8/axolotl_offers_6x_context_length_on_single_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T15:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4mo1r</id>
    <title>Best Way to Use Qwen3-Coder for Local AI Coding?</title>
    <updated>2025-08-31T05:33:56+00:00</updated>
    <author>
      <name>/u/Creative-Scene-6743</name>
      <uri>https://old.reddit.com/user/Creative-Scene-6743</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for some guidance on setting up Qwen Coder models locally for AI-assisted coding work. Normally I’d dive in and figure it out myself, but between work and vacation, I’ve fallen behind and want to avoid a time-consuming rabbit hole.&lt;/p&gt; &lt;p&gt;I have a couple of key questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;How close have you gotten Qwen Code to rival Claude's code capabilities?&lt;/strong&gt; I’m particularly interested in performance for actual dev work, not just benchmarks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What’s the best setup you’ve found so far?&lt;/strong&gt; Are you integrating Qwen into an existing Claude Code by swapping the model? Are you using a like cline integration or something else entirely?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any lessons learned or tips would be hugely appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-Scene-6743"&gt; /u/Creative-Scene-6743 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mo1r/best_way_to_use_qwen3coder_for_local_ai_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mo1r/best_way_to_use_qwen3coder_for_local_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mo1r/best_way_to_use_qwen3coder_for_local_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4rmk7</id>
    <title>If you're not sure if your LLM is right, do this... or the reality check about open weight models - will (have?) they ever hit the frontier again (at all?)?</title>
    <updated>2025-08-31T10:49:09+00:00</updated>
    <author>
      <name>/u/Massive-Shift6641</name>
      <uri>https://old.reddit.com/user/Massive-Shift6641</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4rmk7/if_youre_not_sure_if_your_llm_is_right_do_this_or/"&gt; &lt;img alt="If you're not sure if your LLM is right, do this... or the reality check about open weight models - will (have?) they ever hit the frontier again (at all?)?" src="https://b.thumbs.redditmedia.com/6VYqKtmG8iuqsZkZ-qhUivdNkwuz424KWZ07O760Sak.jpg" title="If you're not sure if your LLM is right, do this... or the reality check about open weight models - will (have?) they ever hit the frontier again (at all?)?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Assume that you have some important question. For example,&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cv7kqz6atamf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3e4a09c16a14d22f59fb0128197ba1ca938a4841"&gt;https://preview.redd.it/cv7kqz6atamf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3e4a09c16a14d22f59fb0128197ba1ca938a4841&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(The correct answers are: key - B, mode - lydian, tempo - fast (120+ BPM), modal interchange &amp;amp; chromaticism - no as there are no non-diatonic notes)&lt;/p&gt; &lt;p&gt;Perfect use case for a LLM! However, LLMs often give vague, equivocal, or uncertain answers. Sometimes you can't be sure about their advice because &lt;strong&gt;you&lt;/strong&gt; lack the expertise to figure it out.&lt;/p&gt; &lt;p&gt;Luckily, since LLM are probabilistic, it is not a problem! The more you annoy them with your stupid questions, the more the odds they will give you a right answer once in a while.&lt;/p&gt; &lt;p&gt;However, it is not enough to have a massive of answers. How do we know &lt;strong&gt;which&lt;/strong&gt; of these answers are correct? Luckily, since LLMs are probabilistic, you can just ask the same question a couple of models some of which smarter than others, and the smarter the model, the more often it gives the right answer.&lt;/p&gt; &lt;p&gt;How do we determine which model is smarter than others? Well, benchmarks, despite how poorly most (all) of them are really designed, provide some information about it. However, a better way would be to ask a couple of models something about the topic &lt;strong&gt;you&lt;/strong&gt; are the expert in - and compare the quality of their answers.&lt;/p&gt; &lt;p&gt;So I went to LM Arena to do exactly this, and oh boy, I do have enough to tell you.&lt;/p&gt; &lt;h1&gt;Models tested on LM Arena&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9fckbnx8xamf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e636ef71cf6e26d2fe04130caac847226c2d3295"&gt;https://preview.redd.it/9fckbnx8xamf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e636ef71cf6e26d2fe04130caac847226c2d3295&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;For this section, remember the right answers:&lt;/p&gt; &lt;p&gt;Key is B&lt;br /&gt; Mode is Lydian&lt;br /&gt; Tempo is fast (120+ BPM)&lt;br /&gt; Modal interchange and chromaticism are not used&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;First of all, benchmarks, despite their poor design, really tell something valuable. We can really see it with Grok 4 and GPT 5 High. Both of them are considered frontier models with astonishing performance at ARC-AGI and other benchmarks, and here, they were the only two models to consistently figure out the mode I wrote this piece in. We can also see that other models were all not that good - which is exactly the picture so many benchmarks tell us.&lt;/p&gt; &lt;p&gt;Second, you can see that models tend to repeat the same answer to the same question over and over again, with minor differences, so you don't need to annoy your favorite models by asking them the same question a hundred times in a row - ten, or even five, may be enough. But if you want to be sure, you can always make just another API call.&lt;/p&gt; &lt;p&gt;Now let's meet the most notable participants of this test.&lt;/p&gt; &lt;h1&gt;Proprietary models: frontier level&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GPT-5 High&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GPT-5 High was the strongest of all models. It figured out the Lydian mode 7 times out of 10, and guessed the correct tempo 10 times out of 10. Even when it mistakenly determined the mode to be Ionian, it applied modal interchange and chromaticism correctly - it once said that a chord was &amp;quot;borrowed from Lydian&amp;quot;, which was just very close enough to the right answer. If not the high workload, I believe that GPT-5 High would answer correctly 10 times of 10.&lt;/p&gt; &lt;p&gt;The only time when GPT-5 gave me a WTF was when it said that the piece was in D# minor. When I asked it in another context, it said that it is likely a brain fart because &amp;quot;the organ chords start on D# minor but it is very easy to overlook the modulations&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pvoan41m3bmf1.png?width=623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9e0fe7f1e19b423014fdb65f598b75874a8b4aa"&gt;Average GPT 5 High answer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0zbth7no3bmf1.png?width=623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00899ef540e2a4102bbd3b9b92a7d7e328d91a4c"&gt;Average GPT 5 High answer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Grok 4&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Strongest contender to GPT-5, it gave correct answers around half of all time, and even the rest of its answers were just very close enough. It applied chromaticism logically to explain the use of non-diatonic chords when it thought that the piece was in Ionian mode. Grok 4 was a very solid performer, very close to the level of GPT-5.&lt;/p&gt; &lt;p&gt;My only complaint about it is that it, apparently, was not able to respond to a couple of prompts. Probably high workload.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/38cts6pw4bmf1.png?width=623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08c333fd8704efb287911de7fa870bb2ef89221a"&gt;Average Grok 4 answer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7ywoln605bmf1.png?width=623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a555b62aab819dbb293e5c384fed2ef9b38d2b56"&gt;https://preview.redd.it/7ywoln605bmf1.png?width=623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a555b62aab819dbb293e5c384fed2ef9b38d2b56&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now since there aren't any other notable proprietary models that I tested (Opus 4.1 wouldn't let me make more than 5 requests per hour -_-), that were able to achieve comparable performance, let's talk about the open weight ones.&lt;/p&gt; &lt;p&gt;The bad news is, there are currently no open weight models that really compare to the frontier. The good news is, DeepSeek has a very good chance to dethrone the frontier in the near future.&lt;/p&gt; &lt;h1&gt;Open source models: the good&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;DeepSeek V3.1 Thinking&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The GOAT of open weight models and the reason the US stock market and Sam Altman can't sleep well, whale delivers the best analysis among all open source models ever. It &amp;quot;hears&amp;quot; the piece either in Ionian or Mixolydian mode, which are both major modes and are either one or two notes different from Lydian, the mode the song actually is, and correctly determines the tempo as either fast or moderately fast. When tested in the official chat outside of LM Arena, it gave even more accurate answers, insisting on Mixolydian and Ionian modes only.&lt;/p&gt; &lt;p&gt;The main problem of DeepSeek was overcomplicating the analysis - it correctly (as far as I can tell) explained how modal interchange and chromaticism would work in Ionian or Mixolydian, but missed that B Lydian explains the used chords far better than any chromaticism. But it did it best to keep things simple!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/amhkn8gl9bmf1.png?width=763&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2399b121b6410df3b42755af8a8ffbeb518457f"&gt;https://preview.redd.it/amhkn8gl9bmf1.png?width=763&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2399b121b6410df3b42755af8a8ffbeb518457f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p8zyo2wp9bmf1.png?width=763&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f79c7131e59e5abaa68e403f073a9be6f45a119c"&gt;Typical whale performance&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Open source models: the bad&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen&lt;/strong&gt; &lt;strong&gt;Max 2025-08-15 &amp;amp; 235b-a22b-Instruct-2507&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen models are widely considered to be competitors to DeepSeek series, but to my surprise, none of them actually live up to this title. Just take a look at this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/joaemreccbmf1.png?width=439&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19f3fa3588f56a04b32fead057376254585de77f"&gt;https://preview.redd.it/joaemreccbmf1.png?width=439&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19f3fa3588f56a04b32fead057376254585de77f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am sorry, but what even the hell is this? I wrote this piece in Lydian mode, using Lydian chords. GPT-5 High, the most intelligent model on the planet, determined the mode of the song as Lydian. Grok 4 was very likely to believe that it was Lydian as well. Okay, even if it is not really Lydian - GPT, Grok, Gemini and DeepSeek all agree that it is still some major mode, and there are only three major modes - Lydian, Ionian, and Mixolydian.&lt;/p&gt; &lt;p&gt;So how the hell Qwen think that it is a goddamn Aeolian? Or Phrygian dominant, literally the second darkest scale right next to the infamous Locrian? Dorian? Harmonic Minor? It makes completely ZERO sense! And it is not to mention that Qwen-Max did not even determine the correct tempo.&lt;/p&gt; &lt;p&gt;You may say - wait a second, but these are only base models, maybe the thinking version would be better? Well, first of all, the base models for GPT-5 and DeepSeek V3.1 Thinking are already far better:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xtplgkb5ebmf1.png?width=761&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03bdd4b37680ee863679a14ab52ec8a790f7f1a7"&gt;https://preview.redd.it/xtplgkb5ebmf1.png?width=761&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=03bdd4b37680ee863679a14ab52ec8a790f7f1a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Disclaimer: in the chat app, V3.1 determined the modes even worse. However, at least the tempo wasn't nonsensically slow)&lt;/p&gt; &lt;p&gt;And second, I tried Qwen Thinking - not even in the LM Arena, but at &lt;a href="http://chat.qwen.ai"&gt;chat.qwen.ai&lt;/a&gt; - and it was still horrible:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tnpq22t1fbmf1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32b363312835b1af8619950376cd0f1e58b3d8db"&gt;https://preview.redd.it/tnpq22t1fbmf1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=32b363312835b1af8619950376cd0f1e58b3d8db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lriblihbfbmf1.png?width=974&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab3a3b8db68a7be1183289785789ceabfbe29e7f"&gt;Average Qwenslop&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unbelievable! Not only it did the same mistake GPT-5 did (which it admitted to be a brain fart), it literally hyperfixated on TWO bass notes out of EIGHT and even completely made up this &amp;quot;resolve to G# as tonic&amp;quot; to justify its hyperfixation.&lt;/p&gt; &lt;p&gt;And it's not to mention how abhorrent its UX is. Some chats never even get saved in the app, and even its thinking process is orders of magnitude slower than DeepSeek's - only to get this nonsense in the output.&lt;/p&gt; &lt;p&gt;Each time Qwen Thinking did not insist on Ionian mode, it complicated its analysis with unnecessary harmonic minors and chromatic mediants so much that it made me realize how hard DeepSeek actually tried even though it is not as good as GPT 5 yet.&lt;/p&gt; &lt;p&gt;Overall, I can't believe that Qwen is ranked this high in so many benchmarks. Maybe there are use cases where it is better than DeepSeek, which is why it is so hyped, but to me, it looks like the gap between Qwen and DeepSeek is just as wide as between DeepSeek and GPT 5, if not wider.&lt;/p&gt; &lt;h1&gt;Open source models: the ugly&lt;/h1&gt; &lt;p&gt;Qwen already makes me feel bad about the state of open source models, but these two make my heart bleed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kimi K2&lt;/strong&gt;'s answers are not only wrong, they are internally inconsistent. Take a look at this example:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4mui4ppimbmf1.png?width=766&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c0ff3125be0eaf9e47b4e7740247509fafa78f36"&gt;https://preview.redd.it/4mui4ppimbmf1.png?width=766&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c0ff3125be0eaf9e47b4e7740247509fafa78f36&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here, it correctly determined the mode as B Lydian! However, there is a catch:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;the F5dim and A#4min chords borrow tones (A natural, F natural)&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;First of all, there is F natural in Lydian - under the name of E#, which it incorrectly showed as just E.&lt;br /&gt; Second, there is &lt;strong&gt;no&lt;/strong&gt; A natural in A#min chord - it is A#. K2 thinks that there is A natural in this chord because it literally hallucinated the # (diesis) out &lt;strong&gt;in the same sentence&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;You can see the consequences of these hallucinations in the modal and chromaticism scores (bottom two rows):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bgt0rbiolbmf1.png?width=371&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2eb3fd86533827d4973fda14e7fe7e9664fcae9"&gt;https://preview.redd.it/bgt0rbiolbmf1.png?width=371&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b2eb3fd86533827d4973fda14e7fe7e9664fcae9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Once upon a time, the mode is B Aeolian and there are modal and chromatic alterations. Another time the mode is still B Aeolian - but there are no alternations at this time! Why? Because K2 can't keep track of its own thoughts, that's why.&lt;/p&gt; &lt;p&gt;Unfortunately, &lt;strong&gt;GLM 4.5&lt;/strong&gt; is not much better:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ebu41rbcpbmf1.png?width=971&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ea0fa8df0adf5e63242ec322d0e52619634faea"&gt;https://preview.redd.it/ebu41rbcpbmf1.png?width=971&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ea0fa8df0adf5e63242ec322d0e52619634faea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The pizzicato triad is B major chord. GLM doesn't realize it because it just can't count the distance between the notes.&lt;/p&gt; &lt;p&gt;Sadly, it seems that even the most popular open source models today are currently far behind DeepSeek. They may be not worse in some domains (like programming or creative writing), but for general purposes, they are more likely to trail behind the whale than to keep up.&lt;/p&gt; &lt;h1&gt;Conclusions, implications and discussion&lt;/h1&gt; &lt;p&gt;Today we learnt how to evaluate the answers of your LLMs when you are not certain in them, how to test the general intelligence of LLMs with your own expertise, and which models are smarter. Sadly, tests like this discover that some models, regardless how much we love or hate them, are really just better than others, and so far, open weight models are not those that are generally better.&lt;/p&gt; &lt;p&gt;There is, however, very good probability that the open source scene will soon catch up to the frontier. Aside of the cult following of DeepSeek, I can tell that its performance on a number of difficult tasks already approaches that of frontier models. Music analysis can be a non-trivial problem if it requires to utilize knowledge that even most musicians don't care about. To give an idea how difficult it may be for a LLM, I also gave this task to ChatGPT with different reasoning levels, from nano to high, and so far, it solves it successfully in high reasoning mode only. The fact that Deepseek comes close enough as V3.1 already tells something about its capabilities, and I think that it is very likely to grow much, much more in the following releases.&lt;/p&gt; &lt;p&gt;However, if DeepSeek or any other OSS model won't catch up in any near future, it will be disappointing given how far the progress has led us as we won't be okay about leaving it up to the corporations whose interests do not necessarily align with ours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive-Shift6641"&gt; /u/Massive-Shift6641 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4rmk7/if_youre_not_sure_if_your_llm_is_right_do_this_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4rmk7/if_youre_not_sure_if_your_llm_is_right_do_this_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4rmk7/if_youre_not_sure_if_your_llm_is_right_do_this_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T10:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4tvku</id>
    <title>MMLU Pro: Gpt-oss-20b and Gemma3-27b-it-qat on Ollama</title>
    <updated>2025-08-31T12:50:47+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For my curiosity, I ran the full benchmark to compare Gemma3-27B (QAT) and GPT-OSS-20B (MXFP4) on Ollama. Rather than the official 5-run average, this is just single run.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama v0.11.7&lt;/li&gt; &lt;li&gt;GPT-OSS with the latest template fix and the medium reasoning effort&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The tests took about a week on my M3 Max.&lt;/p&gt; &lt;p&gt;It's interesting that Gemma did better on social science like law, philosophy, psychology. Maybe GPT-OSS did better at natural science because it's better at math.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;overall&lt;/th&gt; &lt;th&gt;biology&lt;/th&gt; &lt;th&gt;business&lt;/th&gt; &lt;th&gt;chemistry&lt;/th&gt; &lt;th&gt;computer science&lt;/th&gt; &lt;th&gt;economics&lt;/th&gt; &lt;th&gt;engineering&lt;/th&gt; &lt;th&gt;health&lt;/th&gt; &lt;th&gt;history&lt;/th&gt; &lt;th&gt;law&lt;/th&gt; &lt;th&gt;math&lt;/th&gt; &lt;th&gt;philosophy&lt;/th&gt; &lt;th&gt;physics&lt;/th&gt; &lt;th&gt;psychology&lt;/th&gt; &lt;th&gt;other&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemma3&lt;/td&gt; &lt;td&gt;61.12&lt;/td&gt; &lt;td&gt;79.36&lt;/td&gt; &lt;td&gt;68.69&lt;/td&gt; &lt;td&gt;59.45&lt;/td&gt; &lt;td&gt;62.20&lt;/td&gt; &lt;td&gt;72.04&lt;/td&gt; &lt;td&gt;39.22&lt;/td&gt; &lt;td&gt;67.36&lt;/td&gt; &lt;td&gt;57.74&lt;/td&gt; &lt;td&gt;39.60&lt;/td&gt; &lt;td&gt;68.02&lt;/td&gt; &lt;td&gt;55.71&lt;/td&gt; &lt;td&gt;60.51&lt;/td&gt; &lt;td&gt;72.68&lt;/td&gt; &lt;td&gt;60.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPT-OSS&lt;/td&gt; &lt;td&gt;70.24&lt;/td&gt; &lt;td&gt;83.26&lt;/td&gt; &lt;td&gt;78.96&lt;/td&gt; &lt;td&gt;77.47&lt;/td&gt; &lt;td&gt;78.78&lt;/td&gt; &lt;td&gt;78.44&lt;/td&gt; &lt;td&gt;52.01&lt;/td&gt; &lt;td&gt;69.93&lt;/td&gt; &lt;td&gt;60.10&lt;/td&gt; &lt;td&gt;38.15&lt;/td&gt; &lt;td&gt;88.97&lt;/td&gt; &lt;td&gt;54.31&lt;/td&gt; &lt;td&gt;78.98&lt;/td&gt; &lt;td&gt;68.92&lt;/td&gt; &lt;td&gt;64.39&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4tvku/mmlu_pro_gptoss20b_and_gemma327bitqat_on_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4tvku/mmlu_pro_gptoss20b_and_gemma327bitqat_on_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4tvku/mmlu_pro_gptoss20b_and_gemma327bitqat_on_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T12:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4o0sw</id>
    <title>Building a BMO voice assistant with Raspberry Pi 5 — OpenAI &amp; Mistral support</title>
    <updated>2025-08-31T06:57:26+00:00</updated>
    <author>
      <name>/u/Strange-Dimension675</name>
      <uri>https://old.reddit.com/user/Strange-Dimension675</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4o0sw/building_a_bmo_voice_assistant_with_raspberry_pi/"&gt; &lt;img alt="Building a BMO voice assistant with Raspberry Pi 5 — OpenAI &amp;amp; Mistral support" src="https://preview.redd.it/64ik5rfbzamf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c15775d90e863fae68ed2abc166444fd9ad0d057" title="Building a BMO voice assistant with Raspberry Pi 5 — OpenAI &amp;amp; Mistral support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I’m a 20yo student and this is my first project. I’m building a BMO robot from scratch using a Raspberry Pi 5. This repo is the voice assistant part, but it’s also useful as a general-purpose voice assistant.&lt;/p&gt; &lt;p&gt;Key features: • Automatic switching between OpenAI and local Mistral server (script included) • Easily extensible to other APIs or custom agents • Classes interacting with APIs follow a consistent structure for simplicity&lt;/p&gt; &lt;p&gt;It’s still a hobby project and far from perfect, but I thought it could be interesting for anyone experimenting with local LLMs or voice assistants.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ivegotanheadache/BMO"&gt;https://github.com/ivegotanheadache/BMO&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Strange-Dimension675"&gt; /u/Strange-Dimension675 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/64ik5rfbzamf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4o0sw/building_a_bmo_voice_assistant_with_raspberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4o0sw/building_a_bmo_voice_assistant_with_raspberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T06:57:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4zo0a</id>
    <title>Drummer's Behemoth X 123B v2 - A creative finetune of Mistral Large 2411 that packs a punch, now better than ever for your entertainment! (and with 50% more info in the README!)</title>
    <updated>2025-08-31T16:49:25+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4zo0a/drummers_behemoth_x_123b_v2_a_creative_finetune/"&gt; &lt;img alt="Drummer's Behemoth X 123B v2 - A creative finetune of Mistral Large 2411 that packs a punch, now better than ever for your entertainment! (and with 50% more info in the README!)" src="https://external-preview.redd.it/lci6um6P0-wNteMe0vq3qeiBXe7Q5rNLEGJXrjSC4F4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fb361a7ef0f1b7c8f29357dfff067fa18ed656a" title="Drummer's Behemoth X 123B v2 - A creative finetune of Mistral Large 2411 that packs a punch, now better than ever for your entertainment! (and with 50% more info in the README!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those wondering what my finetuning goals are, please expand and read &amp;quot;Who is Drummer?&amp;quot; and &amp;quot;What are my models like?&amp;quot; in the model card.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Behemoth-X-123B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4zo0a/drummers_behemoth_x_123b_v2_a_creative_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4zo0a/drummers_behemoth_x_123b_v2_a_creative_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T16:49:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4v76j</id>
    <title>GPT-OSS 120B on a 3060Ti (25T/s!) vs 3090</title>
    <updated>2025-08-31T13:49:05+00:00</updated>
    <author>
      <name>/u/Wrong-Historian</name>
      <uri>https://old.reddit.com/user/Wrong-Historian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are some very simple benchmarks of running GPT-OSS 120B (native quant) on a 3060Ti vs a RTX3090.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;3060Ti (--n-cpu-moe 999) 8GB VRAM use: 24.85 tokens per second 3090: (--n-cpu-moe 999) 8GB VRAM use: 26.08 tokens per second 3090: (--n-cpu-moe 28) 21GB VRAM use: 30.44 tokens per second &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is for the simplest prompt &amp;quot;write a poem of 200 words&amp;quot;. Maybe at larger context there would be more differentiation between the 3060Ti and 3090 (TBD). Otherwise there is not much difference between 3060Ti and 3090 (CPU limited)&lt;/p&gt; &lt;p&gt;The system: 14900K,96GB DDR5 6800, RTX3090 on PCIe4.0x16, 3060Ti on PCIe4.0x4&lt;/p&gt; &lt;p&gt;When running all of the MOE layers on CPU, the rest of the model (attention, KV cache) etc. just fits within 8GB with full context length (-c 0). The only issue with the 3060Ti is that there still seems to be a bug in llama-cpp that prefill cache doesn't work, and my workaround for the 3090 was to use -swa-full parameter (using slightly more VRAM, running out of cuda memory on the 3060Ti with full context length...)&lt;/p&gt; &lt;p&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=1 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;~/build/llama.cpp/build-cuda/bin/llama-server \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-m $LLAMA_MODEL_DIR/gpt-oss-120b-mxfp4-00001-of-00003.gguf \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--n-cpu-moe 28 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--n-gpu-layers 999 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--threads 8 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-c 0 -fa \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--cache-reuse 256 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--jinja --reasoning-format auto \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8502 --api-key &amp;quot;dummy&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Fun thing: On the 14900K 96GB and 3090, I can run GPT-OSS 120B and Qwen3-Coder-30B-A3B-Instruct-Q8_0 &lt;strong&gt;simultaneous.&lt;/strong&gt; Eg, both models can be completely loaded and ready to go. Ofcourse when doing inference with both of them at the same time they both will slow down, but each of them separate runs at full speed (~30T/s). Amazing for just a single-GPU system!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wrong-Historian"&gt; /u/Wrong-Historian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v76j/gptoss_120b_on_a_3060ti_25ts_vs_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v76j/gptoss_120b_on_a_3060ti_25ts_vs_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v76j/gptoss_120b_on_a_3060ti_25ts_vs_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T13:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4vv0y</id>
    <title>Am I doing something wrong, or this expected, the beginning of every LLM generation I start is fast and then as it types it slows to a crawl.</title>
    <updated>2025-08-31T14:17:07+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a machine running 4x 3090's with 128 GB of RAM. I'm running gpt-oss-120b with 64k of context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My issue is this.&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;I ask the model a question, maybe &amp;quot;write a story about a rabbit named frank who fights crime&amp;quot;.&lt;/li&gt; &lt;li&gt;It answers, the beginning of the story starts at about 120 tk/s, but towards the end gets to 20 tk/s.&lt;/li&gt; &lt;li&gt;I ask it to continue the story.&lt;/li&gt; &lt;li&gt;It answers, the beginning of the response starts at about 120 tk/s, but towards the end gets to 20 tk/s.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Additional notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- I'm using LM STUDIO (easiest to quick tweak settings to see what helps/hurts)&lt;/p&gt; &lt;p&gt;- I'm utilizing flash attention, but leaving the K-cache and V-cache unchecked/unchanged as changing them to anything besides F16 has a massive performance hit.&lt;/p&gt; &lt;p&gt;- Everything is fitting into the 96 GB of VRAM including the context.&lt;/p&gt; &lt;p&gt;Am I experiencing something that's... expected?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4vv0y/am_i_doing_something_wrong_or_this_expected_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4vv0y/am_i_doing_something_wrong_or_this_expected_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4vv0y/am_i_doing_something_wrong_or_this_expected_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T14:17:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4mxrj</id>
    <title>MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max.</title>
    <updated>2025-08-31T05:50:09+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/"&gt; &lt;img alt="MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max." src="https://preview.redd.it/co3taaq1mamf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a7f592ad4e744736d9c8af891ea21008331265e" title="MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/co3taaq1mamf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T05:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4lh7s</id>
    <title>GPT-OSS-120B on Single RTX 6000 PRO</title>
    <updated>2025-08-31T04:24:33+00:00</updated>
    <author>
      <name>/u/Ill_Recipe7620</name>
      <uri>https://old.reddit.com/user/Ill_Recipe7620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting 120 token/second with a single RTX 6000 PRO. It took 33 seconds to crank out the following answer after thinking for just 3 seconds.&lt;/p&gt; &lt;p&gt;If people aren't impressed with this model then they cannot be impressed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &amp;quot;compare the heat transfer coefficient of liquid water at a high pressure to steam at these conditions and IGNORE the boiling part only convection, search the web if you ned any information&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Response:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Heat‑transfer‑coefficient ( h ) comparison – liquid water vs. steam at the same high pressure&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;(boiling/phase‑change is deliberately excluded – only convective transport is considered)&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;1. What “the same conditions” really means&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantity&lt;/th&gt; &lt;th align="left"&gt;Typical high‑pressure case&lt;/th&gt; &lt;th align="left"&gt;Why it matters for h&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Pressure&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5–20 MPa (≈50–200 bar) – common in super‑critical boilers, geothermal wells, high‑pressure reactors&lt;/td&gt; &lt;td align="left"&gt;Raises density of both phases, compresses the gas, and moves steam toward the liquid side of the phase diagram.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Bulk temperature&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;350–550 °C (well above the normal boiling point of water)&lt;/td&gt; &lt;td align="left"&gt;Determines fluid properties (µ, k, ρ, Cp, Pr) that appear in the dimensionless groups governing convection.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Flow regime&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Forced convection in a pipe or channel, fully developed turbulent flow (Re &amp;gt; 10 000) – the regime that gives the largest, most useful h values.&lt;/td&gt; &lt;td align="left"&gt;The Nusselt number correlations (Dittus‑Boelter, Gnielinski, Sieder‑Tate…) are based on turbulent forced flow.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Geometry&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Typically a circular tube, D = 10–30 mm; length long enough for fully‑developed flow.&lt;/td&gt; &lt;td align="left"&gt;Geometry enters the Reynolds number and the hydraulic diameter used in correlations.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mass‑flow rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Chosen so that the &lt;strong&gt;mean velocity&lt;/strong&gt; &lt;em&gt;U&lt;/em&gt; is the same for both fluids (e.g., 2 m s⁻¹). This isolates the effect of the fluid properties.&lt;/td&gt; &lt;td align="left"&gt;With the same U, the only differences in h arise from ρ, µ, k, Cp, Pr.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;h1&gt;2. Governing dimensionless groups&lt;/h1&gt; &lt;p&gt;For turbulent forced convection in a smooth pipe the Nusselt number is usually expressed as&lt;/p&gt; &lt;p&gt;where&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Re&lt;/em&gt;=&lt;em&gt;μρUD&lt;/em&gt;​ – Reynolds number (inertia/viscous)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Pr&lt;/em&gt;=&lt;em&gt;kcp&lt;/em&gt;​&lt;em&gt;μ&lt;/em&gt;​ – Prandtl number (momentum/thermal diffusivity)&lt;/li&gt; &lt;li&gt;&lt;em&gt;Nu&lt;/em&gt;=&lt;em&gt;khD&lt;/em&gt;​ – Nusselt number (convective/ conductive)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The constants &lt;em&gt;C&lt;/em&gt;,&lt;em&gt;m&lt;/em&gt;,&lt;em&gt;n&lt;/em&gt; depend on the correlation (Dittus‑Boelter, Gnielinski, etc.).&lt;br /&gt; The heat‑transfer coefficient follows directly:&lt;/p&gt; &lt;p&gt;Thus &lt;strong&gt;differences in h&lt;/strong&gt; arise from the three property groups (&lt;em&gt;ρ&lt;/em&gt;,&lt;em&gt;μ&lt;/em&gt;,&lt;em&gt;k&lt;/em&gt;,&lt;em&gt;cp&lt;/em&gt;​) that appear in &lt;em&gt;Re&lt;/em&gt;, &lt;em&gt;Pr&lt;/em&gt; and the explicit factor &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;3. Thermophysical properties at high pressure&lt;/h1&gt; &lt;p&gt;Below is a representative data set for &lt;strong&gt;10 MPa (≈100 bar)&lt;/strong&gt; at &lt;strong&gt;500 °C&lt;/strong&gt; (the pressure is the same for both fluids; the temperature is above the critical temperature of water, 374 °C, so both fluids are in the &lt;em&gt;supercritical&lt;/em&gt; regime, but we still distinguish “liquid‑like” (dense) and “steam‑like” (less dense) states).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Property&lt;/th&gt; &lt;th align="left"&gt;Liquid‑like water (ρ ≈ 650 kg m⁻³)&lt;/th&gt; &lt;th align="left"&gt;Steam‑like water (ρ ≈ 120 kg m⁻³)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Density, ρ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;650 kg m⁻³&lt;/td&gt; &lt;td align="left"&gt;120 kg m⁻³&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Dynamic viscosity, µ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.4 × 10⁻⁴ Pa s&lt;/td&gt; &lt;td align="left"&gt;1.6 × 10⁻⁴ Pa s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Thermal conductivity, k&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.70 W m⁻¹ K⁻¹&lt;/td&gt; &lt;td align="left"&gt;0.45 W m⁻¹ K⁻¹&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Specific heat, cₚ&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2.1 kJ kg⁻¹ K⁻¹&lt;/td&gt; &lt;td align="left"&gt;2.4 kJ kg⁻¹ K⁻¹&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Prandtl number, Pr = cₚµ/k&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;≈ 7.3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;≈ 0.85&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;em&gt;Sources&lt;/em&gt;: NIST REFPROP 10.0, IAPWS‑95 formulation, extrapolated to 10 MPa and 500 °C.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What the numbers tell us&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Density&lt;/strong&gt; – liquid‑like water is ~5 × denser, giving a Reynolds number ~5 × larger for the same velocity and pipe diameter.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Viscosity&lt;/strong&gt; – steam is slightly less viscous, which &lt;em&gt;increases&lt;/em&gt; its Reynolds number a little, but the dominant factor is density.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thermal conductivity&lt;/strong&gt; – liquid water conducts heat ≈ 55 % better.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prandtl number&lt;/strong&gt; – liquid water has a &lt;strong&gt;high Pr&lt;/strong&gt; (≈ 7) while steam has a &lt;strong&gt;low Pr&lt;/strong&gt; (≈ 0.85). A high Pr means the thermal boundary layer is thinner than the velocity boundary layer, boosting h.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Quantitative h‑comparison (forced turbulent flow)&lt;/h1&gt; &lt;h1&gt;4.1. Chosen flow parameters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Pipe diameter &lt;em&gt;D&lt;/em&gt;=0.020m (20 mm)&lt;/li&gt; &lt;li&gt;Mean velocity &lt;em&gt;U&lt;/em&gt;=2.0m s−1 – typical for high‑pressure steam turbines or heat‑exchanger tubes.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4.2. Reynolds numbers&lt;/h1&gt; &lt;p&gt;Both are &lt;strong&gt;turbulent&lt;/strong&gt; (Re &amp;gt; 4 000).&lt;/p&gt; &lt;h1&gt;4.3. Nusselt numbers (Gnielinski correlation – valid for 0.1&amp;lt;Pr&amp;lt;2000 and 3000&amp;lt;Re&amp;lt;5×106)&lt;/h1&gt; &lt;p&gt;Carrying out the calculation:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Fluid&lt;/th&gt; &lt;th align="left"&gt;Friction factor &lt;em&gt;f&lt;/em&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;em&gt;Nu&lt;/em&gt; (Gnielinski)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Liquid‑like water&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;f&lt;/em&gt;≈0.014&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Nu&lt;/em&gt;liq​≈1.0×103&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Steam‑like water&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;f&lt;/em&gt;≈0.028&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Nu&lt;/em&gt;steam​≈3.1×102&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;4.4. Heat‑transfer coefficient&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In other words, &lt;em&gt;under the same pressure, temperature, geometry and bulk velocity, liquid water transfers heat by convection roughly five times more efficiently than steam&lt;/em&gt;.&lt;/p&gt; &lt;h1&gt;5. Why the liquid is so much better (physics insight)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Aspect&lt;/th&gt; &lt;th align="left"&gt;Liquid water (high‑P)&lt;/th&gt; &lt;th align="left"&gt;Steam (high‑P)&lt;/th&gt; &lt;th align="left"&gt;Effect on h&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Density (ρ)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5× larger → larger momentum → higher Re → thinner velocity boundary layer&lt;/td&gt; &lt;td align="left"&gt;Low ρ → smaller Re → thicker velocity boundary layer&lt;/td&gt; &lt;td align="left"&gt;Higher Re → higher Nu&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Thermal conductivity (k)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;55 % larger → more conductive path across the thermal BL&lt;/td&gt; &lt;td align="left"&gt;Smaller k → poorer conduction&lt;/td&gt; &lt;td align="left"&gt;Directly multiplies Nu in h = Nu·k/D&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Prandtl number (Pr)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;≈ 7 (thermal diffusion slower than momentum) → thermal BL &lt;strong&gt;much thinner&lt;/strong&gt; than velocity BL&lt;/td&gt; &lt;td align="left"&gt;≈ 0.85 (thermal diffusion faster) → thermal BL comparable to or thicker than velocity BL&lt;/td&gt; &lt;td align="left"&gt;Thin thermal BL → larger temperature gradient at wall → larger h&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Viscosity (µ)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Slightly higher, but the density effect dominates the Reynolds number&lt;/td&gt; &lt;td align="left"&gt;Lower µ but low ρ keeps Re low&lt;/td&gt; &lt;td align="left"&gt;Net: liquid still gets larger Re&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Compressibility&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Near‑incompressible; flow properties change little with pressure&lt;/td&gt; &lt;td align="left"&gt;Gas‑like compressibility reduces effective velocity and thickens BL&lt;/td&gt; &lt;td align="left"&gt;Keeps liquid’s convective performance high&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;6. Influence of pressure itself&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Pressure change&lt;/th&gt; &lt;th align="left"&gt;Liquid water&lt;/th&gt; &lt;th align="left"&gt;Steam&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Increasing pressure&lt;/strong&gt; (while staying sub‑critical for the liquid)&lt;/td&gt; &lt;td align="left"&gt;ρ ↑, µ ↓ (slightly) → &lt;strong&gt;Re ↑&lt;/strong&gt;; k ↑ modestly; Pr ↑ → &lt;strong&gt;h rises&lt;/strong&gt; (often 10‑30 % per 100 bar).&lt;/td&gt; &lt;td align="left"&gt;ρ ↑ dramatically (≈ 5–10× as you approach the critical point), µ ↓, k ↑, Pr ↑ from ~0.7 toward 1‑2. When steam becomes &lt;em&gt;dense&lt;/em&gt; (near‑critical), its Re and Pr increase enough that &lt;strong&gt;h can climb 2–4×&lt;/strong&gt;. However, even at 20 MPa the steam’s Pr never reaches the ~7 of liquid water, so &lt;em&gt;liquid still beats steam&lt;/em&gt; unless you cross into the &lt;em&gt;supercritical fluid&lt;/em&gt; regime where the distinction blurs.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Very near the critical point&lt;/strong&gt; (≈ 22 MPa, 374 °C)&lt;/td&gt; &lt;td align="left"&gt;Water is a &lt;em&gt;supercritical fluid&lt;/em&gt; with properties intermediate between liquid and gas; h can be &lt;strong&gt;10⁴–2×10⁴ W m⁻² K⁻¹&lt;/strong&gt; – comparable to high‑pressure liquid.&lt;/td&gt; &lt;td align="left"&gt;Same supercritical fluid – the “steam” label loses meaning; h values become similar to the liquid side.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Take‑away:&lt;/strong&gt; Pressure improves h for both phases, but because liquid water already starts with a much higher baseline, the &lt;em&gt;ratio&lt;/em&gt; &lt;em&gt;h&lt;/em&gt;liq​/&lt;em&gt;h&lt;/em&gt;steam​ stays in the &lt;strong&gt;3–6 range&lt;/strong&gt; for most engineering‑relevant pressures (5–20 MPa). Only when you go extremely close to the critical point does the gap shrink dramatically.&lt;/p&gt; &lt;h1&gt;7. Practical implications&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Application&lt;/th&gt; &lt;th align="left"&gt;Typical operating pressure &amp;amp; temperature&lt;/th&gt; &lt;th align="left"&gt;Which phase gives the larger convective h?&lt;/th&gt; &lt;th align="left"&gt;Design note&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;High‑pressure water‑to‑steam heat exchangers (e.g., nuclear steam generators)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;15 MPa, 300 °C liquid → 15 MPa, 350 °C steam&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;Liquid side&lt;/em&gt; (primary) has &lt;em&gt;h&lt;/em&gt; 4–6× larger than the steam side.&lt;/td&gt; &lt;td align="left"&gt;The steam side is usually the limiting resistance; designers boost its turbulence (twisted tapes, ribbed tubes) to compensate.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Supercritical CO₂ heat recovery (analogy)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.5 MPa, 500 °C (fluid is supercritical)&lt;/td&gt; &lt;td align="left"&gt;The fluid behaves like a dense gas; h is &lt;strong&gt;lower&lt;/strong&gt; than a comparable liquid but &lt;strong&gt;higher&lt;/strong&gt; than low‑pressure CO₂ gas.&lt;/td&gt; &lt;td align="left"&gt;Use high‑Re flow (large velocities, small hydraulic diameters) to raise h.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Geothermal wells (high‑P hot water, 10 MPa, 250 °C)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fluid is liquid‑like&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Liquid&lt;/strong&gt; h ≈ 2–3 × 10⁴ W m⁻² K⁻¹, far exceeding what the same well would deliver if the fluid flashed to steam.&lt;/td&gt; &lt;td align="left"&gt;Maintaining liquid conditions (prevent flashing) maximises convective heat extraction.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Supercritical water oxidation reactors (SCWO, &amp;gt; 25 MPa, &amp;gt; 400 °C)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Water is supercritical (no distinct liquid/steam)&lt;/td&gt; &lt;td align="left"&gt;h values of &lt;strong&gt;10⁴–2×10⁴ W m⁻² K⁻¹&lt;/strong&gt;; similar to high‑pressure liquid water.&lt;/td&gt; &lt;td align="left"&gt;The high h is a key advantage for rapid heat removal in exothermic oxidation.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;8. Summary (concise bullet list)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Liquid water at high pressure&lt;/strong&gt; has &lt;strong&gt;much larger density&lt;/strong&gt; and &lt;strong&gt;higher Prandtl number&lt;/strong&gt; than steam at the same pressure/temperature.&lt;/li&gt; &lt;li&gt;These property differences raise the Reynolds number and shrink the thermal boundary layer, leading to &lt;strong&gt;higher Nusselt numbers&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Quantitatively, for a 20 mm tube, 2 m s⁻¹ flow, 10 MPa and 500 °C:→ &lt;strong&gt;Liquid water convects roughly 5 × faster.&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;h_liquid ≈ 3–4 × 10⁴ W m⁻² K⁻¹&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;h_steam ≈ 6–8 × 10³ W m⁻² K⁻¹&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Raising pressure improves h for &lt;strong&gt;both&lt;/strong&gt; fluids, but the &lt;strong&gt;ratio stays between 3 and 6&lt;/strong&gt; over the usual engineering range (5–20 MPa).&lt;/li&gt; &lt;li&gt;Near the critical point the distinction blurs; steam becomes a dense supercritical fluid and its h rises dramatically, approaching that of the liquid side.&lt;/li&gt; &lt;li&gt;In design practice the &lt;strong&gt;steam side is usually the thermal bottleneck&lt;/strong&gt; in high‑pressure heat exchangers; engineers therefore add turbulence promoters, increase velocity, or operate at higher pressure to mitigate the lower h.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Bottom line&lt;/h1&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Recipe7620"&gt; /u/Ill_Recipe7620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4lh7s/gptoss120b_on_single_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T04:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4n9yx</id>
    <title>56GB VRAM achieved: Gigabyte 5090 Windforce OC (65mm width!!) + Galax HOF 3090 barely fit but both running x8/x8 and I just really want to share :)</title>
    <updated>2025-08-31T06:10:19+00:00</updated>
    <author>
      <name>/u/Gerdel</name>
      <uri>https://old.reddit.com/user/Gerdel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4n9yx/56gb_vram_achieved_gigabyte_5090_windforce_oc/"&gt; &lt;img alt="56GB VRAM achieved: Gigabyte 5090 Windforce OC (65mm width!!) + Galax HOF 3090 barely fit but both running x8/x8 and I just really want to share :)" src="https://preview.redd.it/k0im592roamf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf3307b2439da45726bdc7e079057842ef0c0904" title="56GB VRAM achieved: Gigabyte 5090 Windforce OC (65mm width!!) + Galax HOF 3090 barely fit but both running x8/x8 and I just really want to share :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally planned to put the 3090 in a lower x4 slot, but it wouldn't fit to PSU case clearance. Builder put the 3090 in the upper x16 slot instead, and the 5090 just barely fit in the second x16.&lt;br /&gt; Both cards running x8/x8 rather than the original planned x16/x4 configuration - but I'm cool with it. The 3090 fans are literally 1mm from the backplate of the 5090 yet the thermals are fine with 7x 140mm case fans. After the anxiety of my dream build I'm not doing heavy testing yet, but now looking to get into serious fine-tuning pretty soon. &lt;/p&gt; &lt;p&gt;I've the developer of a local AI app designed for dual GPU systems (&lt;a href="https://github.com/boneylizard/Eloquent"&gt;https://github.com/boneylizard/Eloquent&lt;/a&gt;) and I've found that with expanded capabilities comes expanded imagination. Haven't done a git push in a while and there's an issue I really need to get around to addressing, but that explains the build. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gerdel"&gt; /u/Gerdel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0im592roamf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4n9yx/56gb_vram_achieved_gigabyte_5090_windforce_oc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4n9yx/56gb_vram_achieved_gigabyte_5090_windforce_oc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T06:10:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4w0tq</id>
    <title>VibeVoice quantized to 4 bit and 8 bit with some code to run it...</title>
    <updated>2025-08-31T14:23:31+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was playing around with VibeVoice and saw other people were looking for ways to run it on less than 24gb vram so I did a little fiddling.&lt;/p&gt; &lt;p&gt;Here's a huggingface I put up with the 4 and 8 bit pre-quantized models, getting them to sizes that might be able to be crammed (barely) on an 8 gb vram and 12 gb vram card, respectively (you might have to run headless to fit that 7b in 8gb vram, it's really cutting it close, but both should run -fine- in a 12gb+ card).&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DevParker/VibeVoice7b-low-vram"&gt;VibeVoice 4 bit and 8 bit Quantized Models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also included some code to test them out, or to quantize them yourself, or if you're just curious how I did this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Deveraux-Parker/VibeVoice-Low-Vram"&gt;https://github.com/Deveraux-Parker/VibeVoice-Low-Vram&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I haven't bothered making a Gradio for this or anything like that, but there's some python files in there to test inference and it can be bolted into the existing VibeVoice gradio easily.&lt;/p&gt; &lt;p&gt;A quick test:&lt;br /&gt; &lt;a href="https://vocaroo.com/1lPin5ISa2f5"&gt;https://vocaroo.com/1lPin5ISa2f5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4w0tq/vibevoice_quantized_to_4_bit_and_8_bit_with_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4w0tq/vibevoice_quantized_to_4_bit_and_8_bit_with_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4w0tq/vibevoice_quantized_to_4_bit_and_8_bit_with_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T14:23:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4szk5</id>
    <title>Deepseek r1 671b on a $500 server. Interesting lol but you guessed it. 1 tps. If only we can get hardware that cheap to produce 60 tps at a minimum.</title>
    <updated>2025-08-31T12:06:48+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://youtu.be/t_hh2-KG6Bw?feature=shared"&gt;https://youtu.be/t_hh2-KG6Bw?feature=shared&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4szk5/deepseek_r1_671b_on_a_500_server_interesting_lol/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4szk5/deepseek_r1_671b_on_a_500_server_interesting_lol/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4szk5/deepseek_r1_671b_on_a_500_server_interesting_lol/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T12:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4pt0x</id>
    <title>Top-k 0 vs 100 on GPT-OSS-120b</title>
    <updated>2025-08-31T08:52:17+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4pt0x/topk_0_vs_100_on_gptoss120b/"&gt; &lt;img alt="Top-k 0 vs 100 on GPT-OSS-120b" src="https://preview.redd.it/1p4r0n9gibmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a722fa2967b2e81a9bf33eb4c23859a3dd096ec7" title="Top-k 0 vs 100 on GPT-OSS-120b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using a M4 Max Macbook Pro 128 GB I am comparing the speed boost of setting top-k to 100. OpenAI says to set top-k to 0 while Unsloth proposes that one could try 100 instead. &lt;/p&gt; &lt;p&gt;Top-k 0 means use the full vocabulary of the model. Any other value specifies that we should only consider the top k most likely tokens of the vocabulary. If the value is too small, we might get a worse response from the model. Typical values for top-k seems to be 20-40 and 100 would be considered a relatively large value. By using a large value we aim to get the same result as top-k 0 but faster.&lt;/p&gt; &lt;p&gt;My test shows a very substantial gain by using top-k 100.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1p4r0n9gibmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4pt0x/topk_0_vs_100_on_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4pt0x/topk_0_vs_100_on_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T08:52:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4dsym</id>
    <title>128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow.</title>
    <updated>2025-08-30T21:59:06+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"&gt; &lt;img alt="128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow." src="https://preview.redd.it/ld3rckf8b8mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad5697c140ab4e2a23aa8df66f12cc106ee3b653" title="128GB GDDR6, 3PFLOP FP8, Tb/s of interconnect, $6000 total. Build instructions/blog tomorrow." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ld3rckf8b8mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4dsym/128gb_gddr6_3pflop_fp8_tbs_of_interconnect_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T21:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4rj8v</id>
    <title>Fine Tuning Gemma 3 270M to talk Bengaluru!</title>
    <updated>2025-08-31T10:43:24+00:00</updated>
    <author>
      <name>/u/samairtimer</name>
      <uri>https://old.reddit.com/user/samairtimer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;I trained Gemma 3 270M to talk in Bengaluru Slang !&lt;/h1&gt; &lt;p&gt;Okay, you may have heard or read about it by now. Why did Google develop a &lt;a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/"&gt;270-million-parameter model&lt;/a&gt;?&lt;/p&gt; &lt;p&gt;While there are a ton of discussions on the topic, it's interesting to note that now we have a model that can be fully fine-tuned to your choice, without the need to spend a significant amount of money on GPUs.&lt;/p&gt; &lt;p&gt;You can now tune all the layers of the model and make it unlearn things during the process, a big dream of many LLM enthusiasts like me.&lt;/p&gt; &lt;p&gt;So what did I do? I trained Gemma 270M model, to talk back in the famous Bengaluru slang! I am one of those guys who has succumbed to it (in a good way) in the last decade living in Bengaluru, so much so that I found it interesting to train AI on it!!&lt;/p&gt; &lt;p&gt;You can read more on my Substack - &lt;a href="https://samairtimer.substack.com/p/fine-tuning-gemma-3-270m-to-talk"&gt;https://samairtimer.substack.com/p/fine-tuning-gemma-3-270m-to-talk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samairtimer"&gt; /u/samairtimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4rj8v/fine_tuning_gemma_3_270m_to_talk_bengaluru/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4rj8v/fine_tuning_gemma_3_270m_to_talk_bengaluru/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4rj8v/fine_tuning_gemma_3_270m_to_talk_bengaluru/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T10:43:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4wf0j</id>
    <title>Open-Sourcing Medical LLM which Scores 85.8% on USMLE-Style Questions, Beating Similar Models - 𝙽𝙴𝙴𝚃𝙾–𝟷.𝟶–𝟾𝙱 🚀</title>
    <updated>2025-08-31T14:39:22+00:00</updated>
    <author>
      <name>/u/False_Mountain_7289</name>
      <uri>https://old.reddit.com/user/False_Mountain_7289</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wf0j/opensourcing_medical_llm_which_scores_858_on/"&gt; &lt;img alt="Open-Sourcing Medical LLM which Scores 85.8% on USMLE-Style Questions, Beating Similar Models - 𝙽𝙴𝙴𝚃𝙾–𝟷.𝟶–𝟾𝙱 🚀" src="https://preview.redd.it/rcciowx66dmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edef201361f7e43cd16bc481e9d389945c77c84c" title="Open-Sourcing Medical LLM which Scores 85.8% on USMLE-Style Questions, Beating Similar Models - 𝙽𝙴𝙴𝚃𝙾–𝟷.𝟶–𝟾𝙱 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent the last 2 months building something that might change how students prepare USMLE/UKMLE/NEET-PG forever. Meet &lt;strong&gt;Neeto-1.0-8B&lt;/strong&gt; - a specialized, 8-billion-parameter biomedical LLM fine-tuned on a curated dataset of over 500K items. Our goal was clear: create a model that could not only assist with medical exam prep (NEET-PG, USMLE, UKMLE) but also strengthen factual recall and clinical reasoning for practitioners and the model itself outperforming general models by 25% on medical datasets.&lt;/p&gt; &lt;p&gt;Docs + model on Hugging Face 👉 &lt;a href="https://huggingface.co/S4nfs/Neeto-1.0-8b"&gt;https://huggingface.co/S4nfs/Neeto-1.0-8b&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;🤯 The Problem&lt;/h1&gt; &lt;p&gt;While my company was preparing a research paper on USMLE/UKMLE/NEET-PG and medical science, I realized existing AI assistants couldn't handle medical reasoning. They'd hallucinate drug interactions, miss diagnostic nuances, and provide dangerous oversimplifications. So I decided to build something better at my organization.&lt;/p&gt; &lt;h1&gt;🚀 The Breakthrough&lt;/h1&gt; &lt;p&gt;After 1 month of training on more than &lt;strong&gt;410,000+ medical samples&lt;/strong&gt; (MedMCQA, USMLE questions, clinical cases) and private datasets from our my organization's platform medicoplasma[dot]com, we achieved:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Score&lt;/th&gt; &lt;th align="left"&gt;outperforms&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MedQA Accuracy&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;85.8%&lt;/td&gt; &lt;td align="left"&gt;+87% vs general AI&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;PubMedQA&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;79.0%&lt;/td&gt; &lt;td align="left"&gt;+23% vs other medical AIs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Response Time&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&amp;lt;2 seconds&lt;/td&gt; &lt;td align="left"&gt;Real-time clinical use&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;🔧 Technical Deep Dive&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Llama-3.1-8B with full-parameter fine-tuning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: 8×H200 GPUs using FSDP (Fully Sharded Data Parallel)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization&lt;/strong&gt;: 4-bit GGUF for consumer hardware compatibility&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's how we compare to other models:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;MedQA Score&lt;/th&gt; &lt;th align="left"&gt;Medical Reasoning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Neeto-1.0-8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;85.8%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Expert-level&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama-3-8B-Instruct&lt;/td&gt; &lt;td align="left"&gt;62.3%&lt;/td&gt; &lt;td align="left"&gt;Intermediate&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OpenBioLM-8B&lt;/td&gt; &lt;td align="left"&gt;59.1%&lt;/td&gt; &lt;td align="left"&gt;Basic&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Yesterday, I watched a friend use Neeto to diagnose a complex case of &lt;strong&gt;ureteral calculus with aberrant renal artery anatomy&lt;/strong&gt; - something that would take hours in textbooks. Neeto provided the differential diagnosis in &lt;strong&gt;1.7 seconds&lt;/strong&gt; with 92% confidence.&lt;/p&gt; &lt;h1&gt;💻 How to Use It Right Now&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# 1. Install vLLM pip install vllm # 2. Run the medical AI server vllm serve S4nfs/Neeto-1.0-8b # 3. Ask medical questions curl http://localhost:8000/v1/completions -H &amp;quot;Content-Type: application/json&amp;quot; -d '{ &amp;quot;model&amp;quot;: &amp;quot;S4nfs/Neeto-1.0-8b&amp;quot;, &amp;quot;prompt&amp;quot;: &amp;quot;A 55-year-old male with flank pain and hematuria...&amp;quot;, &amp;quot;max_tokens&amp;quot;: 4096, &amp;quot;temperature&amp;quot;: 0.7 }' &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;🌟 What Makes This Different&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Cultural Context&lt;/strong&gt;: Optimized for advanced healthcare system and terminology&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real Clinical Validation&lt;/strong&gt;: Tested by 50+ doctors across global universities&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accessibility&lt;/strong&gt;: Runs on single GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparency&lt;/strong&gt;: Full training data and methodology disclosed (2 datasets are private as i am seeking permission from my org to release)&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;📈 Benchmark Dominance&lt;/h1&gt; &lt;p&gt;We're outperforming every similar-sized model across 7 medical benchmarks, (see docs, for full results):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MedMCQA: 66.2% (+18% over competitors)&lt;/li&gt; &lt;li&gt;MMLU Medical Genetics: 87.1% (Best in class)&lt;/li&gt; &lt;li&gt;Clinical Knowledge: 79.4% (Near-specialist level)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upvote &amp;amp; like the model for medical research. Feedback, criticism &amp;amp; collaborations welcome! 🤗&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False_Mountain_7289"&gt; /u/False_Mountain_7289 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rcciowx66dmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wf0j/opensourcing_medical_llm_which_scores_858_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wf0j/opensourcing_medical_llm_which_scores_858_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T14:39:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n46ify</id>
    <title>Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)</title>
    <updated>2025-08-30T16:52:00+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt; &lt;img alt="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" src="https://preview.redd.it/1wl79kpjs6mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967356c2a208d26c7d657a1b535e795c0332f304" title="Finally China entering the GPU market to destroy the unchallenged monopoly abuse. 96 GB VRAM GPUs under 2000 USD, meanwhile NVIDIA sells from 10000+ (RTX 6000 PRO)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wl79kpjs6mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T16:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4garp</id>
    <title>Creating the brain behind dumb models</title>
    <updated>2025-08-30T23:55:53+00:00</updated>
    <author>
      <name>/u/ChristopherLyon</name>
      <uri>https://old.reddit.com/user/ChristopherLyon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"&gt; &lt;img alt="Creating the brain behind dumb models" src="https://external-preview.redd.it/MWFtYzE2NDZ3OG1mMdkNi0cONUhOlI53Ev_Op4Ah4qsUiMoYY-mZ8OWM03HP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d27b6fcdb4c16b3d3a61e3f5d15d95327d5ebe8" title="Creating the brain behind dumb models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been fascinated by model intelligence enhancement and trying to deploy super tiny models like gemma3:270m in niche domains with high levels of success... &lt;/p&gt; &lt;p&gt;My latest implementation is a &amp;quot;community nested&amp;quot; relational graph knowledgebase pipeline that gives both top down context on knowledge sub-domains, but also a traditional bottom-up search (essentially regular semantic embedding cosine similarity) with a traversal mechanism to grab context from nodes that are not semantically similar but still referentially linked. Turns out there is a LOT of context that does not get picked up through regular embedding based RAG.&lt;/p&gt; &lt;p&gt;I created a quick front-end with nextjs and threejs to visualize how my knowledge base hangs together, and to quickly identify if I had a high level of overall coherence (i.e. number of isolated/disconnected clusters) and to get a better feeling for what context the LLM loads into memory for any given user query in real time (I'm a visual learner)&lt;/p&gt; &lt;p&gt;The KB you can see in the video is from a single 160 page PDF on Industrial Design, taking you anywhere from notable people, material science to manufacturing techniques. I was pleasantly surprised to see that the node for &amp;quot;ergonomics&amp;quot; was by far the most linked and overall strongly referenced in the corpus - essentially linking the &amp;quot;human factor&amp;quot; to some significant contribution to great product design.&lt;/p&gt; &lt;p&gt;If anyone hasn't gotten into graph based retrieval augmented generation I found the best resource and starter to be from Microsoft: &lt;a href="https://github.com/microsoft/graphrag"&gt;https://github.com/microsoft/graphrag&lt;/a&gt;&lt;/p&gt; &lt;p&gt;^ pip install graphrag and use the init and index commands to create your first graph in minutes.&lt;/p&gt; &lt;p&gt;Anyone else been in my shoes and already know what the NEXT step will be? Let me know.&lt;/p&gt; &lt;p&gt;It's 2 am so a quick video shot on my mobile is all I have right now, but I can't sleep thinking about this so thought I'd post what I have. I need to work some more on it and add the local LLM interface for querying the KB through the front end, but I don't mind open sourcing it if anyone is interested. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChristopherLyon"&gt; /u/ChristopherLyon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bwyft136w8mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4garp/creating_the_brain_behind_dumb_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T23:55:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4v0ql</id>
    <title>LongCat-Flash-Chat 560B MoE</title>
    <updated>2025-08-31T13:41:09+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v0ql/longcatflashchat_560b_moe/"&gt; &lt;img alt="LongCat-Flash-Chat 560B MoE" src="https://preview.redd.it/4pfegt9ezcmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=611d4a68a425489022dacb28fc5bd82d9690c441" title="LongCat-Flash-Chat 560B MoE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LongCat-Flash-Chat is a powerful and efficient language model with an innovative Mixture-of-Experts (MoE) architecture. It contains 560 billion total parameters but dynamically activates only 18.6 to 31.3 billion parameters (averaging ~27B) per token, optimizing for both performance and efficiency. It is designed to be a non-thinking foundation model with exceptional strengths in agentic tasks.&lt;/p&gt; &lt;p&gt;Key Features * Efficient Architecture: Uses a Mixture-of-Experts (MoE) design with a &amp;quot;zero-computation experts mechanism&amp;quot; and a &amp;quot;Shortcut-connected MoE&amp;quot; to optimize for computational efficiency and communication overlap. * Robust Scaling Strategy: Employs a comprehensive framework for stable training at a massive scale, including a hyperparameter transfer strategy, a model-growth initialization mechanism, and a multi-pronged stability suite. * Advanced Training Pipeline: A multi-stage pipeline was used to imbue the model with advanced agentic behaviors, focusing on reasoning, coding, and a long context length of 128k. It also uses a multi-agent synthesis framework to create complex training tasks.&lt;/p&gt; &lt;p&gt;Evaluation Highlights&lt;/p&gt; &lt;p&gt;The model demonstrates highly competitive performance across a wide range of benchmarks. Noteworthy strengths include: * Instruction Following: Achieves high scores on benchmarks like IFEval and COLLIE. * Agentic Tool Use: Shows strong results on agent-specific benchmarks such as τ²-Bench and VitaBench. * Mathematical Reasoning: Performs competitively on a variety of math reasoning tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;License: The model is released under the MIT License.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4pfegt9ezcmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v0ql/longcatflashchat_560b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4v0ql/longcatflashchat_560b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T13:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n4wo0y</id>
    <title>The Huawei GPU is not equivalent to an RTX 6000 Pro whatsoever</title>
    <updated>2025-08-31T14:49:29+00:00</updated>
    <author>
      <name>/u/MCH_2000</name>
      <uri>https://old.reddit.com/user/MCH_2000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a response to the recent viral post taking about the “amazing” Huawei GPU offering 96 GB for “only” 2000$ when Nvidia is way more expensive. (Edit: as many in the comments section noted, the Huawei is a dual GPU setup. Depending on the specific packaging, it might not be easy to run inference at peak speed). &lt;/p&gt; &lt;p&gt;The post leaves out important context.&lt;/p&gt; &lt;h1&gt;Performance (Sparsity)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;INT8: 1,000 (2,000) TOPs vs 280 TOPs &lt;/li&gt; &lt;li&gt;FP4 w/FP32 Accumulate: 2,000 (4,000) TFLOPs vs not supported. &lt;/li&gt; &lt;li&gt;Bandwidth: 1792 GB/s vs 408 GB/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Huawei is closer to a mobile SoC than it is to a high end Nvidia dGPU. &lt;/p&gt; &lt;h1&gt;Memory&lt;/h1&gt; &lt;p&gt;The reason the Huawei GPU packs 96 GB is it’s using LPDDR4X. &lt;/p&gt; &lt;p&gt;LPDDR4X (64b) is 8 GB @ 34 GB/s&lt;/p&gt; &lt;p&gt;GDDR7 (64b) is 2-3 GB @ 256 GB/s&lt;/p&gt; &lt;p&gt;The Nvidia has a wider bus, but it doesn’t use the top GDDR7 memory bin. Regardless, Bandwidth is roughly 4.5x. And for the highly memory bound consumer inference, this will translate to 4~5x higher token/s. &lt;/p&gt; &lt;p&gt;One of the two memory technologies trades Bandwidth for capacity. And Huawei is using ancient memory technology. LP4X is outdated and there is already LP5, LP5X, LP5T, LP6 with far higher capacity and bandwidth. Huawei can’t use them because of the entity list. &lt;/p&gt; &lt;p&gt;For the record, it’s for this reason that you can get an AI MAX 395+ w/128 GB MINI PC (not simply a GPU) for the price of the Huawei. It comes with a 16 Core Zen 5 CPU and a 55 TOPs INT8 NPU which supports sparsity. it also comes with an RDNA3.5 iGPU that does 50 TFLOPs FP16 | 50 TOPs INT8. &lt;/p&gt; &lt;h1&gt;Software&lt;/h1&gt; &lt;p&gt;It needs no saying, but the Nvidia GPU will have vastly better software support. &lt;/p&gt; &lt;h1&gt;Context&lt;/h1&gt; &lt;p&gt;The RTX 6000 Pro is banned from being exported to China. The inflated price reflects the reality that it needs to be smuggled. Huawei’s GPU is Chinese domestically produced. No one from memory maker to fab to Huawei are actually making money without the Chinese government subsidizing them. &lt;/p&gt; &lt;p&gt;Nvidia is a private company that needs to make a profit to continue operating in the segment. Nvidia’s recent rise in market valuation is overwhelmingly premised on them expanding their datacenter revenues rather than expanding their consumer margins. &lt;/p&gt; &lt;p&gt;Simply look at the consumer market to see if Nvidia is abusing their monopoly. &lt;/p&gt; &lt;p&gt;Nvidia sells 380mm2 + 16 GB GDDR7 for 750$. (5070Ti)&lt;/p&gt; &lt;p&gt;AMD sells 355mm2 + 16 GB GDDR6 for 700$. (9070XT)&lt;/p&gt; &lt;p&gt;Nvidia is giving more for only slightly more. &lt;/p&gt; &lt;p&gt;The anti-Nvidia circle jerk is getting tiring. Nvidia WILL OFFER high memory capacities in 2026 early. Why then? Because that’s when Micron and SK Hynix 3 GB GDDR7 is ready. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MCH_2000"&gt; /u/MCH_2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wo0y/the_huawei_gpu_is_not_equivalent_to_an_rtx_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wo0y/the_huawei_gpu_is_not_equivalent_to_an_rtx_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n4wo0y/the_huawei_gpu_is_not_equivalent_to_an_rtx_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-31T14:49:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI — The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM – 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
