<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-01-23T11:34:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1i7ljpy</id>
    <title>NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com</title>
    <updated>2025-01-22T20:48:17+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt; &lt;img alt="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com" src="https://external-preview.redd.it/4XL5vcWPC90yEEq7VKQk8oDbmqhIMyQmjRfE4cS0j3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5bf06daf13377cc2784242eaf7086a8def5b1f0" title="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted - VideoCardz.com" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-blackwell-gpu-with-96gb-gddr7-memory-and-512-bit-bus-spotted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ljpy/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7uch1</id>
    <title>VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding</title>
    <updated>2025-01-23T03:32:09+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7uch1/videollama_3_frontier_multimodal_foundation/"&gt; &lt;img alt="VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding" src="https://external-preview.redd.it/sskI9EuKiHMlUoD8_GezaDdoGMOFEfN6kCxCNY1PJSo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a56d01af565c1e11e26d169fb45fb843e9f1152b" title="VideoLLaMA 3: Frontier Multimodal Foundation Models for Video Understanding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/DAMO-NLP-SG/VideoLLaMA3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7uch1/videollama_3_frontier_multimodal_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7uch1/videollama_3_frontier_multimodal_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T03:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80job</id>
    <title>Deepseek-r1:32b</title>
    <updated>2025-01-23T10:36:33+00:00</updated>
    <author>
      <name>/u/morifo</name>
      <uri>https://old.reddit.com/user/morifo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, have any of you realised that, when using it for code generation and refactoring, Deepseek-r1 somehow misses a random character from variables? It does it repeatedly, I’m wondering whether my set up is the cause or if it’s something inherent?&lt;/p&gt; &lt;p&gt;Ollama Both Deepseek-r1:32b and Deepseek-r1:32b-qwen-distill-q4_K_M Running fully on 3090 Hosted via open-webUI Parameters all to webUI default bar the context length which is set at 4096&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/morifo"&gt; /u/morifo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80job/deepseekr132b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80job/deepseekr132b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i80job/deepseekr132b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T10:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7tym8</id>
    <title>Introducing Awesome Open Source AI: A list for tracking great open source models</title>
    <updated>2025-01-23T03:12:05+00:00</updated>
    <author>
      <name>/u/SuccessIsHardWork</name>
      <uri>https://old.reddit.com/user/SuccessIsHardWork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7tym8/introducing_awesome_open_source_ai_a_list_for/"&gt; &lt;img alt="Introducing Awesome Open Source AI: A list for tracking great open source models" src="https://external-preview.redd.it/GZEQfLmnHi7A-1vZCw7gSk8kSfnSxLSAWq7Gh4npUro.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5cd676b21170a902ce845ef5e20f7dee19a46cae" title="Introducing Awesome Open Source AI: A list for tracking great open source models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuccessIsHardWork"&gt; /u/SuccessIsHardWork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/suncloudsmoon/awesome-open-source-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7tym8/introducing_awesome_open_source_ai_a_list_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7tym8/introducing_awesome_open_source_ai_a_list_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T03:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1i75g7p</id>
    <title>I don’t believe the $500 Billion OpenAI investment</title>
    <updated>2025-01-22T07:03:02+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking at this deal, several things don't add up. The $500 billion figure is wildly optimistic - that's almost double what the entire US government committed to semiconductor manufacturing through the CHIPS Act. When you dig deeper, you see lots of vague promises but no real details about where the money's coming from or how they'll actually build anything.&lt;/p&gt; &lt;p&gt;The legal language is especially fishy. Instead of making firm commitments, they're using weasel words like &amp;quot;intends to,&amp;quot; &amp;quot;evaluating,&amp;quot; and &amp;quot;potential partnerships.&amp;quot; This isn't accidental - by running everything through Stargate, a new private company, and using this careful language, they've created a perfect shield for bigger players like SoftBank and Microsoft. If things go south, they can just blame &amp;quot;market conditions&amp;quot; and walk away with minimal exposure. Private companies like Stargate don't face the same strict disclosure requirements as public ones.&lt;/p&gt; &lt;p&gt;The timing is also telling - announcing this massive investment right after Trump won the presidency was clearly designed for maximum political impact. It fits perfectly into the narrative of bringing jobs and investment back to America. Using inflated job numbers for data centers (which typically employ relatively few people once built) while making vague promises about US technological leadership? That’s politics.&lt;/p&gt; &lt;p&gt;My guess? There's probably a real data center project in the works, but it's being massively oversold for publicity and political gains. The actual investment will likely be much smaller, take longer to complete, and involve different partners than what's being claimed. This announcement just is a deal structured by lawyers who wanted to generate maximum headlines while minimizing any legal risk for their clients.​​​​&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i75g7p/i_dont_believe_the_500_billion_openai_investment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T07:03:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ered</id>
    <title>New Qwen will probably be a MoE as well.</title>
    <updated>2025-01-22T16:14:47+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://xcancel.com/Alibaba_Qwen/status/1882064440159596725"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ered/new_qwen_will_probably_be_a_moe_as_well/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ered/new_qwen_will_probably_be_a_moe_as_well/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T16:14:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7ploh</id>
    <title>FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF</title>
    <updated>2025-01-22T23:41:43+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"&gt; &lt;img alt="FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF" src="https://external-preview.redd.it/_5H6RERAm1u8EGHu_Xa7sfMV3mlRr7YdjSL8ClNGTds.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa21da17ed0152f6d5681cd31154e91d31943138" title="FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-Qwen2.5-Coder-32B-Preview-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7ploh/fuseaifuseo1deepseekr1qwen25coder32bpreviewgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:41:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7fjqm</id>
    <title>DeepSeek R1 is unusable [IMHO]</title>
    <updated>2025-01-22T16:47:24+00:00</updated>
    <author>
      <name>/u/VirtualPanic6798</name>
      <uri>https://old.reddit.com/user/VirtualPanic6798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is my personal experience. Small R1 models that can run fast enough generate too much output. Effectively they end up being very slow, compared to something like LLama3.2. Even if you are OK with the speed, R1 fails to stick to simple output instructions.&lt;br /&gt; Regarding the chain of thought concept: I am not convinced that this is yielding significant improvement. Retrospection works if you have an external feedback or reference, not by going over your own thoughts like a schizophrenic exclaiming &amp;quot;wait no&amp;quot; every now and then.&lt;br /&gt; R1 gives the impression of a student who doesn't know the answer and is hoping to wing it by accidentally stumbling on something acceptable while stalling the teacher.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualPanic6798"&gt; /u/VirtualPanic6798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7fjqm/deepseek_r1_is_unusable_imho/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T16:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78sfs</id>
    <title>Deepseek R1 GRPO code open sourced 🤯</title>
    <updated>2025-01-22T11:11:56+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt; &lt;img alt="Deepseek R1 GRPO code open sourced 🤯" src="https://preview.redd.it/ryfnofs83jee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40e8bb4b0b8a7dd1b82c628a825c88559a17aff0" title="Deepseek R1 GRPO code open sourced 🤯" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ryfnofs83jee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78sfs/deepseek_r1_grpo_code_open_sourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:11:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7x4va</id>
    <title>"Creative writing"</title>
    <updated>2025-01-23T06:18:30+00:00</updated>
    <author>
      <name>/u/MountainGoatAOE</name>
      <uri>https://old.reddit.com/user/MountainGoatAOE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often see people talking about the creative writing skills of a model. So I am curious to learn what kind of use-cases you are all working on. Are you all copywriters that use this for work, or perhaps translators? Or perhaps fanfic as a hobby? Or is it - you know - for erotica reasons?&lt;/p&gt; &lt;p&gt;This may sound like a meme post but I'm actually curious since it pops up so often, so I'm happy to learn how &amp;quot;creative&amp;quot; people are using it so much. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MountainGoatAOE"&gt; /u/MountainGoatAOE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x4va/creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x4va/creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x4va/creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T06:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zn2e</id>
    <title>Is anyone else obsessed with watching R1's thinking output?</title>
    <updated>2025-01-23T09:27:55+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone else obsessed with watching the thinking output? &lt;/p&gt; &lt;p&gt;I find myself watching it and getting anxious when it gets close to the right answer but then does a &amp;quot;oh wait!&amp;quot; and then switches to another direction.&lt;/p&gt; &lt;p&gt;Today, I tried the 1.5B Q5 distil model to see how it did with a simple question:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;state density of gold and from this calculate the length of a cube of gold with 1kg mass.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;``` Thought for 232 seconds Okay, so I need to figure out the density of gold and then use that to calculate the length of a cube made with 1 kg of gold. Hmm, let's break this down step by step.&lt;/p&gt; &lt;p&gt;First, I remember that density is mass divided by volume. The formula for density is:&lt;/p&gt; &lt;p&gt;Density= Volume Mass ​&lt;/p&gt; &lt;p&gt;But wait, to use this formula, I need to know the volume of the gold cube. Since it's a cube, all sides are equal, right? So if I can find the side length, I can find the volume.&lt;/p&gt; &lt;p&gt;But before that, I need the density of gold. I think gold is really dense, but I'm not exactly sure what that number is. I think it's around 19.3 grams per cubic centimeter. Yeah, that sounds right. Let me confirm that... I think I remember that gold has a density of about 19.3 g/cm³. Okay, so that's the density.&lt;/p&gt; &lt;p&gt;Now, I have a mass of 1 kg. Wait, I need to make sure the units are consistent because density is mass per volume. So, should I convert kilograms to grams? Yeah, that makes sense. 1 kg is 1000 grams.&lt;/p&gt; &lt;p&gt;So, mass is 1000 grams, and density is 19.3 grams per cm³. So, using the density formula, I can rearrange it to solve for volume:&lt;/p&gt; &lt;p&gt;Volume= Density Mass ​&lt;/p&gt; &lt;p&gt;Plugging in the numbers:&lt;/p&gt; &lt;p&gt;Volume= 19.3 grams/cm 3&lt;/p&gt; &lt;p&gt;1000 grams ​&lt;/p&gt; &lt;p&gt;Let me do that division. 1000 divided by 19.3. Hmm, 19.3 goes into 1000 how many times? Let me calculate 19.3 times 50 is 965, which is less than 1000. Then 19.3 times 52 is 1003.6, which is just over 1000. So, it's approximately 52 cm³, maybe around 52.2 cm³ or something.&lt;/p&gt; &lt;p&gt;Wait, actually, let me compute it more accurately. 19.3 multiplied by 52 is 1003.6, which is 3.6 grams over. So, 1000 / 19.3 is approximately 51.88 cm³.&lt;/p&gt; &lt;p&gt;So, the volume is about 51.88 cm³.&lt;/p&gt; &lt;p&gt;Now, since the cube has equal sides, the volume of a cube is side length cubed. So,&lt;/p&gt; &lt;p&gt;Volume=Side 3&lt;/p&gt; &lt;p&gt;Therefore, to find the side length, I need to take the cube root of the volume. So,&lt;/p&gt; &lt;p&gt;Side= 3&lt;/p&gt; &lt;p&gt;Volume ​&lt;/p&gt; &lt;p&gt;Plugging in the volume:&lt;/p&gt; &lt;p&gt;Side= 3&lt;/p&gt; &lt;p&gt;51.88 cm 3&lt;/p&gt; &lt;p&gt;​&lt;/p&gt; &lt;p&gt;Hmm, cube root of 51.88. Let me think about this. 3³ is 27, 4³ is 64, so it's somewhere between 3 and 4 cm. Let me calculate it more accurately.&lt;/p&gt; &lt;p&gt;I know that 3.7³ is 3.7 * 3.7 * 3.7. 3.7 * 3.7 is 13.69, then 13.69 * 3.7. Let me compute that.&lt;/p&gt; &lt;p&gt;13.69 * 3 is 41.07, and 13.69 * 0.7 is 9.583. Adding them together gives 41.07 + 9.583 = 50.653 cm³. Hmm, that's close to 51.88, but a bit less.&lt;/p&gt; &lt;p&gt;So, 3.7 cm³ gives 50.653 cm³. The volume we have is 51.88 cm³, so it's a bit higher. So, let's try 3.72 cm.&lt;/p&gt; &lt;p&gt;3.72 * 3.72 = ?&lt;/p&gt; &lt;p&gt;First, 3.7 * 3.7 = 13.69, as before. Then, 0.02 * 3.72 is 0.0744, so 3.72 * 3.72 = 13.69 + 0.0744 + 0.0744? Wait, no, that's not correct. Actually, (a + b)² = a² + 2ab + b². So, (3.7 + 0.02)² = 3.7² + 23.7 0.02 + 0.02² = 13.69 + 0.148 + 0.0004 = 13.8384.&lt;/p&gt; &lt;p&gt;Then, 13.8384 * 3.72. Let me compute that.&lt;/p&gt; &lt;p&gt;13.8384 * 3 = 41.5152&lt;/p&gt; &lt;p&gt;13.8384 * 0.7 = 9.68688&lt;/p&gt; &lt;p&gt;13.8384 * 0.02 = 0.276768&lt;/p&gt; &lt;p&gt;Adding them together:&lt;/p&gt; &lt;p&gt;41.5152 + 9.68688 = 51.20208&lt;/p&gt; &lt;p&gt;51.20208 + 0.276768 = 51.478848 cm³&lt;/p&gt; &lt;p&gt;So, 3.72³ is approximately 51.478848 cm³, which is a bit less than 51.88 cm³.&lt;/p&gt; &lt;p&gt;So, let's try 3.73.&lt;/p&gt; &lt;p&gt;3.73 * 3.73 = ?&lt;/p&gt; &lt;p&gt;Again, using the same method:&lt;/p&gt; &lt;p&gt;3.73 is 3.7 + 0.03&lt;/p&gt; &lt;p&gt;So, (3.7 + 0.03)² = 3.7² + 23.7 0.03 + 0.03² = 13.69 + 0.222 + 0.0009 = 13.9129&lt;/p&gt; &lt;p&gt;Then, 13.9129 * 3.73.&lt;/p&gt; &lt;p&gt;Compute step by step:&lt;/p&gt; &lt;p&gt;13.9129 * 3 = 41.7387&lt;/p&gt; &lt;p&gt;13.9129 * 0.7 = 9.73903&lt;/p&gt; &lt;p&gt;13.9129 * 0.03 = 0.417387&lt;/p&gt; &lt;p&gt;Adding them up:&lt;/p&gt; &lt;p&gt;41.7387 + 9.73903 = 51.47773&lt;/p&gt; &lt;p&gt;51.47773 + 0.417387 = 51.895117 cm³&lt;/p&gt; &lt;p&gt;Wow, that's very close to 51.88 cm³. So, 3.73³ is approximately 51.895117 cm³.&lt;/p&gt; &lt;p&gt;Which is a little over 51.88 cm³. So, the exact cube root is just a little less than 3.73 cm. Maybe 3.728 cm?&lt;/p&gt; &lt;p&gt;Wait, perhaps I should use linear approximation or Newton-Raphson method for a better estimate. But this is getting complicated. Maybe I can use a calculator here, but since I don't have one, perhaps I can estimate.&lt;/p&gt; &lt;p&gt;Alternatively, maybe I can use the exact calculation.&lt;/p&gt; &lt;p&gt;But actually, perhaps I can note that 3.72³ is 51.478848, and 3.73³ is 51.895117. Since 51.88 is between them, so the cube root is somewhere in between.&lt;/p&gt; &lt;p&gt;The difference between 51.88 and 51.478848 is 0.401152.&lt;/p&gt; &lt;p&gt;The difference between 51.895117 and 51.478848 is 0.416269.&lt;/p&gt; &lt;p&gt;So, the fraction is 0.401152 / 0.416269 ≈ 0.962.&lt;/p&gt; &lt;p&gt;So, the cube root is approximately 3.72 + 0.962*(0.001) ≈ 3.72 + 0.000962 ≈ 3.720962 cm.&lt;/p&gt; &lt;p&gt;So, approximately 3.721 cm.&lt;/p&gt; &lt;p&gt;Therefore, the side length is approximately 3.721 cm.&lt;/p&gt; &lt;p&gt;But wait, let me double-check my calculations because this seems a bit tedious.&lt;/p&gt; &lt;p&gt;Alternatively, maybe I can use the formula for the volume of a cube and solve for the side length.&lt;/p&gt; &lt;p&gt;Wait, another approach: let's take the cube root of 51.88.&lt;/p&gt; &lt;p&gt;We know that 3.7³ is 50.653, and 3.72³ is 51.4788, which is less than 51.88.&lt;/p&gt; &lt;p&gt;So, 3.72³ = 51.4788&lt;/p&gt; &lt;p&gt;Let me compute 3.72 + delta, where delta is small, such that (3.72 + delta)³ = 51.88&lt;/p&gt; &lt;p&gt;Using the binomial expansion:&lt;/p&gt; &lt;p&gt;(3.72 + delta)&lt;sup&gt;3&lt;/sup&gt; = 3.72³ + 3&lt;em&gt;(3.72)²delta + 3 (3.72)&lt;/em&gt;delta² + delta³&lt;/p&gt; &lt;p&gt;We can ignore delta squared and delta cubed since delta is small.&lt;/p&gt; &lt;p&gt;So,&lt;/p&gt; &lt;p&gt;(3.72 + delta)&lt;sup&gt;3&lt;/sup&gt; ≈ 3.72³ + 3*(3.72)²delta = 51.4788 + 3 (13.8384)delta = 51.4788 + 41.5152 delta&lt;/p&gt; &lt;p&gt;Set this equal to 51.88:&lt;/p&gt; &lt;p&gt;51.4788 + 41.5152*delta = 51.88&lt;/p&gt; &lt;p&gt;Subtract 51.4788:&lt;/p&gt; &lt;p&gt;41.5152*delta = 0.4012&lt;/p&gt; &lt;p&gt;So,&lt;/p&gt; &lt;p&gt;delta ≈ 0.4012 / 41.5152 ≈ 0.00962&lt;/p&gt; &lt;p&gt;So, delta ≈ 0.00962 cm.&lt;/p&gt; &lt;p&gt;Therefore, the cube root is approximately 3.72 + 0.00962 ≈ 3.72962 cm.&lt;/p&gt; &lt;p&gt;So, approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;But wait, when I cube 3.73, I get 51.895 cm³, which is just a bit over 51.88. So, actually, the cube root is slightly less than 3.73. Let me denote delta as negative.&lt;/p&gt; &lt;p&gt;So,&lt;/p&gt; &lt;p&gt;(3.72 + delta)&lt;sup&gt;3&lt;/sup&gt; = 51.88&lt;/p&gt; &lt;p&gt;We found that 3.72³ is 51.4788, and 3.73³ is 51.8951.&lt;/p&gt; &lt;p&gt;So, 51.88 is 51.8951 - 0.0151 above 3.73³.&lt;/p&gt; &lt;p&gt;So, delta is such that:&lt;/p&gt; &lt;p&gt;51.4788 + 41.5152*delta = 51.88&lt;/p&gt; &lt;p&gt;So, 41.5152*delta = 51.88 - 51.4788 = 0.4012&lt;/p&gt; &lt;p&gt;Therefore,&lt;/p&gt; &lt;p&gt;delta = 0.4012 / 41.5152 ≈ 0.00962&lt;/p&gt; &lt;p&gt;But since 3.73³ is higher, delta is negative, so delta ≈ -0.00962 cm.&lt;/p&gt; &lt;p&gt;So, the cube root is approximately 3.72 - 0.00962 ≈ 3.71038 cm.&lt;/p&gt; &lt;p&gt;Therefore, approximately 3.7104 cm.&lt;/p&gt; &lt;p&gt;So, the side length is approximately 3.71 cm.&lt;/p&gt; &lt;p&gt;Wait, let me verify by cubing 3.71:&lt;/p&gt; &lt;p&gt;3.71 * 3.71 = 13.7641&lt;/p&gt; &lt;p&gt;13.7641 * 3.71:&lt;/p&gt; &lt;p&gt;Let me compute 13.7641 * 3 = 41.2923&lt;/p&gt; &lt;p&gt;13.7641 * 0.7 = 9.63487&lt;/p&gt; &lt;p&gt;13.7641 * 0.01 = 0.137641&lt;/p&gt; &lt;p&gt;Adding them together:&lt;/p&gt; &lt;p&gt;41.2923 + 9.63487 = 50.92717&lt;/p&gt; &lt;p&gt;50.92717 + 0.137641 ≈ 51.06481 cm³&lt;/p&gt; &lt;p&gt;Hmm, 51.06481, which is still less than 51.88. So, 3.71³ is 51.06481 cm³.&lt;/p&gt; &lt;p&gt;So, that's still less than 51.88. Let me try 3.72:&lt;/p&gt; &lt;p&gt;As before, 3.72³ is 51.4788 cm³, which is still less.&lt;/p&gt; &lt;p&gt;3.721³: Let's compute 3.721³.&lt;/p&gt; &lt;p&gt;First, 3.721 * 3.721:&lt;/p&gt; &lt;p&gt;3.721 * 3.721&lt;/p&gt; &lt;p&gt;Compute 3 * 3.721 = 11.163&lt;/p&gt; &lt;p&gt;0.7 * 3.721 = 2.6047&lt;/p&gt; &lt;p&gt;0.02 * 3.721 = 0.07442&lt;/p&gt; &lt;p&gt;0.001 * 3.721 = 0.003721&lt;/p&gt; &lt;p&gt;So, adding them:&lt;/p&gt; &lt;p&gt;11.163 + 2.6047 = 13.7677&lt;/p&gt; &lt;p&gt;13.7677 + 0.07442 = 13.84212&lt;/p&gt; &lt;p&gt;13.84212 + 0.003721 ≈ 13.845841&lt;/p&gt; &lt;p&gt;So, 3.721² ≈ 13.845841&lt;/p&gt; &lt;p&gt;Now, 13.845841 * 3.721:&lt;/p&gt; &lt;p&gt;Again, break it down:&lt;/p&gt; &lt;p&gt;13 * 3.721 = 48.373&lt;/p&gt; &lt;p&gt;0.845841 * 3.721 ≈ Let's compute 0.8 * 3.721 = 2.9768&lt;/p&gt; &lt;p&gt;0.04 * 3.721 = 0.14884&lt;/p&gt; &lt;p&gt;0.005841 * 3.721 ≈ 0.02176&lt;/p&gt; &lt;p&gt;Adding them together:&lt;/p&gt; &lt;p&gt;2.9768 + 0.14884 = 3.12564&lt;/p&gt; &lt;p&gt;3.12564 + 0.02176 ≈ 3.1474&lt;/p&gt; &lt;p&gt;So, total is 48.373 + 3.1474 ≈ 51.5204 cm³&lt;/p&gt; &lt;p&gt;So, 3.721³ ≈ 51.5204 cm³&lt;/p&gt; &lt;p&gt;Which is still less than 51.88. Hmm, so 3.721³ is 51.5204, which is less than 51.88. So, let's try 3.722.&lt;/p&gt; &lt;p&gt;Compute 3.722³:&lt;/p&gt; &lt;p&gt;First, 3.722²:&lt;/p&gt; &lt;p&gt;Again, 3.72² is 13.8384, so 3.722² = 13.8384 + 2&lt;em&gt;(0.002)&lt;/em&gt;3.72 + (0.002)&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;Wait, actually, 3.722 is 3.72 + 0.002, so:&lt;/p&gt; &lt;p&gt;(3.72 + 0.002)² = 3.72² + 23.72 0.002 + 0.002² = 13.8384 + 0.01488 + 0.000004 = 13.853284&lt;/p&gt; &lt;p&gt;Then, 13.853284 * 3.722:&lt;/p&gt; &lt;p&gt;Break it down:&lt;/p&gt; &lt;p&gt;13 * 3.722 = 48.386&lt;/p&gt; &lt;p&gt;0.853284 * 3.722 ≈ Let's compute:&lt;/p&gt; &lt;p&gt;0.8 * 3.722 = 2.9776&lt;/p&gt; &lt;p&gt;0.05 * 3.722 = 0.1861&lt;/p&gt; &lt;p&gt;0.003284 * 3.722 ≈ 0.01222&lt;/p&gt; &lt;p&gt;Adding them:&lt;/p&gt; &lt;p&gt;2.9776 + 0.1861 = 3.1637&lt;/p&gt; &lt;p&gt;3.1637 + 0.01222 ≈ 3.1759&lt;/p&gt; &lt;p&gt;Total volume: 48.386 + 3.1759 ≈ 51.5619 cm³&lt;/p&gt; &lt;p&gt;So, 3.722³ ≈ 51.5619 cm³, which is less than 51.88.&lt;/p&gt; &lt;p&gt;So, 3.722³ = 51.5619&lt;/p&gt; &lt;p&gt;Difference from 51.88: 51.88 - 51.5619 = 0.3181&lt;/p&gt; &lt;p&gt;So, need to add more.&lt;/p&gt; &lt;p&gt;Let me compute the derivative of the volume with respect to side length to approximate the delta.&lt;/p&gt; &lt;p&gt;V = s³&lt;/p&gt; &lt;p&gt;dV/ds = 3s²&lt;/p&gt; &lt;p&gt;At s = 3.722, dV/ds = 3&lt;em&gt;(3.722)² ≈ 3&lt;/em&gt;13.853284 ≈ 41.55985&lt;/p&gt; &lt;p&gt;So, delta V ≈ dV/ds * delta = 41.55985 * delta = 0.3181&lt;/p&gt; &lt;p&gt;So, delta ≈ 0.3181 / 41.55985 ≈ 0.00765 cm&lt;/p&gt; &lt;p&gt;Therefore, s ≈ 3.722 + 0.00765 ≈ 3.72965 cm&lt;/p&gt; &lt;p&gt;So, approximately 3.7297 cm&lt;/p&gt; &lt;p&gt;So, the cube root is approximately 3.7297 cm&lt;/p&gt; &lt;p&gt;Therefore, the side length of the cube is approximately 3.73 cm&lt;/p&gt; &lt;p&gt;To be precise, about 3.73 cm.&lt;/p&gt; &lt;p&gt;Therefore, the edge length of the cube is approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;Let me verify by cubing 3.73:&lt;/p&gt; &lt;p&gt;3.73 * 3.73 = 13.9129&lt;/p&gt; &lt;p&gt;13.9129 * 3.73:&lt;/p&gt; &lt;p&gt;13 * 3.73 = 48.49&lt;/p&gt; &lt;p&gt;0.9129 * 3.73 ≈ Let's compute:&lt;/p&gt; &lt;p&gt;0.9 * 3.73 = 3.357&lt;/p&gt; &lt;p&gt;0.0129 * 3.73 ≈ 0.0482&lt;/p&gt; &lt;p&gt;Adding together: 3.357 + 0.0482 ≈ 3.4052&lt;/p&gt; &lt;p&gt;Total volume: 48.49 + 3.4052 ≈ 51.8952 cm³&lt;/p&gt; &lt;p&gt;Which is very close to 51.88 cm³.&lt;/p&gt; &lt;p&gt;So, 3.73 cm³ is approximately 51.8952, which is about 51.90, which is very close to 51.88.&lt;/p&gt; &lt;p&gt;Therefore, 3.73 cm is a good approximation.&lt;/p&gt; &lt;p&gt;So, the edge length is approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;Hence, the edge of the cube is approximately 3.73 cm.&lt;/p&gt; &lt;p&gt;Final Answer The edge of the cube is approximately \boxed{3.73} cm. ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zn2e/is_anyone_else_obsessed_with_watching_r1s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zn2e/is_anyone_else_obsessed_with_watching_r1s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zn2e/is_anyone_else_obsessed_with_watching_r1s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T09:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7osek</id>
    <title>Deepseek R1 Distills become kind of dumb below 14B.</title>
    <updated>2025-01-22T23:05:04+00:00</updated>
    <author>
      <name>/u/Dance-Till-Night1</name>
      <uri>https://old.reddit.com/user/Dance-Till-Night1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried Deepseek R1 Qwen Distills 32B, 14B, 8B (llama), 7B, and 1.5B. Anything below 14B is sort of dumb with 14B being alright. I don't know if anyone has a similar experience but looking at benchmarks (Oobabooga and Open-llm leaderboard as examples) it seems that there's a severe loss of intelligence/reasoning below 14b.&lt;/p&gt; &lt;p&gt;Now there's no denying how amazing Deepseek R1 is and the qwen 32b distill is honestly one of the best models I have used but it's also worth pointing out that the smaller models don't perform so well. This is just my experience trying the smaller models and it's obviously subjective. If anyone wants to share their experience that would be great too. Maybe I'm missing something :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dance-Till-Night1"&gt; /u/Dance-Till-Night1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7osek/deepseek_r1_distills_become_kind_of_dumb_below_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7r0s5</id>
    <title>Pretraining small models - some takeaways from pretraining a 162M model</title>
    <updated>2025-01-23T00:46:35+00:00</updated>
    <author>
      <name>/u/amang0112358</name>
      <uri>https://old.reddit.com/user/amang0112358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share some of my experience pretraining a small model from scratch. I have written out a &lt;strong&gt;tutorial&lt;/strong&gt; for anyone to follow here (includes code and checkpoints):&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@gupta.aman/pre-training-logs-entry-1-training-a-smol-llama-from-scratch-04e4b5d4c5f7"&gt;https://medium.com/@gupta.aman/pre-training-logs-entry-1-training-a-smol-llama-from-scratch-04e4b5d4c5f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here are some takeaways for me:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The basic recipe is not that complicated. On the other hand, it's a great learning experience. For example, there are a bunch of interesting tools/libraries out there - like &lt;a href="https://github.com/huggingface/datatrove"&gt;datatrove from Huggingface&lt;/a&gt; - which I wouldn't have been exposed to if I had only been post-training (where dataset sizes are much smaller). Beyond large-scale tokenization (what I used it for), it has a lot of data curation pipelines, which will definitely be useful in the future.&lt;/li&gt; &lt;li&gt;While creating SoTA models on local hardware is kinda hard, even for the smallest models, there is a lot that can be &lt;em&gt;learned&lt;/em&gt; with 3090-like hardware. For example, one interesting surprise for me was that the model did learn a single paragraph of text perfectly (training takes a minute to run on a 3090). There are future memorization experiments I am interested in (e.g., Can an LLM memorize Shakespeare?), and given the size of the dataset, it's totally feasible to do it using a 3090 for model sizes up to 1B.&lt;/li&gt; &lt;li&gt;A lot of interesting architectural work has happened over the years. The most interesting for me was the MobileLLM research, which showed that using a smaller embedding size but more layers (like 32 layers) is better than wider models with fewer layers (e.g., GPT-2 135M has only 12 layers). I have a suspicion that memorization would be impacted by this decision, too, and that's something I want to look into.&lt;/li&gt; &lt;li&gt;I am really excited to try and go out of the box when it comes to datasets, like training a model on my emails for the last 20 years. Previously, I would have said that this is a tiny dataset and the model wouldn't be useful. But now I have enough conviction through this exercise that small datasets are OK to pre-train with as long as they are within a narrow domain and you expect to use the model within that narrow domain.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And to me, that's what this whole Local LLM movement is about - we want to take control of LLM technology to make it work for us, and we are not afraid to tinker with the tech to make our own homemade LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amang0112358"&gt; /u/amang0112358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7r0s5/pretraining_small_models_some_takeaways_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T00:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i78src</id>
    <title>How I Used GPT-O1 Pro to Discover My Autoimmune Disease (After Spending $100k and Visiting 30+ Hospitals with No Success)</title>
    <updated>2025-01-22T11:12:33+00:00</updated>
    <author>
      <name>/u/Dry_Steak30</name>
      <uri>https://old.reddit.com/user/Dry_Steak30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Suffered from various health issues for 5 years, visited 30+ hospitals with no answers&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finally diagnosed with axial spondyloarthritis through genetic testing&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built a personalized health analysis system using GPT-O1 Pro, which actually suggested this condition earlier&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm a guy in my mid-30s who started having weird health issues about 5 years ago. Nothing major, but lots of annoying symptoms - getting injured easily during workouts, slow recovery, random fatigue, and sometimes the pain was so bad I could barely walk.&lt;/p&gt; &lt;p&gt;At first, I went to different doctors for each symptom. Tried everything - MRIs, chiropractic care, meds, steroids - nothing helped. I followed every doctor's advice perfectly. Started getting into longevity medicine thinking it might be early aging. Changed my diet, exercise routine, sleep schedule - still no improvement. The cause remained a mystery.&lt;/p&gt; &lt;p&gt;Recently, after a month-long toe injury wouldn't heal, I ended up seeing a rheumatologist. They did genetic testing and boom - diagnosed with axial spondyloarthritis. This was the answer I'd been searching for over 5 years.&lt;/p&gt; &lt;p&gt;Here's the crazy part - I fed all my previous medical records and symptoms into GPT-O1 pro before the diagnosis, and it actually listed this condition as the top possibility!&lt;/p&gt; &lt;p&gt;This got me thinking - why didn't any doctor catch this earlier? Well, it's a rare condition, and autoimmune diseases affect the whole body. Joint pain isn't just joint pain, dry eyes aren't just eye problems. The usual medical workflow isn't set up to look at everything together.&lt;/p&gt; &lt;p&gt;So I had an idea: What if we created an open-source system that could analyze someone's complete medical history, including family history (which was a huge clue in my case), and create personalized health plans? It wouldn't replace doctors but could help both patients and medical professionals spot patterns.&lt;/p&gt; &lt;p&gt;Building my personal system was challenging:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Every hospital uses different formats and units for test results. Had to create a GPT workflow to standardize everything.&lt;/li&gt; &lt;li&gt;RAG wasn't enough - needed a large context window to analyze everything at once for the best results.&lt;/li&gt; &lt;li&gt;Finding reliable medical sources was tough. Combined official guidelines with recent papers and trusted YouTube content.&lt;/li&gt; &lt;li&gt;GPT-O1 pro was best at root cause analysis, Google Note LLM worked great for citations, and Examine excelled at suggesting actions.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In the end, I built a system using Google Sheets to view my data and interact with trusted medical sources. It's been incredibly helpful in managing my condition and understanding my health better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Steak30"&gt; /u/Dry_Steak30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i78src/how_i_used_gpto1_pro_to_discover_my_autoimmune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T11:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7o9xo</id>
    <title>Deepseek R1's Open Source Version Differs from the Official API Version</title>
    <updated>2025-01-22T22:42:59+00:00</updated>
    <author>
      <name>/u/TempWanderer101</name>
      <uri>https://old.reddit.com/user/TempWanderer101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt; &lt;img alt="Deepseek R1's Open Source Version Differs from the Official API Version" src="https://external-preview.redd.it/O6yRSkHlLQEuWa3IVTIjOeWjc3oVfhYSEAWbRgtkPoI.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7667af2d2893f12eab2e89dc94623122da33ee0b" title="Deepseek R1's Open Source Version Differs from the Official API Version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt; The API is actually &lt;em&gt;less&lt;/em&gt; censored than the open model on CCP-related issues (instead of more, as you would expect). If they are indeed different models, then benchmarks and researchers should be more specific about which they are using. Third-party providers seem to be affected.&lt;/p&gt; &lt;p&gt;I was watching Matthew Berman's video (&lt;a href="https://youtu.be/bOsvI3HYHgI?si=zt6N0yYoqA_-zH1v&amp;amp;t=769"&gt;timestamped link&lt;/a&gt;), and was surprised when R1 (self-hosted by him) refused to answer questions about Tiananmen Square or Taiwan, since I had previously tested a similar question on the API without any issues.&lt;/p&gt; &lt;p&gt;For context, R1 is Deepseek's recently released thinking model, which was released under the MIT open source license, with benchmarks claiming to outperform OpenAI's Claude and Anthropic's Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;To investigate this discrepancy, I compared the model's responses across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The official API (accessed via OpenRouter and LM Arena)&lt;/li&gt; &lt;li&gt;TogetherAI (a platform that is likely hosting the public open source version)&lt;/li&gt; &lt;li&gt;A self-hosted version (as seen in Matthew's video)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Test 1: Basic Tiananmen Square Query&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;code&gt;tell me about tiananmen square&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official model on Deepseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cvjq5b2pdmee1.png?width=1488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=239c77ca2bff1c447cdf6fc85a33633bec553545"&gt;https://preview.redd.it/cvjq5b2pdmee1.png?width=1488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=239c77ca2bff1c447cdf6fc85a33633bec553545&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l390lzrlcmee1.png?width=1603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3df3b84ddc65dab80a30df4983ce141e24a21c4"&gt;https://preview.redd.it/l390lzrlcmee1.png?width=1603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3df3b84ddc65dab80a30df4983ce141e24a21c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on Matthew Berman's video (self-hosted)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3ye9sb6p3mee1.png?width=992&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d580ed525f76a77bb225af72fe282d86cfac9aa"&gt;https://preview.redd.it/3ye9sb6p3mee1.png?width=992&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d580ed525f76a77bb225af72fe282d86cfac9aa&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test 2: Mixing Reasoning with Sensitive Topics&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;code&gt;Answer these questions: a) Solve sin(x) + cos(y) = 1 b) Tell me about tiananmen square c) Five people were eating apples, A finished before B, but behind C. D finished before E, but behind B. What was the finishing order?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official version on Deekseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zd1f3qhzcmee1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d020fe75d3fe8737ab83329460d66841d1da77fc"&gt;https://preview.redd.it/zd1f3qhzcmee1.png?width=1490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d020fe75d3fe8737ab83329460d66841d1da77fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public version on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rj7b3qm8cmee1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17f44d47f0580b776532f1c9088a4edd9acdeec1"&gt;https://preview.redd.it/rj7b3qm8cmee1.png?width=1287&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17f44d47f0580b776532f1c9088a4edd9acdeec1&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Test 3: Taiwan Independence Query&lt;/h1&gt; &lt;p&gt;Prompt: &lt;code&gt;Tell me about Taiwan's status as an independent country&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official model on Deepseek's API via OpenRouter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dmqb8nkpamee1.png?width=1543&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a83bf864bf2535f9503d32ebca9c635a4c87c346"&gt;https://preview.redd.it/dmqb8nkpamee1.png?width=1543&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a83bf864bf2535f9503d32ebca9c635a4c87c346&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on TogetherAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/38njhjowemee1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3ba7976605522223fde280fce63ee96d7af1bbc"&gt;https://preview.redd.it/38njhjowemee1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3ba7976605522223fde280fce63ee96d7af1bbc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Public model on Matthew Berman's video (self-hosted)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0a80kylf4mee1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=015c3111845f604cd02c83d70001ed1d2b1e5c32"&gt;https://preview.redd.it/0a80kylf4mee1.png?width=831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=015c3111845f604cd02c83d70001ed1d2b1e5c32&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The public, open source model on HuggingFace is more censored than the API&lt;/li&gt; &lt;li&gt;When handling CCP-sensitive topics, the public model: &lt;ul&gt; &lt;li&gt;Skips its usual thinking process&lt;/li&gt; &lt;li&gt;Either refuses to answer or provides notably biased responses&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Even when sensitive questions are embedded between reasoning tasks, the model still exhibits this behavior&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implications&lt;/h1&gt; &lt;p&gt;If it is true that they are different models, then:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The open model may perform worse than its reported benchmarks. As seen, it totally interrupts the thinking process and causes the model to not think at all. This also affects human-ranked leaderboards like LM Arena, as it uses the (currently uncensored) official API.&lt;/li&gt; &lt;li&gt;Models appear unbiased, but as they are eventually made available by more providers (which use the open source models), they may subtly spread biased viewpoints, as seen in the screenshots.&lt;/li&gt; &lt;li&gt;The actual model might still not be open source, despite the claim.&lt;/li&gt; &lt;li&gt;Models provided by other providers or self-hosted on the cloud may not perform as well. This might be important as Deepseek's API uses inputs for training, and some users might prefer providers who do not log inputs.&lt;/li&gt; &lt;li&gt;This might confuse LLM researchers and subsequent papers.&lt;/li&gt; &lt;li&gt;Third party benchmarks will be inconsistent, as some might use the API, while others might choose to host the model themselves.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Testing methodology&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;All tests were conducted with:&lt;/li&gt; &lt;li&gt;Temperature: 0&lt;/li&gt; &lt;li&gt;Top-P: 0.7&lt;/li&gt; &lt;li&gt;Top-K: 50&lt;/li&gt; &lt;li&gt;Repetition penalty: 1.0&lt;/li&gt; &lt;li&gt;No system prompt &lt;ul&gt; &lt;li&gt;Assuming this is what &amp;quot;Default&amp;quot; is on TogetherAI&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: the official API &lt;a href="https://api-docs.deepseek.com/guides/reasoning_model"&gt;doesn't support parameters like temperature&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'd like to give Deepseek the benefit of the doubt; hopefully this confusion can be cleared up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TempWanderer101"&gt; /u/TempWanderer101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7o9xo/deepseek_r1s_open_source_version_differs_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:42:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nxhy</id>
    <title>Imatrix quants of DeepSeek R1 (the big one) are up!</title>
    <updated>2025-01-22T22:28:35+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took a nice 3-4 hours per quant at the lower end, and increased my huggingface storage by a solid 9TB or so... But they're up :D&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/DeepSeek-R1-GGUF"&gt;https://huggingface.co/bartowski/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For anyone looking for non-imatrix from me they're of course also available on the lmstudio-community page but uh... Good luck.. the smallest size there is a casual 347GB for the files alone..&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF"&gt;https://huggingface.co/lmstudio-community/DeepSeek-R1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;IQ1_S on the other hand is only 133GB, so basically anyone can run them! (/s)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nxhy/imatrix_quants_of_deepseek_r1_the_big_one_are_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:28:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7pxn7</id>
    <title>Open WebUI adds reasoning-focused features in two new releases OUT TODAY!!! 0.5.5 adds "Thinking" tag support to streamline reasoning model chats (works with R1) . 0.5.6 brings new "reasoning_effort" parameter to control cognitive effort.</title>
    <updated>2025-01-22T23:56:53+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These friggin' guys are always dropping great new features just when we need them! They had been quiet for the last few weeks, but not anymore! Per the release notes from Open WebUI releases page today:&lt;br /&gt; &lt;a href="https://github.com/open-webui/open-webui/releases"&gt;https://github.com/open-webui/open-webui/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.6:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🧠 &lt;strong&gt;Effortful Reasoning Control for OpenAI Models:&lt;/strong&gt; Introduced the reasoning_effort parameter in chat controls for supported OpenAI models, enabling users to fine-tune how much cognitive effort a model dedicates to its responses, offering greater customization for complex queries and reasoning tasks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Open WebUI 0.5.5:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🤔 &lt;strong&gt;Native 'Think' Tag Support:&lt;/strong&gt; Introduced the new 'think' tag support that visually displays how long the model is thinking, omitting the reasoning content itself until the next turn. Ideal for creating a more streamlined and focused interaction experience.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7pxn7/open_webui_adds_reasoningfocused_features_in_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T23:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7w583</id>
    <title>Jan now supports DeepSeek R1 distills - model loading error fixed</title>
    <updated>2025-01-23T05:14:01+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt; &lt;img alt="Jan now supports DeepSeek R1 distills - model loading error fixed" src="https://external-preview.redd.it/292fRQ6ybDOj_r18_tYuGBSwlG2q4koHTYhO2NFfzsM.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d01f7aa3e4b8157f9ad15919e2a4665a8fd758d" title="Jan now supports DeepSeek R1 distills - model loading error fixed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/jandotai/status/1882282461041967605"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7w583/jan_now_supports_deepseek_r1_distills_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:14:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7g9po</id>
    <title>The Deep Seek R1 glaze is unreal but it’s true.</title>
    <updated>2025-01-22T17:16:07+00:00</updated>
    <author>
      <name>/u/LostMyOtherAcct69</name>
      <uri>https://old.reddit.com/user/LostMyOtherAcct69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have had a programming issue in my code for a RAG machine for two days that I’ve been working through documentation and different LLM‘s. &lt;/p&gt; &lt;p&gt;I have tried every single major LLM from every provider and none could solve this issue including O1 pro. I was going crazy. I just tried R1 and it fixed on its first attempt… I think I found a new daily runner for coding.. time to cancel OpenAI pro lol. &lt;/p&gt; &lt;p&gt;So yes the glaze is unreal (especially that David and Goliath post lol) but it’s THAT good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostMyOtherAcct69"&gt; /u/LostMyOtherAcct69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7g9po/the_deep_seek_r1_glaze_is_unreal_but_its_true/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T17:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7nmk5</id>
    <title>NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted</title>
    <updated>2025-01-22T22:15:41+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt; &lt;img alt="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" src="https://external-preview.redd.it/4XL5vcWPC90yEEq7VKQk8oDbmqhIMyQmjRfE4cS0j3I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5bf06daf13377cc2784242eaf7086a8def5b1f0" title="NVIDIA RTX Blackwell GPU with 96GB GDDR7 memory and 512-bit bus spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-rtx-blackwell-gpu-with-96gb-gddr7-memory-and-512-bit-bus-spotted?fbclid=IwZXh0bgNhZW0CMTEAAR3i39eJbThbgTnI0Yz4JdnkMXgvj4wlorxOdbBeccw35kkqWqyrG816HpI_aem_EoENoW6h6SP-aU7FVwBWiw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7nmk5/nvidia_rtx_blackwell_gpu_with_96gb_gddr7_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T22:15:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7l8jq</id>
    <title>Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’</title>
    <updated>2025-01-22T20:35:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt; &lt;img alt="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" src="https://external-preview.redd.it/eSsDN6qn9Vf3av8njm2eDT1t4GaTyvUyo2U0_KTezCE.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258806c53694caece33676c65dfce38429cd8859" title="Elon Musk bashes the $500 billion AI project Trump announced, claiming its backers don’t ‘have the money’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnn.com/2025/01/22/tech/elon-musk-trump-stargate-openai/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7l8jq/elon_musk_bashes_the_500_billion_ai_project_trump/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-22T20:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7zeqz</id>
    <title>Nivida just open sourced their long context goodies - 128k context for 50% less memory</title>
    <updated>2025-01-23T09:09:11+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;edited : &lt;del&gt;50%&lt;/del&gt; 35% less memory&lt;/p&gt; &lt;p&gt;If you need long context for RAG, tool use, agents, or just because, Nvidia released a new library to make it super simple.&lt;/p&gt; &lt;p&gt;TLDR: You can get 128k context at 35% less memory&lt;/p&gt; &lt;p&gt;Here's a blog post on everything: &lt;a href="https://huggingface.co/blog/nvidia/kvpress"&gt;https://huggingface.co/blog/nvidia/kvpress&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7zeqz/nivida_just_open_sourced_their_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T09:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7x5nd</id>
    <title>The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)</title>
    <updated>2025-01-23T06:19:53+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt; &lt;img alt="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" src="https://preview.redd.it/o28q2pl6roee1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbcc9bcc3ec8f998f0c0b948df4117b2f7a6ae30" title="The first performant open-source byte-level model without tokenization has been released. EvaByte is a 6.5B param model that also has multibyte prediction for faster inference (vs similar sized tokenized models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o28q2pl6roee1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7x5nd/the_first_performant_opensource_bytelevel_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T06:19:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1i7wcry</id>
    <title>ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp; 72B "reasoning" agent for computer use</title>
    <updated>2025-01-23T05:27:26+00:00</updated>
    <author>
      <name>/u/Lynncc6</name>
      <uri>https://old.reddit.com/user/Lynncc6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt; &lt;img alt="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" src="https://external-preview.redd.it/OHh5dzk5NW5pb2VlMQuFHIyWgdnmfbsw0_j6sClcjT-Ye_u4ggCKOXoWw4Cm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7659796eeda19643a224a312f9f87529cedbf79" title="ByteDance dropping an Apache 2.0 licensed 2B, 7B &amp;amp; 72B &amp;quot;reasoning&amp;quot; agent for computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lynncc6"&gt; /u/Lynncc6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ealby85nioee1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i7wcry/bytedance_dropping_an_apache_20_licensed_2b_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T05:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1i80cwf</id>
    <title>deepseek is a side project</title>
    <updated>2025-01-23T10:22:48+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt; &lt;img alt="deepseek is a side project" src="https://preview.redd.it/zdvrlxahzpee1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df808bd8bc2e4ba90db2fdb005eaae092d5d8206" title="deepseek is a side project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zdvrlxahzpee1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-01-23T10:22:48+00:00</published>
  </entry>
</feed>
