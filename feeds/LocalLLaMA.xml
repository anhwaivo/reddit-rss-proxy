<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-22T00:53:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mvnmjo</id>
    <title>My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834</title>
    <updated>2025-08-20T18:49:36+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt; &lt;img alt="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" src="https://external-preview.redd.it/bruJaed8mpWclO3rYYnLL_4tpIRSDSNQT1lxjc08864.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de2bdb3583ed7a714be2ea7450e90270d3b3e83" title="My LLM trained from scratch on only 1800s London texts brings up a real protest from 1834" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôve posted on here a couple times sharing my project. I'm training LLM‚Äôs from scratch on 1800‚Äôs London texts (no fine tune/modern data). I built a dataset using 7,000 texts published between 1800 to 1875 in the city of London, and also trained a custom tokenizer on the dataset itself to get rid of modern vocab. &lt;/p&gt; &lt;p&gt;So far I‚Äôve trained 3 models, 2 with nanoGPT and the latest using Phi 1.5. After training, I messed around with some prompts and used this one:&lt;/p&gt; &lt;p&gt;&amp;quot;It was the year of our Lord 1834&amp;quot; &lt;/p&gt; &lt;p&gt;Here‚Äôs the output:&lt;/p&gt; &lt;p&gt; &amp;quot;It was the year of our Lord 1834 and the streets of London were filled with protest and petition. The cause, as many re counted, was not bound in the way of private, but having taken up the same day in the day of Lord Palmerston, the public will receive a short statement of the difficulties under which the day of law has reached us. It is a matter of deep regret, that the present events in the history of the world are clear, and consequently will be'known. It is not true that the very men who first settled in the Gospel at Jerusalem should have so extensive and so interesting a record of the prosperity and prosperity&amp;quot; (The last sentence is weird but stuff like that shows up a lot probably due to heavy biblical influence)&lt;/p&gt; &lt;p&gt;I was interested to see if a protest had actually occurred in 1834 London and it really did happen but I thought it was maybe just a coincidence. The output also brought up ‚ÄúLord Palmerston‚Äù and after a google search I learned that his actions resulted in the 1834 protests. So this idea is past just mimicking 1800s text and can now actually recall real historical events. &lt;/p&gt; &lt;p&gt;This is all from just 5-6GB of data, imagine the results with 30GB or more. I‚Äôm not sure if just scaling the data up will ever result in reasoning but even now it kinda feels like digital time travel. I want to eventually try different cities also, maybe a Chinese, Russian or Indian or even just another English city model. I‚Äôm just doing this for fun so if anyone would like to collaborate let me know, I‚Äôm open to anything really. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e"&gt;https://preview.redd.it/9e997tbsy7kf1.png?width=1332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbac7818db44afc70c666c677ccae1f94c4a486e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mvnmjo/my_llm_trained_from_scratch_on_only_1800s_london/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-20T18:49:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwb6j3</id>
    <title>Where is AMD NPU driver for Linux?</title>
    <updated>2025-08-21T13:38:01+00:00</updated>
    <author>
      <name>/u/gnorrisan</name>
      <uri>https://old.reddit.com/user/gnorrisan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb6j3/where_is_amd_npu_driver_for_linux/"&gt; &lt;img alt="Where is AMD NPU driver for Linux?" src="https://preview.redd.it/l6rwjjqlldkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e11c98d62ab452c2e0f2fdbcd2919fc858d77ee6" title="Where is AMD NPU driver for Linux?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnorrisan"&gt; /u/gnorrisan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l6rwjjqlldkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb6j3/where_is_amd_npu_driver_for_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb6j3/where_is_amd_npu_driver_for_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T13:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwhal0</id>
    <title>PSA: OpenAI GPT-OSS running slow? Do not set top-k to 0!</title>
    <updated>2025-08-21T17:21:44+00:00</updated>
    <author>
      <name>/u/and_human</name>
      <uri>https://old.reddit.com/user/and_human</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was having issues with GPT-OSS 20b running very slowly on my hardware. At first I suspected that I was using shared RAM, but even at much lower context, and thus memory, I still had horrible speeds. Turns out I had followed the directions of Unsloth in their GPT-OSS guide and set the Top_K to 0. This slows down llama.cpp a lot! I went from 35 tokens/s to 90!&lt;/p&gt; &lt;p&gt;See relevant llama.cpp issue: &lt;a href="https://github.com/ggml-org/llama.cpp/issues/15223"&gt;https://github.com/ggml-org/llama.cpp/issues/15223&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope this helps someone :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/and_human"&gt; /u/and_human &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwhal0/psa_openai_gptoss_running_slow_do_not_set_topk_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwhal0/psa_openai_gptoss_running_slow_do_not_set_topk_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwhal0/psa_openai_gptoss_running_slow_do_not_set_topk_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T17:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3j7l</id>
    <title>Deepseek V3.1 is not so bad after all..</title>
    <updated>2025-08-21T06:40:50+00:00</updated>
    <author>
      <name>/u/Trevor050</name>
      <uri>https://old.reddit.com/user/Trevor050</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"&gt; &lt;img alt="Deepseek V3.1 is not so bad after all.." src="https://b.thumbs.redditmedia.com/T9qzPo7Bvut8hGvtV6Xg52JupsxwiKXPdU34MXCHojI.jpg" title="Deepseek V3.1 is not so bad after all.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like it just was a different purpose, speed and agency. Its pretty good at what its meant for&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trevor050"&gt; /u/Trevor050 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mw3j7l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3j7l/deepseek_v31_is_not_so_bad_after_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwigpz</id>
    <title>Deepseek + Claude Code Working Flawlessly! ü§Ø (haven't experience error like other proxy project yet)</title>
    <updated>2025-08-21T18:05:04+00:00</updated>
    <author>
      <name>/u/GTHell</name>
      <uri>https://old.reddit.com/user/GTHell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwigpz/deepseek_claude_code_working_flawlessly_havent/"&gt; &lt;img alt="Deepseek + Claude Code Working Flawlessly! ü§Ø (haven't experience error like other proxy project yet)" src="https://preview.redd.it/r9a6y15twekf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb3f0fd1cd4085afa2b1911b3e0248337172f484" title="Deepseek + Claude Code Working Flawlessly! ü§Ø (haven't experience error like other proxy project yet)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTHell"&gt; /u/GTHell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r9a6y15twekf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwigpz/deepseek_claude_code_working_flawlessly_havent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwigpz/deepseek_claude_code_working_flawlessly_havent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T18:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwpbol</id>
    <title>Evaluating Deepseek v3.1 chat with a minimal agent on SWE-bench verified: Still slightly behind Qwen 3 coder</title>
    <updated>2025-08-21T22:29:13+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwpbol/evaluating_deepseek_v31_chat_with_a_minimal_agent/"&gt; &lt;img alt="Evaluating Deepseek v3.1 chat with a minimal agent on SWE-bench verified: Still slightly behind Qwen 3 coder" src="https://external-preview.redd.it/tIi6zR2hpoqmGtk624mlkKyTEp-c3zXW1o-hpjWtxiI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b431a224df8b1233fc9f779fad764ff1ce7cafb" title="Evaluating Deepseek v3.1 chat with a minimal agent on SWE-bench verified: Still slightly behind Qwen 3 coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We evaluated Deepseek v3.1 chat using a minimal agent (no tools other than bash, common-sense prompts, main agent class implemented in some 100 lines of python) and get 53.8% on SWE-bench verified (if you want to reproduce it, you can install &lt;a href="https://github.com/SWE-agent/mini-swe-agent"&gt;https://github.com/SWE-agent/mini-swe-agent&lt;/a&gt; and it's a one-liner to evaluate on SWE-bench).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d1dmlmo78gkf1.png?width=780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=449eca28d86413e9259d33e66c7df67036c317a5"&gt;https://preview.redd.it/d1dmlmo78gkf1.png?width=780&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=449eca28d86413e9259d33e66c7df67036c317a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It currently gets on 2nd place among open source models on our leaderboard (SWE-bench bash-only, where we compare all models with this exact setup, see &lt;a href="https://www.swebench.com/"&gt;https://www.swebench.com/&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;Still working on adding some more models, in particular open source ones. We haven't evaluated DeepSeek v3.1 reasoning so far (it doesn't have tool calls, so it's probably going to be less used for agents).&lt;/p&gt; &lt;p&gt;One of the interesting things is that Deepseek v3.1 chat maxes out later with respect to the number of steps taken by the agent, especially compared to the GPT models. To squeeze out the maximum performance you might have to run for 150 steps.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ok2y7rta8gkf1.png?width=2157&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=add6cf27c09da63de3a0169e76a577a038eaa9d2"&gt;https://preview.redd.it/ok2y7rta8gkf1.png?width=2157&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=add6cf27c09da63de3a0169e76a577a038eaa9d2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As a result of the high step numbers, I'd say the effective cost is somewhere near that of GPT-5 mini if you use the official API (the next plot basically shows different cost to performance points depending on how high you set the step limit of the agent ‚Äî agents succeed fast, but fail very slowly, so you can spend a lot of money without getting a higher resolve rate).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8dfgx8cc8gkf1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff3667c6de5ebb0deafc5b4f7c7a031d70af833b"&gt;https://preview.redd.it/8dfgx8cc8gkf1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ff3667c6de5ebb0deafc5b4f7c7a031d70af833b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(sorry that the cost/step plots still mostly show proprietary models, we'll have a more complete plot soon).&lt;/p&gt; &lt;p&gt;(note: xpost from &lt;a href="https://www.reddit.com/r/DeepSeek/comments/1mwp8ji/evaluating%5C_deepseek%5C_v31%5C_chat%5C_with%5C_a%5C_minimal%5C_agent/"&gt;https://www.reddit.com/r/DeepSeek/comments/1mwp8ji/evaluating\_deepseek\_v31\_chat\_with\_a\_minimal\_agent/&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwpbol/evaluating_deepseek_v31_chat_with_a_minimal_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwpbol/evaluating_deepseek_v31_chat_with_a_minimal_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwpbol/evaluating_deepseek_v31_chat_with_a_minimal_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T22:29:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwlxo6</id>
    <title>Kimi K2 locally, my results and appreciation post</title>
    <updated>2025-08-21T20:15:26+00:00</updated>
    <author>
      <name>/u/koibKop4</name>
      <uri>https://old.reddit.com/user/koibKop4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've just run Kimi K2 locally and I'm amazed that I can run it completely locally. I'm fucking loving K2. &lt;/p&gt; &lt;p&gt;I'm just script kiddie, until now I was using ollama so any suggestions are very welcome.&lt;/p&gt; &lt;p&gt;My setup:&lt;br /&gt; AMD Ryzen Threadripper PRO 3945WX&lt;br /&gt; Asrock wrx80 creator 2.0 mobo&lt;br /&gt; 512 GB DDR4 3200 MHz (8 64gb sticks)&lt;br /&gt; Dual Kingston KC3000 2TB ZFS raid 0 for fast load&lt;br /&gt; Dual RTX 3090&lt;br /&gt; Proxmox 9 as host&lt;/p&gt; &lt;p&gt;I get 4-5t/s on IQ3_KS.&lt;/p&gt; &lt;p&gt;Command I use to run it:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./ik_llama.cpp/build/bin/llama-server \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--model ./ubergarm_Kimi-K2-Instruct-GGUF/IQ3_KS/Kimi-K2-Instruct-IQ3_KS-00001-of-00010.gguf \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--alias Kimi-K2 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--ctx-size 32768 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-ctk q8_0 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-fa -fmoe \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-mla 3 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-ngl 60 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-ot &amp;quot;blk\.(1|2).ffn_.*=CUDA0&amp;quot; \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-ot &amp;quot;blk\.(3|4)\.ffn_.*=CUDA1&amp;quot; \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-ot exps=CPU \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--parallel 1 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--threads 24 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--port 8080 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;\&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--no-mmap&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Is there anything I can do with this command to improve t/s?&lt;/p&gt; &lt;p&gt;Is changing CPU with more CCDs will improve t/s?&lt;/p&gt; &lt;p&gt;Also ik llama.cpp loads model with 1,4GB/s and system easily can read files form this pool with 12GB/s speed. What am I missing?&lt;/p&gt; &lt;p&gt;Also &lt;strong&gt;big thank you&lt;/strong&gt; and shutout to &lt;a href="/u/DigitalSpaceport"&gt;u/DigitalSpaceport&lt;/a&gt; and Ubergarm. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koibKop4"&gt; /u/koibKop4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwlxo6/kimi_k2_locally_my_results_and_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwlxo6/kimi_k2_locally_my_results_and_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwlxo6/kimi_k2_locally_my_results_and_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T20:15:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwpmkb</id>
    <title>Interesting (Opposite) decisions from Qwen and DeepSeek</title>
    <updated>2025-08-21T22:41:51+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;p&gt;Qwen&lt;/p&gt; &lt;ul&gt; &lt;li&gt;(Before) v3: hybrid thinking/non-thinking mode&lt;/li&gt; &lt;li&gt;(Now) v3-2507: thinking/non-thinking separated&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;DeepSeek:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;(Before) chat/r1 separated&lt;/li&gt; &lt;li&gt;(Now) v3.1: hybrid thinking/non-thinking mode&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwpmkb/interesting_opposite_decisions_from_qwen_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwpmkb/interesting_opposite_decisions_from_qwen_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwpmkb/interesting_opposite_decisions_from_qwen_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T22:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwb5ix</id>
    <title>Intern-S1-mini 8B multimodal is out!</title>
    <updated>2025-08-21T13:36:55+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Intern-S1-mini is a lightweight multimodal reasoning large language model ü§ñ.&lt;/p&gt; &lt;p&gt;Base: Built on Qwen3-8B üß† + InternViT-0.3B üëÅÔ∏è.&lt;/p&gt; &lt;p&gt;Training: Pretrained on 5 trillion tokens üìö, more than half from scientific domains (chemistry, physics, biology, materials science üß™).&lt;/p&gt; &lt;p&gt;Strengths: Can handle text, images, and video üí¨üñºÔ∏èüé•, excelling at scientific reasoning tasks like interpreting chemical structures, proteins, and materials data, while still performing well in general-purpose benchmarks.&lt;/p&gt; &lt;p&gt;Deployment: Small enough to run on a single GPU ‚ö°, and designed for compatibility with OpenAI-style APIs üîå, tool calling, and local inference frameworks like vLLM, LMDeploy, and Ollama.&lt;/p&gt; &lt;p&gt;Use case: A research assistant for real-world scientific applications, but still capable of general multimodal chat and reasoning.&lt;/p&gt; &lt;p&gt;‚ö° In short: it‚Äôs a science-focused, multimodal LLM optimized to be lightweight and high-performing.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-mini"&gt;https://huggingface.co/internlm/Intern-S1-mini&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb5ix/interns1mini_8b_multimodal_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb5ix/interns1mini_8b_multimodal_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwb5ix/interns1mini_8b_multimodal_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T13:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwl09x</id>
    <title>GitHub - karpathy/rendergit: Render any git repo into a single static HTML page for humans or LLMs</title>
    <updated>2025-08-21T19:40:24+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwl09x/github_karpathyrendergit_render_any_git_repo_into/"&gt; &lt;img alt="GitHub - karpathy/rendergit: Render any git repo into a single static HTML page for humans or LLMs" src="https://external-preview.redd.it/Z_OzWvTnBaBBexR3EpJ1SvbmUAtHerUflPeHuklK1G8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=775d586e3da9eceecb53bc5d089382254875755c" title="GitHub - karpathy/rendergit: Render any git repo into a single static HTML page for humans or LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Karpathy's at it again!&lt;/p&gt; &lt;p&gt;Simple, one file python script to flatten git repos into a single HTML file&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/karpathy/rendergit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwl09x/github_karpathyrendergit_render_any_git_repo_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwl09x/github_karpathyrendergit_render_any_git_repo_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T19:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw73uz</id>
    <title>DeepSeek has revealed that the next generation of China-made chips is about to be released</title>
    <updated>2025-08-21T10:25:14+00:00</updated>
    <author>
      <name>/u/Dry-Ad8947</name>
      <uri>https://old.reddit.com/user/Dry-Ad8947</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"&gt; &lt;img alt="DeepSeek has revealed that the next generation of China-made chips is about to be released" src="https://b.thumbs.redditmedia.com/Ior1GVKeeGS67aHoSqPUGnAsxjCS7RxbgR-JhoE9meg.jpg" title="DeepSeek has revealed that the next generation of China-made chips is about to be released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5j7osgkanckf1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bad0b7a62ad023889c86de7320fda3c7f4871f03"&gt;https://preview.redd.it/5j7osgkanckf1.png?width=1205&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bad0b7a62ad023889c86de7320fda3c7f4871f03&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In an official post on DeepSeek's official WeChat account, DeepSeek further explained that UE8M0 FP8 is designed for the upcoming next-generation domestic chip.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry-Ad8947"&gt; /u/Dry-Ad8947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw73uz/deepseek_has_revealed_that_the_next_generation_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T10:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw7x6f</id>
    <title>New DeepSeek API pricing: -chat prices increasing, -reasoner prices decreasing</title>
    <updated>2025-08-21T11:09:58+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7x6f/new_deepseek_api_pricing_chat_prices_increasing/"&gt; &lt;img alt="New DeepSeek API pricing: -chat prices increasing, -reasoner prices decreasing" src="https://preview.redd.it/d2xgmwobvckf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d9ce209cc69d0fc5c15c3d82253b16c89c0b2ac" title="New DeepSeek API pricing: -chat prices increasing, -reasoner prices decreasing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New API pricing scheme goes into effect on September 5, 2025: &lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;https://api-docs.deepseek.com/quick_start/pricing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2xgmwobvckf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7x6f/new_deepseek_api_pricing_chat_prices_increasing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw7x6f/new_deepseek_api_pricing_chat_prices_increasing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T11:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw86zw</id>
    <title>I‚Äôm gonna say it:</title>
    <updated>2025-08-21T11:24:07+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw86zw/im_gonna_say_it/"&gt; &lt;img alt="I‚Äôm gonna say it:" src="https://preview.redd.it/gxbn2ofuxckf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d114958447f9d74be0e026a7a772ce1a19fab790" title="I‚Äôm gonna say it:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gxbn2ofuxckf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw86zw/im_gonna_say_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw86zw/im_gonna_say_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T11:24:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3nat</id>
    <title>DeepSeek-V3.1 implements Anthropic API compatibility</title>
    <updated>2025-08-21T06:47:55+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"&gt; &lt;img alt="DeepSeek-V3.1 implements Anthropic API compatibility" src="https://preview.redd.it/0pp8mwjkkbkf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44e7a0529d636accee5763e0a807e41d636629b2" title="DeepSeek-V3.1 implements Anthropic API compatibility" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/guides/anthropic_api"&gt;https://api-docs.deepseek.com/guides/anthropic_api&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0pp8mwjkkbkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3nat/deepseekv31_implements_anthropic_api_compatibility/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw2lme</id>
    <title>Frontier AI labs‚Äô publicized 100k-H100 training runs under-deliver because software and systems don‚Äôt scale efficiently, wasting massive GPU fleets</title>
    <updated>2025-08-21T05:44:41+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"&gt; &lt;img alt="Frontier AI labs‚Äô publicized 100k-H100 training runs under-deliver because software and systems don‚Äôt scale efficiently, wasting massive GPU fleets" src="https://b.thumbs.redditmedia.com/qEAZdKzAv0zTNBneqhh9EDw7ENWMaMbCmlZSPzIj5Hw.jpg" title="Frontier AI labs‚Äô publicized 100k-H100 training runs under-deliver because software and systems don‚Äôt scale efficiently, wasting massive GPU fleets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mw2lme"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw2lme/frontier_ai_labs_publicized_100kh100_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T05:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwdgdw</id>
    <title>Command A Reasoning: Enterprise-grade control for AI agents</title>
    <updated>2025-08-21T15:02:58+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwdgdw/command_a_reasoning_enterprisegrade_control_for/"&gt; &lt;img alt="Command A Reasoning: Enterprise-grade control for AI agents" src="https://b.thumbs.redditmedia.com/F-wu3xdsFTQXIWp9SHh00FXzZZM-hiWCdK7h7OwtNkI.jpg" title="Command A Reasoning: Enterprise-grade control for AI agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://cohere.com/blog/command-a-reasoning"&gt;https://cohere.com/blog/command-a-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF Link: &lt;a href="https://huggingface.co/CohereLabs/command-a-reasoning-08-2025"&gt;https://huggingface.co/CohereLabs/command-a-reasoning-08-2025&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mwdgdw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwdgdw/command_a_reasoning_enterprisegrade_control_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwdgdw/command_a_reasoning_enterprisegrade_control_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T15:02:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwo7da</id>
    <title>AMA ‚Äì We built the first multimodal model designed for NPUs (runs on phones, PCs, cars &amp; IoT)</title>
    <updated>2025-08-21T21:43:18+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwo7da/ama_we_built_the_first_multimodal_model_designed/"&gt; &lt;img alt="AMA ‚Äì We built the first multimodal model designed for NPUs (runs on phones, PCs, cars &amp;amp; IoT)" src="https://external-preview.redd.it/kZrvcfv0jE0eTOar-xyOAJLNUR3Xhdr3kDPCkNfl-DA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e293ecc6dbbe315a3a4c9e7bb67307c0139d9f4e" title="AMA ‚Äì We built the first multimodal model designed for NPUs (runs on phones, PCs, cars &amp;amp; IoT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA üëã &lt;/p&gt; &lt;h1&gt;Here's what I observed&lt;/h1&gt; &lt;p&gt;GPUs have dominated local AI. But more and more devices now ship with NPUs ‚Äî from the latest Macs and iPhones to AIPC laptops, cars, and IoT.&lt;/p&gt; &lt;p&gt;If you have a dedicated GPU, it will still outperform. But on devices without one (like iPhones or laptops), the NPU can be the best option:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ö° Up to 1.5√ó faster than CPU and 4√ó faster than GPU for inference on Samsung S25 Ultra&lt;/li&gt; &lt;li&gt;üîã 2‚Äì8√ó more efficient than CPU/GPU&lt;/li&gt; &lt;li&gt;üñ•Ô∏è Frees CPU/GPU for multitasking&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Problem is:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Support for state-of-the-art models on NPUs is still very limited due to complexity.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our Solution:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So we built OmniNeural-4B + nexaML ‚Äî the first multimodal model and inference engine designed for NPUs from day one.&lt;/p&gt; &lt;p&gt;üëâ HuggingFace ü§ó: &lt;a href="https://huggingface.co/NexaAI/OmniNeural-4B"&gt;https://huggingface.co/NexaAI/OmniNeural-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OmniNeural is the first NPU-aware multimodal model that natively understands text, images, and audio and can runs across PCs, mobile devices, automotive, IoT, and more.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo Highlights&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üì± Mobile Phone NPU - Demo on Samsung S25 Ultra: Fully local, multimodal, and conversational AI assistant that hears you and sees what you see, running &lt;strong&gt;natively on Snapdragon NPU&lt;/strong&gt; for long battery life and low latency.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mwo7da/video/z8gbckz1zfkf1/player"&gt;https://reddit.com/link/1mwo7da/video/z8gbckz1zfkf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª Laptop demo: Three capabilities, all local on NPU in CLI:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-Image Reasoning ‚Üí ‚Äúspot the difference‚Äù&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Poster + Text ‚Üí function call (‚Äúadd to calendar‚Äù)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Multi-Audio Comparison ‚Üí tell songs apart offline&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1mwo7da/video/fzw7c1d6zfkf1/player"&gt;https://reddit.com/link/1mwo7da/video/fzw7c1d6zfkf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vision: Wins/ties ~75% of prompts vs Apple Foundation, Gemma-3n-E4B, Qwen2.5-Omni-3B&lt;/li&gt; &lt;li&gt;Audio: Clear lead over Gemma3n &amp;amp; Apple baselines&lt;/li&gt; &lt;li&gt;Text: Matches or outperforms leading multimodal baselines&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4hdj26n9zfkf1.png?width=3696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a73ed2a7024df8da14b8c19d48a6fa7ce8a6b942"&gt;https://preview.redd.it/4hdj26n9zfkf1.png?width=3696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a73ed2a7024df8da14b8c19d48a6fa7ce8a6b942&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For a deeper dive, here‚Äôs our 18-min launch video with detailed explanation and demos: &lt;a href="https://x.com/nexa_ai/status/1958197904210002092"&gt;https://x.com/nexa_ai/status/1958197904210002092&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôd like to see more models supported on NPUs, a like on HuggingFace ‚ù§Ô∏è helps us gauge demand. HuggingFace Repo: &lt;a href="https://huggingface.co/NexaAI/OmniNeural-4B"&gt;https://huggingface.co/NexaAI/OmniNeural-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our research and product team will be around to answer questions ‚Äî AMA! Looking forward to the discussion. üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwo7da/ama_we_built_the_first_multimodal_model_designed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwo7da/ama_we_built_the_first_multimodal_model_designed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwo7da/ama_we_built_the_first_multimodal_model_designed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T21:43:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwmyw8</id>
    <title>Deepseek 3.1 on Fiction.liveBench</title>
    <updated>2025-08-21T20:55:18+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwmyw8/deepseek_31_on_fictionlivebench/"&gt; &lt;img alt="Deepseek 3.1 on Fiction.liveBench" src="https://preview.redd.it/pnf13qlerfkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4142c260b451023e42b499b41c0ccdf66ab2e4c" title="Deepseek 3.1 on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pnf13qlerfkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwmyw8/deepseek_31_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwmyw8/deepseek_31_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T20:55:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mw3c7s</id>
    <title>deepseek-ai/DeepSeek-V3.1 ¬∑ Hugging Face</title>
    <updated>2025-08-21T06:28:56+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1 ¬∑ Hugging Face" src="https://external-preview.redd.it/RJXEgvNDm4zhSkGlks1Mt4ppnLOAENNDWYNaVwpLE9k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d409de545b1a58fad7e22e741370f1a55018f432" title="deepseek-ai/DeepSeek-V3.1 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mw3c7s/deepseekaideepseekv31_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T06:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwhfw9</id>
    <title>Drummer's Behemoth R1 123B v2 - A reasoning Largestral 2411 - Absolute Cinema!</title>
    <updated>2025-08-21T17:27:02+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwhfw9/drummers_behemoth_r1_123b_v2_a_reasoning/"&gt; &lt;img alt="Drummer's Behemoth R1 123B v2 - A reasoning Largestral 2411 - Absolute Cinema!" src="https://external-preview.redd.it/hsj8k1X0C_q74iyN-NqkYYL7JWSy6JJ56ZytmJNIMLY.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb956e24aa9edc85fb2c39b0135eafbdeb819d39" title="Drummer's Behemoth R1 123B v2 - A reasoning Largestral 2411 - Absolute Cinema!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Behemoth-R1-123B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwhfw9/drummers_behemoth_r1_123b_v2_a_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwhfw9/drummers_behemoth_r1_123b_v2_a_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T17:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwla9s</id>
    <title>[Model Release] Deca 3 Alpha Ultra 4.6T! Parameters</title>
    <updated>2025-08-21T19:50:55+00:00</updated>
    <author>
      <name>/u/MohamedTrfhgx</name>
      <uri>https://old.reddit.com/user/MohamedTrfhgx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; No commercial use without a commercial license.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deca-ai/3-alpha-ultra"&gt;https://huggingface.co/deca-ai/3-alpha-ultra&lt;/a&gt;&lt;br /&gt; Deca 3 Alpha Ultra is a large-scale language model built on a &lt;strong&gt;DynAMoE (Dynamically Activated Mixture of Experts)&lt;/strong&gt; architecture, differing from traditional MoE systems. With &lt;strong&gt;4.6 trillion parameters&lt;/strong&gt;, it is among the largest publicly described models, developed with funding from &lt;strong&gt;GenLabs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Specs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Architecture: DynAMoE&lt;/li&gt; &lt;li&gt;Parameters: 4.6T&lt;/li&gt; &lt;li&gt;Training: Large multilingual, multi-domain dataset&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Capabilities&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Language understanding and generation&lt;/li&gt; &lt;li&gt;Summarization, content creation, sentiment analysis&lt;/li&gt; &lt;li&gt;Multilingual and contextual reasoning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High compute requirements&lt;/li&gt; &lt;li&gt;Limited interpretability&lt;/li&gt; &lt;li&gt;Shallow coverage in niche domains&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Content generation, conversational AI, research, and educational tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MohamedTrfhgx"&gt; /u/MohamedTrfhgx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwla9s/model_release_deca_3_alpha_ultra_46t_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwla9s/model_release_deca_3_alpha_ultra_46t_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwla9s/model_release_deca_3_alpha_ultra_46t_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T19:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwexgd</id>
    <title>DeepSeek V3.1 (Thinking) aggregated benchmarks (vs. gpt-oss-120b)</title>
    <updated>2025-08-21T15:56:14+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwexgd/deepseek_v31_thinking_aggregated_benchmarks_vs/"&gt; &lt;img alt="DeepSeek V3.1 (Thinking) aggregated benchmarks (vs. gpt-oss-120b)" src="https://b.thumbs.redditmedia.com/LSriMSIBiKRIhiPtaWtN1ed8AonIRYXPSVg0ORTkNXA.jpg" title="DeepSeek V3.1 (Thinking) aggregated benchmarks (vs. gpt-oss-120b)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was personally interested in comparing with gpt-oss-120b on intelligence vs. speed, tabulating those numbers below for reference:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;DeepSeek 3.1 (Thinking)&lt;/th&gt; &lt;th align="left"&gt;gpt-oss-120b (High)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Total parameters&lt;/td&gt; &lt;td align="left"&gt;671B&lt;/td&gt; &lt;td align="left"&gt;120B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Active parameters&lt;/td&gt; &lt;td align="left"&gt;37B&lt;/td&gt; &lt;td align="left"&gt;5.1B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Context&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;131K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Intelligence Index&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Coding Index&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Math Index&lt;/td&gt; &lt;td align="left"&gt;?&lt;/td&gt; &lt;td align="left"&gt;?&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Response Time (500 tokens + thinking)&lt;/td&gt; &lt;td align="left"&gt;127.8 s&lt;/td&gt; &lt;td align="left"&gt;11.5 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Output Speed (tokens / s)&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;228&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Cheapest Openrouter Provider Pricing (input / output)&lt;/td&gt; &lt;td align="left"&gt;$0.32 / $1.15&lt;/td&gt; &lt;td align="left"&gt;$0.072 / $0.28&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mwexgd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwexgd/deepseek_v31_thinking_aggregated_benchmarks_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwexgd/deepseek_v31_thinking_aggregated_benchmarks_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T15:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwevt4</id>
    <title>Why low-bit models aren't totally braindead: A guide from 1-bit meme to FP16 research</title>
    <updated>2025-08-21T15:54:35+00:00</updated>
    <author>
      <name>/u/Small-Fall-6500</name>
      <uri>https://old.reddit.com/user/Small-Fall-6500</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwevt4/why_lowbit_models_arent_totally_braindead_a_guide/"&gt; &lt;img alt="Why low-bit models aren't totally braindead: A guide from 1-bit meme to FP16 research" src="https://preview.redd.it/5t58iz5u9ekf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=100a692755cfd42ca5aa55904085997b9a5c2344" title="Why low-bit models aren't totally braindead: A guide from 1-bit meme to FP16 research" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright, it's not exactly the same picture, but the core idea is quite similar. This post will explain how, by breaking down LLM quantization into varying levels of precision, starting from a 1-bit meme, then a 2-bit TL;DR, 4-bit overview, 8-bit further reading, and lastly the highest precision FP16 research itself.&lt;/p&gt; &lt;h1&gt;Q1 Version (The Meme Above)&lt;/h1&gt; &lt;p&gt;That's it. A high-compression, low-nuance, instant-takeaway version of the entire concept.&lt;/p&gt; &lt;h1&gt;Q2 Version (The TL;DR)&lt;/h1&gt; &lt;p&gt;LLM quantization is JPEG compression for an AI brain.&lt;/p&gt; &lt;p&gt;It‚Äôs all about smart sacrifices, throwing away the least important information to make the model massively smaller, while keeping the core of its intelligence intact. JPEG keeps the general shapes and colors of an image while simplifying the details you won't miss. Quantization does the same to a model's &amp;quot;weights&amp;quot; (its learned knowledge), keeping the most critical parts at high precision while squashing the rest to low precision.&lt;/p&gt; &lt;h1&gt;Q4 Version (Deeper Dive)&lt;/h1&gt; &lt;p&gt;Like a JPEG, the more you compress, the more detail you lose. But if the original model is big enough (like a 70B parameter model), you can compress it a lot before quality drops noticeably.&lt;/p&gt; &lt;p&gt;So, can only big models be highly quantized? Not quite. There are a few key tricks that make even small models maintain their usefulness at low-precision:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trick #1: Mixed Precision (Not All Knowledge is Equal)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The parts of the model that handle grammar are probably more important than the part that remembers 14th-century basket-weaving history. Modern quantization schemes understand this. They intelligently assign more bits to the &amp;quot;important&amp;quot; parts of the model and fewer bits to the &amp;quot;less important&amp;quot; parts. It‚Äôs not a uniform 2-bit model; it's an average of 2-bits, preserving performance where it matters most.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trick #2: Calibration (Smart Rounding)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of just blindly rounding numbers, quantization uses a &amp;quot;calibration dataset.&amp;quot; It runs a small amount of data through the model to figure out the best way to group and round the weights to minimize information loss. It tunes the compression algorithm specifically for that one model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trick #3: New Architectures (Building for Compression)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Why worry about quantization after training a model when you can just start with the model already quantized? It turns out, it‚Äôs possible to design models from the ground up to run at super low precision. Microsoft's BitNet is the most well-known example, which started with a true 1-bit precision model, for both training and inference. They expanded this to a more efficient ~1.58 bit precision (using only -1, 0, or 1 for each of its weights).&lt;/p&gt; &lt;h1&gt;Q8 Resources (Visuals &amp;amp; Docs)&lt;/h1&gt; &lt;p&gt;A higher-precision look at the concepts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual Overview (Article):&lt;/strong&gt; &lt;a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization"&gt;A Visual Guide to Quantization&lt;/a&gt; - An intuitive breakdown of these ideas.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specific Implementations (Docs):&lt;/strong&gt; &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;Unsloth Dynamic 2.0 GGUFs&lt;/a&gt; - See how a recent quantization method uses these tricks to maximize performance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Great Overview (Video):&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=WBm0nyDkVYM"&gt;The myth of 1-bit LLMs&lt;/a&gt; - A fantastic video explaining Quantization-Aware Training.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;FP16 Resources (Foundational Research)&lt;/h1&gt; &lt;p&gt;The full precision source material:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Original BitNet Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers&lt;/a&gt; - The paper that started the 1-bit hype.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Updated Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs (1.58-bit)&lt;/a&gt; - Microsoft's follow-up showing incredible results with ternary weights.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Bitnet Model Weights:&lt;/strong&gt; &lt;a href="https://huggingface.co/microsoft/bitnet-b1.58-2B-4T"&gt;microsoft/bitnet-b1.58-2B-4T&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Small-Fall-6500"&gt; /u/Small-Fall-6500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5t58iz5u9ekf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwevt4/why_lowbit_models_arent_totally_braindead_a_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwevt4/why_lowbit_models_arent_totally_braindead_a_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T15:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwme5c</id>
    <title>Pewdiepie‚Äôs monstrous 160GB Vram build</title>
    <updated>2025-08-21T20:32:55+00:00</updated>
    <author>
      <name>/u/joseph_the_69th</name>
      <uri>https://old.reddit.com/user/joseph_the_69th</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwme5c/pewdiepies_monstrous_160gb_vram_build/"&gt; &lt;img alt="Pewdiepie‚Äôs monstrous 160GB Vram build" src="https://external-preview.redd.it/zQgZCeoj46IUkydlNZy5fsyhmsqrk550dmk1a_cyvRo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61bb8f79b44594ea4057ee2fd836a112ed1846ce" title="Pewdiepie‚Äôs monstrous 160GB Vram build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;He was talking about running llama 3 70B on half of the gpus. so we might be getting a pewdiepie local llm arc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joseph_the_69th"&gt; /u/joseph_the_69th &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/2JzOe1Hs26Q?si=9Ck53vK9hja3BZD7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwme5c/pewdiepies_monstrous_160gb_vram_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwme5c/pewdiepies_monstrous_160gb_vram_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T20:32:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwbsww</id>
    <title>Love small but mighty team of DeepSeek</title>
    <updated>2025-08-21T14:02:32+00:00</updated>
    <author>
      <name>/u/dbhalla4</name>
      <uri>https://old.reddit.com/user/dbhalla4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwbsww/love_small_but_mighty_team_of_deepseek/"&gt; &lt;img alt="Love small but mighty team of DeepSeek" src="https://preview.redd.it/38d427vmpdkf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54c26ac5a1090a8e75470e0b3b24a163e5402cee" title="Love small but mighty team of DeepSeek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They are working so hard they are even inventing new spellings!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dbhalla4"&gt; /u/dbhalla4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/38d427vmpdkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwbsww/love_small_but_mighty_team_of_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwbsww/love_small_but_mighty_team_of_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-21T14:02:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
