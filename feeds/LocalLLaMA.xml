<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-06T00:25:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ii9tzt</id>
    <title>What do people use their AI for?</title>
    <updated>2025-02-05T13:24:08+00:00</updated>
    <author>
      <name>/u/salixfire</name>
      <uri>https://old.reddit.com/user/salixfire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just gotten started with LLMs and have Kobold CPP and Silly Tavern running on my PC to use for RP. I'm quite enjoying it, but I know the AI can be used for all sorts of things. Due to a disability, reading through pages and pages of stuff about it has left me more confused than ever. What sorts of things can it actually be used for nowadays? I've heard of everything from coding to it making money for people to videos and a whole lot more. What is the truth of the matter and how do you use it in your day to day lives. I am always looking to make things easier for myself due to my disabilities. Do you think there's specific things I could use it for that would help me? Please keep things simple in the explanation, not quite ELI5 level but probably not far off. I'm very out of the loop.&lt;/p&gt; &lt;p&gt;Thank you 😊&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salixfire"&gt; /u/salixfire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9tzt/what_do_people_use_their_ai_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9tzt/what_do_people_use_their_ai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9tzt/what_do_people_use_their_ai_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1iimzg3</id>
    <title>New AI Tool that Roasts your PowerPoints</title>
    <updated>2025-02-05T22:34:07+00:00</updated>
    <author>
      <name>/u/Embarrassed_Author68</name>
      <uri>https://old.reddit.com/user/Embarrassed_Author68</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lol this new tool roasts your slides before your teacher/boss does &lt;a href="https://roastmypowerpoint.com"&gt;https://roastmypowerpoint.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed_Author68"&gt; /u/Embarrassed_Author68 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iimzg3/new_ai_tool_that_roasts_your_powerpoints/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iimzg3/new_ai_tool_that_roasts_your_powerpoints/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iimzg3/new_ai_tool_that_roasts_your_powerpoints/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T22:34:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihqwnd</id>
    <title>OpenAI deep research but it's open source</title>
    <updated>2025-02-04T20:01:31+00:00</updated>
    <author>
      <name>/u/Thomjazz</name>
      <uri>https://old.reddit.com/user/Thomjazz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt; &lt;img alt="OpenAI deep research but it's open source" src="https://external-preview.redd.it/VhiwZJj7J5TUIfA6ujpiUeYD8CI4AKINeo7sLJZlD5Q.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e05cfb506bcac565f349480a4b0d9ba18f4768b1" title="OpenAI deep research but it's open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46"&gt;https://preview.redd.it/533g8jx5h6he1.png?width=2510&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e137b60ba2abab06e5e2f711091beea5958d6f46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://huggingface.co/blog/open-deep-research"&gt;https://huggingface.co/blog/open-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thomjazz"&gt; /u/Thomjazz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihqwnd/openai_deep_research_but_its_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T20:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ihph9f</id>
    <title>In case you thought your feedback was not being heard</title>
    <updated>2025-02-04T19:03:04+00:00</updated>
    <author>
      <name>/u/takuonline</name>
      <uri>https://old.reddit.com/user/takuonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt; &lt;img alt="In case you thought your feedback was not being heard" src="https://preview.redd.it/nvf2f1j876he1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c8871a5964dc8f2db918ba181e1157fc626b71" title="In case you thought your feedback was not being heard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/takuonline"&gt; /u/takuonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nvf2f1j876he1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ihph9f/in_case_you_thought_your_feedback_was_not_being/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-04T19:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiizaa</id>
    <title>Alternative to DeepResearch</title>
    <updated>2025-02-05T19:49:27+00:00</updated>
    <author>
      <name>/u/konilse</name>
      <uri>https://old.reddit.com/user/konilse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiizaa/alternative_to_deepresearch/"&gt; &lt;img alt="Alternative to DeepResearch" src="https://b.thumbs.redditmedia.com/sgkPYpI38cW2E8rJ26rSE9b3Bp4zRafoewXmPvpNVJc.jpg" title="Alternative to DeepResearch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HuggingFace published an alternative to deepresearch that seems quite interesting &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3duhicf9kdhe1.png?width=765&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e158c9a5283d76f9cd578e6047a90ba2a5f7abd"&gt;https://preview.redd.it/3duhicf9kdhe1.png?width=765&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7e158c9a5283d76f9cd578e6047a90ba2a5f7abd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/konilse"&gt; /u/konilse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiizaa/alternative_to_deepresearch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiizaa/alternative_to_deepresearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiizaa/alternative_to_deepresearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiixsv</id>
    <title>Good MoE Models smaller than R1?</title>
    <updated>2025-02-05T19:47:47+00:00</updated>
    <author>
      <name>/u/And1mon</name>
      <uri>https://old.reddit.com/user/And1mon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for MoE models of similar sizes like the original Mixtral 8x7B. Is there anything competitive available at the moment?&lt;/p&gt; &lt;p&gt;Background:&lt;br /&gt; I have a pc with 12GB vram and 64GB ram, large models like Llama 3.3 theoretically fit in my ram, but are slow of course (slightly over 1 t/s). However, similar sized MoEs like Mixtral 8x7 are a lot faster, like 4 to 5 t/s, which is at least usable for some things. Of course, fitting into vram is the fastest, but I like experimenting with larger models and don't want to buy new gpus yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/And1mon"&gt; /u/And1mon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiixsv/good_moe_models_smaller_than_r1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiixsv/good_moe_models_smaller_than_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiixsv/good_moe_models_smaller_than_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1iic1ks</id>
    <title>Kokoro voice model extrapolation, blending, and experimenting python application</title>
    <updated>2025-02-05T15:06:21+00:00</updated>
    <author>
      <name>/u/rodbiren</name>
      <uri>https://old.reddit.com/user/rodbiren</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I have been playing around with blending the kokoro voice models (excellent&lt;a href="https://github.com/thewh1teagle/kokoro-onnx"&gt; text to speech library&lt;/a&gt; and &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;model&lt;/a&gt;) and determined I wanted more capability to create voices. I made an application that uses sqlite queries to select groups of voices based on the query. It then creates a linear model between the two voice groups which allows for easy blending of the voices, but also allows for &lt;strong&gt;extrapolation&lt;/strong&gt; of the voices.&lt;/p&gt; &lt;p&gt;For instance, if I make a group of British and a group of American voices I can model between and beyond them. This effectively allows you to make &amp;quot;extreme&amp;quot; versions of the difference in vocal traits between groups. You can make &lt;strong&gt;very&lt;/strong&gt; British and &lt;strong&gt;very&lt;/strong&gt; American accents. The code also allows for exporting voice models into other formats for use in other applications. Examples, codes, and instructions in the github.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RobViren/kokovoicelab"&gt;https://github.com/RobViren/kokovoicelab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rodbiren"&gt; /u/rodbiren &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iic1ks/kokoro_voice_model_extrapolation_blending_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iic1ks/kokoro_voice_model_extrapolation_blending_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iic1ks/kokoro_voice_model_extrapolation_blending_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T15:06:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiml6u</id>
    <title>Which models are you most excited about for 2025?</title>
    <updated>2025-02-05T22:17:25+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which do you think will be most shockingly amazing for math/coding/vision/general intelligence or something else entirely?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiml6u/which_models_are_you_most_excited_about_for_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiml6u/which_models_are_you_most_excited_about_for_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiml6u/which_models_are_you_most_excited_about_for_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T22:17:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7dfq</id>
    <title>Upgrading 3090's from 24GB to 48GB</title>
    <updated>2025-02-05T10:49:34+00:00</updated>
    <author>
      <name>/u/CertainlyBright</name>
      <uri>https://old.reddit.com/user/CertainlyBright</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm able to do this conversion but the raw parts cost is 550$ for all the new vram. &lt;/p&gt; &lt;p&gt;Plus labor, is that even worth it anymore for others to pay for the upgrade if I offer it as a service?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CertainlyBright"&gt; /u/CertainlyBright &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7dfq/upgrading_3090s_from_24gb_to_48gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7dfq/upgrading_3090s_from_24gb_to_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7dfq/upgrading_3090s_from_24gb_to_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T10:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiaa1r</id>
    <title>GRPO (the method used by Deepseek) will be worse than the original model if you make a mistake in the reward function.</title>
    <updated>2025-02-05T13:47:13+00:00</updated>
    <author>
      <name>/u/dahara111</name>
      <uri>https://old.reddit.com/user/dahara111</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiaa1r/grpo_the_method_used_by_deepseek_will_be_worse/"&gt; &lt;img alt="GRPO (the method used by Deepseek) will be worse than the original model if you make a mistake in the reward function." src="https://preview.redd.it/vt6h2sj2rbhe1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10a4c4476b0b3303072dc9e8ed1aa00784dc242e" title="GRPO (the method used by Deepseek) will be worse than the original model if you make a mistake in the reward function." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dahara111"&gt; /u/dahara111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vt6h2sj2rbhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiaa1r/grpo_the_method_used_by_deepseek_will_be_worse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiaa1r/grpo_the_method_used_by_deepseek_will_be_worse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiitl5</id>
    <title>Andrej Karpathy: Deep Dive into LLMs Like ChatGPT</title>
    <updated>2025-02-05T19:43:08+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiitl5/andrej_karpathy_deep_dive_into_llms_like_chatgpt/"&gt; &lt;img alt="Andrej Karpathy: Deep Dive into LLMs Like ChatGPT" src="https://external-preview.redd.it/DmIKQR1Qh6xD2A-jd67MgvOzKDXIcFDp0jJD5ODYpIY.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b94b4626807e1636998d3593911753a052fdde2" title="Andrej Karpathy: Deep Dive into LLMs Like ChatGPT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=7xTGNNLPyMI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiitl5/andrej_karpathy_deep_dive_into_llms_like_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiitl5/andrej_karpathy_deep_dive_into_llms_like_chatgpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii7qfy</id>
    <title>I created a website that tracks AI regulations around the world</title>
    <updated>2025-02-05T11:15:59+00:00</updated>
    <author>
      <name>/u/techie_ray</name>
      <uri>https://old.reddit.com/user/techie_ray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To help you stay on top of what governments are doing on AI, I created an interactive world map that tracks AI regulatory and policy developments around the world. Click on a region (or use the search bar) to view its profile. This website is updated regularly (including new regions to be added).&lt;/p&gt; &lt;p&gt;Free to access. No login required. This is for the community :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techieray.com/GlobalAIRegulationTracker"&gt;https://www.techieray.com/GlobalAIRegulationTracker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techie_ray"&gt; /u/techie_ray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii7qfy/i_created_a_website_that_tracks_ai_regulations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iihjq1</id>
    <title>Announcing Sage: Open-source voice chat with LLMs</title>
    <updated>2025-02-05T18:52:04+00:00</updated>
    <author>
      <name>/u/felixatwood</name>
      <uri>https://old.reddit.com/user/felixatwood</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iihjq1/announcing_sage_opensource_voice_chat_with_llms/"&gt; &lt;img alt="Announcing Sage: Open-source voice chat with LLMs" src="https://external-preview.redd.it/Do-ORxNEK_7XDMp8cozVLzedSBauVAy968xPLSTBvJg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=321ce755c82cbf201d2d52979b4c2663df16e1df" title="Announcing Sage: Open-source voice chat with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/felixatwood"&gt; /u/felixatwood &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/farshed/sage"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iihjq1/announcing_sage_opensource_voice_chat_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iihjq1/announcing_sage_opensource_voice_chat_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T18:52:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iij58e</id>
    <title>S1-32B: The $6 R1 Competitor?</title>
    <updated>2025-02-05T19:56:02+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://timkellogg.me/blog/2025/02/03/s1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iij58e/s132b_the_6_r1_competitor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iij58e/s132b_the_6_r1_competitor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:56:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii3qvv</id>
    <title>Google Lifts a Ban on Using Its AI for Weapons and Surveillance</title>
    <updated>2025-02-05T06:18:38+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"&gt; &lt;img alt="Google Lifts a Ban on Using Its AI for Weapons and Surveillance" src="https://external-preview.redd.it/NrA5s-vSHIcN9SaUL0ETUoJ_dGVpcnD0UsdffV5wGi8.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afb92e2820ff849c88d693e6467404d7df633be8" title="Google Lifts a Ban on Using Its AI for Weapons and Surveillance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/google-responsible-ai-principles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii3qvv/google_lifts_a_ban_on_using_its_ai_for_weapons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T06:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iikhg3</id>
    <title>I found a way to speed up CPU based LLM inference using a HNSW index on the output embeddings</title>
    <updated>2025-02-05T20:50:51+00:00</updated>
    <author>
      <name>/u/martinloretz</name>
      <uri>https://old.reddit.com/user/martinloretz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To get the next token from an LLM, we compute the probabilities for each individual token in the LLM's vocabulary by multiplying the last hidden state with the output embedding matrix. This matrix is massive, accounting for up to 20% of the total parameters in small multilingual LLMs. &lt;/p&gt; &lt;p&gt;When sampling the next token with top-k sampling, we're only sampling from the 40 most probable tokens out of 128,256 (for Llama 3.2 models). By using an HNSW vector index, we can retrieve these 40 most probable tokens directly through an approximate nearest neighbor search over the output embeddings, avoiding the full matrix multiplication with the output embeddings. &lt;/p&gt; &lt;p&gt;This reduces memory accesses and computation, resulting in up to 28% faster CPU-based inference for Llama 2.1 1B on mid-range laptops.&lt;/p&gt; &lt;h3&gt;For more details, read the full blog post on &lt;a href="https://martinloretz.com/blog/vector-index-cpu/"&gt;martinloretz.com/blog/vector-index-cpu/&lt;/a&gt;&lt;/h3&gt; &lt;h2&gt;Benchmarks&lt;/h2&gt; &lt;p&gt;&lt;code&gt;llama-bench&lt;/code&gt; for Llama 1B F16 (Ubuntu = Intel® Core™ i7-10750H x 12, 2 x 16GiB DDR4 2933 MHz, MacBook = MacBook Pro 16&amp;quot; M4 Pro, vec = vector index, MM = matrix multiplication (reference)):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="right"&gt;threads&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;Vec t/s&lt;/th&gt; &lt;th align="right"&gt;MM t/s&lt;/th&gt; &lt;th align="right"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Ubuntu&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;5.99 ± 0.05&lt;/td&gt; &lt;td align="right"&gt;4.73 ± 0.04&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.27&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ubuntu&lt;/td&gt; &lt;td align="right"&gt;6&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;12.51 ± 0.30&lt;/td&gt; &lt;td align="right"&gt;9.72 ± 0.13&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.29&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;23.56 ± 0.24&lt;/td&gt; &lt;td align="right"&gt;20.11 ± 0.44&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.17&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MacBook&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;tg256&lt;/td&gt; &lt;td align="right"&gt;12.52 ± 0.31&lt;/td&gt; &lt;td align="right"&gt;11.80 ± 0.18&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1.06&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LLama 3.2 1B was selected for these benchmarks because of its relatively large embedding matrix (21% of all parameters). Full model speedups for larger models are lower because less time is spent computing the output embeddings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;To replicate these benchmarks, checkout this code of the &lt;a href="https://github.com/martinloretzzz/llama.cpp"&gt;fork of llama.cpp&lt;/a&gt;.&lt;/strong&gt; Installation instructions are in the Readme.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinloretz"&gt; /u/martinloretz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iikhg3/i_found_a_way_to_speed_up_cpu_based_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iikhg3/i_found_a_way_to_speed_up_cpu_based_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iikhg3/i_found_a_way_to_speed_up_cpu_based_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T20:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii9lab</id>
    <title>2B model beats 72B model</title>
    <updated>2025-02-05T13:10:56+00:00</updated>
    <author>
      <name>/u/TheLogiqueViper</name>
      <uri>https://old.reddit.com/user/TheLogiqueViper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9lab/2b_model_beats_72b_model/"&gt; &lt;img alt="2B model beats 72B model" src="https://preview.redd.it/nxx7b0kblbhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7a412b056534d115469db02236cac1fd22d5d1a" title="2B model beats 72B model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Deep-Agent/R1-V"&gt;https://github.com/Deep-Agent/R1-V&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 2B model outperforms the 72B model. &lt;/p&gt; &lt;p&gt;Only 100 training steps, costing less than $3.&lt;/p&gt; &lt;p&gt;The outperformance is in both effectiveness and out-of-distribution (OOD) robustness for vision language models.&lt;/p&gt; &lt;p&gt;in OOD tests within just 100 training steps. &lt;/p&gt; &lt;p&gt;R1-V is released, and fully open-sourced. &lt;/p&gt; &lt;p&gt;The project shows a 2B-parameter model surpassing a 72B-parameter counterpart in generalization tests. &lt;/p&gt; &lt;p&gt;With only 100 training steps (vs. thousands in conventional methods), 30 minutes on 8 A100 GPUs and $2.62 total cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLogiqueViper"&gt; /u/TheLogiqueViper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nxx7b0kblbhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9lab/2b_model_beats_72b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii9lab/2b_model_beats_72b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:10:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iik4y9</id>
    <title>Train your own reasoning model in 30 minutes with Deepseek R1 and Kiln AI</title>
    <updated>2025-02-05T20:36:22+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"&gt; &lt;img alt="Train your own reasoning model in 30 minutes with Deepseek R1 and Kiln AI" src="https://external-preview.redd.it/fkk_hfuiSuMOZjLy_dEtjSiqJMOwZz9w_oAKY_5Q2Nk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3dadc03291c7ac04f201561f33b9b740f85a835" title="Train your own reasoning model in 30 minutes with Deepseek R1 and Kiln AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've just released an update of Kiln on Github which allows you to distill a custom fine-tuned model from Deepseek R1 (or any reasoning model/chain-of-thought). The whole process only takes about 30 minutes, including generating a synthetic training dataset. It doesn't require any coding or command line work.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The attached video shows the process&lt;/li&gt; &lt;li&gt;Our docs have &lt;a href="https://docs.getkiln.ai/docs/guide-train-a-reasoning-model"&gt;a guide for distilling R1&lt;/a&gt; if you want to try it out yourself&lt;/li&gt; &lt;li&gt;Here's the &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Github repo&lt;/a&gt; with all of the source code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also wanted to add a huge thanks to &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt; for the awesome reception to on my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i1ffid/i_accidentally_built_an_open_alternative_to/"&gt;last post&lt;/a&gt;. It really inspires me to keep building. I've already made about 30 improvements and built feature requests which came from people who found it via &lt;a href="/r/localllama"&gt;r/localllama&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Kiln runs locally and we never have access to your dataset. Unsloth is fully supported if you have the GPUs to train locally. You can also use a training service like Fireworks &amp;amp; OpenAI if you prefer (data is sent to them with your keys, we still never have access to it). &lt;/p&gt; &lt;p&gt;If anyone wants to try Kiln, here's the &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;GitHub repository&lt;/a&gt; and &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;docs are here&lt;/a&gt;. Getting started is super easy - it's a one-click install to get setup and running.&lt;/p&gt; &lt;p&gt;I'm curious to get any feedback/ideas. It really helps me improve Kiln. Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1iik4y9/video/1vnufrecrdhe1/player"&gt;Kiln AI demo - distilling Deepseek R1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iik4y9/train_your_own_reasoning_model_in_30_minutes_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T20:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iihbxk</id>
    <title>Those moments in time you wish would last forever</title>
    <updated>2025-02-05T18:43:04+00:00</updated>
    <author>
      <name>/u/beezbos_trip</name>
      <uri>https://old.reddit.com/user/beezbos_trip</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iihbxk/those_moments_in_time_you_wish_would_last_forever/"&gt; &lt;img alt="Those moments in time you wish would last forever" src="https://preview.redd.it/awwwugte8dhe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b62ddb1e6dac5bc36f41ce25fb146a4bdd9fae8d" title="Those moments in time you wish would last forever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beezbos_trip"&gt; /u/beezbos_trip &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awwwugte8dhe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iihbxk/those_moments_in_time_you_wish_would_last_forever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iihbxk/those_moments_in_time_you_wish_would_last_forever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T18:43:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1iih21v</id>
    <title>Google's been at work, not Gemma 3 sadly</title>
    <updated>2025-02-05T18:32:11+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iih21v/googles_been_at_work_not_gemma_3_sadly/"&gt; &lt;img alt="Google's been at work, not Gemma 3 sadly" src="https://preview.redd.it/x5uaqeak6dhe1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87ac5db8ef998927b98f0559468dd0f99a87fa19" title="Google's been at work, not Gemma 3 sadly" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x5uaqeak6dhe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iih21v/googles_been_at_work_not_gemma_3_sadly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iih21v/googles_been_at_work_not_gemma_3_sadly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T18:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiafzm</id>
    <title>We have to fight back now.</title>
    <updated>2025-02-05T13:55:35+00:00</updated>
    <author>
      <name>/u/mr_happy_nice</name>
      <uri>https://old.reddit.com/user/mr_happy_nice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-source innovation is the lifeblood of American progress, and any attempt to lock it down is a threat to our future. Banning open-source AI under harsh penalties will only stifle the creativity, transparency, and collaboration that have fueled our tech breakthroughs for decades. When anyone can build on and improve each other’s work, we all win—especially in the race for a safer, smarter tomorrow.&lt;/p&gt; &lt;p&gt;We need to stand together for a future where ideas flow freely and innovation isn’t held hostage. Embracing open-source means a stronger, more competitive American tech ecosystem that benefits everyone, from citizens to startups to established giants. The open road is the best road—let’s keep it that way.&lt;/p&gt; &lt;p&gt;The only thing that these people understand is money. So, follow the money. Here are some of Hawley’s contributors to get you started. You have a right to have your voice be heard. Let them hear. &lt;/p&gt; &lt;h1&gt;Smead Capital Management&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Addresses &amp;amp; Phone Numbers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Phoenix Office:&lt;/em&gt; 2502 E. Camelback Rd, Suite 210, Phoenix, AZ 85016 Phone: 602.889.3660&lt;/li&gt; &lt;li&gt;&lt;em&gt;Jersey City Office:&lt;/em&gt; 30 Montgomery St, Suite 920, Jersey City, NJ 07302 Phone: 484.535.5121&lt;/li&gt; &lt;li&gt;&lt;em&gt;London Office (UK):&lt;/em&gt; 18th Floor, 100 Bishopsgate, London EC2N 4AG Phone: +44 (0)20.8819.6490&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sales Desk (US):&lt;/strong&gt; 877.701.2883&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@smeadcap.com"&gt;info@smeadcap.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@smeadcap.com"&gt;info@smeadcap.com&lt;/a&gt;) &lt;em&gt;(Additional verified contact: Cole Smead can be reached at&lt;/em&gt; [&lt;em&gt;&lt;a href="mailto:cole@smeadcap.com"&gt;cole@smeadcap.com&lt;/a&gt;&lt;/em&gt;](mailto:&lt;a href="mailto:cole@smeadcap.com"&gt;cole@smeadcap.com&lt;/a&gt;)&lt;em&gt;.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Indeck Energy Services&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;600 N. Buffalo Grove Road, Suite 300, Buffalo Grove, IL 60089&lt;/li&gt; &lt;li&gt;Phone: 847-520-3212&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@indeckenergy.com"&gt;info@indeckenergy.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@indeckenergy.com"&gt;info@indeckenergy.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Peck Enterprises, LLC&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number (as listed via Swagelok Alabama):&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;7290 Cahaba Valley Rd, Birmingham, AL 35242&lt;/li&gt; &lt;li&gt;Phone: 205.988.4812&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@ALFL.Swagelok.com"&gt;info@ALFL.Swagelok.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@ALFL.Swagelok.com"&gt;info@ALFL.Swagelok.com&lt;/a&gt;) &lt;em&gt;(Note: An alternate email – Roderick Douglass at&lt;/em&gt; [&lt;em&gt;&lt;a href="mailto:rodd.douglass@gmail.com"&gt;rodd.douglass@gmail.com&lt;/a&gt;&lt;/em&gt;](mailto:&lt;a href="mailto:rodd.douglass@gmail.com"&gt;rodd.douglass@gmail.com&lt;/a&gt;) &lt;em&gt;– was found on a third‑party directory, but the verified contact on the official Swagelok page is used here.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Northwestern Mutual&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;3601 North Point Parkway, Glendale, WI 53217&lt;/li&gt; &lt;li&gt;Phone: 800-225-5945&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; &lt;em&gt;(None published – inquiries are typically directed through the website’s contact form.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Prime Inc&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;4201 E. Kentucky Ave, Lincoln, NE 68504&lt;/li&gt; &lt;li&gt;Phone: 800-866-2747&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; &lt;em&gt;(No verified email found on the official website; please use the website contact form.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Veterans United Home Loans&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;1701 Wynnton Road, Suite 500, Columbia, MD 21046&lt;/li&gt; &lt;li&gt;Phone: 855-852-4189&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@veteransunited.com"&gt;info@veteransunited.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@veteransunited.com"&gt;info@veteransunited.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Diamond Pet Foods&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Number:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;1200 West Kemper Road, Eagan, MN 55122&lt;/li&gt; &lt;li&gt;Phone: 952-787-3400&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@diamondpet.com"&gt;info@diamondpet.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@diamondpet.com"&gt;info@diamondpet.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Leggett &amp;amp; Platt&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Numbers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;One Leggett Parkway, Carson, CA 90746&lt;/li&gt; &lt;li&gt;Customer Care: 800-232-8534; Corporate: (562) 467-2000&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; &lt;em&gt;(No verified email address was confirmed on their official site.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Opko Health&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mailing Address &amp;amp; Phone Numbers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;One Opko Way, Miami, FL 33131&lt;/li&gt; &lt;li&gt;Phone: 800-543-4741 or (305) 300-1234&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verified Email:&lt;/strong&gt; [&lt;a href="mailto:info@opko.com"&gt;info@opko.com&lt;/a&gt;](mailto:&lt;a href="mailto:info@opko.com"&gt;info@opko.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Edward Jones&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phone Numbers&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Client Relations: (800) 441-2357 (7 a.m. – 5:30 p.m. CT, Monday–Friday)&lt;/li&gt; &lt;li&gt;Headquarters: (314) 515-2000 (7 a.m. – 6 p.m. CT, Monday–Friday)&lt;/li&gt; &lt;li&gt;Toll-Free: (800) 803-3333&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Address&lt;/strong&gt;: 12555 Manchester Road, St. Louis County, Missouri 63131, USA&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Edward Jones does not list a public email for customer service; inquiries are handled via phone or their online access portal.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Diamond Pet Foods&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phone Number&lt;/strong&gt;: (800) 442-0402&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Address&lt;/strong&gt;: PO Box 156, Meta, Missouri 65058, USA&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Diamond Pet Foods does not publicly provide a direct email but offers a contact form on their website for inquiries.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Hunter Engineering Company&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Corporate Headquarters Address&lt;/strong&gt;: 11250 Hunter Drive, Bridgeton, Missouri 63044, USA&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Phone Numbers&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Corporate Office: (314) 731-3020 or (800) 448-6848&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: [&lt;a href="mailto:canadainfo@hunter.com"&gt;canadainfo@hunter.com&lt;/a&gt;](mailto:&lt;a href="mailto:canadainfo@hunter.com"&gt;canadainfo@hunter.com&lt;/a&gt;) (Canada-specific inquiries); [&lt;a href="mailto:info@huntereng.de"&gt;info@huntereng.de&lt;/a&gt;](mailto:&lt;a href="mailto:info@huntereng.de"&gt;info@huntereng.de&lt;/a&gt;) (Germany-specific inquiries)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Hallmark Cards&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phone Numbers&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Toll-Free in the U.S.: (800) 425-5627&lt;/li&gt; &lt;li&gt;Customer Service: (816) 274-3613&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Hallmark does not list a direct customer service email but allows inquiries through a contact form on their website.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For further assistance with these companies, it is recommended to use the provided phone numbers or visit their official websites for additional contact options.&lt;/p&gt; &lt;h1&gt;Fisher Realty (North Carolina)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Belle Hart Schmidt LLC&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GJ Grewe Inc&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Holland Law Firm (Missouri)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found; please refer to a state bar directory for direct contact.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Wilson Logistics&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified email address was found; contact information is available via the company’s “Contact Us” page.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;AGC Partners&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Warren David Properties LLC&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details were found in public sources.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Durham Co&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact email was found. Public details are not available for inclusion.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Ozarks Coca‑Cola Bottling&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Information:&lt;/strong&gt; &lt;em&gt;(No verified contact details or email address were found in the public sources.)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;*edit *link:&lt;br /&gt; &lt;a href="https://economictimes.indiatimes.com/news/international/us/if-you-download-deepseek-in-the-u-s-you-could-face-20-years-in-prison-and-a-100-million-fine-this-is-what-a-new-bill-introduced-in-the-senate-proposes-to-do/articleshow/117954136.cms?from=mdr"&gt;https://economictimes.indiatimes.com/news/international/us/if-you-download-deepseek-in-the-u-s-you-could-face-20-years-in-prison-and-a-100-million-fine-this-is-what-a-new-bill-introduced-in-the-senate-proposes-to-do/articleshow/117954136.cms?from=mdr&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sen Josh Hawley put forth a bill. This bill fines you up to $1,000,000 and up to 20 years in jail for downloading a Chinese LLM. But the way it is worded, it can apply to even a research paper from a Chinese lab. Has support from Elizabeth Warren. &lt;/p&gt; &lt;p&gt;Also, I would just like to thank the people that called me an idiot. I'm not always correct. I *hope* I'm wrong and maybe this will be nothing and he is grand-standing or trying to get political points. Obviously that's a possibility. But the way things are going in the US, it is very much in the realm of possibility that this could get support and pass in a red congress. Peace :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_happy_nice"&gt; /u/mr_happy_nice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiafzm/we_have_to_fight_back_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiafzm/we_have_to_fight_back_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiafzm/we_have_to_fight_back_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T13:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ii82yg</id>
    <title>DeepSeek just released an official demo for DeepSeek VL2 Small - It's really powerful at OCR, text extraction and chat use-cases (Hugging Face Space)</title>
    <updated>2025-02-05T11:40:12+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Space: &lt;a href="https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small"&gt;https://huggingface.co/spaces/deepseek-ai/deepseek-vl2-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Vaibhav (VB) Srivastav on X: &lt;a href="https://x.com/reach_vb/status/1887094223469515121"&gt;https://x.com/reach_vb/status/1887094223469515121&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Zizheng Pan on X: Our official huggingface space demo for DeepSeek-VL2 Small is out! A 16B MoE model for various vision-language tasks: &lt;a href="https://x.com/zizhpan/status/1887110842711162900"&gt;https://x.com/zizhpan/status/1887110842711162900&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ii82yg/deepseek_just_released_an_official_demo_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T11:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiij1d</id>
    <title>DeepSeek R1 ties o1 for first place on the Generalization Benchmark.</title>
    <updated>2025-02-05T19:30:56+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiij1d/deepseek_r1_ties_o1_for_first_place_on_the/"&gt; &lt;img alt="DeepSeek R1 ties o1 for first place on the Generalization Benchmark." src="https://preview.redd.it/7na44xs3gdhe1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=77dd2e43eb2352bf9c4ab11068ca9221f8b83934" title="DeepSeek R1 ties o1 for first place on the Generalization Benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7na44xs3gdhe1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiij1d/deepseek_r1_ties_o1_for_first_place_on_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiij1d/deepseek_r1_ties_o1_for_first_place_on_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iilrym</id>
    <title>Gemma 3 on the way!</title>
    <updated>2025-02-05T21:43:33+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"&gt; &lt;img alt="Gemma 3 on the way!" src="https://preview.redd.it/q2q4555s4ehe1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be6c986a18108bcff251eb781a9cd1a0f4bcbd3" title="Gemma 3 on the way!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/osanseviero/status/1887247587776069957?t=xQ9khq5p-lBM-D2ntK7ZJw&amp;amp;s=19"&gt;https://x.com/osanseviero/status/1887247587776069957?t=xQ9khq5p-lBM-D2ntK7ZJw&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q2q4555s4ehe1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iilrym/gemma_3_on_the_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T21:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1iiio9u</id>
    <title>Anthropic: ‘Please don’t use AI’</title>
    <updated>2025-02-05T19:36:56+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"&gt; &lt;img alt="Anthropic: ‘Please don’t use AI’" src="https://external-preview.redd.it/XdLbwNiaDfP6hGsSmn44MWaR_4YQK7L36Ar5RuZkt4s.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f5b6dec6d423a65124ea27edb0de0e52f12e6ef" title="Anthropic: ‘Please don’t use AI’" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;While we encourage people to use AI systems during their role to help them work faster and more effectively, please do not use AI assistants during the application process. We want to understand your personal interest in Anthropic without mediation through an AI system, and we also want to evaluate your non-AI-assisted communication skills. Please indicate ‘Yes’ if you have read and agree.&amp;quot;&lt;/p&gt; &lt;p&gt;There's a certain irony in having one of the biggest AI labs coming against AI applications and acknowledging the enshittification of the whole job application process.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/9b1e6af4-94f2-41c6-bb91-96a74b9b2da1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iiio9u/anthropic_please_dont_use_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-05T19:36:56+00:00</published>
  </entry>
</feed>
