<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-03T23:48:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1l1rb18</id>
    <title>I made LLMs respond with diff patches rather than standard code blocks and the result is simply amazing!</title>
    <updated>2025-06-02T19:32:00+00:00</updated>
    <author>
      <name>/u/carlrobertoh</name>
      <uri>https://old.reddit.com/user/carlrobertoh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1rb18/i_made_llms_respond_with_diff_patches_rather_than/"&gt; &lt;img alt="I made LLMs respond with diff patches rather than standard code blocks and the result is simply amazing!" src="https://external-preview.redd.it/MXRsZjNqNWZmazRmMXGOM2N51B2QnCZhafa7NKplGti0671pTg7o1NRLqsqm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1f73f62c3f1d72a7c956c94ca78e646d7393252" title="I made LLMs respond with diff patches rather than standard code blocks and the result is simply amazing!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been developing a coding assistant for JetBrains IDEs called &lt;strong&gt;ProxyAI&lt;/strong&gt; (previously CodeGPT), and I wanted to experiment with an idea where LLM is instructed to produce diffs as opposed to regular code blocks, which ProxyAI then applies directly to your project.&lt;/p&gt; &lt;p&gt;I was fairly skeptical about this at first, but after going back-and-forth with the initial version and getting it where I wanted it to be, it simply started to amaze me. The model began generating paths and diffs for files it had never seen before and somehow these &amp;quot;hallucinations&amp;quot; were correct (this mostly happened with modifications to build files that typically need a fixed path).&lt;/p&gt; &lt;p&gt;What really surprised me was how natural the workflow became. You just describe what you want changed, and the diffs appear in &lt;em&gt;near real-time&lt;/em&gt;, almost always with the correct diff patch - can't praise enough how good it feels for &lt;strong&gt;quick iterations&lt;/strong&gt;! In most cases, it takes &lt;em&gt;less than a minute&lt;/em&gt; for the LLM to make edits across many different files. When smaller models mess up (which happens fairly often), there's a simple retry mechanism that usually gets it right on the second attempt - fairly similar logic to Cursor's Fast Apply.&lt;/p&gt; &lt;p&gt;This whole functionality is &lt;strong&gt;free&lt;/strong&gt;, &lt;strong&gt;open-source&lt;/strong&gt;, and available for &lt;strong&gt;every model and provider&lt;/strong&gt;, regardless of tool calling capabilities. &lt;strong&gt;No vendor lock-in&lt;/strong&gt;, &lt;strong&gt;no premium features&lt;/strong&gt; - just plug in your API key or connect to a local model and give it a go!&lt;/p&gt; &lt;p&gt;For me, this feels much more intuitive than the typical &lt;em&gt;&amp;quot;switch to edit mode&amp;quot;&lt;/em&gt; dance that most AI coding tools require. I'd definitely encourage you to give it a try and let me know what you think, or what the current solution lacks. Always looking to improve!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tryproxy.io/"&gt;https://www.tryproxy.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Best regards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carlrobertoh"&gt; /u/carlrobertoh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zcq3wk5ffk4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1rb18/i_made_llms_respond_with_diff_patches_rather_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1rb18/i_made_llms_respond_with_diff_patches_rather_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T19:32:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2qtle</id>
    <title>Building an extension that lets you try ANY clothing on with AI! Who wants me to open source it?</title>
    <updated>2025-06-03T23:31:14+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qtle/building_an_extension_that_lets_you_try_any/"&gt; &lt;img alt="Building an extension that lets you try ANY clothing on with AI! Who wants me to open source it?" src="https://external-preview.redd.it/OGdrcWRjaGZyczRmMRmQT4-0lQTlIfgftoYeHfX8nRIDSoRoafHzyMNvPJv5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3005be52e75a398d231ba8f435b53c5e79f8054" title="Building an extension that lets you try ANY clothing on with AI! Who wants me to open source it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y0z3cehfrs4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qtle/building_an_extension_that_lets_you_try_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qtle/building_an_extension_that_lets_you_try_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T23:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2qv7z</id>
    <title>Help Me Understand MOE vs Dense</title>
    <updated>2025-06-03T23:33:16+00:00</updated>
    <author>
      <name>/u/Express_Seesaw_8418</name>
      <uri>https://old.reddit.com/user/Express_Seesaw_8418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems SOTA LLMS are moving towards MOE architectures. The smartest models in the world &lt;a href="https://lmarena.ai/leaderboard"&gt;seem to be using it&lt;/a&gt;. But why? When you use a MOE model, only a fraction of parameters are actually active. Wouldn't the model be &amp;quot;smarter&amp;quot; if you just use all parameters? Efficiency is awesome, but there are many problems that the smartest models cannot solve (i.e., cancer, a bug in my code, etc.). So, are we moving towards MOE because we discovered some kind of intelligence scaling limit in dense models (for example, a dense 2T LLM could never outperform a well architected MOE 2T LLM) or is it just for efficiency, or both?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Seesaw_8418"&gt; /u/Express_Seesaw_8418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qv7z/help_me_understand_moe_vs_dense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qv7z/help_me_understand_moe_vs_dense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qv7z/help_me_understand_moe_vs_dense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T23:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2letx</id>
    <title>Is there any small models for home budgets</title>
    <updated>2025-06-03T19:48:36+00:00</updated>
    <author>
      <name>/u/DueRuin3912</name>
      <uri>https://old.reddit.com/user/DueRuin3912</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, Is there any small local models I could feed my bank statements into and have it done a full budget breakdown? What would be the best way to go about this for a beginner?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DueRuin3912"&gt; /u/DueRuin3912 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2letx/is_there_any_small_models_for_home_budgets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2letx/is_there_any_small_models_for_home_budgets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2letx/is_there_any_small_models_for_home_budgets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T19:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2r3rt</id>
    <title>How my open-source extension does with a harder virtual try on outfit!</title>
    <updated>2025-06-03T23:44:31+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2r3rt/how_my_opensource_extension_does_with_a_harder/"&gt; &lt;img alt="How my open-source extension does with a harder virtual try on outfit!" src="https://external-preview.redd.it/a2N0Z3RxMGN0czRmMZEFLO9nCJikC7mtBpPcIQAr59c4sK2P034UkenC8j1x.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b4c8b1e419e3cb0287d40d2cdcbb46142474977" title="How my open-source extension does with a harder virtual try on outfit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm open sourcing a chrome extension that lets you try on anything that you see on the internet. Feels like magic.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/parsakhaz/fashn-tryon-extension"&gt;click here to visit the github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e8m2fq0cts4f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2r3rt/how_my_opensource_extension_does_with_a_harder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2r3rt/how_my_opensource_extension_does_with_a_harder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T23:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2dizc</id>
    <title>Checkout this FREE and FAST semantic deduplication app on Hugging Face</title>
    <updated>2025-06-03T14:35:29+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's no point only hashing deduplication of datasets. You might as well use semantic deduplication too. This space for semantic deduplication works on multiple massive datasets. Removing near duplicates, not just exact matches!&lt;/p&gt; &lt;p&gt;This is how it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You pick one all more datasets from the Hub&lt;/li&gt; &lt;li&gt;It make a semantic embedding of each row&lt;/li&gt; &lt;li&gt;It remove removes near duplicates based on a threshold like 0.9&lt;/li&gt; &lt;li&gt;You can push the deduplicated dataset back to a new repo, and get to work.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is super useful if you’re training models or building evals.&lt;/p&gt; &lt;p&gt;You can also clone the repo and run it locally. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/minishlab/semantic-deduplication"&gt;https://huggingface.co/spaces/minishlab/semantic-deduplication&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2dizc/checkout_this_free_and_fast_semantic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2dizc/checkout_this_free_and_fast_semantic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2dizc/checkout_this_free_and_fast_semantic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T14:35:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2k7rk</id>
    <title>Cooling question</title>
    <updated>2025-06-03T18:58:51+00:00</updated>
    <author>
      <name>/u/johnfkngzoidberg</name>
      <uri>https://old.reddit.com/user/johnfkngzoidberg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k7rk/cooling_question/"&gt; &lt;img alt="Cooling question" src="https://preview.redd.it/vd9n6tpyer4f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=169e304b6a6b509345ece64e6e13cdaaa08c98c0" title="Cooling question" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got a “new” 3090 and I got the bright idea to go buy a 1200W power supply and put my 3070 in the same case instead of the upgrade. Before I go buy the new PS, I tried the fit and it feels like that’s pretty tight. Is that enough room between the cards for airflow or am I about to start a fire? I’m adding two new case fans at the bottom anyway, but I’m worried about the top card. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnfkngzoidberg"&gt; /u/johnfkngzoidberg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vd9n6tpyer4f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k7rk/cooling_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k7rk/cooling_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T18:58:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2dowh</id>
    <title>Postman like client for local MCP servers</title>
    <updated>2025-06-03T14:42:11+00:00</updated>
    <author>
      <name>/u/Mysterious-Coat5856</name>
      <uri>https://old.reddit.com/user/Mysterious-Coat5856</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test my custom MCP server on Linux but none of the options seemed right. So I built my own on a weekend.&lt;/p&gt; &lt;p&gt;It's MIT licensed so do with it what you like!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious-Coat5856"&gt; /u/Mysterious-Coat5856 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/faraazahmad/mcp_debug"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2dowh/postman_like_client_for_local_mcp_servers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2dowh/postman_like_client_for_local_mcp_servers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T14:42:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2afie</id>
    <title>PipesHub - Open Source Enterprise Search Platform(Generative-AI Powered)</title>
    <updated>2025-06-03T12:20:03+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I’m excited to share something we’ve been building for the past few months – &lt;strong&gt;PipesHub&lt;/strong&gt;, a fully open-source Enterprise Search Platform.&lt;/p&gt; &lt;p&gt;In short, PipesHub is your &lt;strong&gt;customizable, scalable, enterprise-grade RAG platform&lt;/strong&gt; for everything from intelligent search to building agentic apps — all powered by your own models and data.&lt;/p&gt; &lt;p&gt;We also connect with tools like Google Workspace, Slack, Notion and more — so your team can quickly find answers and trained on &lt;em&gt;your&lt;/em&gt; company’s internal knowledge.&lt;/p&gt; &lt;p&gt;You can run also it locally and use any AI Model out of the box including Ollama.&lt;br /&gt; &lt;strong&gt;We’re looking for early feedback&lt;/strong&gt;, so if this sounds useful (or if you’re just curious), we’d love for you to check it out and tell us what you think!&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2afie/pipeshub_open_source_enterprise_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2afie/pipeshub_open_source_enterprise_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2afie/pipeshub_open_source_enterprise_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T12:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2agpu</id>
    <title>Attention by Hand - Practice attention mechanism on an interactive webpage</title>
    <updated>2025-06-03T12:21:40+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2agpu/attention_by_hand_practice_attention_mechanism_on/"&gt; &lt;img alt="Attention by Hand - Practice attention mechanism on an interactive webpage" src="https://external-preview.redd.it/nhMEOcQ2pY2cWvmMC1a4Ya8l-ZpFkuu1hArRGS_70Jo.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d72bc65c67e8fa81fbd23e548bba69e1a0bb3e8" title="Attention by Hand - Practice attention mechanism on an interactive webpage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/fmji9oswfp4f1.gif"&gt;https://i.redd.it/fmji9oswfp4f1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Try this: &lt;a href="https://vizuara-ai-learning-lab.vercel.app/"&gt;https://vizuara-ai-learning-lab.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nuts-And-Bolts-AI is an interactive web environment where you can practice AI concepts by writing down matrix multiplications. &lt;/p&gt; &lt;p&gt;(1) Let’s take the attention mechanism in language models as an example. &lt;/p&gt; &lt;p&gt;(2) Using Nuts-And-Bolts-AI, you can actively engage with the step-by-step calculation of the scaled dot-product attention mechanism. &lt;/p&gt; &lt;p&gt;(3) Users can input values and work through each matrix operation (Q, K, V, scores, softmax, weighted sum) manually within a guided, interactive environment. &lt;/p&gt; &lt;p&gt;Eventually, we will add several modules on this website: &lt;/p&gt; &lt;p&gt;- Neural Networks from scratch&lt;/p&gt; &lt;p&gt;- CNNs from scratch&lt;/p&gt; &lt;p&gt;- RNNs from scratch&lt;/p&gt; &lt;p&gt;- Diffusion from scratch&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2agpu/attention_by_hand_practice_attention_mechanism_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2agpu/attention_by_hand_practice_attention_mechanism_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2agpu/attention_by_hand_practice_attention_mechanism_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T12:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2bgc1</id>
    <title>Semantic Search PoC for Hugging Face – Now with Parameter Size Filters (0-1B to 70B+)</title>
    <updated>2025-06-03T13:08:16+00:00</updated>
    <author>
      <name>/u/dvanstrien</name>
      <uri>https://old.reddit.com/user/dvanstrien</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! &lt;/p&gt; &lt;p&gt;I’ve recently updated my prototype semantic search for Hugging Face Space, which makes it easier to discover models not only via semantic search but also by &lt;strong&gt;parameter size&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;There are currently over 1.5 million models on the Hub, and finding the right one can be a challenge. &lt;/p&gt; &lt;p&gt;This PoC helps you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Semantic search using the &lt;strong&gt;summaries&lt;/strong&gt; generated by a small LLM (&lt;a href="https://huggingface.co/davanstrien/Smol-Hub-tldr"&gt;https://huggingface.co/davanstrien/Smol-Hub-tldr&lt;/a&gt;) &lt;/li&gt; &lt;li&gt;Filter models by &lt;strong&gt;parameter size&lt;/strong&gt;, from 0-1B all the way to 70B+&lt;/li&gt; &lt;li&gt;It also allows you to find similar models/datasets. For datasets in particular, I've found this can be a nice way to find a bunch of datasets super quickly. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try it here: &lt;a href="https://huggingface.co/spaces/librarian-bots/huggingface-semantic-search"&gt;https://huggingface.co/spaces/librarian-bots/huggingface-semantic-search&lt;/a&gt;&lt;/p&gt; &lt;p&gt;FWIW, for this Space, I also tried a different approach to developing it. Basically, I did the backend API dev myself (since I'm familiar enough with that kind of dev work for it to be quick), but vibe coded the frontend using the OpenAPI Specification for the backed as context for the LLM). Seems to work quite well (at least the front end is better than anything I would do on my own...) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dvanstrien"&gt; /u/dvanstrien &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2bgc1/semantic_search_poc_for_hugging_face_now_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2bgc1/semantic_search_poc_for_hugging_face_now_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2bgc1/semantic_search_poc_for_hugging_face_now_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T13:08:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2qtbo</id>
    <title>B vs Quantization</title>
    <updated>2025-06-03T23:30:53+00:00</updated>
    <author>
      <name>/u/Empty_Object_9299</name>
      <uri>https://old.reddit.com/user/Empty_Object_9299</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been reading about different configurations for my Large Language Model (LLM) and had a question. I understand that Q4 models are generally less accurate (less perplexity) compared to 8 quantization settings (am i wright?).&lt;/p&gt; &lt;p&gt;To clarify, I'm trying to decide between two configurations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4B_Q8: fewer parameters with potentially better perplexity&lt;/li&gt; &lt;li&gt;12B_Q4_0: more parameters with potentially lower perplexity&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In general, is it better to prioritize more perplexity with fewer parameters or less perplexity with more parameters?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Object_9299"&gt; /u/Empty_Object_9299 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qtbo/b_vs_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qtbo/b_vs_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2qtbo/b_vs_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T23:30:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2m4q9</id>
    <title>live transcription</title>
    <updated>2025-06-03T20:16:02+00:00</updated>
    <author>
      <name>/u/Away_Expression_3713</name>
      <uri>https://old.reddit.com/user/Away_Expression_3713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to use whisper or any other model similar accuracy on device android with inference. PLease suggest me the one with best latency. Please help me if i am missing out something - onnx, Tflite , ctranslate2&lt;/p&gt; &lt;p&gt;if you know anything about this category any open source proejcts that can help me pull off a live transcription on android. Please help me out&lt;/p&gt; &lt;p&gt;Also i am building in java so would consider doing a binding or using libraries to build other projects&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Away_Expression_3713"&gt; /u/Away_Expression_3713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2m4q9/live_transcription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2m4q9/live_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2m4q9/live_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T20:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2pl4l</id>
    <title>Llama 3.3 70b Vs Newer Models</title>
    <updated>2025-06-03T22:35:36+00:00</updated>
    <author>
      <name>/u/BalaelGios</name>
      <uri>https://old.reddit.com/user/BalaelGios</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On my MBP (M3 Max 16/40 64GB), the largest model I can run seems to be Llama 3.3 70b. The swathe of new models don't have any options with this many parameters its either 30b or 200b+.&lt;/p&gt; &lt;p&gt;My question is does Llama 3.3 70b, compete or even is it still my best option for local use, or even with the much lower amount of parameters are the likes of Qwen3 30b a3b, Qwen3 32b, Gemma3 27b, DeepSeek R1 0528 Qwen3 8b, are these newer models still &amp;quot;better&amp;quot; or smarter? &lt;/p&gt; &lt;p&gt;I primarily use LLMs for search engine via perplexica and as code assitants. I have attempted to test this myself and honestly they all seem to work at times, can't say I've tested consistently enough yet though to say for sure if there is a front runner. &lt;/p&gt; &lt;p&gt;So yeah is Llama 3.3 dead in the water now? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BalaelGios"&gt; /u/BalaelGios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2pl4l/llama_33_70b_vs_newer_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2pl4l/llama_33_70b_vs_newer_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2pl4l/llama_33_70b_vs_newer_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T22:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2fj2k</id>
    <title>I'm collecting dialogue from anime, games, and visual novels — is this actually useful for improving AI?</title>
    <updated>2025-06-03T15:54:51+00:00</updated>
    <author>
      <name>/u/Akowmako</name>
      <uri>https://old.reddit.com/user/Akowmako</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I’m not a programmer or AI developer, but I’ve been doing something on my own for a while out of passion.&lt;/p&gt; &lt;p&gt;I’ve noticed that most AI responses — especially in roleplay or emotional dialogue — tend to sound repetitive, shallow, or generic. They often reuse the same phrases and don’t adapt well to different character personalities like tsundere, kuudere, yandere, etc.&lt;/p&gt; &lt;p&gt;So I started collecting and organizing dialogue from games, anime, visual novels, and even NSFW content. I'm manually extracting lines directly from files and scenes, then categorizing them based on tone, personality type, and whether it's SFW or NSFW.&lt;/p&gt; &lt;p&gt;I'm trying to build a kind of &amp;quot;word and emotion library&amp;quot; so AI could eventually talk more like real characters, with variety and personality. It’s just something I care about and enjoy working on.&lt;/p&gt; &lt;p&gt;My question is: Is this kind of work actually useful for improving AI models? And if yes, where can I send or share this kind of dialogue dataset?&lt;/p&gt; &lt;p&gt;I tried giving it to models like Gemini, but it didn’t really help since the model doesn’t seem trained on this kind of expressive or emotional language. I haven’t contacted any open-source teams yet, but maybe I will if I know it’s worth doing.&lt;/p&gt; &lt;p&gt;Edit: I should clarify — my main goal isn’t just collecting dialogue, but actually expanding the language and vocabulary AI can use, especially in emotional or roleplay conversations.&lt;/p&gt; &lt;p&gt;A lot of current AI responses feel repetitive or shallow, even with good prompts. I want to help models express emotions better and have more variety in how characters talk — not just the same 10 phrases recycled over and over.&lt;/p&gt; &lt;p&gt;So this isn’t just about training on what characters say, but how they say it, and giving AI access to a wider, richer way of speaking like real personalities.&lt;/p&gt; &lt;p&gt;Any advice would mean a lot — thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Akowmako"&gt; /u/Akowmako &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2fj2k/im_collecting_dialogue_from_anime_games_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2fj2k/im_collecting_dialogue_from_anime_games_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2fj2k/im_collecting_dialogue_from_anime_games_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T15:54:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2imqv</id>
    <title>I would really like to start digging deeper into LLMs. If I have $1500-$2000 to spend, what hardware setup would you recommend assuming I have nothing currently.</title>
    <updated>2025-06-03T17:54:32+00:00</updated>
    <author>
      <name>/u/BokehJunkie</name>
      <uri>https://old.reddit.com/user/BokehJunkie</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have very little idea of what I'm looking for with regard to hardware. I'm a mac guy generally, so i'm familiar with their OS, so that's a plus for me. I also like that their memory is all very fast and shared with the GPU, which I *think* helps run things faster instead of being memory or CPU bound, but I'm not 100% certain. I'd like for thise to be a twofold thing - learning the software side of LLMs, but also to eventually run my own LLM at home in &amp;quot;production&amp;quot; for privacy purposes.&lt;/p&gt; &lt;p&gt;I'm a systems engineer / cloud engineer as my job, so I'm not completely technologically illiterate, but I really don't know much about consumer hardware, especially CPUs and CPUs, nor do I totally understand what I should be prioritizing. &lt;/p&gt; &lt;p&gt;I don't mind building something from scratch, but pre-built is a huge win, and something small is also a big win - so again I lean more toward a mac mini or mac studio. &lt;/p&gt; &lt;p&gt;I would love some other perspectives here, as long as it's not simply &amp;quot;apple bad. mac bad. boo&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BokehJunkie"&gt; /u/BokehJunkie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2imqv/i_would_really_like_to_start_digging_deeper_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2imqv/i_would_really_like_to_start_digging_deeper_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2imqv/i_would_really_like_to_start_digging_deeper_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T17:54:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2oywk</id>
    <title>What GUI are you using for local LLMs? (AnythingLLM, LM Studio, etc.)</title>
    <updated>2025-06-03T22:09:03+00:00</updated>
    <author>
      <name>/u/Aaron_MLEngineer</name>
      <uri>https://old.reddit.com/user/Aaron_MLEngineer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying out AnythingLLM and LM Studio lately to run models like LLaMA and Gemma locally. Curious what others here are using.&lt;/p&gt; &lt;p&gt;What’s been your experience with these or other GUI tools like GPT4All, Oobabooga, PrivateGPT, etc.?&lt;/p&gt; &lt;p&gt;What do you like, what’s missing, and what would you recommend for someone looking to do local inference with documents or RAG?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaron_MLEngineer"&gt; /u/Aaron_MLEngineer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2oywk/what_gui_are_you_using_for_local_llms_anythingllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2oywk/what_gui_are_you_using_for_local_llms_anythingllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2oywk/what_gui_are_you_using_for_local_llms_anythingllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T22:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2k4nw</id>
    <title>GuidedQuant: Boost LLM layer-wise PTQ methods using the end loss guidance (Qwen3, Gemma3, Llama3.3 / 2~4bit Quantization)</title>
    <updated>2025-06-03T18:54:54+00:00</updated>
    <author>
      <name>/u/jusjinuk</name>
      <uri>https://old.reddit.com/user/jusjinuk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k4nw/guidedquant_boost_llm_layerwise_ptq_methods_using/"&gt; &lt;img alt="GuidedQuant: Boost LLM layer-wise PTQ methods using the end loss guidance (Qwen3, Gemma3, Llama3.3 / 2~4bit Quantization)" src="https://b.thumbs.redditmedia.com/vtLtRQARCD-YrSYoV47w8VcpAELX-_6KIrebjV3YZEY.jpg" title="GuidedQuant: Boost LLM layer-wise PTQ methods using the end loss guidance (Qwen3, Gemma3, Llama3.3 / 2~4bit Quantization)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Paper (ICML 2025):&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2505.07004"&gt;https://arxiv.org/abs/2505.07004&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/snu-mllab/GuidedQuant"&gt;https://github.com/snu-mllab/GuidedQuant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HuggingFace Collection:&lt;/strong&gt; 2~4-bit quantized Qwen3-32B, gemma-3-27b-it, Llama-3.1-8B-Instruct, Llama-3.3-70B-Instruct → &lt;a href="https://huggingface.co/collections/jusjinuk/instruction-tuned-models-guidedquant-68334269c44cd3eb21f7bd61"&gt;Link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; &lt;strong&gt;GuidedQuant&lt;/strong&gt; boosts layer-wise PTQ methods by integrating end loss guidance into the objective. We also introduce &lt;strong&gt;LNQ&lt;/strong&gt;, a non-uniform scalar quantization algorithm which is guaranteed to monotonically decrease the quantization objective value.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6sggk1y3ir4f1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=24a408ae268ed34bbb92d3d00fe5880d05c61fa7"&gt;Runs on a single RTX 3090 GPU!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jusjinuk"&gt; /u/jusjinuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k4nw/guidedquant_boost_llm_layerwise_ptq_methods_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k4nw/guidedquant_boost_llm_layerwise_ptq_methods_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2k4nw/guidedquant_boost_llm_layerwise_ptq_methods_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T18:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2820t</id>
    <title>nvidia/Nemotron-Research-Reasoning-Qwen-1.5B · Hugging Face</title>
    <updated>2025-06-03T10:06:22+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2820t/nvidianemotronresearchreasoningqwen15b_hugging/"&gt; &lt;img alt="nvidia/Nemotron-Research-Reasoning-Qwen-1.5B · Hugging Face" src="https://external-preview.redd.it/RP_o1NnFnVgqmDAj8haRnOnwD5ZnZcjaUEqHghtS6ig.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbe8cb87d4ec3c680868083410ff0f4da7c3636d" title="nvidia/Nemotron-Research-Reasoning-Qwen-1.5B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2820t/nvidianemotronresearchreasoningqwen15b_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2820t/nvidianemotronresearchreasoningqwen15b_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T10:06:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1l1qqdx</id>
    <title>At the airport people watching while I run models locally:</title>
    <updated>2025-06-02T19:10:02+00:00</updated>
    <author>
      <name>/u/Current-Ticket4214</name>
      <uri>https://old.reddit.com/user/Current-Ticket4214</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"&gt; &lt;img alt="At the airport people watching while I run models locally:" src="https://preview.redd.it/55ab38z0ck4f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee34cc7e6232ae1fc31a5076b1cc4064bd66305d" title="At the airport people watching while I run models locally:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Current-Ticket4214"&gt; /u/Current-Ticket4214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/55ab38z0ck4f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-02T19:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2gv3a</id>
    <title>Sakana AI proposes the Darwin Gödel Machine, an self-learning AI system that leverages an evolution algorithm to iteratively rewrite its own code, thereby continuously improving its performance on programming tasks</title>
    <updated>2025-06-03T16:46:59+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gv3a/sakana_ai_proposes_the_darwin_gödel_machine_an/"&gt; &lt;img alt="Sakana AI proposes the Darwin Gödel Machine, an self-learning AI system that leverages an evolution algorithm to iteratively rewrite its own code, thereby continuously improving its performance on programming tasks" src="https://external-preview.redd.it/ll0sI2kj9OWJW1iOriHpZm1jSfC278YnLF-jisELKs4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f30bf0b3bae15b4dee53ba7bd37f2486072c04" title="Sakana AI proposes the Darwin Gödel Machine, an self-learning AI system that leverages an evolution algorithm to iteratively rewrite its own code, thereby continuously improving its performance on programming tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sakana.ai/dgm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gv3a/sakana_ai_proposes_the_darwin_gödel_machine_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gv3a/sakana_ai_proposes_the_darwin_gödel_machine_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T16:46:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2b83p</id>
    <title>Vision Language Models are Biased</title>
    <updated>2025-06-03T12:58:13+00:00</updated>
    <author>
      <name>/u/taesiri</name>
      <uri>https://old.reddit.com/user/taesiri</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taesiri"&gt; /u/taesiri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://vlmsarebiased.github.io/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2b83p/vision_language_models_are_biased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2b83p/vision_language_models_are_biased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T12:58:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2diwk</id>
    <title>Arcee Homunculus-12B</title>
    <updated>2025-06-03T14:35:23+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Homunculus&lt;/strong&gt; is a 12 billion-parameter instruction model distilled from &lt;strong&gt;Qwen3-235B&lt;/strong&gt; onto the &lt;strong&gt;Mistral-Nemo&lt;/strong&gt; backbone.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Homunculus"&gt;https://huggingface.co/arcee-ai/Homunculus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Homunculus-GGUF"&gt;https://huggingface.co/arcee-ai/Homunculus-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2diwk/arcee_homunculus12b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2diwk/arcee_homunculus12b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2diwk/arcee_homunculus12b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T14:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1l2gvar</id>
    <title>New META Paper - How much do language models memorize?</title>
    <updated>2025-06-03T16:47:12+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very interesting paper on dataset size, parameter size, and grokking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2505.24832"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gvar/new_meta_paper_how_much_do_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l2gvar/new_meta_paper_how_much_do_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T16:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1l27g8d</id>
    <title>Google opensources DeepSearch stack</title>
    <updated>2025-06-03T09:25:47+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"&gt; &lt;img alt="Google opensources DeepSearch stack" src="https://external-preview.redd.it/jtUtL7EqwS5bMEk8XfF81tFd6n1MgnQyQL0hQG-jzRk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0633532a1f9e627adaa6246fd5a299a809f2654" title="Google opensources DeepSearch stack" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While it's not evident if this is the exact same stack they use in the Gemini user app, it sure looks very promising! Seems to work with Gemini and Google Search. Maybe this can be adapted for any local model and SearXNG?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1l27g8d/google_opensources_deepsearch_stack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-03T09:25:47+00:00</published>
  </entry>
</feed>
