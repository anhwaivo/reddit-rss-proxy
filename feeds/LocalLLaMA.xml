<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-01T06:25:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lnrd1t</id>
    <title>You can just RL a model to beat any "AI detectors"</title>
    <updated>2025-06-29T22:22:55+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt; &lt;img alt="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="You can just RL a model to beat any &amp;quot;AI detectors&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d"&gt;https://preview.redd.it/p4binxqqvx9f1.png?width=783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5af26533b3e667d6f0382d11163331aedf6bc42d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd"&gt;https://preview.redd.it/k4tcfdmsvx9f1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=934ff9d043c7021764743c443feff0f0767c25cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Baseline&lt;br /&gt; • Model: Llama-3.1 8B-Instruct&lt;br /&gt; • Prompt: plain &amp;quot;Write an essay about X&amp;quot;&lt;br /&gt; • Detector: ZeroGPT&lt;br /&gt; Result: 100 % AI-written&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d"&gt;https://preview.redd.it/09nmithvvx9f1.png?width=1204&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82d0071d8579effb1f1b75eaa5c037a56385ef9d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Data&lt;br /&gt; • Synthetic dataset of 150 school-style prompts (history, literature, tech). Nothing fancy, just json lines + system prompt &amp;quot;You are a human essay writer&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e"&gt;https://preview.redd.it/d189whuxvx9f1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5fdd406d1df4a40f3f4c1623b6b049026559f29e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First training run&lt;br /&gt; After ~30 GRPO steps on a single A100:&lt;br /&gt; • ZeroGPT score drops from 100 → 42 %&lt;br /&gt; The model learned:&lt;br /&gt; Write a coherent intro&lt;br /&gt; Stuff one line of high-entropy junk&lt;br /&gt; Finish normally&lt;br /&gt; Average &amp;quot;human-ness&amp;quot; skyrockets because detector averages per-sentence scores&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75"&gt;https://preview.redd.it/c4bkar70wx9f1.png?width=941&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e3a86287c2d0cc273fd9f3854634cbd7c8ecf75&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Patch #1&lt;br /&gt; Added a gibberish classifier (tiny DistilRoBERTa) and multiplied reward by its minimum &amp;quot;clean&amp;quot; score. Junk lines now tank reward → behaviour disappears. GRPO’s beta ≈ how harshly to penalize incoherence. Set β = 0.4 and reward curve stabilized; no more oscillation between genius &amp;amp; garbage. Removed reasoning (memory constraints).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252"&gt;https://preview.redd.it/prmgkja2wx9f1.png?width=652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79f46c100445337e257dc3b7666ffdf2ba826252&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tiny models crush it&lt;br /&gt; Swapped in Qwen 0.5B LoRA rank 8, upped num_generations → 64.&lt;br /&gt; Result after 7 steps: best sample already at 28 % &amp;quot;human&amp;quot;. Smaller vocab seems to help leak less LM &amp;quot;signature&amp;quot; (the model learned to use lots of proper nouns to trick the detector).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2"&gt;https://preview.redd.it/2e6g1pm7wx9f1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cfbcaa7fd8c6baa2a05d063a3989ba282c8d31a2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&lt;/a&gt;-GRPO.ipynb)&lt;/p&gt; &lt;p&gt;Detector bug?&lt;br /&gt; ZeroGPT sometimes marks the first half AI, second half human for the same paragraph. The RL agent locks onto that gradient and exploits it. Classifier clearly over-fits surface patterns rather than semantics&lt;/p&gt; &lt;p&gt;Single scalar feedback is enough for LMs to reverse-engineer public detectors &lt;/p&gt; &lt;p&gt;Add even a tiny auxiliary reward (gibberish, length) to stop obvious failure modes &lt;/p&gt; &lt;p&gt;Public &amp;quot;AI/Not-AI&amp;quot; classifiers are security-through-obscurity&lt;/p&gt; &lt;p&gt;Reward function: &lt;a href="https://codefile.io/f/R4O9IdGEhg"&gt;https://codefile.io/f/R4O9IdGEhg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnrd1t/you_can_just_rl_a_model_to_beat_any_ai_detectors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T22:22:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lo84yj</id>
    <title>[2506.21734] Hierarchical Reasoning Model</title>
    <updated>2025-06-30T13:54:40+00:00</updated>
    <author>
      <name>/u/absolooot1</name>
      <uri>https://old.reddit.com/user/absolooot1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract:&lt;/p&gt; &lt;p&gt;Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/absolooot1"&gt; /u/absolooot1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2506.21734"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T13:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnlxp1</id>
    <title>4x 4090 48GB inference box (I may have overdone it)</title>
    <updated>2025-06-29T18:33:40+00:00</updated>
    <author>
      <name>/u/101m4n</name>
      <uri>https://old.reddit.com/user/101m4n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt; &lt;img alt="4x 4090 48GB inference box (I may have overdone it)" src="https://external-preview.redd.it/o67J1SHcLKrQAlXicnfT20w0glJr7s4wb4-c1GOwiA8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=936572d5a67f4298cbb8ecc135d737e991ade403" title="4x 4090 48GB inference box (I may have overdone it)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I discovered that 48GB 4090s were starting to show up on the western market in large numbers. I didn't think much of it at the time, but then I got my payout from the mt.gox bankruptcy filing (which has been ongoing for over 10 years now), and decided to blow a chunk of it on an inference box for local machine learning experiments.&lt;/p&gt; &lt;p&gt;After a delay receiving some of the parts (and admittedly some procrastination on my end), I've finally found the time to put the whole machine together!&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Asrock romed8-2t motherboard (SP3)&lt;/li&gt; &lt;li&gt;32 core epyc&lt;/li&gt; &lt;li&gt;256GB 2666V memory&lt;/li&gt; &lt;li&gt;4x &amp;quot;tronizm&amp;quot; rtx 4090D 48GB modded GPUs from china&lt;/li&gt; &lt;li&gt;2x 1tb nvme (striped) for OS and local model storage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The cards are very well built. I have no doubts as to their quality whatsoever. They were heavy, the heatsinks made contact with all the board level components and the shrouds were all-metal and very solid. It was almost a shame to take them apart! They were however incredibly loud. At idle, the fan sits at 30%, and at that level they are already as loud as the loudest blower cards for gaming. At full load, they are truly deafening and definitely not something you want to share space with. Hence the water-cooling.&lt;/p&gt; &lt;p&gt;There are however no full-cover waterblocks for these GPUs (they use a custom PCB), so to cool them I had to get a little creative. Corsair makes a (kinda) &lt;a href="https://www.corsair.com/uk/en/p/custom-liquid-cooling/cx-9025001-ww/icue-link-xg3-rgb-hybrid-gpu-water-block-4090-4080-cx-9025001-ww?srsltid=AfmBOopBdweqKN5Wpj6wHKLSR9SEYZmNpOpOyaFZTLLdld7hLBrg1iCg"&gt;generic block&lt;/a&gt; called the xg3. The product itself is a bit rubbish, requiring corsairs proprietary i-cue system to run the fan which is supposed to cool the components not covered by the coldplate. It's also overpriced. However these are more or less the only option here. As a side note, these &amp;quot;generic&amp;quot; blocks only work work because the mounting hole and memory layout around the core is actually standardized to some extent, something I learned during my research.&lt;/p&gt; &lt;p&gt;The cold-plate on these blocks turned out to foul one of the components near the core, so I had to modify them a bit. I also couldn't run the aforementioned fan without corsairs i-cue link nonsense and the fan and shroud were too thick anyway and would have blocked the next GPU anyway. So I removed the plastic shroud and fabricated a frame + heatsink arrangement to add some support and cooling for the VRMs and other non-core components.&lt;/p&gt; &lt;p&gt;As another side note, the marketing material for the xg3 claims that the block contains a built-in temperature sensor. However I saw no indication of a sensor anywhere when disassembling the thing. Go figure.&lt;/p&gt; &lt;p&gt;Lastly there's the case. I couldn't find a case that I liked the look of that would support three 480mm radiators, so I built something out of pine furniture board. Not the easiest or most time efficient approach, but it was fun and it does the job (fire hazard notwithstanding).&lt;/p&gt; &lt;p&gt;As for what I'll be using it for, I'll be hosting an LLM for local day-to-day usage, but I also have some more unique project ideas, some of which may show up here in time. Now that such projects won't take up resources on my regular desktop, I can afford to do a lot of things I previously couldn't!&lt;/p&gt; &lt;p&gt;P.S. If anyone has any questions or wants to replicate any of what I did here, feel free to DM me with any questions, I'm glad to help any way I can!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/101m4n"&gt; /u/101m4n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lnlxp1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnlxp1/4x_4090_48gb_inference_box_i_may_have_overdone_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-29T18:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1loj134</id>
    <title>arXiv2Docker: Computational Reproducibility with the ExperimentOps Agent</title>
    <updated>2025-06-30T20:57:06+00:00</updated>
    <author>
      <name>/u/remyxai</name>
      <uri>https://old.reddit.com/user/remyxai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loj134/arxiv2docker_computational_reproducibility_with/"&gt; &lt;img alt="arXiv2Docker: Computational Reproducibility with the ExperimentOps Agent" src="https://preview.redd.it/rak71t31n4af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcce9df76f1ce564209c9bfa33c8883ba2b4cbc5" title="arXiv2Docker: Computational Reproducibility with the ExperimentOps Agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've all been there, spend a morning setting up to find out it's not gonna work for your application.&lt;/p&gt; &lt;p&gt;From &lt;a href="https://arxiv.org/pdf/2409.07440"&gt;SUPER&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;em&gt;As a recent study shows (Storks et al., 2023), both novice and advanced researchers find the challenge of &amp;quot;setting up the code base&amp;quot; to be the most difficult part of reproducing experiments.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I'm sharing auto-generated Docker images for papers my agent recommends based on what I'm building.&lt;/p&gt; &lt;p&gt;Today's recommendation: &lt;strong&gt;LLaVA-Scissor&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker pull remyxai/2506.21862v1:latest docker run --gpus all -it remyxai/2506.21862v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More on &lt;a href="https://remyxai.substack.com/p/the-experimentops-agent"&gt;ExperimentOps&lt;/a&gt; and computational reproducibility. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remyxai"&gt; /u/remyxai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rak71t31n4af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loj134/arxiv2docker_computational_reproducibility_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loj134/arxiv2docker_computational_reproducibility_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T20:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojd3e</id>
    <title>Gemma-3n VRAM usage</title>
    <updated>2025-06-30T21:10:17+00:00</updated>
    <author>
      <name>/u/el_pr3sid3nt3</name>
      <uri>https://old.reddit.com/user/el_pr3sid3nt3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello fellow redditors,&lt;/p&gt; &lt;p&gt;I am trying to run Gemma-3n-E2B and E4B advertised as 2gb-3gb VRAM models. However, I couldn't run E4B due to torch outOfMemory, but when I ran E2B it took 10gbs and after few requests I went out of memory.&lt;/p&gt; &lt;p&gt;I am trying to understand, is there a way to run these models really on 2gb-3gb VRAM, and if yes how so, and what I missed?&lt;/p&gt; &lt;p&gt;Thank you all&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el_pr3sid3nt3"&gt; /u/el_pr3sid3nt3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:10:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1lotzy4</id>
    <title>Video Cards &amp; GPUs SPARKLE intros new Arc Pro B60 cards: one is a dual-GPU workstation card with 48GB of VRAM</title>
    <updated>2025-07-01T05:52:37+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tweaktown.com/news/106121/sparkle-intros-new-arc-pro-b60-cards-one-is-dual-gpu-workstation-card-with-48gb-of-vram/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lotzy4/video_cards_gpus_sparkle_intros_new_arc_pro_b60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lotzy4/video_cards_gpus_sparkle_intros_new_arc_pro_b60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T05:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnxo8y</id>
    <title>Major AI platforms will eventually have ads</title>
    <updated>2025-06-30T03:40:35+00:00</updated>
    <author>
      <name>/u/MattDTO</name>
      <uri>https://old.reddit.com/user/MattDTO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see this as a huge reason to continue advancement of local LLMs. OpenAI, Google, Microsoft, Anthropic, etc. all the big players have investors to answer to, and will eventually need to stop burning money. They will get pressured into a sustainable business model. I think Google has already lost a lot of traffic to AI search that they will try to win back. Right now, they are giving LLM access in exchange for data to train on. Eventually they will have enough that it won’t be worth it anymore. &lt;/p&gt; &lt;p&gt;Anyone else see this coming?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MattDTO"&gt; /u/MattDTO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnxo8y/major_ai_platforms_will_eventually_have_ads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T03:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1lop94b</id>
    <title>A Llama near the top for every size except small</title>
    <updated>2025-07-01T01:32:57+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/"&gt; &lt;img alt="A Llama near the top for every size except small" src="https://preview.redd.it/o941j62s16af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90263e1c05cfd35fd17eb13ba1e683470b488b2f" title="A Llama near the top for every size except small" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting pattern I noticed for non-reasoning models (I am in the process of picking one to fine-tune): there is a Llama at/near the top of the intelligence index for &lt;em&gt;every&lt;/em&gt; model size class &lt;em&gt;except&lt;/em&gt; small models! Also interesting: the small model class is the &lt;em&gt;most&lt;/em&gt; crowded model class by far.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img fgwkkzv116af1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img gcfpkrz916af1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img 2nxh432b16af1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img lmjustob16af1...&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o941j62s16af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T01:32:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1load8a</id>
    <title>[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?</title>
    <updated>2025-06-30T15:23:52+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt; &lt;img alt="[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?" src="https://b.thumbs.redditmedia.com/QrXwS0MMtdu4-LCvZnP-VTv25rOcYvXpPucHJrkYiSQ.jpg" title="[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It Matter?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7"&gt;https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you’ve ever peeked inside models like GPT or BERT and wondered &lt;em&gt;how&lt;/em&gt; they understand the &lt;em&gt;order&lt;/em&gt; of words, the secret sauce is something called positional embedding.&lt;/p&gt; &lt;p&gt;Without it, a language model can’t tell the difference between:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“The cat sat on the mat”&lt;/li&gt; &lt;li&gt;“The mat sat on the cat”&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Problem: Transformers Don’t Understand Word Order&lt;/h1&gt; &lt;p&gt;Transformers process all tokens at once, which is great for speed, but unlike RNNs, they don’t read text sequentially. That means they don’t naturally know the order of words.&lt;/p&gt; &lt;p&gt;To a plain Transformer, “I love AI” could mean the same as “AI love I.”&lt;/p&gt; &lt;h1&gt;The Solution: Positional Embeddings&lt;/h1&gt; &lt;p&gt;To fix this, we add a second layer of information: positional embeddings. These vectors tell the model &lt;em&gt;where&lt;/em&gt; each word appears in the input sequence.&lt;/p&gt; &lt;p&gt;So instead of just using word embeddings, we do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Final Input = Word Embedding + Positional Embedding &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the model knows both the meaning of each word and its position in the sentence.&lt;/p&gt; &lt;h1&gt;Why Not Let the Model Learn Position on Its Own?&lt;/h1&gt; &lt;p&gt;In theory, a large model &lt;em&gt;could&lt;/em&gt; infer word order from patterns. But in practice, that’s inefficient and unreliable. Positional embeddings provide the model with a strong starting point, akin to adding page numbers to a shuffled book.&lt;/p&gt; &lt;h1&gt;Two Common Types of Positional Embeddings&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Sinusoidal Positional Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Used in the original Transformer paper&lt;/li&gt; &lt;li&gt;Not learned, uses sine and cosine functions&lt;/li&gt; &lt;li&gt;Good for generalizing to longer sequences&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learned Positional Embeddings&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Used in models like BERT&lt;/li&gt; &lt;li&gt;Learned during training, like word embeddings&lt;/li&gt; &lt;li&gt;Flexible, but may not generalize well to unseen sequence lengths&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Real Example: Why It Matters&lt;/h1&gt; &lt;p&gt;Compare:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;“The dog chased the cat.”&lt;/li&gt; &lt;li&gt;“The cat chased the dog”&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Same words, totally different meaning. Without positional embeddings, the model can’t tell which animal is doing the chasing.&lt;/p&gt; &lt;h1&gt;What’s New: Rotary Positional Embeddings (RoPE)&lt;/h1&gt; &lt;p&gt;Modern models, such as DeepSeek and LLaMA, utilize RoPE to integrate position into the attention mechanism itself. It’s more efficient for long sequences and performs better in certain settings.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Positional embeddings help Transformers make sense of word order. Without them, a model is just guessing how words relate to each other, like trying to read a book with the pages shuffled.&lt;/p&gt; &lt;p&gt;👉 Tomorrow, we’re going to code positional embeddings from scratch—so stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T15:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lnu4zl</id>
    <title>Baidu releases ERNIE 4.5 models on huggingface</title>
    <updated>2025-06-30T00:34:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt; &lt;img alt="Baidu releases ERNIE 4.5 models on huggingface" src="https://external-preview.redd.it/Wyzo5BvQjbbXvCrrpLypEcj3XicuXWigLyl_Acs2b5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=33f879948e7c84df63582ea3398eb078c0298c8e" title="Baidu releases ERNIE 4.5 models on huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp support for ERNIE 4.5 0.3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14408"&gt;https://github.com/ggml-org/llama.cpp/pull/14408&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vllm Ernie4.5 and Ernie4.5MoE Model Support&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/20220"&gt;https://github.com/vllm-project/vllm/pull/20220&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/baidu/ernie-45-6861cd4c9be84540645f35c9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lnu4zl/baidu_releases_ernie_45_models_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T00:34:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lobyx5</id>
    <title>Upcoming Coding Models?</title>
    <updated>2025-06-30T16:25:45+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Based on past threads from this sub, I see that below coding models are coming.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3 Coder - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt;Recent thread&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Deep Cogito - Preview models there&lt;/li&gt; &lt;li&gt;Polaris - Preview models there&lt;/li&gt; &lt;li&gt;Granite releasing any new coding models? Preview (General) models there for upcoming Version 4. How good is their existing coding models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What other coding models coming apart from above ones?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lobyx5/upcoming_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1losjpq</id>
    <title>Intel GPU vLLM Docker Compose Bootstrap with Phi-lthy4 on A770</title>
    <updated>2025-07-01T04:25:16+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;This weekend I started tinkering with vLLM after a discussion we had over at the &lt;a href="https://github.com/SearchSavior/OpenArc"&gt;OpenArc&lt;/a&gt; &lt;a href="https://discord.gg/Bzz9hax9Jq"&gt;discord server&lt;/a&gt; last week about getting better performance.&lt;/p&gt; &lt;p&gt;Between vLLM and IPEX documentation they make it easy enough to get things rolling once you are setup; however if you are new to docker/containerization like I was when I got started building a compose from scratch can be hard, and the documentation does not cover that yet it makes deployment cleaner and reproducible.&lt;/p&gt; &lt;p&gt;&lt;code&gt; services: ipex-llm-serving: image: intelanalytics/ipex-llm-serving-xpu:0.8.3-b21 container_name: ipex-vllm stdin_open: true tty: true network_mode: host devices: - /dev/dri:/dev/dri volumes: - path/to/your/models:/llm/models environment: - HTTP_PROXY= - HTTPS_PROXY= - http_proxy= - https_proxy= restart: unless-stopped &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Turns out that most of the cooking to get this running smoothly on multi-GPU requires environment variables that configure oneCCL and oneDNN that I have not figured out yet. Will share an update once I get that sorted, as I'm eager to test. &lt;/p&gt; &lt;p&gt;In the meantime, I wanted to share this bare minimum bootstrap for anyone interested. &lt;/p&gt; &lt;p&gt;Benchmarks:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Phi-lthy4?not-for-all-audiences=true"&gt;SicariusSicariiStuff/Phi-lthy4&lt;/a&gt; @ woq_int4 (which should be close to q4km)&lt;/p&gt; &lt;p&gt;1x A770 Xeon W-2255 Ubuntu 24.04 6.14.4-061404-generic Context 2048 (~4gb vram to spare)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Serving Benchmark Result&lt;/strong&gt; Successful requests: 3000&lt;/p&gt; &lt;p&gt;Benchmark duration (s): 7850.31&lt;/p&gt; &lt;p&gt;Total input tokens: 3072000&lt;/p&gt; &lt;p&gt;Total generated tokens: 1536000&lt;/p&gt; &lt;p&gt;Request throughput (req/s): 0.38&lt;/p&gt; &lt;p&gt;Output token throughput (tok/s): 195.66&lt;/p&gt; &lt;p&gt;Total Token throughput (tok/s): 586.98&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Time to First Token&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Mean TTFT (ms): 3887736.67&lt;/p&gt; &lt;p&gt;Median TTFT (ms): 3873859.76&lt;/p&gt; &lt;p&gt;P99 TTFT (ms): 7739753.88&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Time per Output Token (excl. 1st token)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Mean TPOT (ms): 122.82&lt;/p&gt; &lt;p&gt;Median TPOT (ms): 111.34&lt;/p&gt; &lt;p&gt;P99 TPOT (ms): 210.83&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inter-token Latency&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Mean ITL (ms): 122.90&lt;/p&gt; &lt;p&gt;Median ITL (ms): 75.30&lt;/p&gt; &lt;p&gt;P99 ITL (ms): 900.24&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1losjpq/intel_gpu_vllm_docker_compose_bootstrap_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1losjpq/intel_gpu_vllm_docker_compose_bootstrap_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1losjpq/intel_gpu_vllm_docker_compose_bootstrap_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T04:25:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1loswvr</id>
    <title>New to the scene. Yesterday, got 4 t/s on R1 671b q4. Today, I'm getting about 0.15 t/s... What did I break lol</title>
    <updated>2025-07-01T04:46:12+00:00</updated>
    <author>
      <name>/u/sourpatchgrownadults</name>
      <uri>https://old.reddit.com/user/sourpatchgrownadults</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5975wx, 512gb DDR4 3200, dual 3090s. Ollama + OpenWebUI. Running on LMDE.&lt;/p&gt; &lt;p&gt;Idk what went wrong now but I'm struggling to get it back to 4 t/s... I can work with 4 t/s, but 0.15 t/s is just terrible.&lt;/p&gt; &lt;p&gt;Any ideas? Happy to provide information upon request.&lt;/p&gt; &lt;p&gt;Total noob here, just built this a few days ago and very little terminal experience lol but have an open mind and a will to learn.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sourpatchgrownadults"&gt; /u/sourpatchgrownadults &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loswvr/new_to_the_scene_yesterday_got_4_ts_on_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T04:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lomilz</id>
    <title>[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀</title>
    <updated>2025-06-30T23:22:23+00:00</updated>
    <author>
      <name>/u/Awkward-Dare-1127</name>
      <uri>https://old.reddit.com/user/Awkward-Dare-1127</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt; &lt;img alt="[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀" src="https://external-preview.redd.it/4LxPZe7g9FzOelbXz3aOGVDeWd4mgBA3OWlLsXGi_Bc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a61aa76d902ab96a1963a6d4338aa8b21a38657e" title="[Tool] Run GPT-style models from a USB stick – no install, no internet, no GPU – meet Local LLM Notepad 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Copy one portable&lt;/em&gt; &lt;code&gt;.exe&lt;/code&gt; &lt;em&gt;+ a&lt;/em&gt; &lt;code&gt;.gguf&lt;/code&gt; &lt;em&gt;model to a flash drive → double-click on any Windows PC → start chatting offline in seconds.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;GitHub ▶︎ &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad"&gt;&lt;strong&gt;https://github.com/runzhouye/Local_LLM_Notepad&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8"&gt;https://preview.redd.it/fai7y6o0f5af1.png?width=644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fd486f9b42b09cc5810e3f225d3148c500d36f8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/lz6e4zmpd5af1.gif"&gt;https://i.redd.it/lz6e4zmpd5af1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;30-second Quick-Start&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Grab &lt;strong&gt;Local_LLM_Notepad-portable.exe&lt;/strong&gt; from the &lt;a href="https://github.com/runzhouye/Local_LLM_Notepad/releases"&gt;latest release&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download a small CPU model like &lt;strong&gt;gemma-3-1b-it-Q4_K_M.gguf&lt;/strong&gt; (≈0.8 GB) from &lt;a href="https://huggingface.co/ggml-org/gemma-3-1b-it-GGUF/tree/main"&gt;Hugging Face&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Copy both files onto a USB stick.&lt;/li&gt; &lt;li&gt;Double-click the EXE on any Windows box → first run loads the model.&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;✅&lt;/th&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;What it means&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Plug-and-play&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Single 45 MB EXE runs without admin rights&lt;/td&gt; &lt;td align="left"&gt;Run on any computer—no install needed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Source-word highlighting&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Bold-underlines every word/number from your prompt&lt;/td&gt; &lt;td align="left"&gt;Ctrl-click to trace facts &amp;amp; tables for quick fact-checking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Hotkeys&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;Ctrl + SCtrl + ZCtrl + FCtrl + X&lt;/code&gt; send, stop, search, clear, etc.&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Portable chat logs&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;One-click JSON export&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward-Dare-1127"&gt; /u/Awkward-Dare-1127 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lomilz/tool_run_gptstyle_models_from_a_usb_stick_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:22:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lompd5</id>
    <title>I've built a spec for LLM-to-LLM comms by combining semantic patterns with structured syntax</title>
    <updated>2025-06-30T23:31:00+00:00</updated>
    <author>
      <name>/u/sbuswell</name>
      <uri>https://old.reddit.com/user/sbuswell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I'm a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I've found something that's pretty effective (well it has been for me anyway).&lt;/p&gt; &lt;p&gt;Long story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, → for causality and progression, etc) and I've combined these two layers to create a DSL that's more token-efficient but also richer and more logically sound.&lt;/p&gt; &lt;p&gt;This isn't a library you need to install; it's just a spec. Any LLM I've tested it on can understand it out of the box. I've documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.&lt;/p&gt; &lt;p&gt;I'm sharing this because I think it's a genuinely useful technique, and I'd love to get your feedback to help improve it. Or even someone tell me it already exists and I'll use the proper version!&lt;/p&gt; &lt;p&gt;Link to the repo: &lt;a href="https://github.com/elevanaltd/octave"&gt;https://github.com/elevanaltd/octave&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sbuswell"&gt; /u/sbuswell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokkpc</id>
    <title>A Meta-Framework for Self-Improving LLMs with Transparent Reasoning</title>
    <updated>2025-06-30T21:59:34+00:00</updated>
    <author>
      <name>/u/henryb213</name>
      <uri>https://old.reddit.com/user/henryb213</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt; &lt;img alt="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" src="https://external-preview.redd.it/GF7LOLNV1EkT3j_WQj3wN6pKRBc62ktaNGoxeqmHjug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31f4c15b33f9e40cd80aee5e1468225b045437e8" title="A Meta-Framework for Self-Improving LLMs with Transparent Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Framework overview:&lt;/strong&gt; LLMs iteratively refine their own outputs—typically through a three‑phase cycle &lt;strong&gt;draft → critique → revision&lt;/strong&gt;, repeat until convergence (all phases &amp;amp; stop rules are configurable). I started coding three weeks ago after an eight‑year break and zero professional dev experience.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;The classes work as Python callables with built in observability: instances are callable -&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Python,tabs=4 from recursive_companion.base import MarketingCompanion agent = MarketingCompanion() answer = agent(&amp;quot;question or problem…&amp;quot;) # final refined output print(answer) print(agent.run_log) # list[dict] of every draft, critique &amp;amp; revision &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Why it stays clean &amp;amp; modular&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Templates are plain text files (system prompts, user prompts, protocol). &lt;em&gt;Swap harsh critiques for creative ones by swapping files.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;build_templates()&lt;/code&gt; lets you compose any combination.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Protocol injection&lt;/strong&gt; cleanly separates reasoning patterns from implementation.&lt;/li&gt; &lt;li&gt;New agents in &lt;strong&gt;3 lines&lt;/strong&gt;—just inherit from &lt;code&gt;BaseCompanion&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Convergence uses &lt;strong&gt;embedding‑based cosine similarity&lt;/strong&gt; by default, but the metric is fully pluggable.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;How it came together&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The design emerged from recursive dialogues with multiple LLMs—the same iterative process the framework now automates. No legacy assumptions meant every piece became independent: swap models, add phases, change convergence logic—no rewiring required.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Extras&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Streamlit app&lt;/strong&gt; shows the thinking live as it happens.&lt;/li&gt; &lt;li&gt;Demos cover raw orchestration &lt;em&gt;and&lt;/em&gt; LangGraph integration (agents as graph nodes).&lt;/li&gt; &lt;li&gt;Full architecture docs, comprehensive docstrings, commenting, and worked examples included.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Repo (MIT)&lt;/strong&gt; &lt;a href="https://github.com/hankbesser/recursive-companion"&gt;https://github.com/hankbesser/recursive-companion&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Built by questioning everything. Learning by building, built for learning.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Thanks for reading and really looking for any feedback and open to contributors, no question or discussion is too big or small.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/henryb213"&gt; /u/henryb213 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/hankbesser/recursive-companion"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokkpc/a_metaframework_for_selfimproving_llms_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:59:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1loo2u3</id>
    <title>Struggling with vLLM. The instructions make it sound so simple to run, but it’s like my Kryptonite. I give up.</title>
    <updated>2025-07-01T00:35:22+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m normally the guy they call in to fix the IT stuff nobody else can fix. I’ll laser focus on whatever it is and figure it out probably 99% of the time. I’ve been in IT for over 28+ years. I’ve been messing with AI stuff for nearly 2 years now. Getting my Masters in AI right now. All that being said, I’ve never encountered a more difficult software package to run than trying to get vLLM working in Docker. I can run nearly anything else in Docker except for vLLM. I feel like I’m really close, but every time I think it’s going to run, BAM! some new error that i find very little information on. - I’m running Ubuntu 24.04 - I have a 4090, 3090, and 64GB of RAM on AERO-D TRX50 motherboard. - Yes I have the Nvidia runtime container working - Yes I have the hugginface token generated is there an easy button somewhere that I’m missing? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T00:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lol3na</id>
    <title>[Dataset] 4,000 hours of full-body, in-person, human face-to-face interaction videos</title>
    <updated>2025-06-30T22:20:44+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dataset on Huggingface: &lt;a href="https://huggingface.co/datasets/facebook/seamless-interaction"&gt;https://huggingface.co/datasets/facebook/seamless-interaction&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.aidemos.meta.com/seamless_interaction_dataset"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lol3na/dataset_4000_hours_of_fullbody_inperson_human/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:20:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lodmc6</id>
    <title>ERNIE 4.5 Collection from Baidu</title>
    <updated>2025-06-30T17:27:55+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lodmc6/ernie_45_collection_from_baidu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T17:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lok3r2</id>
    <title>[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News</title>
    <updated>2025-06-30T21:40:05+00:00</updated>
    <author>
      <name>/u/EasternBeyond</name>
      <uri>https://old.reddit.com/user/EasternBeyond</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt; &lt;img alt="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" src="https://external-preview.redd.it/7cRnC2dFTB8VTd7qs9tim3BVul_HOXlhVu97BYC8mXw.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfea0e06944005f53398ccc99f53814a8c4923f4" title="[News] Datacenter GPUs May Have an Astonishingly Short Lifespan of Only 1 to 3 Years | TrendForce News" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasternBeyond"&gt; /u/EasternBeyond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.trendforce.com/news/2024/10/31/news-datacenter-gpus-may-have-an-astonishingly-short-lifespan-of-only-1-to-3-years/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lok3r2/news_datacenter_gpus_may_have_an_astonishingly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lom2r9</id>
    <title>With the OpenAI employees that Meta hired, do you think this will be positive for local models?</title>
    <updated>2025-06-30T23:02:37+00:00</updated>
    <author>
      <name>/u/LarDark</name>
      <uri>https://old.reddit.com/user/LarDark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt; &lt;img alt="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" src="https://preview.redd.it/ymsyhfb2b5af1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6adc725dda988a88523c2dd76383f72148e4d67a" title="With the OpenAI employees that Meta hired, do you think this will be positive for local models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, if these people hired were so important to developing powerful and important OpenAI models. Hopefully the next Llama models will be much better than Llama 4... and raise the bar like Llama did before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LarDark"&gt; /u/LarDark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymsyhfb2b5af1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lom2r9/with_the_openai_employees_that_meta_hired_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T23:02:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1lococc</id>
    <title>Open Source AI Editor: First Milestone</title>
    <updated>2025-06-30T16:52:52+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt; &lt;img alt="Open Source AI Editor: First Milestone" src="https://external-preview.redd.it/7W6FU5na7gC1vKxg3pWb3QkD0a8T5GyzeaLh8U3roNc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d188c22d72aa036de764ff96aa9d951cba5ae6b3" title="Open Source AI Editor: First Milestone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know if you have any questions about open sourcing. Happy to answer. &lt;/p&gt; &lt;p&gt;vscode pm here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lococc/open_source_ai_editor_first_milestone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T16:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lokp88</id>
    <title>Intel Arc Pro B60 Dual 48G Turbo Maxsun GPU Pricing Revealed</title>
    <updated>2025-06-30T22:04:32+00:00</updated>
    <author>
      <name>/u/Airwalker19</name>
      <uri>https://old.reddit.com/user/Airwalker19</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many others, I was hyped for the dual GPU Intel Arc Pro B60, so I emailed Maxsun for a quote. Their US distributor hit me back with $5k per unit for 3 GPUs, or $4.5k each for 5+.&lt;/p&gt; &lt;p&gt;Sure, dual GPUs should cost more, but this is &lt;em&gt;10x&lt;/em&gt; the rumored MSRP of the 24GB card. Space savings are nice, but not &lt;em&gt;that&lt;/em&gt; nice.&lt;/p&gt; &lt;p&gt;RIP my hopes for an (affordable) AI desktop win.&lt;/p&gt; &lt;p&gt;Anyone else think this pricing is delusional, or just me?&lt;/p&gt; &lt;p&gt;UPDATE:&lt;/p&gt; &lt;p&gt;Here's a screenshot of the email &lt;a href="https://imgur.com/a/Qh1nYb1"&gt;https://imgur.com/a/Qh1nYb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I also talked on the phone with a rep and talked him down to $3,800 for 4 units. 5+ units down to $3,000. Still not worth it if the $500 price point for the 24GB cards are to be believed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Airwalker19"&gt; /u/Airwalker19 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T22:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lorbc5</id>
    <title>Is the rumours true about Apple abandoning MLX?</title>
    <updated>2025-07-01T03:17:23+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some folks on X are saying&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lorbc5/is_the_rumours_true_about_apple_abandoning_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-01T03:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lojlrw</id>
    <title>[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team</title>
    <updated>2025-06-30T21:19:51+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt; &lt;img alt="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team" src="https://external-preview.redd.it/hHtdtWWuX05qlxJIIZFgrRzaMxdrmlIQ8OiqTPog1_w.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e97f33d6160ce6f067a79278cab0942d295e3325" title="[WIRED] Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta’s ‘Superintelligence’ Team" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lojlrw/wired_here_is_everyone_mark_zuckerberg_has_hired/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-30T21:19:51+00:00</published>
  </entry>
</feed>
