<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-02-16T08:06:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1iq0n6g</id>
    <title>DeepSeek-R1-Distill tokenization mess</title>
    <updated>2025-02-15T12:48:48+00:00</updated>
    <author>
      <name>/u/remixer_dec</name>
      <uri>https://old.reddit.com/user/remixer_dec</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to discuss the tokenization issues with the DeepSeek-R1-Distill-Qwen-32B model. This may be relevant towards other R1-Distill family models (or at least qwen-based, as pointed out in one of the issues linked), I only tested it on 32B.&lt;/p&gt; &lt;p&gt;Its tokenizer config was &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B/commits/main/tokenizer_config.json"&gt;changed&lt;/a&gt; multiple times. They changed &lt;strong&gt;add_bos_token&lt;/strong&gt; parameter and the template. Last two revisions have both &amp;quot;&lt;strong&gt;add_bos_token&amp;quot;: true&lt;/strong&gt; in the config and &lt;code&gt;{{bos_token}}&lt;/code&gt; in the chat template. vLLM renders both of these tokens, so chat completions requests end up with &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B/discussions/31"&gt;2 bos tokens&lt;/a&gt;, as mentioned in this issue. Llama.cpp for some reason does not render the bos token inside chat template, possibly because it is used as a variable.&lt;/p&gt; &lt;p&gt;They also changed qwen's tokenizer.json, and the markup formatting tokens used for instruction tuning / chat-completions are set as &lt;code&gt;special:false&lt;/code&gt; which causes .GGUF converted models (in &lt;a href="https://github.com/vllm-project/vllm/issues/12985"&gt;vllm&lt;/a&gt; and &lt;a href="https://github.com/sgl-project/sglang/issues/3427"&gt;sglang&lt;/a&gt;; llama.cpp does not have such problem) to behave poorly due to incorrect tokenization.&lt;/p&gt; &lt;p&gt;Apparently, they also &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B/discussions/28"&gt;messed up&lt;/a&gt; the bos_token_id in config.json&lt;/p&gt; &lt;p&gt;Just wanted to bring more attention to this issue to maybe get some clarity whether this model really requires two BOS tokens or is it just currently in a buggy state.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/remixer_dec"&gt; /u/remixer_dec &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq0n6g/deepseekr1distill_tokenization_mess/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq0n6g/deepseekr1distill_tokenization_mess/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq0n6g/deepseekr1distill_tokenization_mess/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T12:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqljmt</id>
    <title>SanDisk's High Bandwidth Flash might help local llm</title>
    <updated>2025-02-16T05:50:35+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like it should be at least 128GB/s and 4TB max at size in the first gen. If the pricing is right, it can be a solution for MoE models like R1 and multi-LLM workflow.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/dram/sandisks-new-hbf-memory-enables-up-to-4tb-of-vram-on-gpus-matches-hbm-bandwidth-at-higher-capacity"&gt;https://www.tomshardware.com/pc-components/dram/sandisks-new-hbf-memory-enables-up-to-4tb-of-vram-on-gpus-matches-hbm-bandwidth-at-higher-capacity&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqljmt/sandisks_high_bandwidth_flash_might_help_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqljmt/sandisks_high_bandwidth_flash_might_help_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqljmt/sandisks_high_bandwidth_flash_might_help_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T05:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipsnck</id>
    <title>How I created LlamaThink-8b-Instruct</title>
    <updated>2025-02-15T03:30:52+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;LlamaThink-8b-Instruct Finetuning Process&lt;/h1&gt; &lt;p&gt;I recently created &lt;a href="https://huggingface.co/DavidBrowne17/LlamaThink-8B-instruct"&gt;LlamaThink-8b-Instruct Full Instruct model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GGUF:&lt;/strong&gt; &lt;a href="https://huggingface.co/DavidBrowne17/LlamaThink-8B-instruct-GGUF"&gt;LlamaThink-8b-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and a few of you were curious as to how I made it, here is the process to finetune a model with &lt;strong&gt;GRPO reinforcement learning&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;So our goal is to make a thinker model, its super easy, first we need a dataset. Here is a script for llama cpp python to create a dataset.&lt;/p&gt; &lt;p&gt;```python import json import gc import random import re from llama_cpp import Llama import textwrap&lt;/p&gt; &lt;p&gt;MODEL_PATHS = [ &amp;quot;YOUR MODEL GGUF HERE&amp;quot; ]&lt;/p&gt; &lt;p&gt;OUTPUT_FILE = &amp;quot;./enhanced_simple_dataset.jsonl&amp;quot;&lt;/p&gt; &lt;p&gt;NUM_CONVERSATIONS = 5000 TURNS_PER_CONVO = 1 MAX_TOKENS = 100&lt;/p&gt; &lt;p&gt;STOP_TOKENS = [ &amp;quot;&amp;lt;/s&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;&amp;lt;USR&amp;gt;&amp;gt;&amp;quot;, &amp;quot;&amp;lt;&amp;lt;/USR&amp;gt;&amp;gt;&amp;quot;, &amp;quot;&amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt;&amp;quot;, &amp;quot;&amp;lt;&amp;lt;/USER&amp;gt;&amp;gt;&amp;quot;, &amp;quot;&amp;lt;&amp;lt;/ASSISTANT&amp;gt;&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|im_end|&amp;gt;&amp;quot;, &amp;quot;user:&amp;quot;, &amp;quot;User:&amp;quot;, &amp;quot;user :&amp;quot;, &amp;quot;User :&amp;quot;, &amp;quot;[assistant]&amp;quot;, &amp;quot;[[assistant]]&amp;quot;, &amp;quot;[user]&amp;quot;, &amp;quot;[[user]]&amp;quot;, &amp;quot;[/assistant]&amp;quot;, &amp;quot;[/user]&amp;quot;, &amp;quot;[\assistant]&amp;quot; ]&lt;/p&gt; &lt;p&gt;USER_INSTRUCTION = ( &amp;quot;You are engaging in a conversation with an AI designed for deep reasoning and structured thinking. &amp;quot; &amp;quot;Ask questions naturally while expecting insightful, multi-layered responses. &amp;quot; &amp;quot;Ask a unique, relevant question. &amp;quot; &amp;quot;Keep messages clear and concise. Respond only with the Question, nothing else.&amp;quot; )&lt;/p&gt; &lt;p&gt;INSTRUCTIONS = { &amp;quot;system_prompt&amp;quot;: textwrap.dedent(&amp;quot;&amp;quot;&amp;quot; Generate a system prompt for an AI to follow. This is a prompt for how the AI should behave, e.g., You are a chatbot, assistant, maths teacher, etc. It should not be instructions for a specific task. Do not add any explanations, headers, or formatting. Only output the system prompt text. &amp;quot;&amp;quot;&amp;quot;).strip(),&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;thinking&amp;quot;: ( &amp;quot;You are an AI designed to think deeply about the conversation topic. &amp;quot; &amp;quot;This is your internal thought process which is not visible to the user. &amp;quot; &amp;quot;Explain to yourself how you figure out the answer. &amp;quot; &amp;quot;Consider the user's question carefully, analyze the context, and formulate a coherent response strategy. &amp;quot; &amp;quot;Ensure your thought process is logical and well-structured. Do not generate any headers.&amp;quot; ), &amp;quot;final&amp;quot;: ( &amp;quot;You are the final reviewer ensuring the response meets high standards of quality and insight. &amp;quot; &amp;quot;Your goal is to:\n&amp;quot; &amp;quot;1. Maximize logical depth and engagement.\n&amp;quot; &amp;quot;2. Ensure the response is precise, well-reasoned, and helpful.\n&amp;quot; &amp;quot;3. Strengthen structured argumentation and clarity.\n&amp;quot; &amp;quot;4. Maintain a professional and well-organized tone.\n&amp;quot; &amp;quot;In your final response, reference the user-provided system prompt to ensure consistency and relevance. &amp;quot; &amp;quot;Be concise and give the final answer.&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;def load_model(path): &amp;quot;&amp;quot;&amp;quot;Loads a single model.&amp;quot;&amp;quot;&amp;quot; try: return Llama(model_path=path, n_ctx=16000, n_gpu_layers=-1, chat_format=&amp;quot;llama-3&amp;quot;) except Exception as e: print(f&amp;quot;Failed to load model {path}: {e}&amp;quot;) return None&lt;/p&gt; &lt;p&gt;def call_model(llm, messages): &amp;quot;&amp;quot;&amp;quot;Calls the model using chat completion API and retries on failure.&amp;quot;&amp;quot;&amp;quot; attempt = 0 while True: attempt += 1 try: result = llm.create_chat_completion( messages=messages, max_tokens=MAX_TOKENS, temperature=random.uniform(1.4, 1.7), top_k=random.choice([250, 350]), top_p=random.uniform(0.85, 0.95), seed=random.randint(1, 900000000), stop=STOP_TOKENS ) response_text = result[&amp;quot;choices&amp;quot;][0][&amp;quot;message&amp;quot;][&amp;quot;content&amp;quot;].strip() if response_text: return response_text else: print(f&amp;quot;Attempt {attempt}: Empty response. Retrying...&amp;quot;) except ValueError as e: print(f&amp;quot;Attempt {attempt}: Model call error: {e}. Retrying...&amp;quot;) except KeyboardInterrupt: print(&amp;quot;\nManual interruption detected. Exiting retry loop.&amp;quot;) return &amp;quot;Error: Retry loop interrupted by user.&amp;quot; except Exception as e: print(f&amp;quot;Unexpected error on attempt {attempt}: {e}. Retrying...&amp;quot;)&lt;/p&gt; &lt;p&gt;def generate_system_prompt(llm): messages = [{&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: INSTRUCTIONS[&amp;quot;system_prompt&amp;quot;]}] return call_model(llm, messages)&lt;/p&gt; &lt;p&gt;def generate_user_message(llm, system_prompt): messages = [ {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: USER_INSTRUCTION} ] return call_model(llm, messages)&lt;/p&gt; &lt;p&gt;def trim_to_last_complete_sentence(text): &amp;quot;&amp;quot;&amp;quot;Trims text to the last complete sentence.&amp;quot;&amp;quot;&amp;quot; matches = list(re.finditer(r'[.!?]', text)) return text[:matches[-1].end()] if matches else text&lt;/p&gt; &lt;p&gt;def generate_response(llm, conversation_history, system_prompt): thinking = call_model(llm, [ {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: INSTRUCTIONS[&amp;quot;thinking&amp;quot;]} ])&lt;/p&gt; &lt;pre&gt;&lt;code&gt;final_response = call_model(llm, [ {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: INSTRUCTIONS[&amp;quot;final&amp;quot;]} ]) return f&amp;quot;&amp;lt;thinking&amp;gt;{trim_to_last_complete_sentence(thinking)}&amp;lt;/thinking&amp;gt;\n\n&amp;lt;answer&amp;gt;{trim_to_last_complete_sentence(final_response)}&amp;lt;/answer&amp;gt;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;def format_conversation(conversation): return &amp;quot;\n&amp;quot;.join(f&amp;quot;{entry['role']}: {entry['content']}&amp;quot; for entry in conversation)&lt;/p&gt; &lt;p&gt;def generate_conversation(llm): conversation = [] system_prompt = generate_system_prompt(llm)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for _ in range(TURNS_PER_CONVO): user_message_text = generate_user_message(llm, system_prompt) conversation.append({&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: user_message_text}) conv_history_str = format_conversation(conversation) assistant_message_text = generate_response(llm, conv_history_str, system_prompt) conversation.append({&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: assistant_message_text}) return system_prompt, conversation &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;def validate_json(data): &amp;quot;&amp;quot;&amp;quot;Ensures JSON is valid before writing.&amp;quot;&amp;quot;&amp;quot; try: json.loads(json.dumps(data)) return True except json.JSONDecodeError as e: print(f&amp;quot;Invalid JSON detected: {e}&amp;quot;) return False&lt;/p&gt; &lt;p&gt;def main(): llm = load_model(MODEL_PATHS[0]) if not llm: print(&amp;quot;Failed to load the model. Exiting.&amp;quot;) return&lt;/p&gt; &lt;pre&gt;&lt;code&gt;with open(OUTPUT_FILE, &amp;quot;a&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as out_f: for convo_idx in range(NUM_CONVERSATIONS): system_prompt, conversation = generate_conversation(llm) json_output = { &amp;quot;instruction&amp;quot;: system_prompt.strip(), &amp;quot;conversation&amp;quot;: conversation } if validate_json(json_output): json_string = json.dumps(json_output, ensure_ascii=False) out_f.write(json_string + &amp;quot;\n&amp;quot;) else: print(f&amp;quot;Skipping malformed JSON for conversation {convo_idx}&amp;quot;) if convo_idx % 100 == 0: print(f&amp;quot;Wrote conversation {convo_idx}/{NUM_CONVERSATIONS}&amp;quot;) del llm gc.collect() print(f&amp;quot;Dataset complete: {OUTPUT_FILE}&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;: main() ```&lt;/p&gt; &lt;p&gt;I set the limit to 5000 but we really only need about 300 results to finetune our model. I highly recommend changing the prompts slightly as you get more useful data, to get a more diverse dataset, This will improve your final results. Tell it to be a mathematician, historian etc. and to ask complex advanced questions.&lt;/p&gt; &lt;p&gt;Once the dataset is ready, install unsloth. Once your install is done you can create a new file called grpo.py which contains the following code, once the dataset is ready, place it in the same directory as the grpo.py file in the unsloth folder.&lt;/p&gt; &lt;p&gt;```python import sys import os import re import torch from typing import List from sentence_transformers import SentenceTransformer import numpy as np&lt;/p&gt; &lt;p&gt;embedder = SentenceTransformer(&amp;quot;all-MiniLM-L6-v2&amp;quot;) os.environ[&amp;quot;CUDA_LAUNCH_BLOCKING&amp;quot;] = &amp;quot;1&amp;quot;&lt;/p&gt; &lt;p&gt;if sys.platform == &amp;quot;win32&amp;quot;: import types resource = types.ModuleType(&amp;quot;resource&amp;quot;) resource.getrlimit = lambda resource_id: (0, 0) resource.setrlimit = lambda resource_id, limits: None sys.modules[&amp;quot;resource&amp;quot;] = resource&lt;/p&gt; &lt;p&gt;from unsloth import FastLanguageModel, PatchFastRL, is_bfloat16_supported PatchFastRL(&amp;quot;GRPO&amp;quot;, FastLanguageModel) from datasets import load_dataset from trl import GRPOConfig, GRPOTrainer from transformers import AutoModelForCausalLM, AutoTokenizer from peft import LoraConfig, get_peft_model, PeftModel&lt;/p&gt; &lt;h1&gt;Configuration&lt;/h1&gt; &lt;p&gt;MAX_SEQ_LENGTH = 256 LORA_RANK = 16 BASE_MODEL_NAME = &amp;quot;unsloth/Meta-Llama-3.1-8B-instruct&amp;quot; DATASET_PATH = &amp;quot;enhanced_simple_dataset.jsonl&amp;quot; ADAPTER_SAVE_PATH = &amp;quot;grpo_adapter&amp;quot; MERGED_MODEL_PATH = &amp;quot;merged_grpo_full&amp;quot; SYSTEM_PROMPT = &amp;quot;&amp;quot;&amp;quot; Respond in the following format: &amp;lt;thinking&amp;gt; ... &amp;lt;/thinking&amp;gt; &amp;lt;answer&amp;gt; ... &amp;lt;/answer&amp;gt; The thinking and answer portions should be no more than 100 tokens each. &amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;def format_dataset_entry(example): &amp;quot;&amp;quot;&amp;quot;Format dataset entries for GRPO training.&amp;quot;&amp;quot;&amp;quot; system_prompt = example.get(&amp;quot;instruction&amp;quot;, &amp;quot;&amp;quot;) conversation = example.get(&amp;quot;conversation&amp;quot;, [])&lt;/p&gt; &lt;pre&gt;&lt;code&gt;messages = [{&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt + SYSTEM_PROMPT}] if conversation and conversation[-1].get(&amp;quot;role&amp;quot;) == &amp;quot;assistant&amp;quot;: for turn in conversation[:-1]: messages.append(turn) answer = conversation[-1].get(&amp;quot;content&amp;quot;, &amp;quot;&amp;quot;) else: for turn in conversation: messages.append(turn) answer = &amp;quot;&amp;quot; return {&amp;quot;prompt&amp;quot;: messages, &amp;quot;answer&amp;quot;: answer} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;def extract_xml_answer(text: str) -&amp;gt; str: answer = text.split(&amp;quot;&amp;lt;answer&amp;gt;&amp;quot;)[-1] answer = answer.split(&amp;quot;&amp;lt;/answer&amp;gt;&amp;quot;)[0] return answer.strip()&lt;/p&gt; &lt;p&gt;def correctness_reward_func(prompts, completions, answer, **kwargs) -&amp;gt; list[float]: responses = [completion[0]['content'] for completion in completions] q = prompts[0][-1]['content'] extracted_responses = [extract_xml_answer(r) for r in responses]&lt;/p&gt; &lt;pre&gt;&lt;code&gt;print('-' * 20, f&amp;quot;Question:\n{q}&amp;quot;, f&amp;quot;\nAnswer:\n{answer[0]}&amp;quot;, f&amp;quot;\nResponse:\n{responses[0]}&amp;quot;, f&amp;quot;\nExtracted:\n{extracted_responses[0]}&amp;quot;) # Compute embeddings and cosine similarity answer_embedding = embedder.encode(answer, convert_to_numpy=True) response_embeddings = embedder.encode(extracted_responses, convert_to_numpy=True) similarities = [np.dot(r, answer_embedding) / (np.linalg.norm(r) * np.linalg.norm(answer_embedding)) for r in response_embeddings] # Convert similarity to reward (scaled 0-2 range) return [max(0.0, min(2.0, s * 2)) for s in similarities] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;def int_reward_func(completions, **kwargs) -&amp;gt; list[float]: responses = [completion[0]['content'] for completion in completions] extracted_responses = [extract_xml_answer(r) for r in responses] return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]&lt;/p&gt; &lt;p&gt;def strict_format_reward_func(completions, &lt;em&gt;kwargs) -&amp;gt; list[float]: pattern = r&amp;quot;&amp;lt;thinking&amp;gt;\n.?\n&amp;lt;/thinking&amp;gt;\n&amp;lt;answer&amp;gt;\n.&lt;/em&gt;?\n&amp;lt;/answer&amp;gt;\n$&amp;quot; responses = [completion[0][&amp;quot;content&amp;quot;] for completion in completions] matches = [re.match(pattern, r) for r in responses] return [0.5 if match else 0.0 for match in matches]&lt;/p&gt; &lt;p&gt;def soft_format_reward_func(completions, *kwargs) -&amp;gt; list[float]: pattern = r&amp;quot;&amp;lt;thinking&amp;gt;.?&amp;lt;/thinking&amp;gt;\s&amp;lt;answer&amp;gt;.?&amp;lt;/answer&amp;gt;&amp;quot; responses = [completion[0][&amp;quot;content&amp;quot;] for completion in completions] matches = [re.match(pattern, r) for r in responses] return [0.5 if match else 0.0 for match in matches]&lt;/p&gt; &lt;p&gt;def count_xml(text) -&amp;gt; float: count = 0.0 if text.count(&amp;quot;&amp;lt;thinking&amp;gt;\n&amp;quot;) == 1: count += 0.125 if text.count(&amp;quot;\n&amp;lt;/thinking&amp;gt;\n&amp;quot;) == 1: count += 0.125 if text.count(&amp;quot;\n&amp;lt;answer&amp;gt;\n&amp;quot;) == 1: count += 0.125 count -= len(text.split(&amp;quot;\n&amp;lt;/answer&amp;gt;\n&amp;quot;)[-1]) * 0.001 if text.count(&amp;quot;\n&amp;lt;/answer&amp;gt;&amp;quot;) == 1: count += 0.125 count -= (len(text.split(&amp;quot;\n&amp;lt;/answer&amp;gt;&amp;quot;)[-1]) - 1) * 0.001 return count&lt;/p&gt; &lt;p&gt;def xmlcount_reward_func(completions, **kwargs) -&amp;gt; list[float]: contents = [completion[0][&amp;quot;content&amp;quot;] for completion in completions] return [count_xml(c) for c in contents]&lt;/p&gt; &lt;p&gt;def main(): print(&amp;quot;Loading model and tokenizer...&amp;quot;) model, tokenizer = FastLanguageModel.from_pretrained( model_name=BASE_MODEL_NAME, max_seq_length=MAX_SEQ_LENGTH, load_in_4bit=True, fast_inference=False, max_lora_rank=LORA_RANK, gpu_memory_utilization=0.9, device_map={&amp;quot;&amp;quot;: torch.cuda.current_device()} )&lt;/p&gt; &lt;pre&gt;&lt;code&gt;print(&amp;quot;Applying GRPO adapter...&amp;quot;) lora_config = LoraConfig( r=16, lora_alpha=16, target_modules=[ &amp;quot;q_proj&amp;quot;, &amp;quot;k_proj&amp;quot;, &amp;quot;v_proj&amp;quot;, &amp;quot;o_proj&amp;quot;, &amp;quot;gate_proj&amp;quot;, &amp;quot;up_proj&amp;quot;, &amp;quot;down_proj&amp;quot;, &amp;quot;embed_tokens&amp;quot;, &amp;quot;lm_head&amp;quot; ], lora_dropout=0.05, bias=&amp;quot;none&amp;quot;, task_type=&amp;quot;CAUSAL_LM&amp;quot;, inference_mode=False ) print(&amp;quot;Applying QLoRA to the base model.&amp;quot;) model = get_peft_model(model, lora_config) print(&amp;quot;Loading and processing dataset...&amp;quot;) raw_dataset = load_dataset(&amp;quot;json&amp;quot;, data_files=DATASET_PATH, split=&amp;quot;train&amp;quot;) formatted_dataset = raw_dataset.map(format_dataset_entry) print(&amp;quot;Configuring training...&amp;quot;) training_args = GRPOConfig( use_vllm = False, learning_rate = 5e-6, adam_beta1 = 0.9, adam_beta2 = 0.99, weight_decay = 0.1, warmup_ratio = 0.1, lr_scheduler_type = &amp;quot;cosine&amp;quot;, optim = &amp;quot;paged_adamw_8bit&amp;quot;, logging_steps = 1, bf16 = is_bfloat16_supported(), fp16 = not is_bfloat16_supported(), per_device_train_batch_size = 1 gradient_accumulation_steps = 1, num_generations = 6, # Decrease if out of memory max_prompt_length = 256, max_completion_length = 250, max_steps = 250, save_steps = 10, max_grad_norm = 0.1, report_to = &amp;quot;none&amp;quot;, output_dir = &amp;quot;outputs&amp;quot;, ) print(&amp;quot;Initializing trainer...&amp;quot;) trainer = GRPOTrainer( model=model, processing_class=tokenizer, reward_funcs=[ xmlcount_reward_func, soft_format_reward_func, strict_format_reward_func, int_reward_func, correctness_reward_func, ], args=training_args, train_dataset=formatted_dataset, ) print(&amp;quot;Starting training...&amp;quot;) trainer.train() print(f&amp;quot;Saving GRPO adapter to {ADAPTER_SAVE_PATH}&amp;quot;) model.save_pretrained(ADAPTER_SAVE_PATH) tokenizer.save_pretrained(ADAPTER_SAVE_PATH) print(&amp;quot;Loading base model for merging...&amp;quot;) base_model = AutoModelForCausalLM.from_pretrained( BASE_MODEL_NAME, torch_dtype=torch.float16, device_map={&amp;quot;&amp;quot;: torch.cuda.current_device()} ) base_model.config.pad_token_id = tokenizer.pad_token_id print(&amp;quot;Merging GRPO adapter...&amp;quot;) grpo_model = PeftModel.from_pretrained(base_model, ADAPTER_SAVE_PATH) merged_model = grpo_model.merge_and_unload() print(f&amp;quot;Saving merged model to {MERGED_MODEL_PATH}&amp;quot;) merged_model.save_pretrained(MERGED_MODEL_PATH) tokenizer.save_pretrained(MERGED_MODEL_PATH) print(&amp;quot;Process completed successfully!&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;main&amp;quot;: main() ``` We are loading and finetuning the model in 4 bit, but saving the adapter in the full model, this will significantly speed up the training time. For the most part your dataset doesnt need advanced coding info, we just need it to be simple and fit the format well so the model can learn to think. When this is finished you should have a completed finetuned thinking model. This code can be used for smaller models like Llama-3b. Have fun machine learning!&lt;/p&gt; &lt;p&gt;If you crash mid training you can load your latest checkpoint ```python import sys import os import re import torch from typing import List&lt;/p&gt; &lt;p&gt;if sys.platform == &amp;quot;win32&amp;quot;: import types resource = types.ModuleType(&amp;quot;resource&amp;quot;) resource.getrlimit = lambda resource_id: (0, 0) resource.setrlimit = lambda resource_id, limits: None sys.modules[&amp;quot;resource&amp;quot;] = resource&lt;/p&gt; &lt;p&gt;from unsloth import FastLanguageModel, PatchFastRL, is_bfloat16_supported PatchFastRL(&amp;quot;GRPO&amp;quot;, FastLanguageModel) from datasets import load_dataset from trl import GRPOConfig, GRPOTrainer from transformers import AutoModelForCausalLM, AutoTokenizer from peft import LoraConfig, get_peft_model, PeftModel from sentence_transformers import SentenceTransformer import numpy as np&lt;/p&gt; &lt;p&gt;embedder = SentenceTransformer(&amp;quot;all-MiniLM-L6-v2&amp;quot;) MAX_SEQ_LENGTH = 512 LORA_RANK = 32 BASE_MODEL_NAME = &amp;quot;unsloth/meta-Llama-3.1-8B-instruct&amp;quot; DATASET_PATH = &amp;quot;enhanced_dataset.jsonl&amp;quot; ADAPTER_SAVE_PATH = &amp;quot;grpo_adapter&amp;quot; MERGED_MODEL_PATH = &amp;quot;merged_grpo_full&amp;quot; CHECKPOINT_PATH = &amp;quot;YOUR_LATEST_CHECKPOINT&amp;quot; SYSTEM_PROMPT = &amp;quot;&amp;quot;&amp;quot; Respond in the following format: &amp;lt;thinking&amp;gt; ... &amp;lt;/thinking&amp;gt; &amp;lt;answer&amp;gt; ... &amp;lt;/answer&amp;gt; &amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;def format_dataset_entry(example): &amp;quot;&amp;quot;&amp;quot;Format dataset entries for GRPO training.&amp;quot;&amp;quot;&amp;quot; system_prompt = example.get(&amp;quot;instruction&amp;quot;, &amp;quot;&amp;quot;) conversation = example.get(&amp;quot;conversation&amp;quot;, [])&lt;/p&gt; &lt;pre&gt;&lt;code&gt;messages = [{&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt + SYSTEM_PROMPT}] if conversation and conversation[-1].get(&amp;quot;role&amp;quot;) == &amp;quot;assistant&amp;quot;: for turn in conversation[:-1]: messages.append(turn) answer = conversation[-1].get(&amp;quot;content&amp;quot;, &amp;quot;&amp;quot;) else: for turn in conversation: messages.append(turn) answer = &amp;quot;&amp;quot; return {&amp;quot;prompt&amp;quot;: messages, &amp;quot;answer&amp;quot;: answer} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;def extract_xml_answer(text: str) -&amp;gt; str: answer = text.split(&amp;quot;&amp;lt;answer&amp;gt;&amp;quot;)[-1] answer = answer.split(&amp;quot;&amp;lt;/answer&amp;gt;&amp;quot;)[0] return answer.strip()&lt;/p&gt; &lt;p&gt;def correctness_reward_func(prompts, completions, answer, **kwargs) -&amp;gt; list[float]: responses = [completion[0]['content'] for completion in completions] q = prompts[0][-1]['content'] extracted_responses = [extract_xml_answer(r) for r in responses]&lt;/p&gt; &lt;pre&gt;&lt;code&gt;print('-' * 20, f&amp;quot;Question:\n{q}&amp;quot;, f&amp;quot;\nAnswer:\n{answer[0]}&amp;quot;, f&amp;quot;\nResponse:\n{responses[0]}&amp;quot;, f&amp;quot;\nExtracted:\n{extracted_responses[0]}&amp;quot;) # Compute embeddings and cosine similarity answer_embedding = embedder.encode(answer, convert_to_numpy=True) response_embeddings = embedder.encode(extracted_responses, convert_to_numpy=True) similarities = [np.dot(r, answer_embedding) / (np.linalg.norm(r) * np.linalg.norm(answer_embedding)) for r in response_embeddings] # Convert similarity to reward (scaled 0-2 range) return [max(0.0, min(2.0, s * 2)) for s in similarities] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;def int_reward_func(completions, **kwargs) -&amp;gt; list[float]: responses = [completion[0]['content'] for completion in completions] extracted_responses = [extract_xml_answer(r) for r in responses] return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]&lt;/p&gt; &lt;p&gt;def strict_format_reward_func(completions, *&lt;em&gt;kwargs) -&amp;gt; list[float]: pattern = r&amp;quot;&lt;sup&gt;&amp;lt;thinking&amp;gt;\n.&lt;/sup&gt;&lt;/em&gt;?\n&amp;lt;/thinking&amp;gt;\n&amp;lt;answer&amp;gt;\n.*?\n&amp;lt;/answer&amp;gt;\n$&amp;quot; responses = [completion[0][&amp;quot;content&amp;quot;] for completion in completions] matches = [re.match(pattern, r) for r in responses] return [0.5 if match else 0.0 for match in matches]&lt;/p&gt; &lt;p&gt;def soft_format_reward_func(completions, *&lt;em&gt;kwargs) -&amp;gt; list[float]: pattern = r&amp;quot;&amp;lt;thinking&amp;gt;.&lt;/em&gt;?&amp;lt;/thinking&amp;gt;\s&lt;em&gt;&amp;lt;answer&amp;gt;.&lt;/em&gt;?&amp;lt;/answer&amp;gt;&amp;quot; responses = [completion[0][&amp;quot;content&amp;quot;] for completion in completions] matches = [re.match(pattern, r) for r in responses] return [0.5 if match else 0.0 for match in matches]&lt;/p&gt; &lt;p&gt;def count_xml(text) -&amp;gt; float: count = 0.0 if text.count(&amp;quot;&amp;lt;thinking&amp;gt;\n&amp;quot;) == 1: count += 0.125 if text.count(&amp;quot;\n&amp;lt;/thinking&amp;gt;\n&amp;quot;) == 1: count += 0.125 if text.count(&amp;quot;\n&amp;lt;answer&amp;gt;\n&amp;quot;) == 1: count += 0.125 count -= len(text.split(&amp;quot;\n&amp;lt;/answer&amp;gt;\n&amp;quot;)[-1])&lt;em&gt;0.001 if text.count(&amp;quot;\n&amp;lt;/answer&amp;gt;&amp;quot;) == 1: count += 0.125 count -= (len(text.split(&amp;quot;\n&amp;lt;/answer&amp;gt;&amp;quot;)[-1]) - 1)&lt;/em&gt;0.001 return count&lt;/p&gt; &lt;p&gt;def xmlcount_reward_func(completions, **kwargs) -&amp;gt; list[float]: contents = [completion[0][&amp;quot;content&amp;quot;] for completion in completions] return [count_xml(c) for c in contents]&lt;/p&gt; &lt;p&gt;def main(): print(&amp;quot;Loading model and tokenizer...&amp;quot;) model, tokenizer = FastLanguageModel.from_pretrained( model_name=BASE_MODEL_NAME, max_seq_length=MAX_SEQ_LENGTH, load_in_4bit=True, fast_inference=False, max_lora_rank=LORA_RANK, gpu_memory_utilization=0.9, device_map={&amp;quot;&amp;quot;: torch.cuda.current_device()} )&lt;/p&gt; &lt;pre&gt;&lt;code&gt;print(&amp;quot;Applying GRPO adapter...&amp;quot;) lora_config = LoraConfig( r=16, lora_alpha=16, target_modules=[ &amp;quot;q_proj&amp;quot;, &amp;quot;k_proj&amp;quot;, &amp;quot;v_proj&amp;quot;, &amp;quot;o_proj&amp;quot;, &amp;quot;gate_proj&amp;quot;, &amp;quot;up_proj&amp;quot;, &amp;quot;down_proj&amp;quot;, &amp;quot;embed_tokens&amp;quot;, &amp;quot;lm_head&amp;quot; ], lora_dropout=0.05, bias=&amp;quot;none&amp;quot;, task_type=&amp;quot;CAUSAL_LM&amp;quot;, inference_mode=False ) print(&amp;quot;Applying QLoRA to the base model.&amp;quot;) model = get_peft_model(model, lora_config) print(&amp;quot;Loading and processing dataset...&amp;quot;) raw_dataset = load_dataset(&amp;quot;json&amp;quot;, data_files=DATASET_PATH, split=&amp;quot;train&amp;quot;) formatted_dataset = raw_dataset.map(format_dataset_entry) print(&amp;quot;Configuring training...&amp;quot;) training_args = GRPOConfig( use_vllm = False, learning_rate = 5e-6, adam_beta1 = 0.9, adam_beta2 = 0.99, weight_decay = 0.1, warmup_ratio = 0.1, lr_scheduler_type = &amp;quot;cosine&amp;quot;, optim = &amp;quot;paged_adamw_8bit&amp;quot;, logging_steps = 1, bf16 = is_bfloat16_supported(), fp16 = not is_bfloat16_supported(), per_device_train_batch_size = 1, gradient_accumulation_steps = 1, num_generations = 6, max_prompt_length = 256, max_completion_length = 250, num_train_epochs = 1, max_steps = 250, save_steps = 10, max_grad_norm = 0.1, report_to = &amp;quot;none&amp;quot;, output_dir = &amp;quot;outputs&amp;quot;, ) print(&amp;quot;Initializing trainer...&amp;quot;) trainer = GRPOTrainer( model=model, processing_class=tokenizer, reward_funcs=[ xmlcount_reward_func, soft_format_reward_func, strict_format_reward_func, int_reward_func, correctness_reward_func, ], args=training_args, train_dataset=formatted_dataset, ) print(&amp;quot;Starting training...&amp;quot;) try: if os.path.exists(CHECKPOINT_PATH): print(f&amp;quot;Resuming training from checkpoint: {CHECKPOINT_PATH}&amp;quot;) trainer.train(resume_from_checkpoint=CHECKPOINT_PATH) else: print(&amp;quot;No checkpoint found; starting training from scratch...&amp;quot;) trainer.train() # Save the adapter print(f&amp;quot;Saving GRPO adapter to {ADAPTER_SAVE_PATH}&amp;quot;) if not os.path.exists(ADAPTER_SAVE_PATH): os.makedirs(ADAPTER_SAVE_PATH) model.save_pretrained(ADAPTER_SAVE_PATH) tokenizer.save_pretrained(ADAPTER_SAVE_PATH) except Exception as e: print(f&amp;quot;Error during training or saving: {str(e)}&amp;quot;) raise try: print(&amp;quot;Loading base model in full precision...&amp;quot;) base_model = AutoModelForCausalLM.from_pretrained( BASE_MODEL_NAME, torch_dtype=torch.float16, device_map={&amp;quot;&amp;quot;: torch.cuda.current_device()} ) base_model.config.pad_token_id = tokenizer.pad_token_id print(&amp;quot;Loading and merging GRPO adapter...&amp;quot;) grpo_model = PeftModel.from_pretrained(base_model, ADAPTER_SAVE_PATH) merged_model = grpo_model.merge_and_unload() if not os.path.exists(MERGED_MODEL_PATH): os.makedirs(MERGED_MODEL_PATH) print(f&amp;quot;Saving merged model to {MERGED_MODEL_PATH}&amp;quot;) merged_model.save_pretrained(MERGED_MODEL_PATH) tokenizer.save_pretrained(MERGED_MODEL_PATH) print(&amp;quot;Process completed successfully!&amp;quot;) except Exception as e: print(f&amp;quot;Error during model merging: {str(e)}&amp;quot;) raise &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;: main() ```&lt;/p&gt; &lt;p&gt;This is useful if your PC restarts or updates mid training.&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/W2aPnxl"&gt;https://imgur.com/a/W2aPnxl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipsnck/how_i_created_llamathink8binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipsnck/how_i_created_llamathink8binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipsnck/how_i_created_llamathink8binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T03:30:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq0mj5</id>
    <title>What's going on with Mistral Small 24B?</title>
    <updated>2025-02-15T12:47:38+00:00</updated>
    <author>
      <name>/u/martinerous</name>
      <uri>https://old.reddit.com/user/martinerous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What has been your experience when comparing the new Mistral Small 24B to the previous Mistral Small 22B? Which tasks is the new one better at, and when is it worse?&lt;/p&gt; &lt;p&gt;I've been using the previous Mistral Small 22B for long scenario-based roleplays for months. While it was suffering from &amp;quot;GPT-isms&amp;quot;, it still had the strength of the Mistral models, which is following scenarios more to the letter and being quite pragmatic. I was switching between it and Mixtral 8x7B and they both were the best consistent midrangers.&lt;/p&gt; &lt;p&gt;I was pretty hyped to hear about the new Mistral Small 24B and I ran it through my highly subjective &amp;quot;test suite&amp;quot; a few times. It was unpleasant to discover that it seems to have more GPT-isms, and also tends to get caught in repetitive loops more often. But what's worse - a few times it got stuck at following a quite simple instruction that has been working well for the old Mistral Small and all the other models I tested. Essentially, I have a multicharacter frontend with dynamic scene loading, and every scene has `[Write eofscene]` at the end. The system prompt also has `When the scene is completed, the character's message must end with the exact word eofscene.`&lt;/p&gt; &lt;p&gt;The new Mistral got stuck at this a few times. It definitely was able to deduce that it had reached the end of the scene because it kept blabbering about how it was ready for the next phase and even printed &amp;quot;Scene is complete&amp;quot;. No eofscene though. I modified the scene instruction to say `[Write eofscene][Say eofscene][Output eofscene]eofscene`, regenerated the last message a dozen times, and then it finally got unstuck.&lt;/p&gt; &lt;p&gt;I tried it both locally and on OpenRouter, and played with temperature - did not help much.&lt;/p&gt; &lt;p&gt;Now when I have my own frontend where I can visually format output as I want, I can use Gemma 27B, which had formatting issues when using Backyard AI. Gemma 27B can be even better than Mistral 22B for my use case after I have dealt with its formatting quirks. I'm looking forward to new Google models, but I'm worried that their new &amp;quot;Gemma upgrade&amp;quot; might turn out a similar disappointment as Mistral Small. Keeping my fingers crossed. And also saving money for a better inference machine, whichever comes first - Intel's 24GB GPU, 4090 or 3090 for reasonable prices, or something entirely else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinerous"&gt; /u/martinerous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq0mj5/whats_going_on_with_mistral_small_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq0mj5/whats_going_on_with_mistral_small_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq0mj5/whats_going_on_with_mistral_small_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T12:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqngrb</id>
    <title>LM Studio over a LAN?</title>
    <updated>2025-02-16T08:01:42+00:00</updated>
    <author>
      <name>/u/cangaroo_hamam</name>
      <uri>https://old.reddit.com/user/cangaroo_hamam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I have LMStudio installed on a (beefy) PC in my local network. I downloaded some models, and did some configuration. &lt;/p&gt; &lt;p&gt;Now I want to use LMStudio from my (underpowered) laptop, but connect to the instance of LMStudio on the beefy PC, and use the models from there. In other words, I only want the UI on my laptop.&lt;/p&gt; &lt;p&gt;I have seen a LAN option, but I can't find how an instance of LMStudio can access the models in another instance.&lt;/p&gt; &lt;p&gt;Possible?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cangaroo_hamam"&gt; /u/cangaroo_hamam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm_studio_over_a_lan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm_studio_over_a_lan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm_studio_over_a_lan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T08:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipzjs6</id>
    <title>We need a Chatbot Arena for Deep Research</title>
    <updated>2025-02-15T11:36:09+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the recent explosion of Deep Research tools, I think we really could use a ChatBot Arena specifically for comparing these research assistants. Similar to how lmsys.org's arena helped us understand chatbot capabilities, we need a platform where users can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Submit identical research queries to different Deep Research tools simultaneously&lt;/li&gt; &lt;li&gt;Compare their methodologies, sources, and conclusions side-by-side&lt;/li&gt; &lt;li&gt;Rate output quality, source reliability, and overall usefulness&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;With OpenAI, Google, DeepSeek, Hugging Face, and now Perplexity all launching their own versions in the past few months, it's crucial to understand their real-world strengths and weaknesses. This would help users make informed decisions about which tool best suits their needs, while pushing companies to improve their offerings through healthy competition.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipzjs6/we_need_a_chatbot_arena_for_deep_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipzjs6/we_need_a_chatbot_arena_for_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipzjs6/we_need_a_chatbot_arena_for_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T11:36:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipfv03</id>
    <title>The official DeepSeek deployment runs the same model as the open-source version</title>
    <updated>2025-02-14T17:27:29+00:00</updated>
    <author>
      <name>/u/McSnoo</name>
      <uri>https://old.reddit.com/user/McSnoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipfv03/the_official_deepseek_deployment_runs_the_same/"&gt; &lt;img alt="The official DeepSeek deployment runs the same model as the open-source version" src="https://preview.redd.it/to2mbmta35je1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f32442ae047f98573e622827265434a1b704ff70" title="The official DeepSeek deployment runs the same model as the open-source version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/McSnoo"&gt; /u/McSnoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/to2mbmta35je1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipfv03/the_official_deepseek_deployment_runs_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipfv03/the_official_deepseek_deployment_runs_the_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-14T17:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqi5l8</id>
    <title>Latest and greatest setup to run llama 70b locally</title>
    <updated>2025-02-16T02:30:59+00:00</updated>
    <author>
      <name>/u/NetworkEducational81</name>
      <uri>https://old.reddit.com/user/NetworkEducational81</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, all&lt;/p&gt; &lt;p&gt;I’m working on a job site that scrapes and aggregates direct jobs from company websites. Less ghost jobs - woohoo&lt;/p&gt; &lt;p&gt;The app is live but now I hit bottleneck. Searching through half a million job descriptions is slow so user need to wait 5-10 seconds to get results. &lt;/p&gt; &lt;p&gt;So I decided to add a keywords field where I basically extract all the important keywords and search there. It’s much faster now&lt;/p&gt; &lt;p&gt;I used to run o4 mini to extract keywords but now I got around 10k jobs aggregated every day so I pay around $15 a day&lt;/p&gt; &lt;p&gt;I started doing it locally using llama 3.2 3b&lt;/p&gt; &lt;p&gt;I start my local ollama server and feed it data, then record response to DB. I ran it on my 4 years old Dell XPS with rtx 1650TI (4GB), 32GB RAM&lt;/p&gt; &lt;p&gt;I got 11 token/s output - which is about 8 jobs per minute, 480 per hour. I got about 10k jobs daily, So I need to have it running 20 hrs to get all jobs scanned.&lt;/p&gt; &lt;p&gt;In any case I want to increase speed by at least 10 fold. And maybe run 70b instead of 3b.&lt;/p&gt; &lt;p&gt;I want to buy/build a custom PC for around $4K-$5k for my development job plus LLM. I want to do work I do now plus train some LLM as well.&lt;/p&gt; &lt;p&gt;Now As I understand running 70b at 10 fold(100 tokens) per minute with this $5k price is unrealistic. or am I wrong?&lt;/p&gt; &lt;p&gt;Would I be able to run 3b at 100 tokens per minute.&lt;/p&gt; &lt;p&gt;Also I'd rather spend less if I can still run 3b with 100 tokens/m Like I can sacrifice 4090 for 3090 if the speed is not dramatic.&lt;/p&gt; &lt;p&gt;Or should I consider getting one of those jetsons purely for AI work?&lt;/p&gt; &lt;p&gt;I guess what I'm trying to ask is if anyone did it before, what setups worked for you and what speeds did you get.&lt;/p&gt; &lt;p&gt;Sorry for lengthy post. Cheers, Dan&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NetworkEducational81"&gt; /u/NetworkEducational81 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqi5l8/latest_and_greatest_setup_to_run_llama_70b_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqi5l8/latest_and_greatest_setup_to_run_llama_70b_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqi5l8/latest_and_greatest_setup_to_run_llama_70b_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T02:30:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq51ep</id>
    <title>Work just got me a shiny new m4 macbook pro with 48gb ram. What's the best coding llm I can reasonably run on it?</title>
    <updated>2025-02-15T16:27:52+00:00</updated>
    <author>
      <name>/u/gameguy56</name>
      <uri>https://old.reddit.com/user/gameguy56</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gameguy56"&gt; /u/gameguy56 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq51ep/work_just_got_me_a_shiny_new_m4_macbook_pro_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq51ep/work_just_got_me_a_shiny_new_m4_macbook_pro_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq51ep/work_just_got_me_a_shiny_new_m4_macbook_pro_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T16:27:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipztig</id>
    <title>Deepseek R1 Distill 8B Q40 on 4 x Raspberry Pi 5 8GB (evaluation 11.6 tok/s, prediction 6.43 tok/s)</title>
    <updated>2025-02-15T11:54:56+00:00</updated>
    <author>
      <name>/u/b4rtaz</name>
      <uri>https://old.reddit.com/user/b4rtaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipztig/deepseek_r1_distill_8b_q40_on_4_x_raspberry_pi_5/"&gt; &lt;img alt="Deepseek R1 Distill 8B Q40 on 4 x Raspberry Pi 5 8GB (evaluation 11.6 tok/s, prediction 6.43 tok/s)" src="https://external-preview.redd.it/cjRqaGw0c2trYWplMVdprMYSXfXWS_Gex65ktK8HkyTSYr6WajtRk6Vi7phP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=becae57183cfeba267733570440e076ca4b77ceb" title="Deepseek R1 Distill 8B Q40 on 4 x Raspberry Pi 5 8GB (evaluation 11.6 tok/s, prediction 6.43 tok/s)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/b4rtaz"&gt; /u/b4rtaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5t2524skkaje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipztig/deepseek_r1_distill_8b_q40_on_4_x_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipztig/deepseek_r1_distill_8b_q40_on_4_x_raspberry_pi_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T11:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqehm9</id>
    <title>Multilingual creative writing ranking</title>
    <updated>2025-02-15T23:25:20+00:00</updated>
    <author>
      <name>/u/MadScientist-1214</name>
      <uri>https://old.reddit.com/user/MadScientist-1214</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested various LLMs for their ability to generate creative writing in German. Here's how I conducted the evaluation:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Task: Each model was asked to write a 400-word story in German&lt;/li&gt; &lt;li&gt;Evaluation: Both Claude and ChatGPT assessed each story for: &lt;ul&gt; &lt;li&gt;Language quality (grammar, vocabulary, fluency)&lt;/li&gt; &lt;li&gt;Content quality (creativity, coherence, engagement)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Testing environment: &lt;ul&gt; &lt;li&gt;Some models were tested via Huggingface Spaces: &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/CohereForAI/c4ai-command"&gt;https://huggingface.co/spaces/CohereForAI/c4ai-command&lt;/a&gt;&lt;/li&gt; &lt;li&gt;huggingface.co/chat&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Others were run locally with minor parameter tuning (temperature and min_p). And some I tested twice.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Ø Language&lt;/th&gt; &lt;th&gt;Ø Content&lt;/th&gt; &lt;th&gt;Average Ø&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;nvidia/Llama-3.1-Nemotron-70B-Instruct-HF&lt;/td&gt; &lt;td&gt;5.0&lt;/td&gt; &lt;td&gt;4.5&lt;/td&gt; &lt;td&gt;4.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;meta-llama/Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td&gt;4.5&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;4.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;arcee-ai/SuperNova-Medius&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;4.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gghfez/Writer-Large-2411-v2.1-AWQ&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;stelterlab/Mistral-Small-24B-Instruct-2501-AWQ&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;google/gemma-2-27b-it&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NousResearch/Hermes-3-Llama-3.1-8B&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;CohereForAI/c4ai-command-r-plus-08-2024&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Command R 08-2024&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;aya-expanse-32B&lt;/td&gt; &lt;td&gt;4.0&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mistralai/Mistral-Nemo-Instruct-2407&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen/Qwen2.5-72B-Instruct&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen/Qwen2.5-72B-Instruct-AWQ&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;c4ai-command-r-08-2024-awq&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;td&gt;3.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;solidrust/Gemma-2-Ataraxy-9B-AWQ&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;solidrust/gemma-2-9b-it-AWQ&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;modelscope/Yi-1.5-34B-Chat-AWQ&lt;/td&gt; &lt;td&gt;2.5&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;modelscope/Yi-1.5-34B-Chat-AWQ&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Command R7B 12-2024&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.0&lt;/td&gt; &lt;td&gt;2.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Finally, I took a closer look at nvidia/Llama-3.1-Nemotron-70B-Instruct-HF, which got a perfect grammar score. While its German skills are pretty impressive, I wouldn’t quite agree with the perfect score. The model usually gets German right, but there are a couple of spots where the phrasing feels a bit off (maybe 2-3 instances in every 400 words).&lt;/p&gt; &lt;p&gt;I hope this helps anyone. If you have any other model suggestions, feel free to share them. I’d also be interested in seeing results in other languages from native speakers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadScientist-1214"&gt; /u/MadScientist-1214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqehm9/multilingual_creative_writing_ranking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqehm9/multilingual_creative_writing_ranking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqehm9/multilingual_creative_writing_ranking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T23:25:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipxa9d</id>
    <title>KTransformers v0.2.1: Longer Context (from 4K to 8K for 24GB VRAM) and Slightly Faster Speed (+15%) for DeepSeek-V3/R1-q4</title>
    <updated>2025-02-15T08:44:18+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxa9d/ktransformers_v021_longer_context_from_4k_to_8k/"&gt; &lt;img alt="KTransformers v0.2.1: Longer Context (from 4K to 8K for 24GB VRAM) and Slightly Faster Speed (+15%) for DeepSeek-V3/R1-q4" src="https://external-preview.redd.it/Q2FzIhyr5A41HrauCCNzhCnJKCGu57NYpW96FxQ80Do.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0940960fce8e6f68585e5dc18d5e64b64a3e06ea" title="KTransformers v0.2.1: Longer Context (from 4K to 8K for 24GB VRAM) and Slightly Faster Speed (+15%) for DeepSeek-V3/R1-q4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! A huge thanks to the localLLaMa community for the incredible support! It’s amazing to see &lt;a href="https://github.com/kvcache-ai/ktransformers"&gt;KTransformers (https://github.com/kvcache-ai/ktransformers)&lt;/a&gt; been widely deployed across various platforms (Linux/Windows, Intel/AMD, 40X0/30X0/20X0) and surge from 0.8K to 6.6K GitHub stars in just a few days.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/actvpm5fm9je1.png?width=1831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82ce8b01dfff7241adfd17dd9ad8e9f38077ac7d"&gt;https://preview.redd.it/actvpm5fm9je1.png?width=1831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82ce8b01dfff7241adfd17dd9ad8e9f38077ac7d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're working hard to make KTransformers even faster and easier to use. Today, we're excited to release v0.2.1!&lt;br /&gt; In this version, we've integrated the highly efficient Triton MLA Kernel from the fantastic &lt;a href="https://github.com/sgl-project/sglang"&gt;sglang&lt;/a&gt; project into our flexible YAML-based injection framework.&lt;br /&gt; This optimization extending the maximum context length while also slightly speeds up both prefill and decoding. A detailed breakdown of the results can be found below:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: DeepseekV3-q4km&lt;/li&gt; &lt;li&gt;CPU: Intel (R) Xeon (R) Gold 6454S, 32 cores per socket, 2 sockets, each socket with 8×DDR5-4800&lt;/li&gt; &lt;li&gt;GPU: 4090 24G VRAM CPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i4m0gmiim9je1.png?width=1065&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7504033da7c1bc5466fafa6fc6bf5ab7d1f5146c"&gt;https://preview.redd.it/i4m0gmiim9je1.png?width=1065&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7504033da7c1bc5466fafa6fc6bf5ab7d1f5146c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Besides the improvements in speed, we've also significantly updated the documentation to enhance usability, including:&lt;/p&gt; &lt;p&gt;⦁ Added Multi-GPU configuration tutorial.&lt;/p&gt; &lt;p&gt;⦁ Consolidated installation guide.&lt;/p&gt; &lt;p&gt;⦁ Add a detailed tutorial on registering extra GPU memory with ExpertMarlin;&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What’s Next?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Many more features will come to make KTransformers faster and easier to use&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Faster&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;* The FlashInfer (&lt;a href="https://github.com/flashinfer-ai/flashinfer"&gt;https://github.com/flashinfer-ai/flashinfer&lt;/a&gt;) project is releasing an even more efficient fused MLA operator, promising further speedups&lt;br /&gt; &lt;strong&gt;\&lt;/strong&gt;* vLLM has explored multi-token prediction in DeepSeek-V3, and support is on our roadmap for even better performance&lt;br /&gt; &lt;strong&gt;\&lt;/strong&gt;* We are collaborating with Intel to enhance the AMX kernel (v0.3) and optimize for Xeon6/MRDIMM&lt;br /&gt; &lt;strong&gt;Easier&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;* Official Docker images to simplify installation&lt;br /&gt; * Fix the server integration for web API access&lt;br /&gt; * Support for more quantization types, including the highly requested dynamic quantization from unsloth&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Stay tuned for more updates!&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxa9d/ktransformers_v021_longer_context_from_4k_to_8k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxa9d/ktransformers_v021_longer_context_from_4k_to_8k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxa9d/ktransformers_v021_longer_context_from_4k_to_8k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T08:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq6ngx</id>
    <title>KTransformers 2.1 and llama.cpp Comparison with DeepSeek V3</title>
    <updated>2025-02-15T17:39:26+00:00</updated>
    <author>
      <name>/u/CockBrother</name>
      <uri>https://old.reddit.com/user/CockBrother</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone Loves a Graph, Right?&lt;/p&gt; &lt;p&gt;If not, then tables are the next best thing.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Software Used&lt;/th&gt; &lt;th align="left"&gt;Virtual Memory&lt;/th&gt; &lt;th align="left"&gt;Resident Memory&lt;/th&gt; &lt;th align="left"&gt;Model Quantization&lt;/th&gt; &lt;th align="left"&gt;Prompt Eval Rate (tokens/s)&lt;/th&gt; &lt;th align="left"&gt;Eval Rate (tokens/s)&lt;/th&gt; &lt;th align="left"&gt;Relative Performance&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;KTransformers&lt;/td&gt; &lt;td align="left"&gt;714GB&lt;/td&gt; &lt;td align="left"&gt;670GB&lt;/td&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;57.41&lt;/td&gt; &lt;td align="left"&gt;5.80&lt;/td&gt; &lt;td align="left"&gt;1.946&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KTransformers&lt;/td&gt; &lt;td align="left"&gt;426GB&lt;/td&gt; &lt;td align="left"&gt;380GB&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;83.02&lt;/td&gt; &lt;td align="left"&gt;8.66&lt;/td&gt; &lt;td align="left"&gt;1.986&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama.cpp&lt;/td&gt; &lt;td align="left"&gt;976GB&lt;/td&gt; &lt;td align="left"&gt;970GB&lt;/td&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;24.40&lt;/td&gt; &lt;td align="left"&gt;2.98&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama.cpp&lt;/td&gt; &lt;td align="left"&gt;716GB&lt;/td&gt; &lt;td align="left"&gt;682GB&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;4.36&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;A summary of some controlled tests and comparisons between &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;KTransformers&lt;/code&gt; for 8-bit and 4-bit quantization on DeepSeek v3. The versions tested were the latest from each project's &lt;code&gt;main&lt;/code&gt; branch as of a few hours before benchmarking.&lt;/p&gt; &lt;h1&gt;Configuration&lt;/h1&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD EPYC 7773X CPU&lt;/li&gt; &lt;li&gt;Nvidia 3090 Ti GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Software:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ubuntu 24.04.1&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; build: 4722 (68ff663a)&lt;/li&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt; main/&amp;quot;2.1&amp;quot;&lt;/li&gt; &lt;li&gt;CUDA 12.8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Framework-Specific Settings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt;: Partial GPU acceleration using a single 3090 Ti GPU. Claims &amp;quot;8K context support&amp;quot; from the 2.1 release notes.&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt;: CPU-only, 64K context.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmarking Setup&lt;/h1&gt; &lt;p&gt;A significant, but not overly long, prompt of just over 500 tokens was used to ensure it fit within &lt;code&gt;KTransformers&lt;/code&gt;' processing limits. This length was sufficient to benchmark prefill performance.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The default &lt;code&gt;KTransformers&lt;/code&gt; output length of 300 tokens was used for benchmarking generation.&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; output length was set to 300 tokens for consistency.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tuning and Adjustments&lt;/h1&gt; &lt;p&gt;&lt;code&gt;KTransformers&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The model was prompted twice to &amp;quot;warm up&amp;quot; as it does not appear to lock memory to prevent CPU memory from paging out. Letting &lt;code&gt;KTransformers&lt;/code&gt; sit idle for a while caused a ~4x slowdown in prompt evaluation and a ~1.5x slowdown in token evaluation.&lt;/li&gt; &lt;li&gt;Re-prompting restored expected performance.&lt;/li&gt; &lt;li&gt;Other settings were left at their defaults.&lt;/li&gt; &lt;li&gt;The number of CPU threads was set according to the documentation recommendations, not determined by manual tuning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Used the default &amp;quot;warm-up&amp;quot; setting before prompting.&lt;/li&gt; &lt;li&gt;Block and user block sizes were optimized at 1024 for the best balance between prefill and generation performance.&lt;/li&gt; &lt;li&gt;The number of threads was determined through experimentation and set to optimal values for the test system.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Observations&lt;/h1&gt; &lt;h1&gt;Memory Requirements and Context Handling&lt;/h1&gt; &lt;p&gt;The DeepSeek V3/R1 models are large, requiring significant memory. Even with 8-bit quantization, a 671B parameter model will not fit on systems with 512GB RAM.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires 300GB of RAM for 65K context, which is substantial.&lt;/li&gt; &lt;li&gt;If memory is available, &lt;code&gt;llama.cpp&lt;/code&gt; can handle contexts over 8× longer than &lt;code&gt;KTransformers&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;With 4-bit quantization, &lt;code&gt;llama.cpp&lt;/code&gt; can process up to 128K context.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt;' memory scaling efficiency is unclear since it does not yet support significantly larger contexts.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Performance&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt; significantly outperforms &lt;code&gt;llama.cpp&lt;/code&gt; in both prefill and generation, leveraging GPU acceleration.&lt;/li&gt; &lt;li&gt;However, the observed 2× performance gain is lower than expected given &lt;code&gt;KTransformers&lt;/code&gt;' claims.&lt;/li&gt; &lt;li&gt;This suggests potential over-optimization for specific hardware in &lt;code&gt;KTransformers&lt;/code&gt;, rather than broad performance improvements.&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; is not optimized for MoE (Mixture of Experts) models, affecting its performance in this test.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; is a mature, feature-rich project with robust parameter control and a stable web API.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt; lacks many parameter controls but has unique MoE-focused features, including: &lt;ul&gt; &lt;li&gt;The ability to reduce the number of experts used in generation.&lt;/li&gt; &lt;li&gt;Detailed MoE configuration for placing different layers across CPU and GPU resources.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Usage and API Support&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Both frameworks were tested using their command-line &amp;quot;chat&amp;quot; interfaces.&lt;/li&gt; &lt;li&gt;Both provide Python APIs.&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; has a stable, fully compatible web API.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KTransformers&lt;/code&gt;' web interface is currently unavailable due to unspecified bugs.&lt;/li&gt; &lt;li&gt;Prior attempts to use &lt;code&gt;KTransformers&lt;/code&gt; with Open WebUI indicated missing API support, making it incompatible.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;The growing popularity of DeepSeek V3/R1 may encourage better MoE model support in &lt;code&gt;llama.cpp&lt;/code&gt;. Implementing &lt;code&gt;KTransformers&lt;/code&gt;' innovations in &lt;code&gt;llama.cpp&lt;/code&gt; could improve performance significantly.&lt;/p&gt; &lt;p&gt;However, &lt;code&gt;KTransformers&lt;/code&gt; was designed from the ground up for DeepSeek-like models, and its performance benefits reflect this. Yet, limitations in context length, stability, and configurability make it less compelling for users who need greater flexibility.&lt;/p&gt; &lt;p&gt;At present, &lt;code&gt;KTransformers&lt;/code&gt; feels more like a technology demonstrator than a full replacement for &lt;code&gt;llama.cpp&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Both projects are fast-moving, and performance and features may change dramatically in just a few months.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CockBrother"&gt; /u/CockBrother &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ngx/ktransformers_21_and_llamacpp_comparison_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ngx/ktransformers_21_and_llamacpp_comparison_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ngx/ktransformers_21_and_llamacpp_comparison_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T17:39:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1iprs7f</id>
    <title>But... I only said hi.</title>
    <updated>2025-02-15T02:41:35+00:00</updated>
    <author>
      <name>/u/dagerdev</name>
      <uri>https://old.reddit.com/user/dagerdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iprs7f/but_i_only_said_hi/"&gt; &lt;img alt="But... I only said hi." src="https://preview.redd.it/hkh0ibuwt7je1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be66a4d9e15e958afb2cd8bcb6a9f80e10b37a86" title="But... I only said hi." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dagerdev"&gt; /u/dagerdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hkh0ibuwt7je1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iprs7f/but_i_only_said_hi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iprs7f/but_i_only_said_hi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T02:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqe6wv</id>
    <title>Have you guys tried DeepSeek-R1-Zero?</title>
    <updated>2025-02-15T23:10:59+00:00</updated>
    <author>
      <name>/u/CodeMurmurer</name>
      <uri>https://old.reddit.com/user/CodeMurmurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was reading R1 paper and their pure RL model DeepSeek-R1-Zero got 86.7% on AIME 2024. I wasn't able to find any service hosting the model. Deepseek-R1 got 79.8 on AIME 2024. So I was just wondering if some people here ran it locally or have found a service hosting it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodeMurmurer"&gt; /u/CodeMurmurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqe6wv/have_you_guys_tried_deepseekr1zero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqe6wv/have_you_guys_tried_deepseekr1zero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqe6wv/have_you_guys_tried_deepseekr1zero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T23:10:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq54yg</id>
    <title>Why LLMs are always so confident?</title>
    <updated>2025-02-15T16:32:29+00:00</updated>
    <author>
      <name>/u/Consistent_Equal5327</name>
      <uri>https://old.reddit.com/user/Consistent_Equal5327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They're almost never like &amp;quot;I really don't know what to do here&amp;quot;. Sure sometimes they spit out boilerplate like my training data cuts of at blah blah. But given the huge amount of training data, there must be a lot of incidents where data was like &amp;quot;I don't know&amp;quot;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Equal5327"&gt; /u/Consistent_Equal5327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq54yg/why_llms_are_always_so_confident/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq54yg/why_llms_are_always_so_confident/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq54yg/why_llms_are_always_so_confident/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T16:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipvp2h</id>
    <title>LLMs make flying 1000x better</title>
    <updated>2025-02-15T06:45:17+00:00</updated>
    <author>
      <name>/u/Vegetable_Sun_9225</name>
      <uri>https://old.reddit.com/user/Vegetable_Sun_9225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Normally I hate flying, internet is flaky and it's hard to get things done. I've found that i can get a lot of what I want the internet for on a local model and with the internet gone I don't get pinged and I can actually head down and focus. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vegetable_Sun_9225"&gt; /u/Vegetable_Sun_9225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipvp2h/llms_make_flying_1000x_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipvp2h/llms_make_flying_1000x_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipvp2h/llms_make_flying_1000x_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T06:45:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq7yea</id>
    <title>Since it's so hard to find max context windows all in one place, I started a table - contributions welcome!</title>
    <updated>2025-02-15T18:36:22+00:00</updated>
    <author>
      <name>/u/taylorwilsdon</name>
      <uri>https://old.reddit.com/user/taylorwilsdon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq7yea/since_its_so_hard_to_find_max_context_windows_all/"&gt; &lt;img alt="Since it's so hard to find max context windows all in one place, I started a table - contributions welcome!" src="https://external-preview.redd.it/yabl__4Ab0fX56Bb4wbP-vpdHxIRx-xXjgB7Jvk4-sU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d1e1f2fe159585b0d0e6b897ae3b1557ad44856" title="Since it's so hard to find max context windows all in one place, I started a table - contributions welcome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taylorwilsdon"&gt; /u/taylorwilsdon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/taylorwilsdon/llm-context-limits"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq7yea/since_its_so_hard_to_find_max_context_windows_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq7yea/since_its_so_hard_to_find_max_context_windows_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T18:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqmwsl</id>
    <title>I pay for chatGPT (20 USD), I specifically use the 4o model as a writing editor. For this kind of task, am I better off using a local model instead?</title>
    <updated>2025-02-16T07:21:41+00:00</updated>
    <author>
      <name>/u/MisPreguntas</name>
      <uri>https://old.reddit.com/user/MisPreguntas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't use chatGPT for anything else beyond editing my stories, as mentioned in the title, I only use the 4o model, and I tell it to edit my writing (stories) for grammar, and help me figure out better pacing, better approaches to explain a scene. It's like having a personal editor 24/7.&lt;/p&gt; &lt;p&gt;Am I better off using a local model for this kind of task? If so which one? I've got a 8GB RTX 3070 and 32 GB of RAM.&lt;/p&gt; &lt;p&gt;I'm asking since I don't use chatGPT for anything else. I used to use it for coding and used a better model, but I recently quit programming and only need a writer editor :) &lt;/p&gt; &lt;p&gt;Any model suggestions or system prompts are more than welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MisPreguntas"&gt; /u/MisPreguntas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqmwsl/i_pay_for_chatgpt_20_usd_i_specifically_use_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T07:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipy2fg</id>
    <title>Microsoft drops OmniParser V2 - Agent that controls Windows and Browser</title>
    <updated>2025-02-15T09:45:40+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released an open source tool that acts as an Agent that controls Windows and Browser to complete tasks given through prompts.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/"&gt;https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/microsoft/OmniParser-v2.0"&gt;https://huggingface.co/microsoft/OmniParser-v2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/microsoft/OmniParser/tree/master/omnitool"&gt;https://github.com/microsoft/OmniParser/tree/master/omnitool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/OmniParser-v2.0og"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipy2fg/microsoft_drops_omniparser_v2_agent_that_controls/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipy2fg/microsoft_drops_omniparser_v2_agent_that_controls/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T09:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipz13t</id>
    <title>Deepseek R1 just became the most liked model ever on Hugging Face just a few weeks after release - with thousands of variants downloaded over 10 million times now</title>
    <updated>2025-02-15T10:58:27+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipz13t/deepseek_r1_just_became_the_most_liked_model_ever/"&gt; &lt;img alt="Deepseek R1 just became the most liked model ever on Hugging Face just a few weeks after release - with thousands of variants downloaded over 10 million times now" src="https://preview.redd.it/lz0e93q9aaje1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aaf4142f69cd28ee8e23da316f638a807cbb3526" title="Deepseek R1 just became the most liked model ever on Hugging Face just a few weeks after release - with thousands of variants downloaded over 10 million times now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lz0e93q9aaje1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipz13t/deepseek_r1_just_became_the_most_liked_model_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipz13t/deepseek_r1_just_became_the_most_liked_model_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T10:58:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqf4v2</id>
    <title>Created a gui for llama.cpp and other apis - all contained in a single html</title>
    <updated>2025-02-15T23:56:24+00:00</updated>
    <author>
      <name>/u/tar_alex</name>
      <uri>https://old.reddit.com/user/tar_alex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqf4v2/created_a_gui_for_llamacpp_and_other_apis_all/"&gt; &lt;img alt="Created a gui for llama.cpp and other apis - all contained in a single html" src="https://external-preview.redd.it/Nnk1dGhwajI1ZWplMX73soshjT9KG-paaQOq0mm21JJPvLVNQkyV4_Cd4e00.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d972894ad4c00fad5f2bde2993e3727538a2c84d" title="Created a gui for llama.cpp and other apis - all contained in a single html" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tar_alex"&gt; /u/tar_alex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8g1dqnj25eje1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqf4v2/created_a_gui_for_llamacpp_and_other_apis_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqf4v2/created_a_gui_for_llamacpp_and_other_apis_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T23:56:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1iq6ite</id>
    <title>GPT-4o reportedly just dropped on lmarena</title>
    <updated>2025-02-15T17:33:40+00:00</updated>
    <author>
      <name>/u/Worldly_Expression43</name>
      <uri>https://old.reddit.com/user/Worldly_Expression43</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ite/gpt4o_reportedly_just_dropped_on_lmarena/"&gt; &lt;img alt="GPT-4o reportedly just dropped on lmarena" src="https://preview.redd.it/cjz352y89cje1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f9b527fb493206e2b7fe73cec4a70245655f39c" title="GPT-4o reportedly just dropped on lmarena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worldly_Expression43"&gt; /u/Worldly_Expression43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cjz352y89cje1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ite/gpt4o_reportedly_just_dropped_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iq6ite/gpt4o_reportedly_just_dropped_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T17:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ipxszq</id>
    <title>Ridiculous</title>
    <updated>2025-02-15T09:25:02+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxszq/ridiculous/"&gt; &lt;img alt="Ridiculous" src="https://preview.redd.it/95cr17p3u9je1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6000872e6551351c948ff99297bb4130600cc27d" title="Ridiculous" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95cr17p3u9je1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxszq/ridiculous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ipxszq/ridiculous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-15T09:25:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1iqh3k1</id>
    <title>Meta's Brain-to-Text AI</title>
    <updated>2025-02-16T01:35:00+00:00</updated>
    <author>
      <name>/u/Particular-Sea2005</name>
      <uri>https://old.reddit.com/user/Particular-Sea2005</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta's groundbreaking research, conducted in collaboration with the Basque Center on Cognition, Brain and Language, marks a significant advancement in non-invasive brain-to-text communication. The study involved 35 healthy volunteers at BCBL, using both magnetoencephalography (MEG) and electroencephalography (EEG) to record brain activity while participants typed sentences[1][2]. Researchers then trained an AI model to reconstruct these sentences solely from the recorded brain signals, achieving up to 80% accuracy in decoding characters from MEG recordings - at least twice the performance of traditional EEG systems[2].&lt;/p&gt; &lt;p&gt;This research builds upon Meta's previous work in decoding image and speech perception from brain activity, now extending to sentence production[1]. The study's success opens new possibilities for non-invasive brain-computer interfaces, potentially aiding in restoring communication for individuals who have lost the ability to speak[2]. However, challenges remain, including the need for further improvements in decoding performance and addressing the practical limitations of MEG technology, which requires subjects to remain still in a magnetically shielded room[1].&lt;/p&gt; &lt;p&gt;Sources [1] Meta announces technology that uses AI and non-invasive magnetic ... &lt;a href="https://gigazine.net/gsc_news/en/20250210-ai-decode-language-from-brain/"&gt;https://gigazine.net/gsc_news/en/20250210-ai-decode-language-from-brain/&lt;/a&gt; [2] Using AI to decode language from the brain and advance our ... &lt;a href="https://ai.meta.com/blog/brain-ai-research-human-communication/"&gt;https://ai.meta.com/blog/brain-ai-research-human-communication/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular-Sea2005"&gt; /u/Particular-Sea2005 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1iqh3k1/metas_braintotext_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-02-16T01:35:00+00:00</published>
  </entry>
</feed>
