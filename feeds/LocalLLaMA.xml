<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-05T19:49:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1miij6j</id>
    <title>OSS-120B fails the 20 bouncing balls in heptagon test</title>
    <updated>2025-08-05T19:18:25+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miij6j/oss120b_fails_the_20_bouncing_balls_in_heptagon/"&gt; &lt;img alt="OSS-120B fails the 20 bouncing balls in heptagon test" src="https://external-preview.redd.it/Z2RpejZvY3UzOWhmMcZ-lfKrwTWy8sv4rAh8UXEfaOU9tc1wa_tIH6YtjRdO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=418b7b9c6a53ed5785741c2ea9569f5ce3149312" title="OSS-120B fails the 20 bouncing balls in heptagon test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vd59dpcu39hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miij6j/oss120b_fails_the_20_bouncing_balls_in_heptagon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miij6j/oss120b_fails_the_20_bouncing_balls_in_heptagon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T19:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifpr3</id>
    <title>gpt-oss-120b can be fine-tuned on a single H100 node!</title>
    <updated>2025-08-05T17:35:54+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifpr3/gptoss120b_can_be_finetuned_on_a_single_h100_node/"&gt; &lt;img alt="gpt-oss-120b can be fine-tuned on a single H100 node!" src="https://external-preview.redd.it/12ojQ9khZuJRm7jqdMaOtnKaFtBC6Yo7dfwq4qKZ3jA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ae7c659a21f868f6dba51b958c810a90c5bfe24" title="gpt-oss-120b can be fine-tuned on a single H100 node!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Absolutely insane news for fine-tuners. I did &lt;strong&gt;not expect&lt;/strong&gt; a fine-tunable Apache 2.0 model. This is literally a pay bump for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifpr3/gptoss120b_can_be_finetuned_on_a_single_h100_node/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mifpr3/gptoss120b_can_be_finetuned_on_a_single_h100_node/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:35:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mieksq</id>
    <title>OpenAI GPT OSS: 21B &amp; 117B models (3.6B &amp; 5.1B active)</title>
    <updated>2025-08-05T16:54:56+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT OSS is a hugely anticipated open-weights release by OpenAI, designed for powerful reasoning, agentic tasks, and versatile developer use cases. It comprises two models: a big one with 117B parameters (&lt;a href="https://hf.co/openai/gpt-oss-120b"&gt;gpt-oss-120b&lt;/a&gt;), and a smaller one with 21B parameters (&lt;a href="https://hf.co/openai/gpt-oss-20b"&gt;gpt-oss-20b&lt;/a&gt;). Both are mixture-of-experts (MoEs) and use a 4-bit quantization scheme (MXFP4), enabling fast inference (thanks to fewer active parameters, see details below) while keeping resource usage low. The large model fits on a single H100 GPU, while the small one runs within 16GB of memory and is perfect for consumer hardware and on-device applications.&lt;/p&gt; &lt;p&gt;Overview of Capabilities and Architecture:&lt;/p&gt; &lt;p&gt;21B and 117B total parameters, with 3.6B and 5.1B active parameters, respectively.&lt;/p&gt; &lt;p&gt;4-bit quantization scheme using mxfp4 format. Only applied on the MoE weights. As stated, the 120B fits in a single 80 GB GPU and the 20B fits in a single 16GB GPU.&lt;/p&gt; &lt;p&gt;Reasoning, text-only models; with chain-of-thought and adjustable reasoning effort levels.&lt;/p&gt; &lt;p&gt;Instruction following and tool use support.&lt;/p&gt; &lt;p&gt;Inference implementations using transformers, vLLM, llama.cpp, and ollama.&lt;/p&gt; &lt;p&gt;Responses API is recommended for inference. License: Apache 2.0, with a small complementary use policy.&lt;/p&gt; &lt;p&gt;Architecture:&lt;/p&gt; &lt;p&gt;Token-choice MoE with SwiGLU activations.&lt;/p&gt; &lt;p&gt;When calculating the MoE weights, a softmax is taken over selected experts (softmax-after-topk).&lt;/p&gt; &lt;p&gt;Each attention layer uses RoPE with 128K context.&lt;/p&gt; &lt;p&gt;Alternate attention layers: full-context, and sliding 128-token window.&lt;/p&gt; &lt;p&gt;Attention layers use a learned attention sink per-head, where the denominator of the softmax has an additional additive value.&lt;/p&gt; &lt;p&gt;It uses the same tokenizer as GPT-4o and other OpenAI API models.&lt;/p&gt; &lt;p&gt;Some new tokens have been incorporated to enable compatibility with the Responses API.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieksq/openai_gpt_oss_21b_117b_models_36b_51b_active/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieksq/openai_gpt_oss_21b_117b_models_36b_51b_active/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mieksq/openai_gpt_oss_21b_117b_models_36b_51b_active/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mibuho</id>
    <title>Kitten-TTS : Smallest ever TTS model (25MB, 15M params), runs on CPU</title>
    <updated>2025-08-05T15:11:00+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just checked out Kitten-TTS, an open-sourced TTS model 1/5th the size of Kokoro 82M, and giving out decent enough results. The model is optimized for CPU and looks great given its size. Also, the inference is quite fast and is able to generate samples within seconds on a CPU as well. &lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.1"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://youtu.be/oyu58Aei6U4"&gt;https://youtu.be/oyu58Aei6U4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mibuho/kittentts_smallest_ever_tts_model_25mb_15m_params/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mibuho/kittentts_smallest_ever_tts_model_25mb_15m_params/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mibuho/kittentts_smallest_ever_tts_model_25mb_15m_params/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T15:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6bkf</id>
    <title>The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!</title>
    <updated>2025-08-05T11:13:33+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/"&gt; &lt;img alt="The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!" src="https://preview.redd.it/h2p8ceo4p6hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04ddb7dfeb9a7a66e33196efe1d60a4acb157f2a" title="The Chess Arena pairings for today's Kaggle exhibition are out, commentary by grandmasters like Hikaru Nakamura!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h2p8ceo4p6hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6bkf/the_chess_arena_pairings_for_todays_kaggle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T11:13:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifzqz</id>
    <title>GPT-OSS-120B vs GLM 4.5 Air...</title>
    <updated>2025-08-05T17:45:59+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifzqz/gptoss120b_vs_glm_45_air/"&gt; &lt;img alt="GPT-OSS-120B vs GLM 4.5 Air..." src="https://preview.redd.it/w52pmzpcn8hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e464e743e2abb8f3b1740018d19c8f245bf5d459" title="GPT-OSS-120B vs GLM 4.5 Air..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w52pmzpcn8hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifzqz/gptoss120b_vs_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mifzqz/gptoss120b_vs_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:45:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mif1xp</id>
    <title>Open models by OpenAI</title>
    <updated>2025-08-05T17:12:10+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/open-models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mif1xp/open_models_by_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mif1xp/open_models_by_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:12:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifaqv</id>
    <title>gpt-oss-120b and 20b GGUFs</title>
    <updated>2025-08-05T17:20:53+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifaqv/gptoss120b_and_20b_ggufs/"&gt; &lt;img alt="gpt-oss-120b and 20b GGUFs" src="https://external-preview.redd.it/INENbEwV6ABsrDTZjJHvECpyAUiCQ1gcEo-8476Qbvk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c691d4c1f3d07ee3536b3daa6084ba562a0cf9cf" title="gpt-oss-120b and 20b GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gpt-oss-20b-GGUF"&gt;https://huggingface.co/ggml-org/gpt-oss-20b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ggml-org/gpt-oss-120b-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifaqv/gptoss120b_and_20b_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mifaqv/gptoss120b_and_20b_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi0co2</id>
    <title>Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!</title>
    <updated>2025-08-05T05:06:08+00:00</updated>
    <author>
      <name>/u/MrJiks</name>
      <uri>https://old.reddit.com/user/MrJiks</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"&gt; &lt;img alt="Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!" src="https://preview.redd.it/9z1vbpnsu4hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bafb282a6194808b25822f60262b2b9d1dd1570e" title="Anthropic's CEO dismisses open source as 'red herring' - but his reasoning seems to miss the point entirely!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Dario Amodei's recent interview on Big Technology Podcast discussing open source AI models. Thoughts on this reasoning?&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/jikkujose/status/1952588432280051930"&gt;https://x.com/jikkujose/status/1952588432280051930&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrJiks"&gt; /u/MrJiks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9z1vbpnsu4hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi0co2/anthropics_ceo_dismisses_open_source_as_red/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T05:06:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1miaugk</id>
    <title>II-Search-4B: model tuned for reasoning with search tools</title>
    <updated>2025-08-05T14:33:02+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miaugk/iisearch4b_model_tuned_for_reasoning_with_search/"&gt; &lt;img alt="II-Search-4B: model tuned for reasoning with search tools" src="https://preview.redd.it/w6vtnupyo7hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=592d2a50ce20e6ddff73d1324a8bdda6dc148b14" title="II-Search-4B: model tuned for reasoning with search tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most search models need the cloud.&lt;/p&gt; &lt;p&gt;II-Search-4B doesn’t.&lt;/p&gt; &lt;p&gt;4B model tuned for reasoning with search tools, built for local use.&lt;/p&gt; &lt;p&gt;Performance of models 10x its size.&lt;/p&gt; &lt;p&gt;Search that is small, smart, and open.&lt;/p&gt; &lt;p&gt;II-Search-4B: &lt;a href="https://huggingface.co/Intelligent-Internet/II-Search-4B"&gt;https://huggingface.co/Intelligent-Internet/II-Search-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;II-Search-CIR-4B: &lt;a href="https://huggingface.co/Intelligent-Internet/II-Search-CIR-4B"&gt;https://huggingface.co/Intelligent-Internet/II-Search-CIR-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://ii.inc/web/blog/post/ii-search"&gt;https://ii.inc/web/blog/post/ii-search&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w6vtnupyo7hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miaugk/iisearch4b_model_tuned_for_reasoning_with_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miaugk/iisearch4b_model_tuned_for_reasoning_with_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T14:33:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mieyrn</id>
    <title>gpt-oss Benchmarks</title>
    <updated>2025-08-05T17:08:55+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieyrn/gptoss_benchmarks/"&gt; &lt;img alt="gpt-oss Benchmarks" src="https://preview.redd.it/rzxruvqmg8hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ad0296b5e45c1e8edae93cf3af9a01c66d76d7d" title="gpt-oss Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzxruvqmg8hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieyrn/gptoss_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mieyrn/gptoss_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6brm</id>
    <title>Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices.</title>
    <updated>2025-08-05T11:13:52+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"&gt; &lt;img alt="Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices." src="https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c02f020e4a288dbe74ee37d8070bfab6a8b6d543" title="Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fast and local TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB (based on the language). Can train on new voices.&lt;/p&gt; &lt;p&gt;Github Link: &lt;a href="https://github.com/OHF-Voice/piper1-gpl"&gt;https://github.com/OHF-Voice/piper1-gpl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4f9mf37ap6hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T11:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifc2l</id>
    <title>GPT OSS 120b and 20b is Apache 2.0!</title>
    <updated>2025-08-05T17:22:13+00:00</updated>
    <author>
      <name>/u/Synaps3</name>
      <uri>https://old.reddit.com/user/Synaps3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openai.com/index/introducing-gpt-oss/"&gt;https://openai.com/index/introducing-gpt-oss/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Synaps3"&gt; /u/Synaps3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifc2l/gpt_oss_120b_and_20b_is_apache_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifc2l/gpt_oss_120b_and_20b_is_apache_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mifc2l/gpt_oss_120b_and_20b_is_apache_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1migo6d</id>
    <title>I FEEL SO SAFE! THANK YOU SO MUCH OPENAI!</title>
    <updated>2025-08-05T18:10:18+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1migo6d/i_feel_so_safe_thank_you_so_much_openai/"&gt; &lt;img alt="I FEEL SO SAFE! THANK YOU SO MUCH OPENAI!" src="https://preview.redd.it/7e3v67opr8hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c78bd2d594d80d839e43d136cedfee6e05b2b464" title="I FEEL SO SAFE! THANK YOU SO MUCH OPENAI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It also lacks all general knowledge and is terrible at coding compared to the same sized GLM air, what is the use case here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7e3v67opr8hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1migo6d/i_feel_so_safe_thank_you_so_much_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1migo6d/i_feel_so_safe_thank_you_so_much_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T18:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi8lbl</id>
    <title>Qwen3 Coder vs. Kimi K2 vs. Sonnet 4 Coding Comparison (Tested on Qwen CLI)</title>
    <updated>2025-08-05T13:02:08+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba released Qwen3‑Coder (480B → 35B active) alongside Qwen Code CLI, a complete fork of Gemini CLI for agentic coding workflows specifically adapted for Qwen3 Coder. I tested it head-to-head with Kimi K2 and Claude Sonnet 4 in practical coding tasks using the same CLI via &lt;strong&gt;OpenRouter&lt;/strong&gt; to keep things consistent for all models. The results surprised me.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;ℹ️ &lt;strong&gt;Note:&lt;/strong&gt; All test timings are based on the OpenRouter providers.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I've done some real-world coding tests for all three, not just regular prompts. Here are the three questions I asked all three models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CLI Chat MCP Client in Python:&lt;/strong&gt; Build a CLI chat MCP client in Python. More like a chat room. Integrate Composio integration for tool calls (Gmail, Slack, etc.).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Geometry Dash WebApp Simulation:&lt;/strong&gt; Build a web version of Geometry Dash.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Typing Test WebApp:&lt;/strong&gt; Build a monkeytype-like typing test app with a theme switcher (Catppuccin theme) and animations (typing trail).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4 was the most reliable across all tasks, with complete, production-ready outputs. It was also the fastest, usually taking 5–7 minutes.&lt;/li&gt; &lt;li&gt;Qwen3-Coder surprised me with solid results, much faster than Kimi, though not quite on Claude’s level.&lt;/li&gt; &lt;li&gt;Kimi K2 writes good UI and follows standards well, but it is slow (20+ minutes on some tasks) and sometimes non-functional.&lt;/li&gt; &lt;li&gt;On tool-heavy prompts like MCP + Composio, Claude was the only one to get it right in one try.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Verdict&lt;/h1&gt; &lt;p&gt;Honestly, Qwen3-Coder feels like the best middle ground if you want budget-friendly coding without massive compromises. But for real coding speed, Claude still dominates all these recent models.&lt;/p&gt; &lt;p&gt;I can't see much hype around Kimi K2, to be honest. It's just painfully slow and not really as great as they say it is in coding. It's mid! (Keep in mind, timings are noted based on the OpenRouter providers.)&lt;/p&gt; &lt;p&gt;Here's a complete blog post with timings for all the tasks for each model and a nice demo here: &lt;a href="https://composio.dev/blog/qwen-3-coder-vs-kimi-k2-vs-claude-4-sonnet-coding-comparison"&gt;Qwen 3 Coder vs. Kimi K2 vs. Claude 4 Sonnet: Coding comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else has benchmarked these models with real coding projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T13:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1miec9u</id>
    <title>Release v4.55.0: New openai GPT OSS model! · huggingface/transformers</title>
    <updated>2025-08-05T16:46:13+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miec9u/release_v4550_new_openai_gpt_oss_model/"&gt; &lt;img alt="Release v4.55.0: New openai GPT OSS model! · huggingface/transformers" src="https://external-preview.redd.it/o3wB6ioZOZhpNvqAVXe5Ffp8Gi7bbUI44EsDB2_JzvY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b72a9d22224f12c017f5e0c04a52db64d195c9b" title="Release v4.55.0: New openai GPT OSS model! · huggingface/transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/releases/tag/v4.55.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miec9u/release_v4550_new_openai_gpt_oss_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miec9u/release_v4550_new_openai_gpt_oss_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:46:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifuqk</id>
    <title>gpt-oss-120b outperforms DeepSeek-R1-0528 in benchmarks</title>
    <updated>2025-08-05T17:40:56+00:00</updated>
    <author>
      <name>/u/oobabooga4</name>
      <uri>https://old.reddit.com/user/oobabooga4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a table I put together:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Benchmark&lt;/th&gt; &lt;th&gt;DeepSeek-R1&lt;/th&gt; &lt;th&gt;DeepSeek-R1-0528&lt;/th&gt; &lt;th&gt;GPT-OSS-20B&lt;/th&gt; &lt;th&gt;GPT-OSS-120B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPQA Diamond&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;71.5&lt;/td&gt; &lt;td&gt;81.0&lt;/td&gt; &lt;td&gt;71.5&lt;/td&gt; &lt;td&gt;80.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Humanity's Last Exam&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8.5&lt;/td&gt; &lt;td&gt;17.7&lt;/td&gt; &lt;td&gt;17.3&lt;/td&gt; &lt;td&gt;19.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;AIME 2024&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;79.8&lt;/td&gt; &lt;td&gt;91.4&lt;/td&gt; &lt;td&gt;96.0&lt;/td&gt; &lt;td&gt;96.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;AIME 2025&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;70.0&lt;/td&gt; &lt;td&gt;87.5&lt;/td&gt; &lt;td&gt;98.7&lt;/td&gt; &lt;td&gt;97.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;57.5&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;69.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;70.9&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;73.4&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;based on&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/open-models/"&gt;https://openai.com/open-models/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-0528&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Here is the table without AIME, as some have pointed out the GPT-OSS benchmarks used tools while the DeepSeek ones did not:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Benchmark&lt;/th&gt; &lt;th&gt;DeepSeek-R1&lt;/th&gt; &lt;th&gt;DeepSeek-R1-0528&lt;/th&gt; &lt;th&gt;GPT-OSS-20B&lt;/th&gt; &lt;th&gt;GPT-OSS-120B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPQA Diamond&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;71.5&lt;/td&gt; &lt;td&gt;81.0&lt;/td&gt; &lt;td&gt;71.5&lt;/td&gt; &lt;td&gt;80.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Humanity's Last Exam&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8.5&lt;/td&gt; &lt;td&gt;17.7&lt;/td&gt; &lt;td&gt;17.3&lt;/td&gt; &lt;td&gt;19.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;40.0&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;49.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;44.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;49.6&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oobabooga4"&gt; /u/oobabooga4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifuqk/gptoss120b_outperforms_deepseekr10528_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifuqk/gptoss120b_outperforms_deepseekr10528_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mifuqk/gptoss120b_outperforms_deepseekr10528_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi7bem</id>
    <title>New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`</title>
    <updated>2025-08-05T12:04:17+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"&gt; &lt;img alt="New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`" src="https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc3c355c6e5a0c2867654d9ea9c2e7c8ed9c618b" title="New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No more need for super-complex regular expression in the -ot option! Just do &lt;code&gt;--cpu-moe&lt;/code&gt; or &lt;code&gt;--n-cpu-moe #&lt;/code&gt; and reduce the number until the model no longer fits on the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15077"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T12:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1midu35</id>
    <title>GPT-OSS today!</title>
    <updated>2025-08-05T16:27:22+00:00</updated>
    <author>
      <name>/u/Jawshoeadan</name>
      <uri>https://old.reddit.com/user/Jawshoeadan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Keep an eye on these links! &lt;a href="https://github.com/openai/harmony"&gt;https://github.com/openai/harmony&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/open-models"&gt;https://openai.com/open-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://gpt-oss.com"&gt;https://gpt-oss.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: also this &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;https://github.com/ggml-org/llama.cpp/pull/15091&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jawshoeadan"&gt; /u/Jawshoeadan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midu35/gptoss_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midu35/gptoss_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1midu35/gptoss_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1migl0k</id>
    <title>gpt-oss-120b is safetymaxxed (cw: explicit safety)</title>
    <updated>2025-08-05T18:07:05+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o893aealq8hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1migl0k/gptoss120b_is_safetymaxxed_cw_explicit_safety/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1migl0k/gptoss120b_is_safetymaxxed_cw_explicit_safety/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T18:07:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhyzp7</id>
    <title>Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)</title>
    <updated>2025-08-05T03:52:26+00:00</updated>
    <author>
      <name>/u/ElectricalBar7464</name>
      <uri>https://old.reddit.com/user/ElectricalBar7464</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt; &lt;img alt="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" src="https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc51726f166f8072e9a0767410a3b105af874a6d" title="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Kitten ML has released open source code and weights of their new TTS model's preview.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/KittenML/KittenTTS"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.1"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is less than 25 MB, around 15M parameters. The full release next week will include another open source ~80M parameter model with these same 8 voices, that can also run on CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features and Advantages&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Eight Different Expressive voices - 4 female and 4 male voices. For a tiny model, the expressivity sounds pretty impressive. This release will support TTS in English and multilingual support expected in future releases.&lt;/li&gt; &lt;li&gt;Super-small in size: The two text to speech models will be ~15M and ~80M parameters .&lt;/li&gt; &lt;li&gt;Can literally run anywhere lol : Forget “No gpu required.” - this thing can even run on raspberry pi’s and phones. Great news for gpu-poor folks like me.&lt;/li&gt; &lt;li&gt;Open source (hell yeah!): the model can used for free.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalBar7464"&gt; /u/ElectricalBar7464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vdfv5uihi4hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T03:52:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mieqcb</id>
    <title>openai/gpt-oss-120b · Hugging Face</title>
    <updated>2025-08-05T17:00:37+00:00</updated>
    <author>
      <name>/u/ShreckAndDonkey123</name>
      <uri>https://old.reddit.com/user/ShreckAndDonkey123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieqcb/openaigptoss120b_hugging_face/"&gt; &lt;img alt="openai/gpt-oss-120b · Hugging Face" src="https://external-preview.redd.it/12ojQ9khZuJRm7jqdMaOtnKaFtBC6Yo7dfwq4qKZ3jA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ae7c659a21f868f6dba51b958c810a90c5bfe24" title="openai/gpt-oss-120b · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShreckAndDonkey123"&gt; /u/ShreckAndDonkey123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieqcb/openaigptoss120b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mieqcb/openaigptoss120b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mic8kf</id>
    <title>Llama.cpp: Add GPT-OSS</title>
    <updated>2025-08-05T15:25:57+00:00</updated>
    <author>
      <name>/u/atgctg</name>
      <uri>https://old.reddit.com/user/atgctg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mic8kf/llamacpp_add_gptoss/"&gt; &lt;img alt="Llama.cpp: Add GPT-OSS" src="https://external-preview.redd.it/SMmA2lbsQDUuflgGCV0_YBw5k-KcfZS-9iAMN58tb_s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95a9e3e605eab496c9ba148173f1d7e68a1d7e9f" title="Llama.cpp: Add GPT-OSS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atgctg"&gt; /u/atgctg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mic8kf/llamacpp_add_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mic8kf/llamacpp_add_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T15:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1midi67</id>
    <title>GPT-OSS today?</title>
    <updated>2025-08-05T16:14:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midi67/gptoss_today/"&gt; &lt;img alt="GPT-OSS today?" src="https://preview.redd.it/2br9oi8178hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20cd517e2220fa7745b9e909a9f4bfcf589d5f03" title="GPT-OSS today?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;because this is almost merged &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;https://github.com/ggml-org/llama.cpp/pull/15091&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2br9oi8178hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midi67/gptoss_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1midi67/gptoss_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1miezct</id>
    <title>🚀 OpenAI released their open-weight models!!!</title>
    <updated>2025-08-05T17:09:35+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miezct/openai_released_their_openweight_models/"&gt; &lt;img alt="🚀 OpenAI released their open-weight models!!!" src="https://preview.redd.it/1yckal6wg8hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc6b586f5d511d8c0e30969100e707e6e00a1815" title="🚀 OpenAI released their open-weight models!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to the gpt-oss series, OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.&lt;/p&gt; &lt;p&gt;We’re releasing two flavors of the open models:&lt;/p&gt; &lt;p&gt;gpt-oss-120b — for production, general purpose, high reasoning use cases that fits into a single H100 GPU (117B parameters with 5.1B active parameters)&lt;/p&gt; &lt;p&gt;gpt-oss-20b — for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1yckal6wg8hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miezct/openai_released_their_openweight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miezct/openai_released_their_openweight_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:09:35+00:00</published>
  </entry>
</feed>
